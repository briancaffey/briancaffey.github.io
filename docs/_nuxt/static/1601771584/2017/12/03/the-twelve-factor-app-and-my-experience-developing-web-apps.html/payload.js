__NUXT_JSONP__("/2017/12/03/the-twelve-factor-app-and-my-experience-developing-web-apps.html", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT){return {data:[{article:{layout:"post",title:"Reflecting on my web-app development process after reading The Twelve-Factor App",date:"2017-12-03T00:00:00.000Z",comments:true,image:V,toc:[{id:W,depth:o,text:X},{id:Y,depth:o,text:Z},{id:_,depth:o,text:$},{id:aa,depth:o,text:ab},{id:ac,depth:o,text:ad},{id:ae,depth:o,text:af},{id:ag,depth:o,text:ah},{id:ai,depth:o,text:aj},{id:ak,depth:o,text:al},{id:am,depth:o,text:an},{id:ao,depth:o,text:ap},{id:aq,depth:o,text:ar}],body:{type:"root",children:[{type:b,tag:g,props:{},children:[{type:b,tag:"img",props:{alt:"png",src:V},children:[]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I recently had a conversation with a developer on the topic of bridging application development and production. From this conversation I was recommneded to have a look at "},{type:b,tag:as,props:{},children:[{type:a,value:"The Twelve-Factor App"}]},{type:a,value:", a high level guide for building modern, production-ready web applications. In this article I thought it would be interesting to go through each of the twelve sections and reflect on my current development process and how it follows and\u002For deviates from these factors. I also want to talk about the new technologies and techniques I have been learning from "},{type:b,tag:j,props:{href:"https:\u002F\u002Ftestdriven.io\u002F",rel:[L,M,N],target:O},children:[{type:a,value:"testdriven.io"}]},{type:a,value:" and how I hope they can benefit me on the next stage of my learning. "}]},{type:a,value:e},{type:b,tag:p,props:{id:W},children:[{type:b,tag:j,props:{href:"#i-codebase",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:X}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"One codebase tracked in revision control, many deploys"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I have been using git to track my projects since starting "},{type:b,tag:j,props:{href:"https:\u002F\u002Fwww.obeythetestinggoat.com\u002F",rel:[L,M,N],target:O},children:[{type:a,value:"Obey the Testing Goat!"}]},{type:a,value:" and have been gradually exploring many of the different features beyond a linear "},{type:b,tag:h,props:{},children:[{type:a,value:"add"}]},{type:a,value:T},{type:b,tag:h,props:{},children:[{type:a,value:"commit"}]},{type:a,value:T},{type:b,tag:h,props:{},children:[{type:a,value:"push"}]},{type:a,value:" loop. Using Visual Studio Code makes resolving merge conflicts very easy. On this blog I have accepted at least one pull requests from a helpful readers to correct outdated information."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The pattern I have been following for Django uses "},{type:b,tag:h,props:{},children:[{type:a,value:".gitignore"}]},{type:a,value:" to keep local application settings out of the production codebase using the following logic: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,at]},children:[{type:b,tag:h,props:{},children:[{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:P}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"base "},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:I}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:U}]},{type:a,value:au},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:P}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"production "},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:I}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:U}]},{type:a,value:au},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"try"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:av}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:P}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"local "},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:I}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:U}]},{type:a,value:e},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"except"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:av}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"pass"}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:Y},children:[{type:b,tag:j,props:{href:"#ii-dependencies",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:Z}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Explicitly declare and isolate dependencies"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Using python makes this easy and I have had success running "},{type:b,tag:h,props:{},children:[{type:a,value:"pip install -r requirements.txt"}]},{type:a,value:", or: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"ADD .\u002Frequirements.txt \u002Fusr\u002Fsrc\u002Fapp\u002Frequirements.txt\nRUN pip install -r requirements.txt\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"when running Docker. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I have ran into minor dependency issues in experimenting with my personal website. I shouldn't have been doing it this way, but I made a virtual environment with "},{type:b,tag:h,props:{},children:[{type:a,value:"conda create"}]},{type:a,value:" and included all of the packages in the environment in my production "},{type:b,tag:h,props:{},children:[{type:a,value:"requirements.txt"}]},{type:a,value:". The Heroku build process gave me a tricky error that I was able to trace to a dependency that was failing to install on the Heroku instance. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"With Python 3 there are a few different options for creating virtual environments: "},{type:b,tag:h,props:{},children:[{type:a,value:aw}]},{type:a,value:", "},{type:b,tag:h,props:{},children:[{type:a,value:"venv"}]},{type:a,value:ax},{type:b,tag:h,props:{},children:[{type:a,value:"conda"}]},{type:a,value:" are three that I have used, and "},{type:b,tag:h,props:{},children:[{type:a,value:aw}]},{type:a,value:" is a reliable tool for building webapps that I have seen used widely. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Another interesting tip from this section relates to common system tools: "}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Twelve-factor apps also do not rely on the implicit existence of any system tools. Examples include shelling out to ImageMagick or curl. "}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is something that I have been grappling with in Docker. Tools like "},{type:b,tag:h,props:{},children:[{type:a,value:"wget"}]},{type:a,value:" aren't part of \"base images\" and need to be intalled in the "},{type:b,tag:h,props:{},children:[{type:a,value:"Dockerfile"}]},{type:a,value:" or in scripts called from "},{type:b,tag:h,props:{},children:[{type:a,value:ay}]},{type:a,value:". "}]},{type:a,value:e},{type:b,tag:p,props:{id:_},children:[{type:b,tag:j,props:{href:"#iii-config",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:$}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Store config in the environment"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Heroku's command line utilities make setting production environments very easy. I think I can improve the way I organize environment variables locally since I often have many different projects it would be easy for projects to accidentally share the same variable name. One idea I have thought about would be to have an untracked bash script that sets environment variables that I run when starting the development environment, something like: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"export SECRET_KEY=\"my_secret_key\"\nexport DB_URL=\"postgres:\u002F\u002Fmy_db_url\"\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Docker wins points again on this factor because environemnt variables can simple be defined in a development and production "},{type:b,tag:h,props:{},children:[{type:a,value:"docker-compose-*.yml"}]},{type:a,value:" files:"}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"    environment:\n      - APP_SETTINGS=project.config.DevelopmentConfig\n      - DATABASE_URL=postgres:\u002F\u002Fpostgres:postgres@users-db:5432\u002Fusers_dev\n      - DATABASE_TEST_URL=postgres:\u002F\u002Fpostgres:postgres@users-db:5432\u002Fusers_test\n"}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:aa},children:[{type:b,tag:j,props:{href:"#iv-backing-services",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:ab}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Treat backing services as attached resources"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The key takeaway for backing services (databases, message\u002Fqueuing systems, etc.) is: "}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The code for a twelve-factor app makes no distinction between local and third party services."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This factor made me think of an interesting syntax that I saw for the first time when learning the microservices architecture with docker in "},{type:b,tag:j,props:{href:"https:\u002F\u002Fgithub.com\u002Fjakewright\u002Ftutorials\u002Ftree\u002Fmaster\u002Fdocker\u002F02-docker-compose",rel:[L,M,N],target:O},children:[{type:a,value:"this Docker example"}]},{type:a,value:" that uses two Flask apps to provide 1) front-end templates and 2) API backend. When the front end calls the API backend, it does so by referencing the service name in the URL. Here is an example with PHP, but it would work similarly in any other framework: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,az]},children:[{type:b,tag:h,props:{},children:[{type:b,tag:c,props:{className:[d,"php",az]},children:[{type:b,tag:c,props:{className:[d,aA,aB]},children:[{type:a,value:"\u003C?php"}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aC}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:Q}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"file_get_contents"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,"single-quoted-string",F]},children:[{type:a,value:"'http:\u002F\u002Fproduct-service\u002F'"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aD}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:Q}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"json_decode"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aE}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:Q}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aD}]},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:T}]},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:"\u003E"}]},{type:b,tag:c,props:{className:[d,"property"]},children:[{type:a,value:"products"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"foreach"}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aE}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"as"}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aF}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:R}]},{type:a,value:"\n        "},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"echo"}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,"double-quoted-string",F]},children:[{type:a,value:"\"\u003Cli\u003E"},{type:b,tag:c,props:{className:[d,aG]},children:[{type:b,tag:c,props:{className:[d,C]},children:[{type:a,value:aF}]}]},{type:a,value:"\u003C\u002Fli\u003E\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:E},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:e},{type:b,tag:c,props:{className:[d,aA,aB]},children:[{type:a,value:"?\u003E"}]}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This front-end code hits the backend API endpoint "},{type:b,tag:h,props:{},children:[{type:a,value:"http:\u002F\u002Fproduct-service\u002F"}]},{type:a,value:", where "},{type:b,tag:h,props:{},children:[{type:a,value:aH}]},{type:a,value:" is the name of a service included in "},{type:b,tag:h,props:{},children:[{type:a,value:ay}]},{type:a,value:aI}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"version: '3'\n\nservices:\n  product-service:\n    build: .\u002Fproduct\n    volumes:\n      - .\u002Fproduct:\u002Fusr\u002Fsrc\u002Fapp\n    ports:\n      - 5001:80\n\n  website:\n    image: php:apache\n    volumes:\n      - .\u002Fwebsite:\u002Fvar\u002Fwww\u002Fhtml\n    ports:\n      - 5000:80\n    depends_on:\n      - product-service\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is docker-compose file creates a network that includes both "},{type:b,tag:h,props:{},children:[{type:a,value:"website"}]},{type:a,value:ax},{type:b,tag:h,props:{},children:[{type:a,value:aH}]},{type:a,value:" that can be accessed by simply creating a URL with the name of the service in the domain. Coming back to the fourth factor, "},{type:b,tag:u,props:{},children:[{type:a,value:"The code for a twelve-factor app makes no distinction between local and third party services"}]},{type:a,value:", multiple docker containers can be thought of as "},{type:b,tag:as,props:{},children:[{type:a,value:"separate services"}]},{type:a,value:" even though they may be running on the same virtual environment in either development or production, and the unique domain cooresponding to the service name seems to reinforce this concept. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This URL could also be referenced with environment variables: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,"language-javascript"]},children:[{type:b,tag:h,props:{},children:[{type:b,tag:c,props:{className:[d,D]},children:[{type:a,value:"getUsers"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:R}]},{type:a,value:"\n  axios"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,K,D,H]},children:[{type:a,value:"get"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,"template-string"]},children:[{type:b,tag:c,props:{className:[d,aJ,F]},children:[{type:a,value:aK}]},{type:b,tag:c,props:{className:[d,aG]},children:[{type:b,tag:c,props:{className:[d,aL,f]},children:[{type:a,value:"${"}]},{type:a,value:"process"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,H]},children:[{type:a,value:"env"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,"constant"]},children:[{type:a,value:"REACT_APP_USERS_SERVICE_URL"}]},{type:b,tag:c,props:{className:[d,aL,f]},children:[{type:a,value:J}]}]},{type:b,tag:c,props:{className:[d,F]},children:[{type:a,value:"\u002Fusers"}]},{type:b,tag:c,props:{className:[d,aJ,F]},children:[{type:a,value:aK}]}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,K,D,H]},children:[{type:a,value:"then"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,aN]},children:[{type:a,value:aO}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,aP,z]},children:[{type:a,value:aQ}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:R}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,S,aR]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,K,D,H]},children:[{type:a,value:aS}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:a,value:aO},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:aM},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,K,D,H]},children:[{type:a,value:"catch"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,aN]},children:[{type:a,value:aT}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,aP,z]},children:[{type:a,value:aQ}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:R}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,S,aR]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,K,D,H]},children:[{type:a,value:aS}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:a,value:aT},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:G}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:e},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:ac},children:[{type:b,tag:j,props:{href:"#v-build-release-run",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:ad}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Strictly separate build and run stages"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is an important stage in getting from development to production. It is the handoff from a local repo to a live, running web application. Here's the process live in action:"}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:" $ git push heroku master\nCounting objects: 3, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (3\u002F3), done.\nWriting objects: 100% (3\u002F3), 303 bytes | 303.00 KiB\u002Fs, done.\nTotal 3 (delta 2), reused 0 (delta 0)\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----\u003E Python app detected\nremote: -----\u003E Installing requirements with pip\nremote: \nremote: -----\u003E Discovering process types\nremote:        Procfile declares types -\u003E web\nremote: \nremote: -----\u003E Compressing...\nremote:        Done: 193.7M\nremote: -----\u003E Launching...\nremote:        Released v410\nremote:        https:\u002F\u002Fbriancaffey.herokuapp.com\u002F deployed to Heroku\nremote: \nremote: Verifying deploy... done.\nTo https:\u002F\u002Fgit.heroku.com\u002Fbriancaffey.git\n   cd6ce76..d8981a3  master -\u003E master\n(briancaffey) brian@archthinkpad ~\u002FDocuments\u002Fgithub\u002Fbriancaffey\u002Fsrc\n"}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:ae},children:[{type:b,tag:j,props:{href:"#vi-processes",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:af}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Execute the app as one or more stateless processes"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is handled in Heroku by the Procfile. For simple Django apps on Heroku this is usually always the same one line: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"web: gunicorn projectname.wsgi --log-file -\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Here's what the Django project says about using gunicorn:"}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"When Gunicorn is installed, a gunicorn command is available which starts the Gunicorn server process. At its simplest, gunicorn just needs to be called with the location of a module containing a WSGI application object named application."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"So for a typical Django project, invoking gunicorn would look like:"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:h,props:{},children:[{type:a,value:"gunicorn myproject.wsgi"}]}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This will start one process running one thread listening on 127.0.0.1:8000. It requires that your project be on the Python path; the simplest way to ensure that is to run this command from the same directory as your manage.py file."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"In a Django project, the "},{type:b,tag:h,props:{},children:[{type:a,value:"wsgi.py"}]},{type:a,value:" file in the main folder of the project root directory has the following contents: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,at]},children:[{type:b,tag:h,props:{},children:[{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:I}]},{type:a,value:" os\n\n"},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:P}]},{type:a,value:" django"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"core"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"wsgi "},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:I}]},{type:a,value:" get_wsgi_application\n\nos"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"environ"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:l}]},{type:a,value:"setdefault"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,F]},children:[{type:a,value:"\"DJANGO_SETTINGS_MODULE\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:","}]},{type:a,value:i},{type:b,tag:c,props:{className:[d,F]},children:[{type:a,value:"\"brianblog.settings\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:"\n\napplication "},{type:b,tag:c,props:{className:[d,z]},children:[{type:a,value:Q}]},{type:a,value:" get_wsgi_application"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:m}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:n}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:ag},children:[{type:b,tag:j,props:{href:"#vi-port-binding",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:ah}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Export services via port binding"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is an area that I'm trying to learn more about. I feel like I have a pretty good grasp of what is going on regarding port binding in the microservice architecture with docker I have seen. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"From the docker-compose docs, the \"short syntax\" for mapping ports between hosts and containers is "},{type:b,tag:h,props:{},children:[{type:a,value:"HOST:CONTAINER"}]},{type:a,value:aI}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen)."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The following are examples of how this could work: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n - \"6060:6060\u002Fudp\"\n"}]}]}]},{type:a,value:e},{type:b,tag:p,props:{id:ai},children:[{type:b,tag:j,props:{href:"#viii-concurrency",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:aj}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Scale out via the process model"}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is an important area, but it is something I haven't had to be aware of since the apps I have developed don't require scaling processes. I belive that Heroku makes this fairly simple by allowing you to increase the number of web or worker processes through the CLI: "}]},{type:a,value:e},{type:b,tag:v,props:{className:[w]},children:[{type:b,tag:x,props:{className:[y,B]},children:[{type:b,tag:h,props:{},children:[{type:a,value:"heroku ps:scale web=1 worker=5\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I haven't covered Part 5 of testdriven.io yet, but it has a section on Elastic Load Balancing with EC2 which should cover this area. "}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Twelve-factor app processes should never daemonize or write PID files. Instead, rely on the operating system’s process manager to manage output streams, respond to crashed processes, and handle user-initiated restarts and shutdowns."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I have been learning more about "},{type:b,tag:h,props:{},children:[{type:a,value:"systemd"}]},{type:a,value:" and customizing "}]},{type:a,value:e},{type:b,tag:p,props:{id:ak},children:[{type:b,tag:j,props:{href:"#ix-disposability",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:al}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Maximize robustness with fast startup and graceful shutdown"}]}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. This facilitates fast elastic scaling, rapid deployment of code or config changes, and robustness of production deploys."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I have used some Heroku tools to start and stop web workers, and docker commands make this factor fairly easy to do correctly. Here's an excerpt from "},{type:b,tag:j,props:{href:"https:\u002F\u002Fwww.ctl.io\u002Fdevelopers\u002Fblog\u002Fpost\u002Fgracefully-stopping-docker-containers\u002F",rel:[L,M,N],target:O},children:[{type:a,value:"Century Link"}]},{type:a,value:" about the "},{type:b,tag:h,props:{},children:[{type:a,value:"docker stop"}]},{type:a,value:" command: "}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The docker stop command attempts to stop a running container first by sending a SIGTERM signal to the root process (PID 1) in the container. If the process hasn't exited within the timeout period a SIGKILL signal will be sent."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:p,props:{id:am},children:[{type:b,tag:j,props:{href:"#x-devprod-parity",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:an}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Keep development, staging, and production as similar as possible"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This is exactly why I'm so interested in using Docker. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"In one of my personal projects I did with Heroku I was relying on a feature of Postgres that is not available in sqlite3, the default database that comes with Django. This produced friction that I wouldn't have had to deal with if I was using Docker. I could have set up a local postgres server, but it would have been much easier to run a docker container that ran the server. "}]},{type:a,value:e},{type:b,tag:p,props:{id:ao},children:[{type:b,tag:j,props:{href:"#xi-logs",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:ap}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Treat logs as event streams"}]}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Running "},{type:b,tag:h,props:{},children:[{type:a,value:"heroku log"}]},{type:a,value:" has been helpful in debugging deployment issues. "}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Docker also produces helpful logs for all the containers currently running. "}]},{type:a,value:e},{type:b,tag:p,props:{id:aq},children:[{type:b,tag:j,props:{href:"#xii-admin-processes",ariaHidden:q,tabIndex:r},children:[{type:b,tag:c,props:{className:[s,t]},children:[]}]},{type:a,value:ar}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:u,props:{},children:[{type:a,value:"Run admin\u002Fmanagement tasks as one-off processes"}]}]},{type:a,value:e},{type:b,tag:A,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release. Admin code must ship with application code to avoid synchronization issues."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This seems to be true about the way I run admin processes on my Django apps. "}]}]},dir:"\u002F2017\u002F12\u002F03",path:"\u002F2017\u002F12\u002F03\u002Fthe-twelve-factor-app-and-my-experience-developing-web-apps.html",extension:".md",slug:"the-twelve-factor-app-and-my-experience-developing-web-apps.html",createdAt:"1970-01-01T00:00:00.000Z",updatedAt:"2020-10-01T02:23:22.995Z",raw:"\n![png](\u002Fstatic\u002Fthe-12-factor-app.png)\n\nI recently had a conversation with a developer on the topic of bridging application development and production. From this conversation I was recommneded to have a look at *The Twelve-Factor App*, a high level guide for building modern, production-ready web applications. In this article I thought it would be interesting to go through each of the twelve sections and reflect on my current development process and how it follows and\u002For deviates from these factors. I also want to talk about the new technologies and techniques I have been learning from [testdriven.io](https:\u002F\u002Ftestdriven.io\u002F) and how I hope they can benefit me on the next stage of my learning. \n\n## I. Codebase\n\n**One codebase tracked in revision control, many deploys**\n\nI have been using git to track my projects since starting [Obey the Testing Goat!](https:\u002F\u002Fwww.obeythetestinggoat.com\u002F) and have been gradually exploring many of the different features beyond a linear `add`-`commit`-`push` loop. Using Visual Studio Code makes resolving merge conflicts very easy. On this blog I have accepted at least one pull requests from a helpful readers to correct outdated information.\n\nThe pattern I have been following for Django uses `.gitignore` to keep local application settings out of the production codebase using the following logic: \n\n```python\nfrom .base import *\n\nfrom .production import *\n\ntry:\n\tfrom .local import *\nexcept:\n\tpass\n```\n\n## II. Dependencies\n\n**Explicitly declare and isolate dependencies**\n\nUsing python makes this easy and I have had success running `pip install -r requirements.txt`, or: \n\n```\nADD .\u002Frequirements.txt \u002Fusr\u002Fsrc\u002Fapp\u002Frequirements.txt\nRUN pip install -r requirements.txt\n```\n\nwhen running Docker. \n\nI have ran into minor dependency issues in experimenting with my personal website. I shouldn't have been doing it this way, but I made a virtual environment with `conda create` and included all of the packages in the environment in my production `requirements.txt`. The Heroku build process gave me a tricky error that I was able to trace to a dependency that was failing to install on the Heroku instance. \n\nWith Python 3 there are a few different options for creating virtual environments: `virtualenv`, `venv` and `conda` are three that I have used, and `virtualenv` is a reliable tool for building webapps that I have seen used widely. \n\nAnother interesting tip from this section relates to common system tools: \n\n\u003E Twelve-factor apps also do not rely on the implicit existence of any system tools. Examples include shelling out to ImageMagick or curl. \n\nThis is something that I have been grappling with in Docker. Tools like `wget` aren't part of \"base images\" and need to be intalled in the `Dockerfile` or in scripts called from `docker-compose.yml`. \n\n## III. Config\n\n**Store config in the environment**\n\nHeroku's command line utilities make setting production environments very easy. I think I can improve the way I organize environment variables locally since I often have many different projects it would be easy for projects to accidentally share the same variable name. One idea I have thought about would be to have an untracked bash script that sets environment variables that I run when starting the development environment, something like: \n\n```terminal\nexport SECRET_KEY=\"my_secret_key\"\nexport DB_URL=\"postgres:\u002F\u002Fmy_db_url\"\n```\n\nDocker wins points again on this factor because environemnt variables can simple be defined in a development and production `docker-compose-*.yml` files:\n\n```\n    environment:\n      - APP_SETTINGS=project.config.DevelopmentConfig\n      - DATABASE_URL=postgres:\u002F\u002Fpostgres:postgres@users-db:5432\u002Fusers_dev\n      - DATABASE_TEST_URL=postgres:\u002F\u002Fpostgres:postgres@users-db:5432\u002Fusers_test\n```\n\n## IV. Backing services\n\n**Treat backing services as attached resources**\n\nThe key takeaway for backing services (databases, message\u002Fqueuing systems, etc.) is: \n\n\u003E The code for a twelve-factor app makes no distinction between local and third party services.\n\nThis factor made me think of an interesting syntax that I saw for the first time when learning the microservices architecture with docker in [this Docker example](https:\u002F\u002Fgithub.com\u002Fjakewright\u002Ftutorials\u002Ftree\u002Fmaster\u002Fdocker\u002F02-docker-compose) that uses two Flask apps to provide 1) front-end templates and 2) API backend. When the front end calls the API backend, it does so by referencing the service name in the URL. Here is an example with PHP, but it would work similarly in any other framework: \n\n```php\n\u003C?php\n    $json = file_get_contents('http:\u002F\u002Fproduct-service\u002F');\n    $obj = json_decode($json);\n    $products = $obj-\u003Eproducts;\n    foreach ($products as $product) {\n        echo \"\u003Cli\u003E$product\u003C\u002Fli\u003E\";\n    }\n?\u003E\n```\n\nThis front-end code hits the backend API endpoint `http:\u002F\u002Fproduct-service\u002F`, where `product-service` is the name of a service included in `docker-compose.yml`: \n\n```\nversion: '3'\n\nservices:\n  product-service:\n    build: .\u002Fproduct\n    volumes:\n      - .\u002Fproduct:\u002Fusr\u002Fsrc\u002Fapp\n    ports:\n      - 5001:80\n\n  website:\n    image: php:apache\n    volumes:\n      - .\u002Fwebsite:\u002Fvar\u002Fwww\u002Fhtml\n    ports:\n      - 5000:80\n    depends_on:\n      - product-service\n```\n\nThis is docker-compose file creates a network that includes both `website` and `product-service` that can be accessed by simply creating a URL with the name of the service in the domain. Coming back to the fourth factor, **The code for a twelve-factor app makes no distinction between local and third party services**, multiple docker containers can be thought of as *separate services* even though they may be running on the same virtual environment in either development or production, and the unique domain cooresponding to the service name seems to reinforce this concept. \n\nThis URL could also be referenced with environment variables: \n\n```javascript\ngetUsers() {\n  axios.get(`${process.env.REACT_APP_USERS_SERVICE_URL}\u002Fusers`)\n  .then((res) =\u003E { console.log(res); })\n  .catch((err) =\u003E { console.log(err); })\n}\n```\n\n## V. Build, release, run\n\n**Strictly separate build and run stages**\n\nThis is an important stage in getting from development to production. It is the handoff from a local repo to a live, running web application. Here's the process live in action:\n\n```terminal\n $ git push heroku master\nCounting objects: 3, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (3\u002F3), done.\nWriting objects: 100% (3\u002F3), 303 bytes | 303.00 KiB\u002Fs, done.\nTotal 3 (delta 2), reused 0 (delta 0)\nremote: Compressing source files... done.\nremote: Building source:\nremote: \nremote: -----\u003E Python app detected\nremote: -----\u003E Installing requirements with pip\nremote: \nremote: -----\u003E Discovering process types\nremote:        Procfile declares types -\u003E web\nremote: \nremote: -----\u003E Compressing...\nremote:        Done: 193.7M\nremote: -----\u003E Launching...\nremote:        Released v410\nremote:        https:\u002F\u002Fbriancaffey.herokuapp.com\u002F deployed to Heroku\nremote: \nremote: Verifying deploy... done.\nTo https:\u002F\u002Fgit.heroku.com\u002Fbriancaffey.git\n   cd6ce76..d8981a3  master -\u003E master\n(briancaffey) brian@archthinkpad ~\u002FDocuments\u002Fgithub\u002Fbriancaffey\u002Fsrc\n```\n\n## VI. Processes\n\n**Execute the app as one or more stateless processes**\n\nThis is handled in Heroku by the Procfile. For simple Django apps on Heroku this is usually always the same one line: \n\n```\nweb: gunicorn projectname.wsgi --log-file -\n```\n\nHere's what the Django project says about using gunicorn:\n\n\u003E When Gunicorn is installed, a gunicorn command is available which starts the Gunicorn server process. At its simplest, gunicorn just needs to be called with the location of a module containing a WSGI application object named application.\n\u003E\n\u003E So for a typical Django project, invoking gunicorn would look like:\n\u003E \n \u003E `gunicorn myproject.wsgi`\n\n\u003E This will start one process running one thread listening on 127.0.0.1:8000. It requires that your project be on the Python path; the simplest way to ensure that is to run this command from the same directory as your manage.py file.\n\nIn a Django project, the `wsgi.py` file in the main folder of the project root directory has the following contents: \n\n```python\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"brianblog.settings\")\n\napplication = get_wsgi_application()\n```\n\n## VI. Port binding\n\n**Export services via port binding**\n\nThis is an area that I'm trying to learn more about. I feel like I have a pretty good grasp of what is going on regarding port binding in the microservice architecture with docker I have seen. \n\nFrom the docker-compose docs, the \"short syntax\" for mapping ports between hosts and containers is `HOST:CONTAINER`: \n\n\u003E Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen).\n\nThe following are examples of how this could work: \n\n```\nports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n - \"6060:6060\u002Fudp\"\n```\n\n## VIII. Concurrency\n\n**Scale out via the process model**\n\nThis is an important area, but it is something I haven't had to be aware of since the apps I have developed don't require scaling processes. I belive that Heroku makes this fairly simple by allowing you to increase the number of web or worker processes through the CLI: \n\n```terminal\nheroku ps:scale web=1 worker=5\n```\n\nI haven't covered Part 5 of testdriven.io yet, but it has a section on Elastic Load Balancing with EC2 which should cover this area. \n\n\u003E Twelve-factor app processes should never daemonize or write PID files. Instead, rely on the operating system’s process manager to manage output streams, respond to crashed processes, and handle user-initiated restarts and shutdowns.\n\nI have been learning more about `systemd` and customizing \n\n## IX. Disposability\n\n**Maximize robustness with fast startup and graceful shutdown**\n\n\u003E The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. This facilitates fast elastic scaling, rapid deployment of code or config changes, and robustness of production deploys.\n\nI have used some Heroku tools to start and stop web workers, and docker commands make this factor fairly easy to do correctly. Here's an excerpt from [Century Link](https:\u002F\u002Fwww.ctl.io\u002Fdevelopers\u002Fblog\u002Fpost\u002Fgracefully-stopping-docker-containers\u002F) about the `docker stop` command: \n\n\u003E The docker stop command attempts to stop a running container first by sending a SIGTERM signal to the root process (PID 1) in the container. If the process hasn't exited within the timeout period a SIGKILL signal will be sent.\n\n## X. Dev\u002Fprod parity\n\n\u003E Keep development, staging, and production as similar as possible\n\nThis is exactly why I'm so interested in using Docker. \n\nIn one of my personal projects I did with Heroku I was relying on a feature of Postgres that is not available in sqlite3, the default database that comes with Django. This produced friction that I wouldn't have had to deal with if I was using Docker. I could have set up a local postgres server, but it would have been much easier to run a docker container that ran the server. \n\n## XI. Logs\n\n**Treat logs as event streams**\n\n\u003E A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior.\n\nRunning `heroku log` has been helpful in debugging deployment issues. \n\nDocker also produces helpful logs for all the containers currently running. \n\n## XII. Admin processes \n\n**Run admin\u002Fmanagement tasks as one-off processes**\n\n\u003E One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release. Admin code must ship with application code to avoid synchronization issues.\n\nThis seems to be true about the way I run admin processes on my Django apps. "}}],fetch:[],mutations:void 0}}("text","element","span","token","\n","punctuation","p","code"," ","a","keyword",".","(",")",2,"h2","true",-1,"icon","icon-link","strong","div","nuxt-content-highlight","pre","line-numbers","operator","blockquote","language-text","variable","function","\n    ","string",";","property-access","import","}","method","nofollow","noopener","noreferrer","_blank","from","=","{","console","-","*","\u002Fstatic\u002Fthe-12-factor-app.png","i-codebase","I. Codebase","ii-dependencies","II. Dependencies","iii-config","III. Config","iv-backing-services","IV. Backing services","v-build-release-run","V. Build, release, run","vi-processes","VI. Processes","vi-port-binding","VI. Port binding","viii-concurrency","VIII. Concurrency","ix-disposability","IX. Disposability","x-devprod-parity","X. Dev\u002Fprod parity","xi-logs","XI. Logs","xii-admin-processes","XII. Admin processes","em","language-python","\n\n",":","virtualenv"," and ","docker-compose.yml","language-php","delimiter","important","$json","$obj","$products","$product","interpolation","product-service",": ","template-punctuation","`","interpolation-punctuation","\n  ","parameter","res","arrow","=\u003E","class-name","log","err")));