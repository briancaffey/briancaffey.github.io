<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Related subreddit graph exploration with NetworkX</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-d2ff5fdf]{padding:0}.selected[data-v-d2ff5fdf]{text-shadow:.25px 0 0 #000}.picker[data-v-d2ff5fdf]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-d2ff5fdf]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-ed0088f5]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-2649ce61]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-2649ce61]:hover{transform:scale(1.3)}.centered[data-v-2649ce61]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>.emoji-mart,.emoji-mart *{-webkit-box-sizing:border-box;box-sizing:border-box;line-height:1.15}.emoji-mart{display:-webkit-box;display:-ms-flexbox;display:flex;font-family:-apple-system,BlinkMacSystemFont,Helvetica Neue,sans-serif;font-size:16px;-webkit-box-orient:vertical;-webkit-box-direction:normal;background:#fff;border:1px solid #d9d9d9;border-radius:5px;color:#222427;-ms-flex-direction:column;flex-direction:column;height:420px}.emoji-mart-emoji{background:none;border:none;-webkit-box-shadow:none;box-shadow:none;padding:6px}.emoji-mart-emoji span{display:inline-block}.emoji-mart-preview-emoji .emoji-mart-emoji span{font-size:32px;height:38px;width:38px}.emoji-type-native{font-family:Segoe UI Emoji,Segoe UI Symbol,Segoe UI,Apple Color Emoji,Twemoji Mozilla,Noto Color Emoji,EmojiOne Color,Android Emoji;word-break:keep-all}.emoji-type-image{background-size:6100%}.emoji-type-image.emoji-set-apple{background-image:url(https://unpkg.com/emoji-datasource-apple@15.0.1/img/apple/sheets-256/64.png)}.emoji-type-image.emoji-set-facebook{background-image:url(https://unpkg.com/emoji-datasource-facebook@15.0.1/img/facebook/sheets-256/64.png)}.emoji-type-image.emoji-set-google{background-image:url(https://unpkg.com/emoji-datasource-google@15.0.1/img/google/sheets-256/64.png)}.emoji-type-image.emoji-set-twitter{background-image:url(https://unpkg.com/emoji-datasource-twitter@15.0.1/img/twitter/sheets-256/64.png)}.emoji-mart-bar{border:0 solid #d9d9d9}.emoji-mart-bar:first-child{border-bottom-width:1px;border-top-left-radius:5px;border-top-right-radius:5px}.emoji-mart-bar:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-top-width:1px}.emoji-mart-scroll{overflow-y:scroll;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-anchors{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;color:#858585;justify-content:space-between;line-height:0;padding:0 6px}.emoji-mart-anchor{display:block;position:relative;-webkit-box-flex:1;background:none;border:none;-webkit-box-shadow:none;box-shadow:none;-ms-flex:1 1 auto;flex:1 1 auto;overflow:hidden;padding:12px 4px;text-align:center;-webkit-transition:color .1s ease-out;transition:color .1s ease-out}.emoji-mart-anchor-selected,.emoji-mart-anchor:hover{color:#464646}.emoji-mart-anchor-selected .emoji-mart-anchor-bar{bottom:0}.emoji-mart-anchor-bar{background-color:#464646;bottom:-3px;height:3px;left:0;position:absolute;width:100%}.emoji-mart-anchors i{display:inline-block;max-width:22px;width:100%}.emoji-mart-anchors svg{fill:currentColor;max-height:18px}.emoji-mart .scroller{height:250px;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-search{margin-top:6px;padding:0 6px}.emoji-mart-search input{border:1px solid #d9d9d9;border-radius:25px;display:block;font-size:16px;outline:0;padding:.2em .6em;width:100%}.emoji-mart-search-results{height:250px;overflow-y:scroll}.emoji-mart-category{position:relative}.emoji-mart-category .emoji-mart-emoji span{cursor:default;position:relative;text-align:center;z-index:1}.emoji-mart-category .emoji-mart-emoji:hover:before,.emoji-mart-emoji-selected:before{background-color:#f4f4f4;border-radius:100%;content:"";height:100%;left:0;opacity:1;position:absolute;top:0;width:100%;z-index:0}.emoji-mart-category-label{position:-webkit-sticky;position:sticky;top:0}.emoji-mart-static .emoji-mart-category-label{position:relative;z-index:2}.emoji-mart-category-label h3{background-color:#fff;background-color:#fffffff2;display:block;font-size:16px;font-weight:500;padding:5px 6px;width:100%}.emoji-mart-emoji{display:inline-block;font-size:0;position:relative}.emoji-mart-no-results{color:#858585;font-size:14px;padding-top:70px;text-align:center}.emoji-mart-no-results .emoji-mart-category-label{display:none}.emoji-mart-no-results .emoji-mart-no-results-label{margin-top:.2em}.emoji-mart-no-results .emoji-mart-emoji:hover:before{content:none}.emoji-mart-preview{height:70px;position:relative}.emoji-mart-preview-data,.emoji-mart-preview-emoji,.emoji-mart-preview-skins{position:absolute;top:50%;-webkit-transform:translateY(-50%);-ms-transform:translateY(-50%);transform:translateY(-50%)}.emoji-mart-preview-emoji{left:12px}.emoji-mart-preview-data{left:68px;right:12px;word-break:break-all}.emoji-mart-preview-skins{right:30px;text-align:right}.emoji-mart-preview-name{font-size:14px}.emoji-mart-preview-shortname{color:#888;font-size:12px}.emoji-mart-preview-emoticon+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-shortname{margin-left:.5em}.emoji-mart-preview-emoticon{color:#bbb;font-size:11px}.emoji-mart-title span{display:inline-block;vertical-align:middle}.emoji-mart-title .emoji-mart-emoji{padding:0}.emoji-mart-title-label{color:#999a9c;font-size:21px;font-weight:300}.emoji-mart-skin-swatches{background-color:#fff;border:1px solid #d9d9d9;border-radius:12px;font-size:0;padding:2px 0}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch{padding:0 2px;width:16px}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch-selected:after{opacity:.75}.emoji-mart-skin-swatch{display:inline-block;-webkit-transition-duration:.125s;transition-duration:.125s;-webkit-transition-property:width,padding;transition-property:width,padding;-webkit-transition-timing-function:ease-out;transition-timing-function:ease-out;vertical-align:middle;width:0}.emoji-mart-skin-swatch:first-child{-webkit-transition-delay:0s;transition-delay:0s}.emoji-mart-skin-swatch:nth-child(2){-webkit-transition-delay:.03s;transition-delay:.03s}.emoji-mart-skin-swatch:nth-child(3){-webkit-transition-delay:.06s;transition-delay:.06s}.emoji-mart-skin-swatch:nth-child(4){-webkit-transition-delay:.09s;transition-delay:.09s}.emoji-mart-skin-swatch:nth-child(5){-webkit-transition-delay:.12s;transition-delay:.12s}.emoji-mart-skin-swatch:nth-child(6){-webkit-transition-delay:.15s;transition-delay:.15s}.emoji-mart-skin-swatch-selected{padding:0 2px;position:relative;width:16px}.emoji-mart-skin-swatch-selected:after{background-color:#fff;border-radius:100%;content:"";height:4px;left:50%;margin:-2px 0 0 -2px;opacity:0;pointer-events:none;position:absolute;top:50%;-webkit-transition:opacity .2s ease-out;transition:opacity .2s ease-out;width:4px}.emoji-mart-skin{border-radius:100%;display:inline-block;max-width:12px;padding-top:100%;width:100%}.emoji-mart-skin-tone-1{background-color:#ffc93a}.emoji-mart-skin-tone-2{background-color:#fadcbc}.emoji-mart-skin-tone-3{background-color:#e0bb95}.emoji-mart-skin-tone-4{background-color:#bf8f68}.emoji-mart-skin-tone-5{background-color:#9b643d}.emoji-mart-skin-tone-6{background-color:#594539}.emoji-mart .vue-recycle-scroller{position:relative}.emoji-mart .vue-recycle-scroller.direction-vertical:not(.page-mode){overflow-y:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal:not(.page-mode){overflow-x:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal{display:-webkit-box;display:-ms-flexbox;display:flex}.emoji-mart .vue-recycle-scroller__slot{-webkit-box-flex:1;-ms-flex:auto 0 0px;flex:auto 0 0}.emoji-mart .vue-recycle-scroller__item-wrapper{-webkit-box-flex:1;-webkit-box-sizing:border-box;box-sizing:border-box;-ms-flex:1;flex:1;overflow:hidden;position:relative}.emoji-mart .vue-recycle-scroller.ready .vue-recycle-scroller__item-view{left:0;position:absolute;top:0;will-change:transform}.emoji-mart .vue-recycle-scroller.direction-vertical .vue-recycle-scroller__item-wrapper{width:100%}.emoji-mart .vue-recycle-scroller.direction-horizontal .vue-recycle-scroller__item-wrapper{height:100%}.emoji-mart .vue-recycle-scroller.ready.direction-vertical .vue-recycle-scroller__item-view{width:100%}.emoji-mart .vue-recycle-scroller.ready.direction-horizontal .vue-recycle-scroller__item-view{height:100%}.emoji-mart .resize-observer[data-v-b329ee4c]{background-color:transparent;border:none;opacity:0}.emoji-mart .resize-observer[data-v-b329ee4c],.emoji-mart .resize-observer[data-v-b329ee4c] object{display:block;height:100%;left:0;overflow:hidden;pointer-events:none;position:absolute;top:0;width:100%;z-index:-1}.emoji-mart-search .hidden{display:none;visibility:hidden}</style><style>span.emoji-mart-emoji[data-v-03afe4fa]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-03afe4fa]:hover{transform:scale(1.3)}.picker[data-v-03afe4fa]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-03afe4fa]{background-color:var(--bg)}.localeText[data-v-03afe4fa]{color:var(--color-primary)}</style><style>.tag[data-v-eb2063c1]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-eb2063c1]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.BAXwoxDU.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2017/03/03/graph_subreddit/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/U2gz5u3s.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D3Q7tgbv.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CAEbC5FA.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BE4wn3fs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ivQeT1ix.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BwRoW_yn.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cz9IHiFo.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_-EP1mu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CeeZmN6j.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/yNn-iIhG.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDdUvEIO.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/PV-tEdEs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/iyVKi4R7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CZYsndi7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C94jHEjt.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BV4pdvGF.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/H6MlkUZ9.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CaawH2jc.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CE5h_3Zy.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Fo6dHlhp.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/711989cd-b755-45e7-b273-f0754c0f755f.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dk67likk.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BU1UZLbD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CZvWFDQc.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/yEdly4JE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CvvIFhEj.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B5fv-1Xq.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DFI9gCi9.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DE3BAxce.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/uYY1jy-2.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BiTVazLu.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DnLIRtEt.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CZsi5-5V.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BDjFqGfp.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Cebmboq7.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CRRfaG9P.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/fV3BeRRD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/6PKCu2pi.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/HSA0rOnf.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dbk39hGM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DlT8tiJO.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/b04OUO_Q.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DPR8_a9D.js"><meta hid="description" name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="Related subreddit graph exploration with NetworkX"><meta property="og:description" content="This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from reddit.com with the goal of visualizing the network of related subreddits (forums on specific topics) and related data."><meta property="og:image" content="https://briancaffey.github.io/static/subreddits.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/subreddits.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/U2gz5u3s.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-d2ff5fdf><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-d2ff5fdf><div class="justify-left flex-grow flex-cols-4" data-v-d2ff5fdf><a href="/" class="text-xl" data-v-d2ff5fdf><span class="hidden sm:inline text-2xl" data-v-d2ff5fdf>Brian Caffey</span><span class="inline sm:hidden" data-v-d2ff5fdf>JBC</span></a></div><div class="flex-grow relative" data-v-d2ff5fdf><nav z-index="10000" data-v-d2ff5fdf data-v-ed0088f5><div data-v-ed0088f5><ul class="items-right float-right hidden md:flex" data-v-ed0088f5><li class="px-4 text-lg" data-v-ed0088f5><a href="/blog/1" class="" data-v-ed0088f5>Blog</a></li><li class="px-4 text-lg" data-v-ed0088f5><a href="/contact" class="" data-v-ed0088f5>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-ed0088f5><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-ed0088f5><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-ed0088f5><title data-v-ed0088f5>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-ed0088f5></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-d2ff5fdf><div class="centered" data-v-d2ff5fdf data-v-2649ce61><div class="grid items-center justify-center" data-v-2649ce61><ul class="flex px-4" data-v-2649ce61><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üñ•Ô∏è, desktop_computer" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåû, sun_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåö, new_moon_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="‚òï, coffee" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><div data-v-2649ce61 data-v-03afe4fa><ul data-v-03afe4fa><li class="md:px-1 px-1 cursor-pointer" data-v-03afe4fa><span aria-label="üá∫üá∏, us, flag-us" class="emoji-mart-emoji" data-v-03afe4fa><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-03afe4fa><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from reddit.com with the goal of visualizing the network of related subreddits (forums on specific topics) and related data." data-nuxt-img srcset="/_ipx/f_webp/static/subreddits.png 1x, /_ipx/f_webp/static/subreddits.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/subreddits.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">Related subreddit graph exploration with NetworkX</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/reddit/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>reddit üè∑Ô∏è <!----></div></a><a href="/blog/tags/python/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>python üè∑Ô∏è <!----></div></a><a href="/blog/tags/scraping/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>scraping üè∑Ô∏è <!----></div></a><a href="/blog/tags/data/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>data üè∑Ô∏è <!----></div></a><a href="/blog/tags/graphs/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>graphs üè∑Ô∏è <!----></div></a><!--]--></div><!----><p class="blog-date text-gray-500 mb-4">Last updated March 3, 2017</p><!----><div class="markdown"><h1 id="graphing-subreddits"><!--[-->Graphing Subreddits<!--]--></h1><p><!--[-->This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from <a href="/2017/03/03/reddit.com" class=""><!--[-->reddit.com<!--]--></a> with the goal of visualizing the network of related subreddits (forums on specific topics) and related data.<!--]--></p><p><!--[-->Reddit is home over 600,000 communities, known as subreddits, where people come to share information, opinions, links, etc. and discuss things in a open forum. Most subreddits display links to related subreddits. For example, /r/apple (the Apple subreddit) links to /r/iPhone, a subreddit all about the iPhone, and over a dozen other Apple-related subreddits.<!--]--></p><p><!--[-->If you visit reddit.com as a guest, you will see a list of popular subreddits. This list is located inside an <code><!--[-->html<!--]--></code> tag called <code><!--[-->drop-choices<!--]--></code>. Here it is:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>from selenium import webdriver
</span></span><span class="line" line="2"><span>import re
</span></span><span class="line" line="3"><span>import time
</span></span><span class="line" line="4"><span>import numpy as np
</span></span><span class="line" line="5"><span>from bs4 import BeautifulSoup
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>driver = webdriver.PhantomJS()
</span></span><span class="line" line="2"><span>driver.get(&#39;https://www.reddit.com/&#39;)
</span></span><span class="line" line="3"><span>time.sleep(4 + np.random.random())
</span></span><span class="line" line="4"><span>html = driver.page_source.encode(&#39;utf-8&#39;)
</span></span><span class="line" line="5"><span emptylineplaceholder="true">
</span></span><span class="line" line="6"><span>s = BeautifulSoup(html)
</span></span><span class="line" line="7"><span>defaults = s.find(&#39;div&#39;, attrs={&#39;class&#39;:&#39;drop-choices&#39;})
</span></span><span class="line" line="8"><span>subs = re.compile(r&quot;\/r\/[\w.]+\/?&quot;)
</span></span><span class="line" line="9"><span>default_subreddits = list(set(subs.findall(str(defaults))))
</span></span><span class="line" line="10"><span emptylineplaceholder="true">
</span></span><span class="line" line="11"><span>for x in default_subreddits: print &#39;[&#39; + x + &#39;](https://reddit.com&#39;+ x + &#39;), &#39;,
</span></span></code><!--]--></pre><p><!--[-->Here are the elements of <code><!--[-->default_subreddits<!--]--></code>:<!--]--></p><blockquote><!--[--><p><!--[--><a href="https://reddit.com/r/LifeProTips/" rel="nofollow"><!--[-->/r/LifeProTips/<!--]--></a>, <a href="https://reddit.com/r/Futurology/" rel="nofollow"><!--[-->/r/Futurology/<!--]--></a>, <a href="https://reddit.com/r/OldSchoolCool/" rel="nofollow"><!--[-->/r/OldSchoolCool/<!--]--></a>, <a href="https://reddit.com/r/mildlyinteresting/" rel="nofollow"><!--[-->/r/mildlyinteresting/<!--]--></a>, <a href="https://reddit.com/r/askscience/" rel="nofollow"><!--[-->/r/askscience/<!--]--></a>, <a href="https://reddit.com/r/UpliftingNews/" rel="nofollow"><!--[-->/r/UpliftingNews/<!--]--></a>, <a href="https://reddit.com/r/aww/" rel="nofollow"><!--[-->/r/aww/<!--]--></a>, <a href="https://reddit.com/r/GetMotivated/" rel="nofollow"><!--[-->/r/GetMotivated/<!--]--></a>, <a href="https://reddit.com/r/personalfinance/" rel="nofollow"><!--[-->/r/personalfinance/<!--]--></a>, <a href="https://reddit.com/r/gadgets/" rel="nofollow"><!--[-->/r/gadgets/<!--]--></a>, <a href="https://reddit.com/r/science/" rel="nofollow"><!--[-->/r/science/<!--]--></a>, <a href="https://reddit.com/r/dataisbeautiful/" rel="nofollow"><!--[-->/r/dataisbeautiful/<!--]--></a>, <a href="https://reddit.com/r/DIY/" rel="nofollow"><!--[-->/r/DIY/<!--]--></a>, <a href="https://reddit.com/r/AskReddit/" rel="nofollow"><!--[-->/r/AskReddit/<!--]--></a>, <a href="https://reddit.com/r/space/" rel="nofollow"><!--[-->/r/space/<!--]--></a>, <a href="https://reddit.com/r/nosleep/" rel="nofollow"><!--[-->/r/nosleep/<!--]--></a>, <a href="https://reddit.com/r/Documentaries/" rel="nofollow"><!--[-->/r/Documentaries/<!--]--></a>, <a href="https://reddit.com/r/todayilearned/" rel="nofollow"><!--[-->/r/todayilearned/<!--]--></a>, <a href="https://reddit.com/r/television/" rel="nofollow"><!--[-->/r/television/<!--]--></a>, <a href="https://reddit.com/r/IAmA/" rel="nofollow"><!--[-->/r/IAmA/<!--]--></a>, <a href="https://reddit.com/r/Art/" rel="nofollow"><!--[-->/r/Art/<!--]--></a>, <a href="https://reddit.com/r/EarthPorn/" rel="nofollow"><!--[-->/r/EarthPorn/<!--]--></a>, <a href="https://reddit.com/r/books/" rel="nofollow"><!--[-->/r/books/<!--]--></a>, <a href="https://reddit.com/r/gifs/" rel="nofollow"><!--[-->/r/gifs/<!--]--></a>, <a href="https://reddit.com/r/Showerthoughts/" rel="nofollow"><!--[-->/r/Showerthoughts/<!--]--></a>, <a href="https://reddit.com/r/blog/" rel="nofollow"><!--[-->/r/blog/<!--]--></a>, <a href="https://reddit.com/r/news/" rel="nofollow"><!--[-->/r/news/<!--]--></a>, <a href="https://reddit.com/r/Jokes/" rel="nofollow"><!--[-->/r/Jokes/<!--]--></a>, <a href="https://reddit.com/r/TwoXChromosomes/" rel="nofollow"><!--[-->/r/TwoXChromosomes/<!--]--></a>, <a href="https://reddit.com/r/videos/" rel="nofollow"><!--[-->/r/videos/<!--]--></a>, <a href="https://reddit.com/r/philosophy/" rel="nofollow"><!--[-->/r/philosophy/<!--]--></a>, <a href="https://reddit.com/r/nottheonion/" rel="nofollow"><!--[-->/r/nottheonion/<!--]--></a>, <a href="https://reddit.com/r/explainlikeimfive/" rel="nofollow"><!--[-->/r/explainlikeimfive/<!--]--></a>, <a href="https://reddit.com/r/movies/" rel="nofollow"><!--[-->/r/movies/<!--]--></a>, <a href="https://reddit.com/r/Music/" rel="nofollow"><!--[-->/r/Music/<!--]--></a>, <a href="https://reddit.com/r/WritingPrompts/" rel="nofollow"><!--[-->/r/WritingPrompts/<!--]--></a>, <a href="https://reddit.com/r/worldnews/" rel="nofollow"><!--[-->/r/worldnews/<!--]--></a>, <a href="https://reddit.com/r/pics/" rel="nofollow"><!--[-->/r/pics/<!--]--></a>, <a href="https://reddit.com/r/history/" rel="nofollow"><!--[-->/r/history/<!--]--></a>, <a href="https://reddit.com/r/listentothis/" rel="nofollow"><!--[-->/r/listentothis/<!--]--></a>, <a href="https://reddit.com/r/sports/" rel="nofollow"><!--[-->/r/sports/<!--]--></a>, <a href="https://reddit.com/r/food/" rel="nofollow"><!--[-->/r/food/<!--]--></a>, <a href="https://reddit.com/r/creepy/" rel="nofollow"><!--[-->/r/creepy/<!--]--></a>, <a href="https://reddit.com/r/announcements/" rel="nofollow"><!--[-->/r/announcements/<!--]--></a>, <a href="https://reddit.com/r/gaming/" rel="nofollow"><!--[-->/r/gaming/<!--]--></a>, <a href="https://reddit.com/r/tifu/" rel="nofollow"><!--[-->/r/tifu/<!--]--></a>, <a href="https://reddit.com/r/funny/" rel="nofollow"><!--[-->/r/funny/<!--]--></a>, <a href="https://reddit.com/r/photoshopbattles/" rel="nofollow"><!--[-->/r/photoshopbattles/<!--]--></a>, <a href="https://reddit.com/r/InternetIsBeautiful/" rel="nofollow"><!--[-->/r/InternetIsBeautiful/<!--]--></a>,<!--]--></p><!--]--></blockquote><p><!--[-->My goal here is to see how many subreddits we can reach as we branch off of these &quot;default&quot; subreddits into their related subreddits.<!--]--></p><p><!--[-->First, we need to set up data structures to hold data for subreddits and their related subreddits. And we need to define an algorithm for collecting data.<!--]--></p><p><!--[-->Here&#39;s an intrdoduction to graphs from <a href="https://www.python.org/doc/essays/graphs/" rel="nofollow"><!--[-->python.org<!--]--></a>:<!--]--></p><blockquote><!--[--><p><!--[-->Few programming languages provide direct support for graphs as a data type, and Python is no exception. However, graphs are easily built out of lists and dictionaries. For instance, here&#39;s a simple graph (I can&#39;t use drawings in these columns, so I write down the graph&#39;s arcs):<!--]--></p><!--]--></blockquote><pre class="language-text"><!--[--><code>A -&gt; B
A -&gt; C
B -&gt; C
B -&gt; D
C -&gt; D
D -&gt; C
E -&gt; F
F -&gt; C
</code><!--]--></pre><p><!--[-->This graph has six nodes (A-F) and eight arcs. It can be represented by the following Python data structure:<!--]--></p><pre class="language-text"><!--[--><code>graph =     {&#39;A&#39;: [&#39;B&#39;, &#39;C&#39;],
             &#39;B&#39;: [&#39;C&#39;, &#39;D&#39;],
             &#39;C&#39;: [&#39;D&#39;],
             &#39;D&#39;: [&#39;C&#39;],
             &#39;E&#39;: [&#39;F&#39;],
             &#39;F&#39;: [&#39;C&#39;]}
</code><!--]--></pre><p><!--[-->First let&#39;s define how we would go only one branch deep into this graph (i.e. find the related subreddits for <em><!--[-->only<!--]--></em> the default subreddits). To collect the data, I first looped through the default subreddits and save the html of each subreddit to its own text file. Here&#39;s a script with comments:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#first we navigate to the correct folder where we will store the first level of related subreddits
</span></span><span class="line" line="2"><span>os.chdir(os.path.expanduser(&#39;~/Documents/Projects/Data/Subreddits/one/&#39;))
</span></span><span class="line" line="3"><span emptylineplaceholder="true">
</span></span><span class="line" line="4"><span>#next we instantiate the webdriver we will be using: PhantomJS
</span></span><span class="line" line="5"><span>driver = webdriver.PhantomJS()
</span></span><span class="line" line="6"><span emptylineplaceholder="true">
</span></span><span class="line" line="7"><span>#loop through the list of default subreddits
</span></span><span class="line" line="8"><span>for num, subreddit in enumerate(default_subreddits):
</span></span><span class="line" line="9"><span emptylineplaceholder="true">
</span></span><span class="line" line="10"><span>    #for each subreddit, we append the /r/subreddit path to the base URL (reddit.com)
</span></span><span class="line" line="11"><span>    driver.get(&#39;https://www.reddit.com&#39;+subreddit)
</span></span><span class="line" line="12"><span emptylineplaceholder="true">
</span></span><span class="line" line="13"><span>    #wait for two seconds
</span></span><span class="line" line="14"><span>    time.sleep(2 + np.random.random())
</span></span><span class="line" line="15"><span emptylineplaceholder="true">
</span></span><span class="line" line="16"><span>    #save the html of the loaded page to a variable: html
</span></span><span class="line" line="17"><span>    html = driver.page_source.encode(&#39;utf-8&#39;)
</span></span><span class="line" line="18"><span emptylineplaceholder="true">
</span></span><span class="line" line="19"><span>    #remove &#39;/r/&#39; from the subreddit name string
</span></span><span class="line" line="20"><span>    name = subreddit.split(&#39;/&#39;)[2]
</span></span><span class="line" line="21"><span emptylineplaceholder="true">
</span></span><span class="line" line="22"><span>    #open a new file and give it the name of the subreddit we just scraped
</span></span><span class="line" line="23"><span>    subreddit_html_file = open(name+&#39;.txt&#39;, &#39;w+&#39;)
</span></span><span class="line" line="24"><span emptylineplaceholder="true">
</span></span><span class="line" line="25"><span>    #write the html contents to the file
</span></span><span class="line" line="26"><span>    subreddit_html_file.write(html)
</span></span><span class="line" line="27"><span emptylineplaceholder="true">
</span></span><span class="line" line="28"><span>    #clost the file
</span></span><span class="line" line="29"><span>    subreddit_html_file.close()
</span></span><span class="line" line="30"><span emptylineplaceholder="true">
</span></span><span class="line" line="31"><span>    #print out the number and name of the subreddit we just scrapped to make sure things are working
</span></span><span class="line" line="32"><span>    print str(num) + &#39; &#39; + subreddit,
</span></span></code><!--]--></pre><p><!--[-->Next, we want to go through each file and extract the information we want. Here&#39;s what we will be getting:<!--]--></p><ul><!--[--><li><!--[-->Number of subscribers<!--]--></li><li><!--[-->Subreddit description<!--]--></li><li><!--[-->Date created<!--]--></li><li><!--[-->Related subreddits<!--]--></li><!--]--></ul><p><!--[-->For this type of project, I prefer to loop through each page and creating several small dictionaries for each data point, then combine the small dictionaries into a large dictionary, and then append the dictionary to a list of dictionaries. Once I have looped through all of the pages, I can create a pandas DataFrame from the list of dictionaries. This allows me to easily manipulate the data. Here&#39;s the script that I used to do this:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#navigate to where the html files are stored (I moved them around a bit so it is not consistent with the script above)
</span></span><span class="line" line="2"><span>os.chdir(&#39;E://DATA/Subreddits/subreddits_html/&#39;)
</span></span><span class="line" line="3"><span emptylineplaceholder="true">
</span></span><span class="line" line="4"><span>#generate a list of files that we will loop through
</span></span><span class="line" line="5"><span>files = os.listdir(&#39;E://DATA/Subreddits/subreddits_html/&#39;)
</span></span><span class="line" line="6"><span emptylineplaceholder="true">
</span></span><span class="line" line="7"><span>#set up an empty list that we will append dictionaries to
</span></span><span class="line" line="8"><span>dict_list = []
</span></span><span class="line" line="9"><span emptylineplaceholder="true">
</span></span><span class="line" line="10"><span>#loop through the files
</span></span><span class="line" line="11"><span>for file_ in files:
</span></span><span class="line" line="12"><span emptylineplaceholder="true">
</span></span><span class="line" line="13"><span>    #print out the name of the current file in the loop
</span></span><span class="line" line="14"><span>    print file_,
</span></span><span class="line" line="15"><span emptylineplaceholder="true">
</span></span><span class="line" line="16"><span>    #open the file
</span></span><span class="line" line="17"><span>    f = open(file_, &#39;r&#39;)
</span></span><span class="line" line="18"><span>    #read the file contents to a local variable
</span></span><span class="line" line="19"><span>    html = f.read()
</span></span><span class="line" line="20"><span>    #create a BeautifulSoup object that we will use to parse the HTML
</span></span><span class="line" line="21"><span>    b = BeautifulSoup(html, &#39;lxml&#39;)
</span></span><span class="line" line="22"><span emptylineplaceholder="true">
</span></span><span class="line" line="23"><span>    #get the subreddit name that we are working with (from the `file` variable)
</span></span><span class="line" line="24"><span>    subreddit_name = &#39;/r/&#39; + file_[:-4].lower()
</span></span><span class="line" line="25"><span>    #put the name into a dictionary
</span></span><span class="line" line="26"><span>    subreddit_name_dict = {&#39;subreddit&#39;:subreddit_name}
</span></span><span class="line" line="27"><span emptylineplaceholder="true">
</span></span><span class="line" line="28"><span>    #get number of subscribers
</span></span><span class="line" line="29"><span>    subs = b.find(&#39;span&#39;, attrs={&#39;class&#39;:&#39;subscribers&#39;})
</span></span><span class="line" line="30"><span>    #if the number of subscribers is displayed on the page, then we find it and add it to a dictionary
</span></span><span class="line" line="31"><span>    if subs:
</span></span><span class="line" line="32"><span>        subs = b.find(&#39;span&#39;, attrs={&#39;class&#39;:&#39;subscribers&#39;}).find(&#39;span&#39;, attrs={&#39;class&#39;:&#39;number&#39;}).text.replace(&#39;,&#39;, &#39;&#39;)
</span></span><span class="line" line="33"><span>        subs_dict = {&#39;subscribers&#39;:int(subs)}
</span></span><span class="line" line="34"><span>    #if the number of subscribers is not displayed on the page, then we set the number of subscribers in the dictionary to None
</span></span><span class="line" line="35"><span>    else:
</span></span><span class="line" line="36"><span>        subs_dict = {&#39;subscribers&#39;:None}
</span></span><span class="line" line="37"><span emptylineplaceholder="true">
</span></span><span class="line" line="38"><span>    #similar process for the description: if the description is displayed, get it and save it to desc
</span></span><span class="line" line="39"><span>    #if it is not available, then desc will be set to `None`
</span></span><span class="line" line="40"><span>    desc = b.find(&#39;div&#39;, attrs={&#39;class&#39;:&#39;md&#39;})
</span></span><span class="line" line="41"><span>    if desc:
</span></span><span class="line" line="42"><span>        desc = b.find(&#39;div&#39;, attrs={&#39;class&#39;:&#39;md&#39;}).text
</span></span><span class="line" line="43"><span>        desc = desc.replace(&#39;\n&#39;, &#39; &#39;)
</span></span><span class="line" line="44"><span>    desc_dict = {&#39;description&#39;:desc}
</span></span><span class="line" line="45"><span emptylineplaceholder="true">
</span></span><span class="line" line="46"><span>    #here we use regular expressions to find links anywhere on the page that have the structure: &quot;/r/something/&quot;
</span></span><span class="line" line="47"><span>    rel_subr = re.compile(r&quot;\/r\/[\w.]+\/?&quot;)
</span></span><span class="line" line="48"><span>    #make a list of these links based on the &quot;/r/something/&quot; pattern
</span></span><span class="line" line="49"><span>    related_subreddits = rel_subr.findall(html)
</span></span><span class="line" line="50"><span emptylineplaceholder="true">
</span></span><span class="line" line="51"><span>    #save the list to a dictionary
</span></span><span class="line" line="52"><span>    subreddits_dict = {&#39;related&#39;:related_subreddits}
</span></span><span class="line" line="53"><span emptylineplaceholder="true">
</span></span><span class="line" line="54"><span>    #same processes for recording the date that the subreddit was created: get the date from an HTML element,
</span></span><span class="line" line="55"><span>    #then save it to a dictionary. There were two different formats available in the HTML so I grabbed both
</span></span><span class="line" line="56"><span>    age = b.find(&#39;span&#39;, attrs={&#39;class&#39;:&#39;age&#39;})
</span></span><span class="line" line="57"><span>    if age:
</span></span><span class="line" line="58"><span>        time1 = age.find(&#39;time&#39;)[&#39;title&#39;]
</span></span><span class="line" line="59"><span>        time2 = age.find(&#39;time&#39;)[&#39;datetime&#39;]
</span></span><span class="line" line="60"><span emptylineplaceholder="true">
</span></span><span class="line" line="61"><span>    #save the date to a dictionary
</span></span><span class="line" line="62"><span>    time_dict = {&quot;date1&quot;:time1, &quot;date2&quot;:time2}
</span></span><span class="line" line="63"><span emptylineplaceholder="true">
</span></span><span class="line" line="64"><span>    #take all the dictionaries we just created and put them together into one big dictionary
</span></span><span class="line" line="65"><span>    dictionary = dict(subs_dict.items()+desc_dict.items()+subreddits_dict.items()+subreddit_name_dict.items()+time_dict.items())
</span></span><span class="line" line="66"><span emptylineplaceholder="true">
</span></span><span class="line" line="67"><span>    #append the big dictionary to the list that we defined right before the beginning of the loop
</span></span><span class="line" line="68"><span>    dict_list.append(dictionary)
</span></span><span class="line" line="69"><span emptylineplaceholder="true">
</span></span><span class="line" line="70"><span>    #deconstruct the Beautiful Soup object (this can eat up memory very quickly, so it is very important when processing lots of data)
</span></span><span class="line" line="71"><span>    b.decompose()
</span></span><span class="line" line="72"><span emptylineplaceholder="true">
</span></span><span class="line" line="73"><span>    #clost the file
</span></span><span class="line" line="74"><span>    f.close()
</span></span></code><!--]--></pre><p><!--[-->Next, let&#39;s save the results into a csv file. This let&#39;s us load the results quickly without having to scrape everyting again. To do this we can use the pandas library.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>import pandas as pd
</span></span><span class="line" line="2"><span>df0 = pd.DataFrame(dict_list, index=None)
</span></span></code><!--]--></pre><p><!--[-->At this point, we can go through the <code><!--[-->related<!--]--></code> column in the DataFrame and put together a list of all the related subreddits. With this list, we can simply repeat the process over and over again. However, each time we start with a new list of subreddits, we want to make sure that they have not already been collected.<!--]--></p><p><!--[-->Next I will read in one DataFrame that represents related subreddits &quot;three levels deep&quot; relative to the default subreddits.<!--]--></p><p><!--[--><strong><!--[-->Default --&gt; Related --&gt; Related --&gt; Related<!--]--></strong><!--]--></p><p><!--[-->This DataFrame represents the collection of subreddits from all of these &quot;layers&quot; of the graph.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>import pandas as pd
</span></span><span class="line" line="2"><span>master_df = pd.read_pickle(&#39;pickle/master_df.p&#39;)
</span></span></code><!--]--></pre><p><!--[-->Now we can do a quick visualization of the growth in number of subreddits since the website&#39;s start in 2005.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>import warnings
</span></span><span class="line" line="2"><span>warnings.filterwarnings(&#39;ignore&#39;)
</span></span><span class="line" line="3"><span>%matplotlib inline
</span></span><span class="line" line="4"><span>import matplotlib.pyplot as plt
</span></span><span class="line" line="5"><span>import seaborn as sns
</span></span><span class="line" line="6"><span>import numpy as np
</span></span><span class="line" line="7"><span emptylineplaceholder="true">
</span></span><span class="line" line="8"><span>master_df_ = master_df[master_df.notnull()]
</span></span><span class="line" line="9"><span>master_df_.date1 = pd.to_datetime(master_df_[&#39;date1&#39;])
</span></span><span class="line" line="10"><span emptylineplaceholder="true">
</span></span><span class="line" line="11"><span>list_of_dates = master_df_.date1.sort_values()
</span></span><span class="line" line="12"><span emptylineplaceholder="true">
</span></span><span class="line" line="13"><span>counts = np.arange(0, len(list_of_dates))
</span></span><span class="line" line="14"><span>_ = plt.plot(list_of_dates, counts)
</span></span><span class="line" line="15"><span>_ = plt.title(&#39;Number of subreddits over time&#39;)
</span></span><span class="line" line="16"><span>_ = plt.xlabel(&#39;Date&#39;)
</span></span><span class="line" line="17"><span>_ = plt.ylabel(&#39;Cummulative Count&#39;)
</span></span><span class="line" line="18"><span>plt.savefig(os.path.expanduser(&#39;~/Documents/GitHub/briancaffey.github.io/static/subreddit_graph/subreddits_count.png&#39;))
</span></span></code><!--]--></pre><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="png" data-nuxt-img srcset="/_ipx/_/static/subreddit_graph/subreddits_count.png 1x, /_ipx/_/static/subreddit_graph/subreddits_count.png 2x" src="/_ipx/_/static/subreddit_graph/subreddits_count.png"><!--]--></p><h1 id="setting-up-a-graph-with-networkx"><!--[-->Setting up a graph with NetworkX<!--]--></h1><p><!--[-->Next we can start to look at the collection of reddits and related subreddits as a graph. I will be using a Python package for network and graph analysis called <a href="https://networkx.github.io" rel="nofollow"><!--[-->NetworkX<!--]--></a>.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#Let&#39;s make sure that we have only unique entries in the dataframe.
</span></span><span class="line" line="2"><span>master_df_u = master_df_.drop_duplicates(&#39;subreddit&#39;)
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>master_df_u = master_df_u.drop(master_df_u.index[master_df_u.subreddit==&#39;/r/track__subreddits_&#39;])
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#here we define a dictionary where the keys are subreddits and the values are lists of related subreddits
</span></span><span class="line" line="2"><span>graph = {x:y for x, y in zip(master_df_u.subreddit, master_df_u.related)}
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#NetworkX comes with the python Anaconda distribution
</span></span><span class="line" line="2"><span>import networkx as nx
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>G=nx.Graph()
</span></span><span class="line" line="2"><span>G=nx.from_dict_of_lists(graph)
</span></span><span class="line" line="3"><span>#making the graph undirected takes all of the vertices between nodes and makes them bi-directional
</span></span><span class="line" line="4"><span>G1 = G.to_undirected()
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>choice = np.random.choice(master_df_u.subreddit, 2)
</span></span><span class="line" line="2"><span>print choice
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>[&#39;/r/streetboarding&#39; &#39;/r/stephenking&#39;]
</code><!--]--></pre><p><!--[-->Let&#39;s test out some of the functions from NetworkX for graph analysis. First, let&#39;s take the two randomly selected nodes defined above and test to see if there exists a path between them:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>nx.has_path(G1, choice[0], choice[1])
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>True
</code><!--]--></pre><h1 id="shortest-path"><!--[-->Shortest path<!--]--></h1><p><!--[-->Now let&#39;s see (at least one of) the shortest path that exists between these nodes:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>nx.shortest_path(G1, choice[0], choice[1])
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>[&#39;/r/streetboarding&#39;,
 &#39;/r/freebord&#39;,
 &#39;/r/adrenaline&#39;,
 &#39;/r/imaginaryadrenaline&#39;,
 &#39;/r/imaginarystephenking&#39;,
 &#39;/r/stephenking&#39;]
</code><!--]--></pre><p><!--[-->Let&#39;s write a function that selects two random subreddits and then prints a shortest path if it exists:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>def short_path():
</span></span><span class="line" line="2"><span>    choices = np.random.choice(master_df_u.subreddit, 2)
</span></span><span class="line" line="3"><span>    if nx.has_path(G1, choices[0], choices[1]) == True:
</span></span><span class="line" line="4"><span>        path = nx.shortest_path(G1, choices[0], choices[1])
</span></span><span class="line" line="5"><span>        print choices[0] + &#39; and &#39; + choices[1] + &#39; are joined by: \n&#39; + str(path)
</span></span><span class="line" line="6"><span>    else:
</span></span><span class="line" line="7"><span>        print &quot;No path exists between &quot; + choices[0] + &#39; and &#39; + choices[1]
</span></span></code><!--]--></pre><p><!--[-->Here&#39;s a collection of results from the <code><!--[-->short_path<!--]--></code> function defined above that start to paint a picuture of the broad set of topics covered by reddit.com:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/personalizationadvice and /r/beautifulfemales are joined by:
[&#39;/r/personalizationadvice&#39;, &#39;/r/coloranalysis&#39;, &#39;/r/fashion&#39;, &#39;/r/redcarpet&#39;, &#39;/r/gentlemanboners&#39;, &#39;/r/beautifulfemales&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/caffeine and /r/shittyramen are joined by:
[&#39;/r/caffeine&#39;, &#39;/r/toast&#39;, &#39;/r/cooking&#39;, &#39;/r/ramen&#39;, &#39;/r/shittyramen&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/watchingcongress and /r/iwantthatonashirt are joined by:
[&#39;/r/watchingcongress&#39;, &#39;/r/stand&#39;, &#39;/r/snowden&#39;, &#39;/r/undelete&#39;, &#39;/r/trees&#39;, &#39;/r/iwantthatonashirt&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/asksciencediscussion and /r/dogsonhardwoodfloors are joined by:
[&#39;/r/asksciencediscussion&#39;, &#39;/r/badscience&#39;, &#39;/r/badlinguistics&#39;, &#39;/r/animalsbeingjerks&#39;, &#39;/r/startledcats&#39;, &#39;/r/dogsonhardwoodfloors&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/randommail and /r/mini are joined by:
[&#39;/r/randommail&#39;, &#39;/r/spiceexchange&#39;, &#39;/r/cameraswapping&#39;, &#39;/r/itookapicture&#39;, &#39;/r/carporn&#39;, &#39;/r/mini&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/catsinsinks and /r/nzmovies are joined by:
[&#39;/r/catsinsinks&#39;, &#39;/r/wetcats&#39;, &#39;/r/tinysubredditoftheday&#39;, &#39;/r/sheep&#39;, &#39;/r/nzmetahub&#39;, &#39;/r/nzmovies&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/thoriumreactor and /r/sailing are joined by:
[&#39;/r/thoriumreactor&#39;, &#39;/r/energy&#39;, &#39;/r/spev&#39;, &#39;/r/sailing&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/deathnote and /r/vegetarianism are joined by:
[&#39;/r/deathnote&#39;, &#39;/r/television&#39;, &#39;/r/netflixbestof&#39;, &#39;/r/naturefilms&#39;, &#39;/r/environment&#39;, &#39;/r/vegetarianism&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/mississippir4r and /r/mathematics are joined by:
[&#39;/r/mississippir4r&#39;, &#39;/r/mississippi&#39;, &#39;/r/prisonreform&#39;, &#39;/r/socialscience&#39;, &#39;/r/alltech&#39;, &#39;/r/mathematics&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/britainsgottalent and /r/irelandbaldwin are joined by:
[&#39;/r/britainsgottalent&#39;, &#39;/r/britishtv&#39;, &#39;/r/that70sshow&#39;, &#39;/r/mila_kunis&#39;, &#39;/r/christinaricci&#39;, &#39;/r/irelandbaldwin&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/the_donald and /r/ladybusiness are joined by:
[&#39;/r/the_donald&#39;, &#39;/r/shitliberalssay&#39;, &#39;/r/trollxchromosomes&#39;, &#39;/r/ladybusiness&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/selfharm and /r/medlabprofessionals are joined by:
[&#39;/r/selfharm&#39;, &#39;/r/adhd&#39;, &#39;/r/neuroimaging&#39;, &#39;/r/pharmacy&#39;, &#39;/r/medlabprofessionals&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/coverart and /r/phillycraftbeer are joined by:
[&#39;/r/coverart&#39;, &#39;/r/nostalgia&#39;, &#39;/r/upvotedbecausegirl&#39;, &#39;/r/wtf&#39;, &#39;/r/remindsmeofdf&#39;, &#39;/r/beer&#39;, &#39;/r/phillycraftbeer&#39;]
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>short_path()
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/hotguyswithlonghair and /r/castles are joined by:
[&#39;/r/hotguyswithlonghair&#39;, &#39;/r/majesticmanes&#39;, &#39;/r/ladyboners&#39;, &#39;/r/imaginaryladyboners&#39;, &#39;/r/imaginarycastles&#39;, &#39;/r/castles&#39;]
</code><!--]--></pre><p><!--[-->Taking a look <a href="http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/unweighted.html?highlight=bidirectional_shortest_path" rel="nofollow"><!--[-->under the hood<!--]--></a> of NetworkX and examining the algorith that finds the <a href="http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/generic.html#shortest_path" rel="nofollow"><!--[-->shortest path<!--]--></a> between any two nodes in a graph, we find that it simply boils down to:<!--]--></p><pre class="language-text"><!--[--><code>def shortest_path(G, source=None, target=None, weight=None):
    paths=nx.bidirectional_shortest_path(G,source,target)
    return paths
</code><!--]--></pre><p><!--[-->You can read more about the <code><!--[-->bidirectional_shortest_path<!--]--></code> function <a href="http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/unweighted.html?highlight=bidirectional_shortest_path" rel="nofollow"><!--[-->here<!--]--></a> in the NetworkX documentation.<!--]--></p><p><!--[-->When I was first experimenting with graph algorithms, I had an interesting result using an algorithm intruduced <a href="https://www.python.org/doc/essays/graphs/" rel="nofollow"><!--[-->here<!--]--></a> in the Python documentation. Here&#39;s the algorithm:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>def find_path(graph, start, end, path=[]):
</span></span><span class="line" line="2"><span>    path = path + [start]
</span></span><span class="line" line="3"><span>    if start == end:
</span></span><span class="line" line="4"><span>        return path
</span></span><span class="line" line="5"><span>    if not graph.has_key(start):
</span></span><span class="line" line="6"><span>        return None
</span></span><span class="line" line="7"><span>    for node in graph[start]:
</span></span><span class="line" line="8"><span>        if node not in path:
</span></span><span class="line" line="9"><span>            newpath = find_path(graph, node, end, path)
</span></span><span class="line" line="10"><span>            if newpath: return newpath
</span></span><span class="line" line="11"><span>    return None
</span></span></code><!--]--></pre><p><!--[-->The above algorthim uses a process called backtracking to exaustively try all possibilities until it returns a solution. It creates an interesting &quot;random walk&quot; through groups of related subreddits. Here&#39;s the result of calling the above function on our graph (only 2 layers deep) with two random nodes: /r/persianrap and /r/nosleep:<!--]--></p><blockquote><!--[--><p><!--[-->/r/persianrap /r/middleeasternmusic /r/arabic /r/arabs /r/libyancrisis /r/syriancivilwar /r/yemenicrisis /r/sinaiinsurgency /r/jihadinfocus /r/credibledefense /r/geopolitics /r/forgottennews /r/libyanconflict /r/menaconflicts /r/iran /r/iranianlgbt /r/zoroastrianism /r/kurdistan /r/rojava /r/anarchism /r/imaginarypolitics /r/imaginaryimmortals /r/imaginaryclerics /r/imaginarylakes /r/imaginaryaliens /r/imaginarygnomes /r/imaginaryladyboners /r/imaginaryturtleworlds /r/imaginarysunnydale /r/imaginarydwarves /r/imaginarywizards /r/imaginaryvikings /r/imaginarycolorscapes /r/imaginarysteampunk /r/imaginarytemples /r/imaginaryblueprints /r/comicbookart /r/imaginarytechnology /r/mtgporn /r/imaginaryoldkingdom /r/imaginaryfactories /r/imaginaryfederation /r/imaginarylovers /r/imaginarynarnia /r/imaginarydwellings /r/imaginaryscience /r/imaginarytaverns /r/imaginarybattlefields /r/cityporn /r/japanpics /r/nationalphotosubs /r/austriapics /r/southkoreapics /r/taiwanpics /r/ghanapics /r/kenyapics /r/norwaypics /r/vzlapics /r/perupics /r/antarcticapics /r/greatlakespics /r/lakeporn /r/pornoverlords /r/thingscutinhalfporn /r/manufacturing /r/cnc /r/askengineers /r/sciencesubreddits /r/math /r/simulate /r/cosmology /r/reddittothefuture /r/scifi /r/lost /r/the100books /r/the100 /r/theblacklist /r/nbc /r/dundermifflin /r/sonsofanarchy /r/twentyfour /r/banshee /r/hbo /r/siliconvalleyhbo /r/siliconvalley /r/california /r/tahoe /r/skiing /r/snowshoeing /r/xcountryskiing /r/wintergear /r/skijumping /r/winter /r/bigmountain /r/mountaineering /r/campingandhiking /r/earthporn /r/nature /r/birding /r/invasivespecies /r/zoology /r/entomology /r/rainforest /r/botany /r/wildlife /r/allscience /r/earthscience /r/energy /r/biomass /r/renewablenews /r/syngas /r/climatenews /r/composting /r/vermiculture /r/organicfarming /r/livestock /r/animalwelfare /r/randomactsofpetfood /r/animalreddits /r/cockatiel /r/catpics /r/tortoises /r/whales /r/cetacea /r/lifeaquatic /r/hrw /r/green_peace /r/environmental_policy /r/conservation /r/depthhub /r/indepthsports /r/deeperhubbeta /r/lectures /r/spacepolicy /r/skylon /r/ula /r/isro /r/engineteststands /r/jupiters /r/imaginarystarscapes /r/spacequestions /r/spaceflight /r/moon /r/dione /r/europa /r/oortcloud /r/dwarfplanetceres /r/saturn /r/asteroidbelt /r/mars /r/rhea /r/venus /r/astrophys /r/spacevideos /r/transhuman /r/timereddits /r/virtualreality /r/vive /r/oculus /r/learnvrdev /r/unity3d /r/gamedev /r/crowdfunding /r/crowdsourcing /r/mturk /r/swagbucks /r/beermoney /r/flipping /r/shoplifting /r/thriftstorehauls /r/dvdcollection /r/televisionposterporn /r/concertposterporn /r/movieposterporn /r/lv426 /r/predator /r/arnoldschwarzenegger /r/alanpartridge /r/americandad /r/timanderic /r/homemovies /r/gravityfalls /r/homestarrunner /r/telltale /r/thewalkingdeadgame /r/thewalkingdeadgifs /r/twdnomansland /r/heycarl /r/twdroadtosurvival /r/thewalkingdead /r/zombies /r/guns /r/swissguns /r/opencarry /r/libertarian /r/geolibertarianism /r/basicincome /r/basicincomeactivism /r/mhoc /r/modelaustralia /r/rmtk /r/thenetherlands /r/tokkiefeesboek /r/nujijinactie /r/ik_ihe /r/youirl /r/fite_me_irl /r/2meirl4meirl /r/depression /r/randomactsofcards /r/philately /r/coins /r/coins4sale /r/ancientcoins /r/ancientrome /r/flatblue /r/bestofwritingprompts /r/writingprompts /r/promptoftheday /r/flashfiction /r/keepwriting /r/getmotivated /r/mentors /r/favors /r/recordthis /r/videography /r/animation /r/3dsmax /r/computergraphics /r/cinema4d /r/design /r/ui_design /r/designjobs /r/heavymind /r/wtfart /r/alternativeart /r/imaginaryninjas /r/imaginaryruins /r/isometric /r/imaginaryislands /r/imaginaryverse /r/icandrawthat /r/caricatures /r/imaginaryneweden /r/imaginaryequestria /r/imaginaryaww /r/imaginarycyberpunk /r/chinafuturism /r/scifirealism /r/inegentlemanboners /r/imaginarywtf /r/imaginaryelementals /r/imaginarydinosaurs /r/dinosaurs /r/speculativeevolution /r/hybridanimals /r/photoshopbattles /r/cutouts /r/battleshops /r/graphic_design /r/visualization /r/statistics /r/oncourtanalytics /r/nbaanalytics /r/nba /r/pacers /r/atlantahawks /r/basketball /r/mavericks /r/fcdallas /r/theticket /r/dallasstars /r/bostonbruins /r/patriots /r/tennesseetitans /r/nashvillesounds /r/predators /r/flyers /r/hockeyfandom /r/caps /r/nhl /r/detroitredwings /r/sabres /r/floridapanthers /r/habs /r/montrealimpact /r/alouettes /r/cfl /r/stadiumporn /r/nfl /r/madden /r/eurobowl /r/fantasyfb /r/fantasyfootball /r/49ers /r/footballgamefilm /r/footballstrategy /r/cfb /r/collegebaseball /r/mlbdraft /r/baseball /r/cubs /r/cardinals /r/saintlouisfc /r/stlouisblues /r/stlouis /r/stlouisbiking /r/mobicycling /r/bicycling /r/vintage_bicycles /r/miamibiking /r/fatbike /r/cycling /r/strava /r/phillycycling /r/wheelbuild /r/bikewrench /r/velo /r/bikepolo /r/bicycletouring /r/bicyclingcirclejerk /r/bikecommuting /r/ukbike /r/leedscycling /r/londoncycling /r/fixedgearbicycle /r/cyclingfashion /r/peloton /r/mtb /r/climbingporn /r/adrenaline /r/motocross /r/bmxracing /r/wake /r/snowboardingnoobs /r/freebord /r/snowboarding /r/sledding /r/outdoors /r/soposts /r/cordcutters /r/netflixviavpn /r/hulu /r/firetv /r/netflixbestof /r/raisinghope /r/madmen /r/earthsgottalent /r/bobsburgers /r/fringe /r/louie /r/theoriginals /r/iansomerhalder /r/kat_graham /r/indianaevans /r/janelevy /r/gagegolightly /r/sarahhyland /r/starlets /r/ninadobrev /r/kathrynnewton /r/arielwinter /r/ashleygreene /r/gentlemanboners /r/bandporn /r/musicpics /r/listentomusic /r/listentonew /r/subraddits /r/dtipics /r/damnthatsinteresting /r/interestingasfuck /r/unexpected /r/wtf /r/weird /r/animalsbeingderps /r/animalsbeingconfused /r/humansbeingbros /r/hulpdiensten /r/askle /r/protectandserve /r/good_cop_free_donut /r/bad_cop_follow_up /r/amifreetogo /r/copwatch /r/puppycide /r/underreportednews /r/mediaquotes /r/savedyouaclick /r/news /r/neutralnews /r/ask_politics /r/politicalopinions /r/gunsarecool /r/renewableenergy /r/web_design /r/somebodymakethis /r/somethingimade /r/crafts /r/kidscrafts /r/daddit /r/formulafeeders /r/boobsandbottles /r/csectioncentral /r/predaddit /r/dadbloggers /r/mombloggers /r/cutekids /r/bigfeats /r/scienceparents /r/lv9hrvv /r/sahp /r/tryingforababy /r/waiting_to_try /r/pcos /r/infertility /r/birthparents /r/tfabchartstalkers /r/firsttimettc /r/cautiousbtb /r/ttchealthy /r/xxketo /r/ketoscience /r/ketogains /r/leangains /r/gettingshredded /r/bulkorcut /r/gainit /r/decidingtobebetter /r/zen /r/buddhism /r/astralprojection /r/spirituality /r/hinduism /r/yoga /r/veganfitness /r/posture /r/health /r/ukhealthcare /r/pharmacy /r/nursing /r/doctorswithoutborders /r/humanitarian /r/assistance /r/paranormalhelp /r/paranormal /r/333 /r/askparanormal /r/intelligence /r/blackhat /r/netsec /r/technology /r/newyorkfuturistparty /r/rad_decentralization /r/massachusettsfp /r/opensource /r/alabamafp /r/darknetplan /r/torrents /r/i2p /r/privacy /r/badgovnofreedom /r/censorship /r/governmentoppression /r/descentintotyranny /r/wikileaks /r/dncleaks /r/hillaryforprison /r/the_donald /r/shitredditsays /r/srsmythos /r/srstrees /r/entwives /r/lesbients /r/actuallesbians /r/lesbianromance /r/lesbianerotica /r/l4l /r/dyke /r/ladyladyboners /r/bisexual /r/bisexy /r/biwomen /r/pansexual /r/genderqueer /r/transspace /r/lgbtlibrary /r/lgbtnews /r/dixiequeer /r/lgbt /r/sex /r/helpmecope /r/bpd /r/rapecounseling /r/trueoffmychest /r/suicidewatch /r/bipolarsos /r/bipolar /r/mentalpod /r/adhd /r/hoarding /r/declutter /r/thrifty /r/tinyhouses /r/leanfire /r/lowcar /r/zerowaste /r/simpleliving /r/livingofftheland /r/hunting /r/animaltracking /r/survival /r/vedc /r/4x4 /r/classiccars /r/automotivetraining /r/autodetailing /r/cartalk /r/mercedes_benz /r/motorsports /r/rallycross /r/worldrallycross /r/blancpain /r/nascarhometracks /r/arcaracing /r/stadiumsupertrucks /r/hydroplanes /r/sailing /r/boatbuilding /r/woodworking /r/cottage_industry /r/farriers /r/blacksmith /r/bladesmith /r/knives /r/swissarmyknives /r/switzerland /r/bern /r/sanktgallen /r/liechtenstein /r/erasmus /r/de /r/germanpuns /r/schland /r/rvacka /r/sloensko /r/slovakia /r/belarus /r/andorra /r/europe /r/hungary /r/francophonie /r/thailand /r/vietnam /r/vietnampics /r/travel /r/geography /r/climate /r/drought /r/waterutilities /r/drylands /r/irrigation /r/water /r/onthewaterfront /r/wetlands /r/marinelife /r/ocean /r/seasteading /r/frontier_colonization /r/arcology /r/retrofuturism /r/goldenpath /r/politics /r/moderationtheory /r/wdp /r/outoftheloop /r/wherearetheynow /r/entertainment /r/portlandia /r/themichaeljfoxshow /r/backtothefuture /r/bladerunner /r/filmnoir /r/vintageladyboners /r/classicfilms /r/foreignmovies /r/britishfilms /r/canadianfilm /r/newjerseyfilm /r/newzealandfilm /r/newzealand /r/wellington /r/nzmetahub /r/newzealandhistory /r/scottishhistory /r/scots /r/scottishproblems /r/britishproblems /r/swedishproblems /r/pinsamt /r/sweden /r/svenskpolitik /r/arbetarrorelsen /r/socialism /r/shittydebatecommunism /r/shittysocialscience /r/shittyideasforadmins /r/shittytheoryofreddit /r/shittybuildingporn /r/shittylifeprotips /r/shittyshitredditsays /r/shittyquotesporn /r/shittyama /r/askashittyparent /r/shittyprogramming /r/shittyaskalawyer /r/badlegaladvice /r/badscience /r/badeconomics /r/badhistory /r/historicalrage /r/metarage /r/ragenovels /r/fffffffuuuuuuuuuuuu /r/gaaaaaaayyyyyyyyyyyy /r/lgbteens /r/needafriend /r/rant /r/showerthoughts /r/markmywords /r/calledit /r/futurewhatif /r/sportswhatif /r/alternatehistory /r/maps /r/xkcd /r/kerbalspaceprogram /r/spacesimgames /r/eve /r/scifigaming /r/masseffect /r/imaginarymasseffect /r/imaginaryvampires /r/imaginarytowers /r/imaginarybestof /r/pics /r/spaceporn /r/auroraporn /r/weatherporn /r/sfwpornnetwork /r/fwepp /r/shittyearthporn /r/shittyaskreddit /r/askashittyphilosopher /r/shittyaskhistory /r/shittysuboftheweek /r/shittyaskcooking /r/shittyhub /r/coolguides /r/trendingsubreddits /r/monkslookingatbeer /r/beerporn /r/beerwithaview /r/shittybeerwithaview /r/shittyfoodporn /r/enttreats /r/trees /r/eldertrees /r/vaporents /r/crainn /r/eirhub /r/fairepublicofireland /r/gaeltacht /r/westmeath /r/tipperary /r/limerick /r/kilkenny /r/ireland /r/irejobs /r/resumes /r/careerguidance /r/flatone /r/centralillinois /r/chicubs /r/whitesox /r/minnesotatwins /r/minnesotavikings /r/greenbaypackers /r/jaguars /r/miamidolphins /r/nflroundtable /r/detroitlions /r/forhonor /r/vikingstv /r/hannibaltv /r/thepathhulu /r/batesmotel /r/hannibal /r/hitchcock /r/silentmoviegifs /r/moviestunts /r/bollywoodrealism /r/indiamain /r/indianews /r/asia /r/oldindia /r/explorepakistan /r/churchporn /r/medievalporn /r/castles /r/historyporn /r/thewaywewere /r/1970s /r/classicmovietrailers /r/warmovies /r/moviecritic /r/trailers /r/liveaction /r/animedeals /r/dbz /r/toonami /r/regularshow /r/thelifeandtimesoftim /r/aquajail /r/modern_family /r/supernatural /r/mishacollins /r/jaredpadalecki /r/fandomnatural /r/fangirls /r/trollxgirlgamers /r/trollmedia /r/trollgaming /r/trollmua /r/justtrollxthings /r/trollxmoms /r/trollmeta /r/trollychromosome /r/oney /r/askwomen /r/okcupid /r/relationship_advice /r/help /r/bugs /r/redditdev /r/enhancement /r/yoursub /r/horrorreviewed /r/truecreepy /r/metatruereddit /r/truepolitics /r/truehub /r/truegaming /r/askgames /r/freegamesonandroid /r/androidapps /r/apphookup /r/browsemyreddit /r/findareddit /r/trap /r/naut /r/militaryfinance /r/army /r/militarystories /r/nationalguard /r/uscg /r/usa /r/murica /r/lonestar /r/whataburger /r/fastfood /r/cocacola /r/kelloggs /r/kellawwggs /r/awwducational /r/marinebiologygifs /r/biologygifs /r/chemicalreactiongifs /r/homechemistry /r/holdmybeaker /r/holdmybeer /r/movieoftheday /r/sharknado /r/syfy /r/killjoys /r/theexpanse /r/truedetective /r/boardwalkempire /r/mobcast /r/1920s /r/1960s /r/beatles /r/minimaluminiumalism /r/ghostsrights /r/botsrights /r/totallynotrobots /r/robotics /r/manna /r/singularity /r/futureporn /r/singularitarianism /r/automate /r/darkfuturology /r/controlproblem /r/aiethics /r/ainothuman /r/neuraljokes /r/3amjokes /r/mommajokes /r/antijokes /r/absolutelynotme_irl /r/toomeirlformeirl /r/meirl /r/tree_irl /r/fishpost /r/mod_irl /r/pics_irl /r/teleshits /r/bitstrips /r/stopbullyingcomics /r/animalsbeingjerks /r/surfinganimals /r/unorthocat /r/catsubs /r/stuffoncats /r/catsinbusinessattire /r/catsinsinks /r/catsonkeyboards /r/mechanicalkeyboards /r/hackedgadgets /r/techsupportmacgyver /r/techsupport /r/programming /r/algorithms /r/datamining /r/datasets /r/wordcloud /r/datavizrequests /r/funnycharts /r/mapporn /r/mapmaking /r/worldbuilding /r/scificoncepts /r/apocalypseporn /r/imaginaryjerk /r/braveryjerk /r/circlejerk /r/politicaldiscussion /r/politicalfactchecking /r/moderatepolitics /r/truereddit /r/malelifestyle /r/fitness /r/swimming /r/freediving /r/bikeshop /r/climbing /r/climbharder /r/bouldering /r/climbergirls /r/womenshredders /r/skatergirls /r/girlsurfers /r/kiteboarding /r/longboarding /r/streetboarding /r/letsgosnowboarding /r/spliddit /r/backcountry /r/wjdbbl2 /r/caving /r/nationalparks /r/parkrangers /r/thesca /r/searchandrescue /r/wildernessbackpacking /r/campinggear /r/flashlight /r/camping /r/yellowstone /r/wmnf /r/pacificcresttrail /r/cdt /r/ultralight /r/backpacking /r/travelpartners /r/adventures /r/libraryofshadows /r/shortscarystories /r/shortscarystoriesooc /r/nosleepooc /r/nosleep<!--]--></p><!--]--></blockquote><h1 id="centrality"><!--[-->Centrality<!--]--></h1><p><!--[-->Centrality is anohter important topic in graph theory. Here&#39;s a brief introduction to centrality from <a href="https://en.wikipedia.org/wiki/Centrality" rel="nofollow"><!--[-->Wikipedia<!--]--></a>:<!--]--></p><blockquote><!--[--><p><!--[-->In graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease.<!--]--></p><!--]--></blockquote><p><!--[-->There are several different methods of measuring centrality in a graph. Here I use <code><!--[-->eigenvector_centrality_numpy<!--]--></code>, a function included in NetworkX. It takes in a graph and returns a dictionary with graph nodes as keys and node centrality as values.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>centrality = nx.eigenvector_centrality_numpy(G1)
</span></span></code><!--]--></pre><p><!--[-->Let&#39;s see which subreddit has the highest centrality:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>print max(centrality, key=centrality.get), centrality[max(centrality, key=centrality.get)]
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/imaginarybattlefields 0.0721530261127
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>len(centrality) == len(sorted(centrality.values(), reverse=True))
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>True
</code><!--]--></pre><p><!--[-->Since all of the centrality values are unique, we can look up nodes by their centrality values.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>subr_list = []
</span></span><span class="line" line="2"><span>for node in centrality:
</span></span><span class="line" line="3"><span>    subr_list.append((node, centrality[node]))
</span></span><span class="line" line="4"><span emptylineplaceholder="true">
</span></span><span class="line" line="5"><span>sorted_subr_list = subr_list.sort(key=lambda x: x[1])
</span></span></code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>for x in sorted(subr_list, key=lambda x: x[1], reverse=True)[:200]: print x[0],
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/imaginarybattlefields /r/imaginarycityscapes /r/imaginarywastelands /r/imaginarywildlands /r/imaginaryleviathans /r/imaginarydragons /r/imaginarystarscapes /r/imaginarywesteros /r/imaginaryartifacts /r/imaginaryangels /r/imaginarymaps /r/imaginarybehemoths /r/imaginarydemons /r/imaginaryelves /r/imaginarycentaurs /r/imaginaryfuturewar /r/imaginarysoldiers /r/imaginaryhistory /r/imaginaryarmor /r/imaginarystarships /r/imaginarynetwork /r/imaginaryjedi /r/imaginarydinosaurs /r/imaginarysteampunk /r/imaginarycyberpunk /r/imaginaryarchers /r/imaginaryvehicles /r/imaginaryanime /r/imaginaryfallout /r/imaginaryastronauts /r/imaginarymusic /r/imaginaryfactories /r/imaginaryequestria /r/imaginarywarships /r/imaginaryazeroth /r/imaginaryarrakis /r/imaginarydisney /r/imaginarypolitics /r/imaginaryhorrors /r/imaginarywinterscapes /r/imaginaryseascapes /r/imaginarypirates /r/imaginarywarriors /r/imaginarymiddleearth /r/imaginarygallifrey /r/imaginarymechs /r/imaginarypropaganda /r/imaginarymerfolk /r/imaginaryvikings /r/imaginaryundead /r/imaginarybeasts /r/imaginarymutants /r/imaginaryruins /r/imaginarytamriel /r/imaginaryforests /r/imaginaryelementals /r/imaginaryskyscapes /r/imaginarymonuments /r/imaginarywaterfalls /r/imaginaryworlds /r/imaginarywizards /r/imaginaryinteriors /r/imaginaryhogwarts /r/imaginarytowers /r/imaginaryarchitecture /r/imaginaryweaponry /r/imaginarygaming /r/imaginarycastles /r/imaginaryrobotics /r/imaginarybooks /r/imaginarygnomes /r/imaginaryvillages /r/imaginarydeserts /r/imaginarywerewolves /r/imaginarydieselpunk /r/imaginaryvampires /r/imaginaryadrenaline /r/imaginarykanto /r/imaginarynatives /r/imaginaryrivers /r/imaginarytemples /r/imaginaryassassins /r/imaginaryvolcanoes /r/imaginaryclerics /r/imaginaryprisons /r/imaginarygiants /r/imaginarycowboys /r/imaginaryhumans /r/imaginarydwarves /r/imaginarycaves /r/imaginarytrolls /r/imaginarywalls /r/imaginarylakes /r/imaginarywitches /r/imaginaryorcs /r/imaginarycanyons /r/imaginaryasylums /r/imaginaryimmortals /r/imaginaryaliens /r/imaginarynobles /r/imaginaryspirits /r/imaginaryaetherpunk /r/imaginarytrees /r/imaginaryislands /r/imaginaryninjas /r/imaginaryscience /r/imaginarymountains /r/imaginaryknights /r/imaginarygoblins /r/imaginaryfaeries /r/imaginarygotham /r/imaginarycybernetics /r/imaginaryooo /r/imaginaryderelicts /r/imaginaryfood /r/imaginaryworldeaters /r/imaginarymindscapes /r/imaginaryaww /r/imaginarymarvel /r/imaginaryweather /r/imaginarynewnewyork /r/imaginaryspidey /r/imaginaryautumnscapes /r/imaginarywarhammer /r/imaginaryfeels /r/imaginarywitcher /r/imaginaryvessels /r/imaginarytaverns /r/imaginarybestof /r/imaginaryairships /r/imaginaryportals /r/imaginaryfashion /r/imaginarylovers /r/imaginarydc /r/imaginaryanimals /r/imaginaryhellscapes /r/imaginarycolorscapes /r/imaginarymonstergirls /r/imaginaryswamps /r/imaginarymythology /r/imaginaryscholars /r/imaginaryladyboners /r/imaginaryfuturism /r/imaginaryaviation /r/imaginarypathways /r/imaginarygatherings /r/imaginarybodyscapes /r/imaginaryoverwatch /r/imaginarydwellings /r/imaginarystephenking /r/specart /r/inegentlemanboners /r/comicbookart /r/imaginarymasseffect /r/imaginaryhalo /r/imaginaryjerk /r/backgroundart /r/futureporn /r/imaginarywallpapers /r/imaginaryfamilies /r/imaginarylibraries /r/imaginaryturtleworlds /r/imaginarydesigns /r/wallpapers /r/apocalypseporn /r/comicbookporn /r/isometric /r/imaginarybakerst /r/imaginaryverse /r/imaginarysunnydale /r/imaginaryfederation /r/imaginarysanctuary /r/starshipporn /r/imaginarystarcraft /r/imaginaryoldkingdom /r/imaginarynarnia /r/imaginarycybertron /r/gameworlds /r/imaginarycarnage /r/imaginaryboners /r/icandrawthat /r/imaginarycosmere /r/imaginaryaperture /r/armoredwomen /r/imaginarywtf /r/unusualart /r/imaginaryblueprints /r/alternativeart /r/sympatheticmonsters /r/adorabledragons /r/imaginarysummerscapes /r/imaginarygayboners /r/imaginarystash /r/artistoftheday /r/imaginaryglaciers /r/imaginaryhybrids /r/imaginaryadventurers /r/imaginarymetropolis /r/craftsoficeandfire /r/popartnouveau
</code><!--]--></pre><p><!--[-->There seems to be a network of &quot;imaginary&quot; subreddits that have the highest centrality. The members of this network probably all link to themselves as well as many other subreddits as the &quot;imaginary&quot; topics span a wide range content. This network may be drowning out other nodes that would otherwise have a high centrality relative to the rest of the subreddits. It might be interesting to eliminate these nodes from the graph and recalculate centrality. Let&#39;s look at the distribution of centrality values:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>_ = plt.plot(sorted(centrality.values(), reverse=True)[:1000])
</span></span><span class="line" line="2"><span>_ = plt.title(&#39;Subreddit Centrality (top 1000)&#39;)
</span></span><span class="line" line="3"><span>_ = plt.xlabel(&#39;Rank&#39;)
</span></span><span class="line" line="4"><span>_ = plt.ylabel(&#39;Centrality&#39;)
</span></span><span class="line" line="5"><span>plt.savefig(os.path.expanduser(&#39;~/Documents/GitHub/briancaffey.github.io/img/subreddit_graph/centrality.png&#39;))
</span></span></code><!--]--></pre><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="png" data-nuxt-img srcset="/_ipx/_/static/subreddit_graph/centrality.png 1x, /_ipx/_/static/subreddit_graph/centrality.png 2x" src="/_ipx/_/static/subreddit_graph/centrality.png"><!--]--></p><h1 id="connectedness"><!--[-->Connectedness<!--]--></h1><p><!--[-->Let&#39;s take a look at the graph as a whole. One thing I&#39;m not sure of is whether or not the entire graph is connected. This means that any node can be reached from any other node. Since we constructed the graph from 49 unrelated nodes, it is possible that the graph is unconnected. This would mean that one or more of the default subreddits and its subreddits is not connected with the rest of the graph. In searching for the shortest path I did not come across any pairs of nodes that did not have a path between themselves. I wouldn&#39;t be surprised if there are a handful of nodes that stand on their own.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>#size of graph: nodes and edges (or, subreddits and connecting links)
</span></span><span class="line" line="2"><span>print &quot;Our graph has &quot; + str(nx.number_of_nodes(G1)) + &#39; nodes and &#39; + str(nx.number_of_edges(G1)) + &#39; edges.&#39;
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>Our graph has 29854 nodes and 149491 edges.
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>print &quot;True of False: our graph is connected... &quot; + str(nx.is_connected(G1)) + &#39;!&#39;
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>True of False: our graph is connected... False!
</code><!--]--></pre><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>Gc = max(nx.connected_component_subgraphs(G1), key=len)
</span></span><span class="line" line="2"><span>print &quot;The largest connected component subgraph has &quot; + str(nx.number_of_nodes(Gc)) + &quot; nodes. &quot;
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>The largest connected component subgraph has 29840 nodes.
</code><!--]--></pre><p><!--[-->There are 14 nodes that are not connected to the main connected component. Let&#39;s list them.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>for x in list(set(nx.to_dict_of_lists(G1, nodelist=None).keys()) - set(nx.to_dict_of_lists(Gc, nodelist=None).keys())): print x,
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>/r/spacediscussions /r/wtfit.gif /r/space. /r/subreddit_graph /r/vidalia /r/listentothis. /r/history. /r/all. /r/ghostdriver /r/personalfinance. /r/toombscounty /r/gaming /r/science /r/books.
</code><!--]--></pre><p><!--[-->Some of the large communities on reddit include /r/books, /r/gaming and /r/science. These subreddits list related subreddits on separate wiki pages since there are many related subreddits for each one. They were most likely all captured in the subsequent levels of the graph, but they also did not link back to /r/science. Here&#39;s an example:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>for x in master_df_u.loc[master_df_u.subreddit==&#39;/r/physics&#39;].related: print x
</span></span></code><!--]--></pre><pre class="language-text"><!--[--><code>[&#39;/r/physicsjokes&#39;, &#39;/r/gradadmissions&#39;, &#39;/r/homeworkhelp&#39;, &#39;/r/scienceimages&#39;, &#39;/r/askacademia&#39;, &#39;/r/physicsgifs&#39;, &#39;/r/physicsstudents&#39;, &#39;/r/gradschool&#39;, &#39;/r/askphysics&#39;, &#39;/r/physics&#39;]
</code><!--]--></pre><p><!--[-->I&#39;ve got some additional ideas to explore in another post on this topic, such as finding cliques and maximual cliques, and doing graph visualizations with D3.js. If you are interested in playing with the data, you can clone <a href="https://github.com/briancaffey/reddit-graph-analysis" rel="nofollow"><!--[-->my GitHub repo<!--]--></a> and load the pickled DataFrames like this:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>import pandas as pd
</span></span><span class="line" line="2"><span>df = pd.read_pickle(&#39;pickle/master_df.p&#39;)
</span></span></code><!--]--></pre><style>html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}</style></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments üí¨ </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> ¬© 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2017/03/03/graph_subreddit/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"graph_subreddit":-1},"/2017/03/03/graph_subreddit",{},1753130132332]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"üá∫üá∏",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Fran√ßais",flag:"üá´üá∑",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"ÁÆÄ‰Ωì‰∏≠Êñá",flag:"üá®üá≥",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"–†—É—Å—Å–∫–∏–π",flag:"üá∑üá∫",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"Êó•Êú¨Ë™û",flag:"üáØüáµ",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"‡§π‡§ø‡§Ç‡§¶‡•Ä",flag:"üáÆüá≥",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"711989cd-b755-45e7-b273-f0754c0f755f",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>