[{"data":1,"prerenderedAt":2001},["ShallowReactive",2],{"graph_subreddit":3},{"id":4,"title":5,"body":6,"comments":108,"date":1984,"description":1985,"draft":1986,"extension":1987,"external":1988,"image":1989,"meta":1990,"navigation":108,"path":1992,"seo":1993,"stem":1994,"tags":1995,"__hash__":2000},"blog/2017/03/03/graph_subreddit.md","Related subreddit graph exploration with NetworkX",{"type":7,"value":8,"toc":1982},"minimark",[9,14,23,26,38,79,145,152,405,408,411,419,424,432,435,441,449,624,627,643,646,1046,1049,1064,1071,1074,1080,1083,1097,1100,1191,1198,1202,1211,1226,1235,1250,1265,1290,1305,1311,1314,1323,1329,1333,1336,1345,1351,1354,1393,1400,1409,1415,1423,1429,1437,1443,1451,1457,1465,1471,1479,1485,1493,1499,1507,1513,1521,1527,1535,1541,1549,1555,1563,1569,1577,1583,1591,1597,1612,1618,1630,1637,1697,1700,1705,1709,1717,1722,1729,1738,1741,1750,1756,1765,1770,1773,1802,1811,1817,1820,1850,1855,1859,1862,1877,1883,1892,1898,1913,1919,1922,1931,1937,1940,1949,1955,1964,1978],[10,11,13],"h1",{"id":12},"graphing-subreddits","Graphing Subreddits",[15,16,17,18,22],"p",{},"This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from ",[19,20,21],"a",{"href":21},"reddit.com"," with the goal of visualizing the network of related subreddits (forums on specific topics) and related data.",[15,24,25],{},"Reddit is home over 600,000 communities, known as subreddits, where people come to share information, opinions, links, etc. and discuss things in a open forum. Most subreddits display links to related subreddits. For example, /r/apple (the Apple subreddit) links to /r/iPhone, a subreddit all about the iPhone, and over a dozen other Apple-related subreddits.",[15,27,28,29,33,34,37],{},"If you visit reddit.com as a guest, you will see a list of popular subreddits. This list is located inside an ",[30,31,32],"code",{},"html"," tag called ",[30,35,36],{},"drop-choices",". Here it is:",[39,40,45],"pre",{"className":41,"code":42,"language":43,"meta":44,"style":44},"language-python shiki shiki-themes github-light github-dark","from selenium import webdriver\nimport re\nimport time\nimport numpy as np\nfrom bs4 import BeautifulSoup\n","python","",[30,46,47,55,61,67,73],{"__ignoreMap":44},[48,49,52],"span",{"class":50,"line":51},"line",1,[48,53,54],{},"from selenium import webdriver\n",[48,56,58],{"class":50,"line":57},2,[48,59,60],{},"import re\n",[48,62,64],{"class":50,"line":63},3,[48,65,66],{},"import time\n",[48,68,70],{"class":50,"line":69},4,[48,71,72],{},"import numpy as np\n",[48,74,76],{"class":50,"line":75},5,[48,77,78],{},"from bs4 import BeautifulSoup\n",[39,80,82],{"className":41,"code":81,"language":43,"meta":44,"style":44},"driver = webdriver.PhantomJS()\ndriver.get('https://www.reddit.com/')\ntime.sleep(4 + np.random.random())\nhtml = driver.page_source.encode('utf-8')\n\ns = BeautifulSoup(html)\ndefaults = s.find('div', attrs={'class':'drop-choices'})\nsubs = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\ndefault_subreddits = list(set(subs.findall(str(defaults))))\n\nfor x in default_subreddits: print '[' + x + '](https://reddit.com'+ x + '), ',\n",[30,83,84,89,94,99,104,110,116,122,128,134,139],{"__ignoreMap":44},[48,85,86],{"class":50,"line":51},[48,87,88],{},"driver = webdriver.PhantomJS()\n",[48,90,91],{"class":50,"line":57},[48,92,93],{},"driver.get('https://www.reddit.com/')\n",[48,95,96],{"class":50,"line":63},[48,97,98],{},"time.sleep(4 + np.random.random())\n",[48,100,101],{"class":50,"line":69},[48,102,103],{},"html = driver.page_source.encode('utf-8')\n",[48,105,106],{"class":50,"line":75},[48,107,109],{"emptyLinePlaceholder":108},true,"\n",[48,111,113],{"class":50,"line":112},6,[48,114,115],{},"s = BeautifulSoup(html)\n",[48,117,119],{"class":50,"line":118},7,[48,120,121],{},"defaults = s.find('div', attrs={'class':'drop-choices'})\n",[48,123,125],{"class":50,"line":124},8,[48,126,127],{},"subs = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n",[48,129,131],{"class":50,"line":130},9,[48,132,133],{},"default_subreddits = list(set(subs.findall(str(defaults))))\n",[48,135,137],{"class":50,"line":136},10,[48,138,109],{"emptyLinePlaceholder":108},[48,140,142],{"class":50,"line":141},11,[48,143,144],{},"for x in default_subreddits: print '[' + x + '](https://reddit.com'+ x + '), ',\n",[15,146,147,148,151],{},"Here are the elements of ",[30,149,150],{},"default_subreddits",":",[153,154,155],"blockquote",{},[15,156,157,163,164,163,169,163,174,163,179,163,184,163,189,163,194,163,199,163,204,163,209,163,214,163,219,163,224,163,229,163,234,163,239,163,244,163,249,163,254,163,259,163,264,163,269,163,274,163,279,163,284,163,289,163,294,163,299,163,304,163,309,163,314,163,319,163,324,163,329,163,334,163,339,163,344,163,349,163,354,163,359,163,364,163,369,163,374,163,379,163,384,163,389,163,394,163,399,404],{},[19,158,162],{"href":159,"rel":160},"https://reddit.com/r/LifeProTips/",[161],"nofollow","/r/LifeProTips/",", ",[19,165,168],{"href":166,"rel":167},"https://reddit.com/r/Futurology/",[161],"/r/Futurology/",[19,170,173],{"href":171,"rel":172},"https://reddit.com/r/OldSchoolCool/",[161],"/r/OldSchoolCool/",[19,175,178],{"href":176,"rel":177},"https://reddit.com/r/mildlyinteresting/",[161],"/r/mildlyinteresting/",[19,180,183],{"href":181,"rel":182},"https://reddit.com/r/askscience/",[161],"/r/askscience/",[19,185,188],{"href":186,"rel":187},"https://reddit.com/r/UpliftingNews/",[161],"/r/UpliftingNews/",[19,190,193],{"href":191,"rel":192},"https://reddit.com/r/aww/",[161],"/r/aww/",[19,195,198],{"href":196,"rel":197},"https://reddit.com/r/GetMotivated/",[161],"/r/GetMotivated/",[19,200,203],{"href":201,"rel":202},"https://reddit.com/r/personalfinance/",[161],"/r/personalfinance/",[19,205,208],{"href":206,"rel":207},"https://reddit.com/r/gadgets/",[161],"/r/gadgets/",[19,210,213],{"href":211,"rel":212},"https://reddit.com/r/science/",[161],"/r/science/",[19,215,218],{"href":216,"rel":217},"https://reddit.com/r/dataisbeautiful/",[161],"/r/dataisbeautiful/",[19,220,223],{"href":221,"rel":222},"https://reddit.com/r/DIY/",[161],"/r/DIY/",[19,225,228],{"href":226,"rel":227},"https://reddit.com/r/AskReddit/",[161],"/r/AskReddit/",[19,230,233],{"href":231,"rel":232},"https://reddit.com/r/space/",[161],"/r/space/",[19,235,238],{"href":236,"rel":237},"https://reddit.com/r/nosleep/",[161],"/r/nosleep/",[19,240,243],{"href":241,"rel":242},"https://reddit.com/r/Documentaries/",[161],"/r/Documentaries/",[19,245,248],{"href":246,"rel":247},"https://reddit.com/r/todayilearned/",[161],"/r/todayilearned/",[19,250,253],{"href":251,"rel":252},"https://reddit.com/r/television/",[161],"/r/television/",[19,255,258],{"href":256,"rel":257},"https://reddit.com/r/IAmA/",[161],"/r/IAmA/",[19,260,263],{"href":261,"rel":262},"https://reddit.com/r/Art/",[161],"/r/Art/",[19,265,268],{"href":266,"rel":267},"https://reddit.com/r/EarthPorn/",[161],"/r/EarthPorn/",[19,270,273],{"href":271,"rel":272},"https://reddit.com/r/books/",[161],"/r/books/",[19,275,278],{"href":276,"rel":277},"https://reddit.com/r/gifs/",[161],"/r/gifs/",[19,280,283],{"href":281,"rel":282},"https://reddit.com/r/Showerthoughts/",[161],"/r/Showerthoughts/",[19,285,288],{"href":286,"rel":287},"https://reddit.com/r/blog/",[161],"/r/blog/",[19,290,293],{"href":291,"rel":292},"https://reddit.com/r/news/",[161],"/r/news/",[19,295,298],{"href":296,"rel":297},"https://reddit.com/r/Jokes/",[161],"/r/Jokes/",[19,300,303],{"href":301,"rel":302},"https://reddit.com/r/TwoXChromosomes/",[161],"/r/TwoXChromosomes/",[19,305,308],{"href":306,"rel":307},"https://reddit.com/r/videos/",[161],"/r/videos/",[19,310,313],{"href":311,"rel":312},"https://reddit.com/r/philosophy/",[161],"/r/philosophy/",[19,315,318],{"href":316,"rel":317},"https://reddit.com/r/nottheonion/",[161],"/r/nottheonion/",[19,320,323],{"href":321,"rel":322},"https://reddit.com/r/explainlikeimfive/",[161],"/r/explainlikeimfive/",[19,325,328],{"href":326,"rel":327},"https://reddit.com/r/movies/",[161],"/r/movies/",[19,330,333],{"href":331,"rel":332},"https://reddit.com/r/Music/",[161],"/r/Music/",[19,335,338],{"href":336,"rel":337},"https://reddit.com/r/WritingPrompts/",[161],"/r/WritingPrompts/",[19,340,343],{"href":341,"rel":342},"https://reddit.com/r/worldnews/",[161],"/r/worldnews/",[19,345,348],{"href":346,"rel":347},"https://reddit.com/r/pics/",[161],"/r/pics/",[19,350,353],{"href":351,"rel":352},"https://reddit.com/r/history/",[161],"/r/history/",[19,355,358],{"href":356,"rel":357},"https://reddit.com/r/listentothis/",[161],"/r/listentothis/",[19,360,363],{"href":361,"rel":362},"https://reddit.com/r/sports/",[161],"/r/sports/",[19,365,368],{"href":366,"rel":367},"https://reddit.com/r/food/",[161],"/r/food/",[19,370,373],{"href":371,"rel":372},"https://reddit.com/r/creepy/",[161],"/r/creepy/",[19,375,378],{"href":376,"rel":377},"https://reddit.com/r/announcements/",[161],"/r/announcements/",[19,380,383],{"href":381,"rel":382},"https://reddit.com/r/gaming/",[161],"/r/gaming/",[19,385,388],{"href":386,"rel":387},"https://reddit.com/r/tifu/",[161],"/r/tifu/",[19,390,393],{"href":391,"rel":392},"https://reddit.com/r/funny/",[161],"/r/funny/",[19,395,398],{"href":396,"rel":397},"https://reddit.com/r/photoshopbattles/",[161],"/r/photoshopbattles/",[19,400,403],{"href":401,"rel":402},"https://reddit.com/r/InternetIsBeautiful/",[161],"/r/InternetIsBeautiful/",",",[15,406,407],{},"My goal here is to see how many subreddits we can reach as we branch off of these \"default\" subreddits into their related subreddits.",[15,409,410],{},"First, we need to set up data structures to hold data for subreddits and their related subreddits. And we need to define an algorithm for collecting data.",[15,412,413,414,151],{},"Here's an intrdoduction to graphs from ",[19,415,418],{"href":416,"rel":417},"https://www.python.org/doc/essays/graphs/",[161],"python.org",[153,420,421],{},[15,422,423],{},"Few programming languages provide direct support for graphs as a data type, and Python is no exception. However, graphs are easily built out of lists and dictionaries. For instance, here's a simple graph (I can't use drawings in these columns, so I write down the graph's arcs):",[39,425,430],{"className":426,"code":428,"language":429,"meta":44},[427],"language-text","A -> B\nA -> C\nB -> C\nB -> D\nC -> D\nD -> C\nE -> F\nF -> C\n","text",[30,431,428],{"__ignoreMap":44},[15,433,434],{},"This graph has six nodes (A-F) and eight arcs. It can be represented by the following Python data structure:",[39,436,439],{"className":437,"code":438,"language":429,"meta":44},[427],"graph =     {'A': ['B', 'C'],\n             'B': ['C', 'D'],\n             'C': ['D'],\n             'D': ['C'],\n             'E': ['F'],\n             'F': ['C']}\n",[30,440,438],{"__ignoreMap":44},[15,442,443,444,448],{},"First let's define how we would go only one branch deep into this graph (i.e. find the related subreddits for ",[445,446,447],"em",{},"only"," the default subreddits). To collect the data, I first looped through the default subreddits and save the html of each subreddit to its own text file. Here's a script with comments:",[39,450,452],{"className":41,"code":451,"language":43,"meta":44,"style":44},"#first we navigate to the correct folder where we will store the first level of related subreddits\nos.chdir(os.path.expanduser('~/Documents/Projects/Data/Subreddits/one/'))\n\n#next we instantiate the webdriver we will be using: PhantomJS\ndriver = webdriver.PhantomJS()\n\n#loop through the list of default subreddits\nfor num, subreddit in enumerate(default_subreddits):\n\n    #for each subreddit, we append the /r/subreddit path to the base URL (reddit.com)\n    driver.get('https://www.reddit.com'+subreddit)\n\n    #wait for two seconds\n    time.sleep(2 + np.random.random())\n\n    #save the html of the loaded page to a variable: html\n    html = driver.page_source.encode('utf-8')\n\n    #remove '/r/' from the subreddit name string\n    name = subreddit.split('/')[2]\n\n    #open a new file and give it the name of the subreddit we just scraped\n    subreddit_html_file = open(name+'.txt', 'w+')\n\n    #write the html contents to the file\n    subreddit_html_file.write(html)\n\n    #clost the file\n    subreddit_html_file.close()\n\n    #print out the number and name of the subreddit we just scrapped to make sure things are working\n    print str(num) + ' ' + subreddit,\n\n",[30,453,454,459,464,468,473,477,481,486,491,495,500,505,510,516,522,527,533,539,544,550,556,561,567,573,578,584,590,595,601,607,612,618],{"__ignoreMap":44},[48,455,456],{"class":50,"line":51},[48,457,458],{},"#first we navigate to the correct folder where we will store the first level of related subreddits\n",[48,460,461],{"class":50,"line":57},[48,462,463],{},"os.chdir(os.path.expanduser('~/Documents/Projects/Data/Subreddits/one/'))\n",[48,465,466],{"class":50,"line":63},[48,467,109],{"emptyLinePlaceholder":108},[48,469,470],{"class":50,"line":69},[48,471,472],{},"#next we instantiate the webdriver we will be using: PhantomJS\n",[48,474,475],{"class":50,"line":75},[48,476,88],{},[48,478,479],{"class":50,"line":112},[48,480,109],{"emptyLinePlaceholder":108},[48,482,483],{"class":50,"line":118},[48,484,485],{},"#loop through the list of default subreddits\n",[48,487,488],{"class":50,"line":124},[48,489,490],{},"for num, subreddit in enumerate(default_subreddits):\n",[48,492,493],{"class":50,"line":130},[48,494,109],{"emptyLinePlaceholder":108},[48,496,497],{"class":50,"line":136},[48,498,499],{},"    #for each subreddit, we append the /r/subreddit path to the base URL (reddit.com)\n",[48,501,502],{"class":50,"line":141},[48,503,504],{},"    driver.get('https://www.reddit.com'+subreddit)\n",[48,506,508],{"class":50,"line":507},12,[48,509,109],{"emptyLinePlaceholder":108},[48,511,513],{"class":50,"line":512},13,[48,514,515],{},"    #wait for two seconds\n",[48,517,519],{"class":50,"line":518},14,[48,520,521],{},"    time.sleep(2 + np.random.random())\n",[48,523,525],{"class":50,"line":524},15,[48,526,109],{"emptyLinePlaceholder":108},[48,528,530],{"class":50,"line":529},16,[48,531,532],{},"    #save the html of the loaded page to a variable: html\n",[48,534,536],{"class":50,"line":535},17,[48,537,538],{},"    html = driver.page_source.encode('utf-8')\n",[48,540,542],{"class":50,"line":541},18,[48,543,109],{"emptyLinePlaceholder":108},[48,545,547],{"class":50,"line":546},19,[48,548,549],{},"    #remove '/r/' from the subreddit name string\n",[48,551,553],{"class":50,"line":552},20,[48,554,555],{},"    name = subreddit.split('/')[2]\n",[48,557,559],{"class":50,"line":558},21,[48,560,109],{"emptyLinePlaceholder":108},[48,562,564],{"class":50,"line":563},22,[48,565,566],{},"    #open a new file and give it the name of the subreddit we just scraped\n",[48,568,570],{"class":50,"line":569},23,[48,571,572],{},"    subreddit_html_file = open(name+'.txt', 'w+')\n",[48,574,576],{"class":50,"line":575},24,[48,577,109],{"emptyLinePlaceholder":108},[48,579,581],{"class":50,"line":580},25,[48,582,583],{},"    #write the html contents to the file\n",[48,585,587],{"class":50,"line":586},26,[48,588,589],{},"    subreddit_html_file.write(html)\n",[48,591,593],{"class":50,"line":592},27,[48,594,109],{"emptyLinePlaceholder":108},[48,596,598],{"class":50,"line":597},28,[48,599,600],{},"    #clost the file\n",[48,602,604],{"class":50,"line":603},29,[48,605,606],{},"    subreddit_html_file.close()\n",[48,608,610],{"class":50,"line":609},30,[48,611,109],{"emptyLinePlaceholder":108},[48,613,615],{"class":50,"line":614},31,[48,616,617],{},"    #print out the number and name of the subreddit we just scrapped to make sure things are working\n",[48,619,621],{"class":50,"line":620},32,[48,622,623],{},"    print str(num) + ' ' + subreddit,\n",[15,625,626],{},"Next, we want to go through each file and extract the information we want. Here's what we will be getting:",[628,629,630,634,637,640],"ul",{},[631,632,633],"li",{},"Number of subscribers",[631,635,636],{},"Subreddit description",[631,638,639],{},"Date created",[631,641,642],{},"Related subreddits",[15,644,645],{},"For this type of project, I prefer to loop through each page and creating several small dictionaries for each data point, then combine the small dictionaries into a large dictionary, and then append the dictionary to a list of dictionaries. Once I have looped through all of the pages, I can create a pandas DataFrame from the list of dictionaries. This allows me to easily manipulate the data. Here's the script that I used to do this:",[39,647,649],{"className":41,"code":648,"language":43,"meta":44,"style":44},"#navigate to where the html files are stored (I moved them around a bit so it is not consistent with the script above)\nos.chdir('E://DATA/Subreddits/subreddits_html/')\n\n#generate a list of files that we will loop through\nfiles = os.listdir('E://DATA/Subreddits/subreddits_html/')\n\n#set up an empty list that we will append dictionaries to\ndict_list = []\n\n#loop through the files\nfor file_ in files:\n\n    #print out the name of the current file in the loop\n    print file_,\n\n    #open the file\n    f = open(file_, 'r')\n    #read the file contents to a local variable\n    html = f.read()\n    #create a BeautifulSoup object that we will use to parse the HTML\n    b = BeautifulSoup(html, 'lxml')\n\n    #get the subreddit name that we are working with (from the `file` variable)\n    subreddit_name = '/r/' + file_[:-4].lower()\n    #put the name into a dictionary\n    subreddit_name_dict = {'subreddit':subreddit_name}\n\n    #get number of subscribers\n    subs = b.find('span', attrs={'class':'subscribers'})\n    #if the number of subscribers is displayed on the page, then we find it and add it to a dictionary\n    if subs:\n        subs = b.find('span', attrs={'class':'subscribers'}).find('span', attrs={'class':'number'}).text.replace(',', '')\n        subs_dict = {'subscribers':int(subs)}\n    #if the number of subscribers is not displayed on the page, then we set the number of subscribers in the dictionary to None\n    else:\n        subs_dict = {'subscribers':None}\n\n    #similar process for the description: if the description is displayed, get it and save it to desc\n    #if it is not available, then desc will be set to `None`\n    desc = b.find('div', attrs={'class':'md'})\n    if desc:\n        desc = b.find('div', attrs={'class':'md'}).text\n        desc = desc.replace('\\n', ' ')\n    desc_dict = {'description':desc}\n\n    #here we use regular expressions to find links anywhere on the page that have the structure: \"/r/something/\"\n    rel_subr = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n    #make a list of these links based on the \"/r/something/\" pattern\n    related_subreddits = rel_subr.findall(html)\n\n    #save the list to a dictionary\n    subreddits_dict = {'related':related_subreddits}\n\n    #same processes for recording the date that the subreddit was created: get the date from an HTML element,\n    #then save it to a dictionary. There were two different formats available in the HTML so I grabbed both\n    age = b.find('span', attrs={'class':'age'})\n    if age:\n        time1 = age.find('time')['title']\n        time2 = age.find('time')['datetime']\n\n    #save the date to a dictionary\n    time_dict = {\"date1\":time1, \"date2\":time2}\n\n    #take all the dictionaries we just created and put them together into one big dictionary\n    dictionary = dict(subs_dict.items()+desc_dict.items()+subreddits_dict.items()+subreddit_name_dict.items()+time_dict.items())\n\n    #append the big dictionary to the list that we defined right before the beginning of the loop\n    dict_list.append(dictionary)\n\n    #deconstruct the Beautiful Soup object (this can eat up memory very quickly, so it is very important when processing lots of data)\n    b.decompose()\n\n    #clost the file\n    f.close()\n",[30,650,651,656,661,665,670,675,679,684,689,693,698,703,707,712,717,721,726,731,736,741,746,751,755,760,765,770,775,779,784,789,794,799,804,810,816,822,828,833,839,845,851,857,863,869,875,880,886,892,898,904,909,915,921,926,932,938,944,950,956,962,967,973,979,984,990,996,1001,1007,1013,1018,1024,1030,1035,1040],{"__ignoreMap":44},[48,652,653],{"class":50,"line":51},[48,654,655],{},"#navigate to where the html files are stored (I moved them around a bit so it is not consistent with the script above)\n",[48,657,658],{"class":50,"line":57},[48,659,660],{},"os.chdir('E://DATA/Subreddits/subreddits_html/')\n",[48,662,663],{"class":50,"line":63},[48,664,109],{"emptyLinePlaceholder":108},[48,666,667],{"class":50,"line":69},[48,668,669],{},"#generate a list of files that we will loop through\n",[48,671,672],{"class":50,"line":75},[48,673,674],{},"files = os.listdir('E://DATA/Subreddits/subreddits_html/')\n",[48,676,677],{"class":50,"line":112},[48,678,109],{"emptyLinePlaceholder":108},[48,680,681],{"class":50,"line":118},[48,682,683],{},"#set up an empty list that we will append dictionaries to\n",[48,685,686],{"class":50,"line":124},[48,687,688],{},"dict_list = []\n",[48,690,691],{"class":50,"line":130},[48,692,109],{"emptyLinePlaceholder":108},[48,694,695],{"class":50,"line":136},[48,696,697],{},"#loop through the files\n",[48,699,700],{"class":50,"line":141},[48,701,702],{},"for file_ in files:\n",[48,704,705],{"class":50,"line":507},[48,706,109],{"emptyLinePlaceholder":108},[48,708,709],{"class":50,"line":512},[48,710,711],{},"    #print out the name of the current file in the loop\n",[48,713,714],{"class":50,"line":518},[48,715,716],{},"    print file_,\n",[48,718,719],{"class":50,"line":524},[48,720,109],{"emptyLinePlaceholder":108},[48,722,723],{"class":50,"line":529},[48,724,725],{},"    #open the file\n",[48,727,728],{"class":50,"line":535},[48,729,730],{},"    f = open(file_, 'r')\n",[48,732,733],{"class":50,"line":541},[48,734,735],{},"    #read the file contents to a local variable\n",[48,737,738],{"class":50,"line":546},[48,739,740],{},"    html = f.read()\n",[48,742,743],{"class":50,"line":552},[48,744,745],{},"    #create a BeautifulSoup object that we will use to parse the HTML\n",[48,747,748],{"class":50,"line":558},[48,749,750],{},"    b = BeautifulSoup(html, 'lxml')\n",[48,752,753],{"class":50,"line":563},[48,754,109],{"emptyLinePlaceholder":108},[48,756,757],{"class":50,"line":569},[48,758,759],{},"    #get the subreddit name that we are working with (from the `file` variable)\n",[48,761,762],{"class":50,"line":575},[48,763,764],{},"    subreddit_name = '/r/' + file_[:-4].lower()\n",[48,766,767],{"class":50,"line":580},[48,768,769],{},"    #put the name into a dictionary\n",[48,771,772],{"class":50,"line":586},[48,773,774],{},"    subreddit_name_dict = {'subreddit':subreddit_name}\n",[48,776,777],{"class":50,"line":592},[48,778,109],{"emptyLinePlaceholder":108},[48,780,781],{"class":50,"line":597},[48,782,783],{},"    #get number of subscribers\n",[48,785,786],{"class":50,"line":603},[48,787,788],{},"    subs = b.find('span', attrs={'class':'subscribers'})\n",[48,790,791],{"class":50,"line":609},[48,792,793],{},"    #if the number of subscribers is displayed on the page, then we find it and add it to a dictionary\n",[48,795,796],{"class":50,"line":614},[48,797,798],{},"    if subs:\n",[48,800,801],{"class":50,"line":620},[48,802,803],{},"        subs = b.find('span', attrs={'class':'subscribers'}).find('span', attrs={'class':'number'}).text.replace(',', '')\n",[48,805,807],{"class":50,"line":806},33,[48,808,809],{},"        subs_dict = {'subscribers':int(subs)}\n",[48,811,813],{"class":50,"line":812},34,[48,814,815],{},"    #if the number of subscribers is not displayed on the page, then we set the number of subscribers in the dictionary to None\n",[48,817,819],{"class":50,"line":818},35,[48,820,821],{},"    else:\n",[48,823,825],{"class":50,"line":824},36,[48,826,827],{},"        subs_dict = {'subscribers':None}\n",[48,829,831],{"class":50,"line":830},37,[48,832,109],{"emptyLinePlaceholder":108},[48,834,836],{"class":50,"line":835},38,[48,837,838],{},"    #similar process for the description: if the description is displayed, get it and save it to desc\n",[48,840,842],{"class":50,"line":841},39,[48,843,844],{},"    #if it is not available, then desc will be set to `None`\n",[48,846,848],{"class":50,"line":847},40,[48,849,850],{},"    desc = b.find('div', attrs={'class':'md'})\n",[48,852,854],{"class":50,"line":853},41,[48,855,856],{},"    if desc:\n",[48,858,860],{"class":50,"line":859},42,[48,861,862],{},"        desc = b.find('div', attrs={'class':'md'}).text\n",[48,864,866],{"class":50,"line":865},43,[48,867,868],{},"        desc = desc.replace('\\n', ' ')\n",[48,870,872],{"class":50,"line":871},44,[48,873,874],{},"    desc_dict = {'description':desc}\n",[48,876,878],{"class":50,"line":877},45,[48,879,109],{"emptyLinePlaceholder":108},[48,881,883],{"class":50,"line":882},46,[48,884,885],{},"    #here we use regular expressions to find links anywhere on the page that have the structure: \"/r/something/\"\n",[48,887,889],{"class":50,"line":888},47,[48,890,891],{},"    rel_subr = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n",[48,893,895],{"class":50,"line":894},48,[48,896,897],{},"    #make a list of these links based on the \"/r/something/\" pattern\n",[48,899,901],{"class":50,"line":900},49,[48,902,903],{},"    related_subreddits = rel_subr.findall(html)\n",[48,905,907],{"class":50,"line":906},50,[48,908,109],{"emptyLinePlaceholder":108},[48,910,912],{"class":50,"line":911},51,[48,913,914],{},"    #save the list to a dictionary\n",[48,916,918],{"class":50,"line":917},52,[48,919,920],{},"    subreddits_dict = {'related':related_subreddits}\n",[48,922,924],{"class":50,"line":923},53,[48,925,109],{"emptyLinePlaceholder":108},[48,927,929],{"class":50,"line":928},54,[48,930,931],{},"    #same processes for recording the date that the subreddit was created: get the date from an HTML element,\n",[48,933,935],{"class":50,"line":934},55,[48,936,937],{},"    #then save it to a dictionary. There were two different formats available in the HTML so I grabbed both\n",[48,939,941],{"class":50,"line":940},56,[48,942,943],{},"    age = b.find('span', attrs={'class':'age'})\n",[48,945,947],{"class":50,"line":946},57,[48,948,949],{},"    if age:\n",[48,951,953],{"class":50,"line":952},58,[48,954,955],{},"        time1 = age.find('time')['title']\n",[48,957,959],{"class":50,"line":958},59,[48,960,961],{},"        time2 = age.find('time')['datetime']\n",[48,963,965],{"class":50,"line":964},60,[48,966,109],{"emptyLinePlaceholder":108},[48,968,970],{"class":50,"line":969},61,[48,971,972],{},"    #save the date to a dictionary\n",[48,974,976],{"class":50,"line":975},62,[48,977,978],{},"    time_dict = {\"date1\":time1, \"date2\":time2}\n",[48,980,982],{"class":50,"line":981},63,[48,983,109],{"emptyLinePlaceholder":108},[48,985,987],{"class":50,"line":986},64,[48,988,989],{},"    #take all the dictionaries we just created and put them together into one big dictionary\n",[48,991,993],{"class":50,"line":992},65,[48,994,995],{},"    dictionary = dict(subs_dict.items()+desc_dict.items()+subreddits_dict.items()+subreddit_name_dict.items()+time_dict.items())\n",[48,997,999],{"class":50,"line":998},66,[48,1000,109],{"emptyLinePlaceholder":108},[48,1002,1004],{"class":50,"line":1003},67,[48,1005,1006],{},"    #append the big dictionary to the list that we defined right before the beginning of the loop\n",[48,1008,1010],{"class":50,"line":1009},68,[48,1011,1012],{},"    dict_list.append(dictionary)\n",[48,1014,1016],{"class":50,"line":1015},69,[48,1017,109],{"emptyLinePlaceholder":108},[48,1019,1021],{"class":50,"line":1020},70,[48,1022,1023],{},"    #deconstruct the Beautiful Soup object (this can eat up memory very quickly, so it is very important when processing lots of data)\n",[48,1025,1027],{"class":50,"line":1026},71,[48,1028,1029],{},"    b.decompose()\n",[48,1031,1033],{"class":50,"line":1032},72,[48,1034,109],{"emptyLinePlaceholder":108},[48,1036,1038],{"class":50,"line":1037},73,[48,1039,600],{},[48,1041,1043],{"class":50,"line":1042},74,[48,1044,1045],{},"    f.close()\n",[15,1047,1048],{},"Next, let's save the results into a csv file. This let's us load the results quickly without having to scrape everyting again. To do this we can use the pandas library.",[39,1050,1052],{"className":41,"code":1051,"language":43,"meta":44,"style":44},"import pandas as pd\ndf0 = pd.DataFrame(dict_list, index=None)\n",[30,1053,1054,1059],{"__ignoreMap":44},[48,1055,1056],{"class":50,"line":51},[48,1057,1058],{},"import pandas as pd\n",[48,1060,1061],{"class":50,"line":57},[48,1062,1063],{},"df0 = pd.DataFrame(dict_list, index=None)\n",[15,1065,1066,1067,1070],{},"At this point, we can go through the ",[30,1068,1069],{},"related"," column in the DataFrame and put together a list of all the related subreddits. With this list, we can simply repeat the process over and over again. However, each time we start with a new list of subreddits, we want to make sure that they have not already been collected.",[15,1072,1073],{},"Next I will read in one DataFrame that represents related subreddits \"three levels deep\" relative to the default subreddits.",[15,1075,1076],{},[1077,1078,1079],"strong",{},"Default --> Related --> Related --> Related",[15,1081,1082],{},"This DataFrame represents the collection of subreddits from all of these \"layers\" of the graph.",[39,1084,1086],{"className":41,"code":1085,"language":43,"meta":44,"style":44},"import pandas as pd\nmaster_df = pd.read_pickle('pickle/master_df.p')\n",[30,1087,1088,1092],{"__ignoreMap":44},[48,1089,1090],{"class":50,"line":51},[48,1091,1058],{},[48,1093,1094],{"class":50,"line":57},[48,1095,1096],{},"master_df = pd.read_pickle('pickle/master_df.p')\n",[15,1098,1099],{},"Now we can do a quick visualization of the growth in number of subreddits since the website's start in 2005.",[39,1101,1103],{"className":41,"code":1102,"language":43,"meta":44,"style":44},"import warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nmaster_df_ = master_df[master_df.notnull()]\nmaster_df_.date1 = pd.to_datetime(master_df_['date1'])\n\nlist_of_dates = master_df_.date1.sort_values()\n\ncounts = np.arange(0, len(list_of_dates))\n_ = plt.plot(list_of_dates, counts)\n_ = plt.title('Number of subreddits over time')\n_ = plt.xlabel('Date')\n_ = plt.ylabel('Cummulative Count')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/static/subreddit_graph/subreddits_count.png'))\n",[30,1104,1105,1110,1115,1120,1125,1130,1134,1138,1143,1148,1152,1157,1161,1166,1171,1176,1181,1186],{"__ignoreMap":44},[48,1106,1107],{"class":50,"line":51},[48,1108,1109],{},"import warnings\n",[48,1111,1112],{"class":50,"line":57},[48,1113,1114],{},"warnings.filterwarnings('ignore')\n",[48,1116,1117],{"class":50,"line":63},[48,1118,1119],{},"%matplotlib inline\n",[48,1121,1122],{"class":50,"line":69},[48,1123,1124],{},"import matplotlib.pyplot as plt\n",[48,1126,1127],{"class":50,"line":75},[48,1128,1129],{},"import seaborn as sns\n",[48,1131,1132],{"class":50,"line":112},[48,1133,72],{},[48,1135,1136],{"class":50,"line":118},[48,1137,109],{"emptyLinePlaceholder":108},[48,1139,1140],{"class":50,"line":124},[48,1141,1142],{},"master_df_ = master_df[master_df.notnull()]\n",[48,1144,1145],{"class":50,"line":130},[48,1146,1147],{},"master_df_.date1 = pd.to_datetime(master_df_['date1'])\n",[48,1149,1150],{"class":50,"line":136},[48,1151,109],{"emptyLinePlaceholder":108},[48,1153,1154],{"class":50,"line":141},[48,1155,1156],{},"list_of_dates = master_df_.date1.sort_values()\n",[48,1158,1159],{"class":50,"line":507},[48,1160,109],{"emptyLinePlaceholder":108},[48,1162,1163],{"class":50,"line":512},[48,1164,1165],{},"counts = np.arange(0, len(list_of_dates))\n",[48,1167,1168],{"class":50,"line":518},[48,1169,1170],{},"_ = plt.plot(list_of_dates, counts)\n",[48,1172,1173],{"class":50,"line":524},[48,1174,1175],{},"_ = plt.title('Number of subreddits over time')\n",[48,1177,1178],{"class":50,"line":529},[48,1179,1180],{},"_ = plt.xlabel('Date')\n",[48,1182,1183],{"class":50,"line":535},[48,1184,1185],{},"_ = plt.ylabel('Cummulative Count')\n",[48,1187,1188],{"class":50,"line":541},[48,1189,1190],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/static/subreddit_graph/subreddits_count.png'))\n",[15,1192,1193],{},[1194,1195],"img",{"alt":1196,"src":1197},"png","/static/subreddit_graph/subreddits_count.png",[10,1199,1201],{"id":1200},"setting-up-a-graph-with-networkx","Setting up a graph with NetworkX",[15,1203,1204,1205,1210],{},"Next we can start to look at the collection of reddits and related subreddits as a graph. I will be using a Python package for network and graph analysis called ",[19,1206,1209],{"href":1207,"rel":1208},"https://networkx.github.io",[161],"NetworkX",".",[39,1212,1214],{"className":41,"code":1213,"language":43,"meta":44,"style":44},"#Let's make sure that we have only unique entries in the dataframe.\nmaster_df_u = master_df_.drop_duplicates('subreddit')\n",[30,1215,1216,1221],{"__ignoreMap":44},[48,1217,1218],{"class":50,"line":51},[48,1219,1220],{},"#Let's make sure that we have only unique entries in the dataframe.\n",[48,1222,1223],{"class":50,"line":57},[48,1224,1225],{},"master_df_u = master_df_.drop_duplicates('subreddit')\n",[39,1227,1229],{"className":41,"code":1228,"language":43,"meta":44,"style":44},"master_df_u = master_df_u.drop(master_df_u.index[master_df_u.subreddit=='/r/track__subreddits_'])\n",[30,1230,1231],{"__ignoreMap":44},[48,1232,1233],{"class":50,"line":51},[48,1234,1228],{},[39,1236,1238],{"className":41,"code":1237,"language":43,"meta":44,"style":44},"#here we define a dictionary where the keys are subreddits and the values are lists of related subreddits\ngraph = {x:y for x, y in zip(master_df_u.subreddit, master_df_u.related)}\n",[30,1239,1240,1245],{"__ignoreMap":44},[48,1241,1242],{"class":50,"line":51},[48,1243,1244],{},"#here we define a dictionary where the keys are subreddits and the values are lists of related subreddits\n",[48,1246,1247],{"class":50,"line":57},[48,1248,1249],{},"graph = {x:y for x, y in zip(master_df_u.subreddit, master_df_u.related)}\n",[39,1251,1253],{"className":41,"code":1252,"language":43,"meta":44,"style":44},"#NetworkX comes with the python Anaconda distribution\nimport networkx as nx\n",[30,1254,1255,1260],{"__ignoreMap":44},[48,1256,1257],{"class":50,"line":51},[48,1258,1259],{},"#NetworkX comes with the python Anaconda distribution\n",[48,1261,1262],{"class":50,"line":57},[48,1263,1264],{},"import networkx as nx\n",[39,1266,1268],{"className":41,"code":1267,"language":43,"meta":44,"style":44},"G=nx.Graph()\nG=nx.from_dict_of_lists(graph)\n#making the graph undirected takes all of the vertices between nodes and makes them bi-directional\nG1 = G.to_undirected()\n",[30,1269,1270,1275,1280,1285],{"__ignoreMap":44},[48,1271,1272],{"class":50,"line":51},[48,1273,1274],{},"G=nx.Graph()\n",[48,1276,1277],{"class":50,"line":57},[48,1278,1279],{},"G=nx.from_dict_of_lists(graph)\n",[48,1281,1282],{"class":50,"line":63},[48,1283,1284],{},"#making the graph undirected takes all of the vertices between nodes and makes them bi-directional\n",[48,1286,1287],{"class":50,"line":69},[48,1288,1289],{},"G1 = G.to_undirected()\n",[39,1291,1293],{"className":41,"code":1292,"language":43,"meta":44,"style":44},"choice = np.random.choice(master_df_u.subreddit, 2)\nprint choice\n",[30,1294,1295,1300],{"__ignoreMap":44},[48,1296,1297],{"class":50,"line":51},[48,1298,1299],{},"choice = np.random.choice(master_df_u.subreddit, 2)\n",[48,1301,1302],{"class":50,"line":57},[48,1303,1304],{},"print choice\n",[39,1306,1309],{"className":1307,"code":1308,"language":429},[427],"['/r/streetboarding' '/r/stephenking']\n",[30,1310,1308],{"__ignoreMap":44},[15,1312,1313],{},"Let's test out some of the functions from NetworkX for graph analysis. First, let's take the two randomly selected nodes defined above and test to see if there exists a path between them:",[39,1315,1317],{"className":41,"code":1316,"language":43,"meta":44,"style":44},"nx.has_path(G1, choice[0], choice[1])\n",[30,1318,1319],{"__ignoreMap":44},[48,1320,1321],{"class":50,"line":51},[48,1322,1316],{},[39,1324,1327],{"className":1325,"code":1326,"language":429},[427],"True\n",[30,1328,1326],{"__ignoreMap":44},[10,1330,1332],{"id":1331},"shortest-path","Shortest path",[15,1334,1335],{},"Now let's see (at least one of) the shortest path that exists between these nodes:",[39,1337,1339],{"className":41,"code":1338,"language":43,"meta":44,"style":44},"nx.shortest_path(G1, choice[0], choice[1])\n",[30,1340,1341],{"__ignoreMap":44},[48,1342,1343],{"class":50,"line":51},[48,1344,1338],{},[39,1346,1349],{"className":1347,"code":1348,"language":429},[427],"['/r/streetboarding',\n '/r/freebord',\n '/r/adrenaline',\n '/r/imaginaryadrenaline',\n '/r/imaginarystephenking',\n '/r/stephenking']\n",[30,1350,1348],{"__ignoreMap":44},[15,1352,1353],{},"Let's write a function that selects two random subreddits and then prints a shortest path if it exists:",[39,1355,1357],{"className":41,"code":1356,"language":43,"meta":44,"style":44},"def short_path():\n    choices = np.random.choice(master_df_u.subreddit, 2)\n    if nx.has_path(G1, choices[0], choices[1]) == True:\n        path = nx.shortest_path(G1, choices[0], choices[1])\n        print choices[0] + ' and ' + choices[1] + ' are joined by: \\n' + str(path)\n    else:\n        print \"No path exists between \" + choices[0] + ' and ' + choices[1]\n",[30,1358,1359,1364,1369,1374,1379,1384,1388],{"__ignoreMap":44},[48,1360,1361],{"class":50,"line":51},[48,1362,1363],{},"def short_path():\n",[48,1365,1366],{"class":50,"line":57},[48,1367,1368],{},"    choices = np.random.choice(master_df_u.subreddit, 2)\n",[48,1370,1371],{"class":50,"line":63},[48,1372,1373],{},"    if nx.has_path(G1, choices[0], choices[1]) == True:\n",[48,1375,1376],{"class":50,"line":69},[48,1377,1378],{},"        path = nx.shortest_path(G1, choices[0], choices[1])\n",[48,1380,1381],{"class":50,"line":75},[48,1382,1383],{},"        print choices[0] + ' and ' + choices[1] + ' are joined by: \\n' + str(path)\n",[48,1385,1386],{"class":50,"line":112},[48,1387,821],{},[48,1389,1390],{"class":50,"line":118},[48,1391,1392],{},"        print \"No path exists between \" + choices[0] + ' and ' + choices[1]\n",[15,1394,1395,1396,1399],{},"Here's a collection of results from the ",[30,1397,1398],{},"short_path"," function defined above that start to paint a picuture of the broad set of topics covered by reddit.com:",[39,1401,1403],{"className":41,"code":1402,"language":43,"meta":44,"style":44},"short_path()\n",[30,1404,1405],{"__ignoreMap":44},[48,1406,1407],{"class":50,"line":51},[48,1408,1402],{},[39,1410,1413],{"className":1411,"code":1412,"language":429},[427],"/r/personalizationadvice and /r/beautifulfemales are joined by:\n['/r/personalizationadvice', '/r/coloranalysis', '/r/fashion', '/r/redcarpet', '/r/gentlemanboners', '/r/beautifulfemales']\n",[30,1414,1412],{"__ignoreMap":44},[39,1416,1417],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1418,1419],{"__ignoreMap":44},[48,1420,1421],{"class":50,"line":51},[48,1422,1402],{},[39,1424,1427],{"className":1425,"code":1426,"language":429},[427],"/r/caffeine and /r/shittyramen are joined by:\n['/r/caffeine', '/r/toast', '/r/cooking', '/r/ramen', '/r/shittyramen']\n",[30,1428,1426],{"__ignoreMap":44},[39,1430,1431],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1432,1433],{"__ignoreMap":44},[48,1434,1435],{"class":50,"line":51},[48,1436,1402],{},[39,1438,1441],{"className":1439,"code":1440,"language":429},[427],"/r/watchingcongress and /r/iwantthatonashirt are joined by:\n['/r/watchingcongress', '/r/stand', '/r/snowden', '/r/undelete', '/r/trees', '/r/iwantthatonashirt']\n",[30,1442,1440],{"__ignoreMap":44},[39,1444,1445],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1446,1447],{"__ignoreMap":44},[48,1448,1449],{"class":50,"line":51},[48,1450,1402],{},[39,1452,1455],{"className":1453,"code":1454,"language":429},[427],"/r/asksciencediscussion and /r/dogsonhardwoodfloors are joined by:\n['/r/asksciencediscussion', '/r/badscience', '/r/badlinguistics', '/r/animalsbeingjerks', '/r/startledcats', '/r/dogsonhardwoodfloors']\n",[30,1456,1454],{"__ignoreMap":44},[39,1458,1459],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1460,1461],{"__ignoreMap":44},[48,1462,1463],{"class":50,"line":51},[48,1464,1402],{},[39,1466,1469],{"className":1467,"code":1468,"language":429},[427],"/r/randommail and /r/mini are joined by:\n['/r/randommail', '/r/spiceexchange', '/r/cameraswapping', '/r/itookapicture', '/r/carporn', '/r/mini']\n",[30,1470,1468],{"__ignoreMap":44},[39,1472,1473],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1474,1475],{"__ignoreMap":44},[48,1476,1477],{"class":50,"line":51},[48,1478,1402],{},[39,1480,1483],{"className":1481,"code":1482,"language":429},[427],"/r/catsinsinks and /r/nzmovies are joined by:\n['/r/catsinsinks', '/r/wetcats', '/r/tinysubredditoftheday', '/r/sheep', '/r/nzmetahub', '/r/nzmovies']\n",[30,1484,1482],{"__ignoreMap":44},[39,1486,1487],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1488,1489],{"__ignoreMap":44},[48,1490,1491],{"class":50,"line":51},[48,1492,1402],{},[39,1494,1497],{"className":1495,"code":1496,"language":429},[427],"/r/thoriumreactor and /r/sailing are joined by:\n['/r/thoriumreactor', '/r/energy', '/r/spev', '/r/sailing']\n",[30,1498,1496],{"__ignoreMap":44},[39,1500,1501],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1502,1503],{"__ignoreMap":44},[48,1504,1505],{"class":50,"line":51},[48,1506,1402],{},[39,1508,1511],{"className":1509,"code":1510,"language":429},[427],"/r/deathnote and /r/vegetarianism are joined by:\n['/r/deathnote', '/r/television', '/r/netflixbestof', '/r/naturefilms', '/r/environment', '/r/vegetarianism']\n",[30,1512,1510],{"__ignoreMap":44},[39,1514,1515],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1516,1517],{"__ignoreMap":44},[48,1518,1519],{"class":50,"line":51},[48,1520,1402],{},[39,1522,1525],{"className":1523,"code":1524,"language":429},[427],"/r/mississippir4r and /r/mathematics are joined by:\n['/r/mississippir4r', '/r/mississippi', '/r/prisonreform', '/r/socialscience', '/r/alltech', '/r/mathematics']\n",[30,1526,1524],{"__ignoreMap":44},[39,1528,1529],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1530,1531],{"__ignoreMap":44},[48,1532,1533],{"class":50,"line":51},[48,1534,1402],{},[39,1536,1539],{"className":1537,"code":1538,"language":429},[427],"/r/britainsgottalent and /r/irelandbaldwin are joined by:\n['/r/britainsgottalent', '/r/britishtv', '/r/that70sshow', '/r/mila_kunis', '/r/christinaricci', '/r/irelandbaldwin']\n",[30,1540,1538],{"__ignoreMap":44},[39,1542,1543],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1544,1545],{"__ignoreMap":44},[48,1546,1547],{"class":50,"line":51},[48,1548,1402],{},[39,1550,1553],{"className":1551,"code":1552,"language":429},[427],"/r/the_donald and /r/ladybusiness are joined by:\n['/r/the_donald', '/r/shitliberalssay', '/r/trollxchromosomes', '/r/ladybusiness']\n",[30,1554,1552],{"__ignoreMap":44},[39,1556,1557],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1558,1559],{"__ignoreMap":44},[48,1560,1561],{"class":50,"line":51},[48,1562,1402],{},[39,1564,1567],{"className":1565,"code":1566,"language":429},[427],"/r/selfharm and /r/medlabprofessionals are joined by:\n['/r/selfharm', '/r/adhd', '/r/neuroimaging', '/r/pharmacy', '/r/medlabprofessionals']\n",[30,1568,1566],{"__ignoreMap":44},[39,1570,1571],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1572,1573],{"__ignoreMap":44},[48,1574,1575],{"class":50,"line":51},[48,1576,1402],{},[39,1578,1581],{"className":1579,"code":1580,"language":429},[427],"/r/coverart and /r/phillycraftbeer are joined by:\n['/r/coverart', '/r/nostalgia', '/r/upvotedbecausegirl', '/r/wtf', '/r/remindsmeofdf', '/r/beer', '/r/phillycraftbeer']\n",[30,1582,1580],{"__ignoreMap":44},[39,1584,1585],{"className":41,"code":1402,"language":43,"meta":44,"style":44},[30,1586,1587],{"__ignoreMap":44},[48,1588,1589],{"class":50,"line":51},[48,1590,1402],{},[39,1592,1595],{"className":1593,"code":1594,"language":429},[427],"/r/hotguyswithlonghair and /r/castles are joined by:\n['/r/hotguyswithlonghair', '/r/majesticmanes', '/r/ladyboners', '/r/imaginaryladyboners', '/r/imaginarycastles', '/r/castles']\n",[30,1596,1594],{"__ignoreMap":44},[15,1598,1599,1600,1605,1606,1611],{},"Taking a look ",[19,1601,1604],{"href":1602,"rel":1603},"http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/unweighted.html?highlight=bidirectional_shortest_path",[161],"under the hood"," of NetworkX and examining the algorith that finds the ",[19,1607,1610],{"href":1608,"rel":1609},"http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/generic.html#shortest_path",[161],"shortest path"," between any two nodes in a graph, we find that it simply boils down to:",[39,1613,1616],{"className":1614,"code":1615,"language":429},[427],"def shortest_path(G, source=None, target=None, weight=None):\n    paths=nx.bidirectional_shortest_path(G,source,target)\n    return paths\n",[30,1617,1615],{"__ignoreMap":44},[15,1619,1620,1621,1624,1625,1629],{},"You can read more about the ",[30,1622,1623],{},"bidirectional_shortest_path"," function ",[19,1626,1628],{"href":1602,"rel":1627},[161],"here"," in the NetworkX documentation.",[15,1631,1632,1633,1636],{},"When I was first experimenting with graph algorithms, I had an interesting result using an algorithm intruduced ",[19,1634,1628],{"href":416,"rel":1635},[161]," in the Python documentation. Here's the algorithm:",[39,1638,1640],{"className":41,"code":1639,"language":43,"meta":44,"style":44},"def find_path(graph, start, end, path=[]):\n    path = path + [start]\n    if start == end:\n        return path\n    if not graph.has_key(start):\n        return None\n    for node in graph[start]:\n        if node not in path:\n            newpath = find_path(graph, node, end, path)\n            if newpath: return newpath\n    return None\n",[30,1641,1642,1647,1652,1657,1662,1667,1672,1677,1682,1687,1692],{"__ignoreMap":44},[48,1643,1644],{"class":50,"line":51},[48,1645,1646],{},"def find_path(graph, start, end, path=[]):\n",[48,1648,1649],{"class":50,"line":57},[48,1650,1651],{},"    path = path + [start]\n",[48,1653,1654],{"class":50,"line":63},[48,1655,1656],{},"    if start == end:\n",[48,1658,1659],{"class":50,"line":69},[48,1660,1661],{},"        return path\n",[48,1663,1664],{"class":50,"line":75},[48,1665,1666],{},"    if not graph.has_key(start):\n",[48,1668,1669],{"class":50,"line":112},[48,1670,1671],{},"        return None\n",[48,1673,1674],{"class":50,"line":118},[48,1675,1676],{},"    for node in graph[start]:\n",[48,1678,1679],{"class":50,"line":124},[48,1680,1681],{},"        if node not in path:\n",[48,1683,1684],{"class":50,"line":130},[48,1685,1686],{},"            newpath = find_path(graph, node, end, path)\n",[48,1688,1689],{"class":50,"line":136},[48,1690,1691],{},"            if newpath: return newpath\n",[48,1693,1694],{"class":50,"line":141},[48,1695,1696],{},"    return None\n",[15,1698,1699],{},"The above algorthim uses a process called backtracking to exaustively try all possibilities until it returns a solution. It creates an interesting \"random walk\" through groups of related subreddits. Here's the result of calling the above function on our graph (only 2 layers deep) with two random nodes: /r/persianrap and /r/nosleep:",[153,1701,1702],{},[15,1703,1704],{},"/r/persianrap /r/middleeasternmusic /r/arabic /r/arabs /r/libyancrisis /r/syriancivilwar /r/yemenicrisis /r/sinaiinsurgency /r/jihadinfocus /r/credibledefense /r/geopolitics /r/forgottennews /r/libyanconflict /r/menaconflicts /r/iran /r/iranianlgbt /r/zoroastrianism /r/kurdistan /r/rojava /r/anarchism /r/imaginarypolitics /r/imaginaryimmortals /r/imaginaryclerics /r/imaginarylakes /r/imaginaryaliens /r/imaginarygnomes /r/imaginaryladyboners /r/imaginaryturtleworlds /r/imaginarysunnydale /r/imaginarydwarves /r/imaginarywizards /r/imaginaryvikings /r/imaginarycolorscapes /r/imaginarysteampunk /r/imaginarytemples /r/imaginaryblueprints /r/comicbookart /r/imaginarytechnology /r/mtgporn /r/imaginaryoldkingdom /r/imaginaryfactories /r/imaginaryfederation /r/imaginarylovers /r/imaginarynarnia /r/imaginarydwellings /r/imaginaryscience /r/imaginarytaverns /r/imaginarybattlefields /r/cityporn /r/japanpics /r/nationalphotosubs /r/austriapics /r/southkoreapics /r/taiwanpics /r/ghanapics /r/kenyapics /r/norwaypics /r/vzlapics /r/perupics /r/antarcticapics /r/greatlakespics /r/lakeporn /r/pornoverlords /r/thingscutinhalfporn /r/manufacturing /r/cnc /r/askengineers /r/sciencesubreddits /r/math /r/simulate /r/cosmology /r/reddittothefuture /r/scifi /r/lost /r/the100books /r/the100 /r/theblacklist /r/nbc /r/dundermifflin /r/sonsofanarchy /r/twentyfour /r/banshee /r/hbo /r/siliconvalleyhbo /r/siliconvalley /r/california /r/tahoe /r/skiing /r/snowshoeing /r/xcountryskiing /r/wintergear /r/skijumping /r/winter /r/bigmountain /r/mountaineering /r/campingandhiking /r/earthporn /r/nature /r/birding /r/invasivespecies /r/zoology /r/entomology /r/rainforest /r/botany /r/wildlife /r/allscience /r/earthscience /r/energy /r/biomass /r/renewablenews /r/syngas /r/climatenews /r/composting /r/vermiculture /r/organicfarming /r/livestock /r/animalwelfare /r/randomactsofpetfood /r/animalreddits /r/cockatiel /r/catpics /r/tortoises /r/whales /r/cetacea /r/lifeaquatic /r/hrw /r/green_peace /r/environmental_policy /r/conservation /r/depthhub /r/indepthsports /r/deeperhubbeta /r/lectures /r/spacepolicy /r/skylon /r/ula /r/isro /r/engineteststands /r/jupiters /r/imaginarystarscapes /r/spacequestions /r/spaceflight /r/moon /r/dione /r/europa /r/oortcloud /r/dwarfplanetceres /r/saturn /r/asteroidbelt /r/mars /r/rhea /r/venus /r/astrophys /r/spacevideos /r/transhuman /r/timereddits /r/virtualreality /r/vive /r/oculus /r/learnvrdev /r/unity3d /r/gamedev /r/crowdfunding /r/crowdsourcing /r/mturk /r/swagbucks /r/beermoney /r/flipping /r/shoplifting /r/thriftstorehauls /r/dvdcollection /r/televisionposterporn /r/concertposterporn /r/movieposterporn /r/lv426 /r/predator /r/arnoldschwarzenegger /r/alanpartridge /r/americandad /r/timanderic /r/homemovies /r/gravityfalls /r/homestarrunner /r/telltale /r/thewalkingdeadgame /r/thewalkingdeadgifs /r/twdnomansland /r/heycarl /r/twdroadtosurvival /r/thewalkingdead /r/zombies /r/guns /r/swissguns /r/opencarry /r/libertarian /r/geolibertarianism /r/basicincome /r/basicincomeactivism /r/mhoc /r/modelaustralia /r/rmtk /r/thenetherlands /r/tokkiefeesboek /r/nujijinactie /r/ik_ihe /r/youirl /r/fite_me_irl /r/2meirl4meirl /r/depression /r/randomactsofcards /r/philately /r/coins /r/coins4sale /r/ancientcoins /r/ancientrome /r/flatblue /r/bestofwritingprompts /r/writingprompts /r/promptoftheday /r/flashfiction /r/keepwriting /r/getmotivated /r/mentors /r/favors /r/recordthis /r/videography /r/animation /r/3dsmax /r/computergraphics /r/cinema4d /r/design /r/ui_design /r/designjobs /r/heavymind /r/wtfart /r/alternativeart /r/imaginaryninjas /r/imaginaryruins /r/isometric /r/imaginaryislands /r/imaginaryverse /r/icandrawthat /r/caricatures /r/imaginaryneweden /r/imaginaryequestria /r/imaginaryaww /r/imaginarycyberpunk /r/chinafuturism /r/scifirealism /r/inegentlemanboners /r/imaginarywtf /r/imaginaryelementals /r/imaginarydinosaurs /r/dinosaurs /r/speculativeevolution /r/hybridanimals /r/photoshopbattles /r/cutouts /r/battleshops /r/graphic_design /r/visualization /r/statistics /r/oncourtanalytics /r/nbaanalytics /r/nba /r/pacers /r/atlantahawks /r/basketball /r/mavericks /r/fcdallas /r/theticket /r/dallasstars /r/bostonbruins /r/patriots /r/tennesseetitans /r/nashvillesounds /r/predators /r/flyers /r/hockeyfandom /r/caps /r/nhl /r/detroitredwings /r/sabres /r/floridapanthers /r/habs /r/montrealimpact /r/alouettes /r/cfl /r/stadiumporn /r/nfl /r/madden /r/eurobowl /r/fantasyfb /r/fantasyfootball /r/49ers /r/footballgamefilm /r/footballstrategy /r/cfb /r/collegebaseball /r/mlbdraft /r/baseball /r/cubs /r/cardinals /r/saintlouisfc /r/stlouisblues /r/stlouis /r/stlouisbiking /r/mobicycling /r/bicycling /r/vintage_bicycles /r/miamibiking /r/fatbike /r/cycling /r/strava /r/phillycycling /r/wheelbuild /r/bikewrench /r/velo /r/bikepolo /r/bicycletouring /r/bicyclingcirclejerk /r/bikecommuting /r/ukbike /r/leedscycling /r/londoncycling /r/fixedgearbicycle /r/cyclingfashion /r/peloton /r/mtb /r/climbingporn /r/adrenaline /r/motocross /r/bmxracing /r/wake /r/snowboardingnoobs /r/freebord /r/snowboarding /r/sledding /r/outdoors /r/soposts /r/cordcutters /r/netflixviavpn /r/hulu /r/firetv /r/netflixbestof /r/raisinghope /r/madmen /r/earthsgottalent /r/bobsburgers /r/fringe /r/louie /r/theoriginals /r/iansomerhalder /r/kat_graham /r/indianaevans /r/janelevy /r/gagegolightly /r/sarahhyland /r/starlets /r/ninadobrev /r/kathrynnewton /r/arielwinter /r/ashleygreene /r/gentlemanboners /r/bandporn /r/musicpics /r/listentomusic /r/listentonew /r/subraddits /r/dtipics /r/damnthatsinteresting /r/interestingasfuck /r/unexpected /r/wtf /r/weird /r/animalsbeingderps /r/animalsbeingconfused /r/humansbeingbros /r/hulpdiensten /r/askle /r/protectandserve /r/good_cop_free_donut /r/bad_cop_follow_up /r/amifreetogo /r/copwatch /r/puppycide /r/underreportednews /r/mediaquotes /r/savedyouaclick /r/news /r/neutralnews /r/ask_politics /r/politicalopinions /r/gunsarecool /r/renewableenergy /r/web_design /r/somebodymakethis /r/somethingimade /r/crafts /r/kidscrafts /r/daddit /r/formulafeeders /r/boobsandbottles /r/csectioncentral /r/predaddit /r/dadbloggers /r/mombloggers /r/cutekids /r/bigfeats /r/scienceparents /r/lv9hrvv /r/sahp /r/tryingforababy /r/waiting_to_try /r/pcos /r/infertility /r/birthparents /r/tfabchartstalkers /r/firsttimettc /r/cautiousbtb /r/ttchealthy /r/xxketo /r/ketoscience /r/ketogains /r/leangains /r/gettingshredded /r/bulkorcut /r/gainit /r/decidingtobebetter /r/zen /r/buddhism /r/astralprojection /r/spirituality /r/hinduism /r/yoga /r/veganfitness /r/posture /r/health /r/ukhealthcare /r/pharmacy /r/nursing /r/doctorswithoutborders /r/humanitarian /r/assistance /r/paranormalhelp /r/paranormal /r/333 /r/askparanormal /r/intelligence /r/blackhat /r/netsec /r/technology /r/newyorkfuturistparty /r/rad_decentralization /r/massachusettsfp /r/opensource /r/alabamafp /r/darknetplan /r/torrents /r/i2p /r/privacy /r/badgovnofreedom /r/censorship /r/governmentoppression /r/descentintotyranny /r/wikileaks /r/dncleaks /r/hillaryforprison /r/the_donald /r/shitredditsays /r/srsmythos /r/srstrees /r/entwives /r/lesbients /r/actuallesbians /r/lesbianromance /r/lesbianerotica /r/l4l /r/dyke /r/ladyladyboners /r/bisexual /r/bisexy /r/biwomen /r/pansexual /r/genderqueer /r/transspace /r/lgbtlibrary /r/lgbtnews /r/dixiequeer /r/lgbt /r/sex /r/helpmecope /r/bpd /r/rapecounseling /r/trueoffmychest /r/suicidewatch /r/bipolarsos /r/bipolar /r/mentalpod /r/adhd /r/hoarding /r/declutter /r/thrifty /r/tinyhouses /r/leanfire /r/lowcar /r/zerowaste /r/simpleliving /r/livingofftheland /r/hunting /r/animaltracking /r/survival /r/vedc /r/4x4 /r/classiccars /r/automotivetraining /r/autodetailing /r/cartalk /r/mercedes_benz /r/motorsports /r/rallycross /r/worldrallycross /r/blancpain /r/nascarhometracks /r/arcaracing /r/stadiumsupertrucks /r/hydroplanes /r/sailing /r/boatbuilding /r/woodworking /r/cottage_industry /r/farriers /r/blacksmith /r/bladesmith /r/knives /r/swissarmyknives /r/switzerland /r/bern /r/sanktgallen /r/liechtenstein /r/erasmus /r/de /r/germanpuns /r/schland /r/rvacka /r/sloensko /r/slovakia /r/belarus /r/andorra /r/europe /r/hungary /r/francophonie /r/thailand /r/vietnam /r/vietnampics /r/travel /r/geography /r/climate /r/drought /r/waterutilities /r/drylands /r/irrigation /r/water /r/onthewaterfront /r/wetlands /r/marinelife /r/ocean /r/seasteading /r/frontier_colonization /r/arcology /r/retrofuturism /r/goldenpath /r/politics /r/moderationtheory /r/wdp /r/outoftheloop /r/wherearetheynow /r/entertainment /r/portlandia /r/themichaeljfoxshow /r/backtothefuture /r/bladerunner /r/filmnoir /r/vintageladyboners /r/classicfilms /r/foreignmovies /r/britishfilms /r/canadianfilm /r/newjerseyfilm /r/newzealandfilm /r/newzealand /r/wellington /r/nzmetahub /r/newzealandhistory /r/scottishhistory /r/scots /r/scottishproblems /r/britishproblems /r/swedishproblems /r/pinsamt /r/sweden /r/svenskpolitik /r/arbetarrorelsen /r/socialism /r/shittydebatecommunism /r/shittysocialscience /r/shittyideasforadmins /r/shittytheoryofreddit /r/shittybuildingporn /r/shittylifeprotips /r/shittyshitredditsays /r/shittyquotesporn /r/shittyama /r/askashittyparent /r/shittyprogramming /r/shittyaskalawyer /r/badlegaladvice /r/badscience /r/badeconomics /r/badhistory /r/historicalrage /r/metarage /r/ragenovels /r/fffffffuuuuuuuuuuuu /r/gaaaaaaayyyyyyyyyyyy /r/lgbteens /r/needafriend /r/rant /r/showerthoughts /r/markmywords /r/calledit /r/futurewhatif /r/sportswhatif /r/alternatehistory /r/maps /r/xkcd /r/kerbalspaceprogram /r/spacesimgames /r/eve /r/scifigaming /r/masseffect /r/imaginarymasseffect /r/imaginaryvampires /r/imaginarytowers /r/imaginarybestof /r/pics /r/spaceporn /r/auroraporn /r/weatherporn /r/sfwpornnetwork /r/fwepp /r/shittyearthporn /r/shittyaskreddit /r/askashittyphilosopher /r/shittyaskhistory /r/shittysuboftheweek /r/shittyaskcooking /r/shittyhub /r/coolguides /r/trendingsubreddits /r/monkslookingatbeer /r/beerporn /r/beerwithaview /r/shittybeerwithaview /r/shittyfoodporn /r/enttreats /r/trees /r/eldertrees /r/vaporents /r/crainn /r/eirhub /r/fairepublicofireland /r/gaeltacht /r/westmeath /r/tipperary /r/limerick /r/kilkenny /r/ireland /r/irejobs /r/resumes /r/careerguidance /r/flatone /r/centralillinois /r/chicubs /r/whitesox /r/minnesotatwins /r/minnesotavikings /r/greenbaypackers /r/jaguars /r/miamidolphins /r/nflroundtable /r/detroitlions /r/forhonor /r/vikingstv /r/hannibaltv /r/thepathhulu /r/batesmotel /r/hannibal /r/hitchcock /r/silentmoviegifs /r/moviestunts /r/bollywoodrealism /r/indiamain /r/indianews /r/asia /r/oldindia /r/explorepakistan /r/churchporn /r/medievalporn /r/castles /r/historyporn /r/thewaywewere /r/1970s /r/classicmovietrailers /r/warmovies /r/moviecritic /r/trailers /r/liveaction /r/animedeals /r/dbz /r/toonami /r/regularshow /r/thelifeandtimesoftim /r/aquajail /r/modern_family /r/supernatural /r/mishacollins /r/jaredpadalecki /r/fandomnatural /r/fangirls /r/trollxgirlgamers /r/trollmedia /r/trollgaming /r/trollmua /r/justtrollxthings /r/trollxmoms /r/trollmeta /r/trollychromosome /r/oney /r/askwomen /r/okcupid /r/relationship_advice /r/help /r/bugs /r/redditdev /r/enhancement /r/yoursub /r/horrorreviewed /r/truecreepy /r/metatruereddit /r/truepolitics /r/truehub /r/truegaming /r/askgames /r/freegamesonandroid /r/androidapps /r/apphookup /r/browsemyreddit /r/findareddit /r/trap /r/naut /r/militaryfinance /r/army /r/militarystories /r/nationalguard /r/uscg /r/usa /r/murica /r/lonestar /r/whataburger /r/fastfood /r/cocacola /r/kelloggs /r/kellawwggs /r/awwducational /r/marinebiologygifs /r/biologygifs /r/chemicalreactiongifs /r/homechemistry /r/holdmybeaker /r/holdmybeer /r/movieoftheday /r/sharknado /r/syfy /r/killjoys /r/theexpanse /r/truedetective /r/boardwalkempire /r/mobcast /r/1920s /r/1960s /r/beatles /r/minimaluminiumalism /r/ghostsrights /r/botsrights /r/totallynotrobots /r/robotics /r/manna /r/singularity /r/futureporn /r/singularitarianism /r/automate /r/darkfuturology /r/controlproblem /r/aiethics /r/ainothuman /r/neuraljokes /r/3amjokes /r/mommajokes /r/antijokes /r/absolutelynotme_irl /r/toomeirlformeirl /r/meirl /r/tree_irl /r/fishpost /r/mod_irl /r/pics_irl /r/teleshits /r/bitstrips /r/stopbullyingcomics /r/animalsbeingjerks /r/surfinganimals /r/unorthocat /r/catsubs /r/stuffoncats /r/catsinbusinessattire /r/catsinsinks /r/catsonkeyboards /r/mechanicalkeyboards /r/hackedgadgets /r/techsupportmacgyver /r/techsupport /r/programming /r/algorithms /r/datamining /r/datasets /r/wordcloud /r/datavizrequests /r/funnycharts /r/mapporn /r/mapmaking /r/worldbuilding /r/scificoncepts /r/apocalypseporn /r/imaginaryjerk /r/braveryjerk /r/circlejerk /r/politicaldiscussion /r/politicalfactchecking /r/moderatepolitics /r/truereddit /r/malelifestyle /r/fitness /r/swimming /r/freediving /r/bikeshop /r/climbing /r/climbharder /r/bouldering /r/climbergirls /r/womenshredders /r/skatergirls /r/girlsurfers /r/kiteboarding /r/longboarding /r/streetboarding /r/letsgosnowboarding /r/spliddit /r/backcountry /r/wjdbbl2 /r/caving /r/nationalparks /r/parkrangers /r/thesca /r/searchandrescue /r/wildernessbackpacking /r/campinggear /r/flashlight /r/camping /r/yellowstone /r/wmnf /r/pacificcresttrail /r/cdt /r/ultralight /r/backpacking /r/travelpartners /r/adventures /r/libraryofshadows /r/shortscarystories /r/shortscarystoriesooc /r/nosleepooc /r/nosleep",[10,1706,1708],{"id":1707},"centrality","Centrality",[15,1710,1711,1712,151],{},"Centrality is anohter important topic in graph theory. Here's a brief introduction to centrality from ",[19,1713,1716],{"href":1714,"rel":1715},"https://en.wikipedia.org/wiki/Centrality",[161],"Wikipedia",[153,1718,1719],{},[15,1720,1721],{},"In graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease.",[15,1723,1724,1725,1728],{},"There are several different methods of measuring centrality in a graph. Here I use ",[30,1726,1727],{},"eigenvector_centrality_numpy",", a function included in NetworkX. It takes in a graph and returns a dictionary with graph nodes as keys and node centrality as values.",[39,1730,1732],{"className":41,"code":1731,"language":43,"meta":44,"style":44},"centrality = nx.eigenvector_centrality_numpy(G1)\n",[30,1733,1734],{"__ignoreMap":44},[48,1735,1736],{"class":50,"line":51},[48,1737,1731],{},[15,1739,1740],{},"Let's see which subreddit has the highest centrality:",[39,1742,1744],{"className":41,"code":1743,"language":43,"meta":44,"style":44},"print max(centrality, key=centrality.get), centrality[max(centrality, key=centrality.get)]\n",[30,1745,1746],{"__ignoreMap":44},[48,1747,1748],{"class":50,"line":51},[48,1749,1743],{},[39,1751,1754],{"className":1752,"code":1753,"language":429},[427],"/r/imaginarybattlefields 0.0721530261127\n",[30,1755,1753],{"__ignoreMap":44},[39,1757,1759],{"className":41,"code":1758,"language":43,"meta":44,"style":44},"len(centrality) == len(sorted(centrality.values(), reverse=True))\n",[30,1760,1761],{"__ignoreMap":44},[48,1762,1763],{"class":50,"line":51},[48,1764,1758],{},[39,1766,1768],{"className":1767,"code":1326,"language":429},[427],[30,1769,1326],{"__ignoreMap":44},[15,1771,1772],{},"Since all of the centrality values are unique, we can look up nodes by their centrality values.",[39,1774,1776],{"className":41,"code":1775,"language":43,"meta":44,"style":44},"subr_list = []\nfor node in centrality:\n    subr_list.append((node, centrality[node]))\n\nsorted_subr_list = subr_list.sort(key=lambda x: x[1])\n",[30,1777,1778,1783,1788,1793,1797],{"__ignoreMap":44},[48,1779,1780],{"class":50,"line":51},[48,1781,1782],{},"subr_list = []\n",[48,1784,1785],{"class":50,"line":57},[48,1786,1787],{},"for node in centrality:\n",[48,1789,1790],{"class":50,"line":63},[48,1791,1792],{},"    subr_list.append((node, centrality[node]))\n",[48,1794,1795],{"class":50,"line":69},[48,1796,109],{"emptyLinePlaceholder":108},[48,1798,1799],{"class":50,"line":75},[48,1800,1801],{},"sorted_subr_list = subr_list.sort(key=lambda x: x[1])\n",[39,1803,1805],{"className":41,"code":1804,"language":43,"meta":44,"style":44},"for x in sorted(subr_list, key=lambda x: x[1], reverse=True)[:200]: print x[0],\n",[30,1806,1807],{"__ignoreMap":44},[48,1808,1809],{"class":50,"line":51},[48,1810,1804],{},[39,1812,1815],{"className":1813,"code":1814,"language":429},[427],"/r/imaginarybattlefields /r/imaginarycityscapes /r/imaginarywastelands /r/imaginarywildlands /r/imaginaryleviathans /r/imaginarydragons /r/imaginarystarscapes /r/imaginarywesteros /r/imaginaryartifacts /r/imaginaryangels /r/imaginarymaps /r/imaginarybehemoths /r/imaginarydemons /r/imaginaryelves /r/imaginarycentaurs /r/imaginaryfuturewar /r/imaginarysoldiers /r/imaginaryhistory /r/imaginaryarmor /r/imaginarystarships /r/imaginarynetwork /r/imaginaryjedi /r/imaginarydinosaurs /r/imaginarysteampunk /r/imaginarycyberpunk /r/imaginaryarchers /r/imaginaryvehicles /r/imaginaryanime /r/imaginaryfallout /r/imaginaryastronauts /r/imaginarymusic /r/imaginaryfactories /r/imaginaryequestria /r/imaginarywarships /r/imaginaryazeroth /r/imaginaryarrakis /r/imaginarydisney /r/imaginarypolitics /r/imaginaryhorrors /r/imaginarywinterscapes /r/imaginaryseascapes /r/imaginarypirates /r/imaginarywarriors /r/imaginarymiddleearth /r/imaginarygallifrey /r/imaginarymechs /r/imaginarypropaganda /r/imaginarymerfolk /r/imaginaryvikings /r/imaginaryundead /r/imaginarybeasts /r/imaginarymutants /r/imaginaryruins /r/imaginarytamriel /r/imaginaryforests /r/imaginaryelementals /r/imaginaryskyscapes /r/imaginarymonuments /r/imaginarywaterfalls /r/imaginaryworlds /r/imaginarywizards /r/imaginaryinteriors /r/imaginaryhogwarts /r/imaginarytowers /r/imaginaryarchitecture /r/imaginaryweaponry /r/imaginarygaming /r/imaginarycastles /r/imaginaryrobotics /r/imaginarybooks /r/imaginarygnomes /r/imaginaryvillages /r/imaginarydeserts /r/imaginarywerewolves /r/imaginarydieselpunk /r/imaginaryvampires /r/imaginaryadrenaline /r/imaginarykanto /r/imaginarynatives /r/imaginaryrivers /r/imaginarytemples /r/imaginaryassassins /r/imaginaryvolcanoes /r/imaginaryclerics /r/imaginaryprisons /r/imaginarygiants /r/imaginarycowboys /r/imaginaryhumans /r/imaginarydwarves /r/imaginarycaves /r/imaginarytrolls /r/imaginarywalls /r/imaginarylakes /r/imaginarywitches /r/imaginaryorcs /r/imaginarycanyons /r/imaginaryasylums /r/imaginaryimmortals /r/imaginaryaliens /r/imaginarynobles /r/imaginaryspirits /r/imaginaryaetherpunk /r/imaginarytrees /r/imaginaryislands /r/imaginaryninjas /r/imaginaryscience /r/imaginarymountains /r/imaginaryknights /r/imaginarygoblins /r/imaginaryfaeries /r/imaginarygotham /r/imaginarycybernetics /r/imaginaryooo /r/imaginaryderelicts /r/imaginaryfood /r/imaginaryworldeaters /r/imaginarymindscapes /r/imaginaryaww /r/imaginarymarvel /r/imaginaryweather /r/imaginarynewnewyork /r/imaginaryspidey /r/imaginaryautumnscapes /r/imaginarywarhammer /r/imaginaryfeels /r/imaginarywitcher /r/imaginaryvessels /r/imaginarytaverns /r/imaginarybestof /r/imaginaryairships /r/imaginaryportals /r/imaginaryfashion /r/imaginarylovers /r/imaginarydc /r/imaginaryanimals /r/imaginaryhellscapes /r/imaginarycolorscapes /r/imaginarymonstergirls /r/imaginaryswamps /r/imaginarymythology /r/imaginaryscholars /r/imaginaryladyboners /r/imaginaryfuturism /r/imaginaryaviation /r/imaginarypathways /r/imaginarygatherings /r/imaginarybodyscapes /r/imaginaryoverwatch /r/imaginarydwellings /r/imaginarystephenking /r/specart /r/inegentlemanboners /r/comicbookart /r/imaginarymasseffect /r/imaginaryhalo /r/imaginaryjerk /r/backgroundart /r/futureporn /r/imaginarywallpapers /r/imaginaryfamilies /r/imaginarylibraries /r/imaginaryturtleworlds /r/imaginarydesigns /r/wallpapers /r/apocalypseporn /r/comicbookporn /r/isometric /r/imaginarybakerst /r/imaginaryverse /r/imaginarysunnydale /r/imaginaryfederation /r/imaginarysanctuary /r/starshipporn /r/imaginarystarcraft /r/imaginaryoldkingdom /r/imaginarynarnia /r/imaginarycybertron /r/gameworlds /r/imaginarycarnage /r/imaginaryboners /r/icandrawthat /r/imaginarycosmere /r/imaginaryaperture /r/armoredwomen /r/imaginarywtf /r/unusualart /r/imaginaryblueprints /r/alternativeart /r/sympatheticmonsters /r/adorabledragons /r/imaginarysummerscapes /r/imaginarygayboners /r/imaginarystash /r/artistoftheday /r/imaginaryglaciers /r/imaginaryhybrids /r/imaginaryadventurers /r/imaginarymetropolis /r/craftsoficeandfire /r/popartnouveau\n",[30,1816,1814],{"__ignoreMap":44},[15,1818,1819],{},"There seems to be a network of \"imaginary\" subreddits that have the highest centrality. The members of this network probably all link to themselves as well as many other subreddits as the \"imaginary\" topics span a wide range content. This network may be drowning out other nodes that would otherwise have a high centrality relative to the rest of the subreddits. It might be interesting to eliminate these nodes from the graph and recalculate centrality. Let's look at the distribution of centrality values:",[39,1821,1823],{"className":41,"code":1822,"language":43,"meta":44,"style":44},"_ = plt.plot(sorted(centrality.values(), reverse=True)[:1000])\n_ = plt.title('Subreddit Centrality (top 1000)')\n_ = plt.xlabel('Rank')\n_ = plt.ylabel('Centrality')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/subreddit_graph/centrality.png'))\n",[30,1824,1825,1830,1835,1840,1845],{"__ignoreMap":44},[48,1826,1827],{"class":50,"line":51},[48,1828,1829],{},"_ = plt.plot(sorted(centrality.values(), reverse=True)[:1000])\n",[48,1831,1832],{"class":50,"line":57},[48,1833,1834],{},"_ = plt.title('Subreddit Centrality (top 1000)')\n",[48,1836,1837],{"class":50,"line":63},[48,1838,1839],{},"_ = plt.xlabel('Rank')\n",[48,1841,1842],{"class":50,"line":69},[48,1843,1844],{},"_ = plt.ylabel('Centrality')\n",[48,1846,1847],{"class":50,"line":75},[48,1848,1849],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/subreddit_graph/centrality.png'))\n",[15,1851,1852],{},[1194,1853],{"alt":1196,"src":1854},"/static/subreddit_graph/centrality.png",[10,1856,1858],{"id":1857},"connectedness","Connectedness",[15,1860,1861],{},"Let's take a look at the graph as a whole. One thing I'm not sure of is whether or not the entire graph is connected. This means that any node can be reached from any other node. Since we constructed the graph from 49 unrelated nodes, it is possible that the graph is unconnected. This would mean that one or more of the default subreddits and its subreddits is not connected with the rest of the graph. In searching for the shortest path I did not come across any pairs of nodes that did not have a path between themselves. I wouldn't be surprised if there are a handful of nodes that stand on their own.",[39,1863,1865],{"className":41,"code":1864,"language":43,"meta":44,"style":44},"#size of graph: nodes and edges (or, subreddits and connecting links)\nprint \"Our graph has \" + str(nx.number_of_nodes(G1)) + ' nodes and ' + str(nx.number_of_edges(G1)) + ' edges.'\n",[30,1866,1867,1872],{"__ignoreMap":44},[48,1868,1869],{"class":50,"line":51},[48,1870,1871],{},"#size of graph: nodes and edges (or, subreddits and connecting links)\n",[48,1873,1874],{"class":50,"line":57},[48,1875,1876],{},"print \"Our graph has \" + str(nx.number_of_nodes(G1)) + ' nodes and ' + str(nx.number_of_edges(G1)) + ' edges.'\n",[39,1878,1881],{"className":1879,"code":1880,"language":429},[427],"Our graph has 29854 nodes and 149491 edges.\n",[30,1882,1880],{"__ignoreMap":44},[39,1884,1886],{"className":41,"code":1885,"language":43,"meta":44,"style":44},"print \"True of False: our graph is connected... \" + str(nx.is_connected(G1)) + '!'\n",[30,1887,1888],{"__ignoreMap":44},[48,1889,1890],{"class":50,"line":51},[48,1891,1885],{},[39,1893,1896],{"className":1894,"code":1895,"language":429},[427],"True of False: our graph is connected... False!\n",[30,1897,1895],{"__ignoreMap":44},[39,1899,1901],{"className":41,"code":1900,"language":43,"meta":44,"style":44},"Gc = max(nx.connected_component_subgraphs(G1), key=len)\nprint \"The largest connected component subgraph has \" + str(nx.number_of_nodes(Gc)) + \" nodes. \"\n",[30,1902,1903,1908],{"__ignoreMap":44},[48,1904,1905],{"class":50,"line":51},[48,1906,1907],{},"Gc = max(nx.connected_component_subgraphs(G1), key=len)\n",[48,1909,1910],{"class":50,"line":57},[48,1911,1912],{},"print \"The largest connected component subgraph has \" + str(nx.number_of_nodes(Gc)) + \" nodes. \"\n",[39,1914,1917],{"className":1915,"code":1916,"language":429},[427],"The largest connected component subgraph has 29840 nodes.\n",[30,1918,1916],{"__ignoreMap":44},[15,1920,1921],{},"There are 14 nodes that are not connected to the main connected component. Let's list them.",[39,1923,1925],{"className":41,"code":1924,"language":43,"meta":44,"style":44},"for x in list(set(nx.to_dict_of_lists(G1, nodelist=None).keys()) - set(nx.to_dict_of_lists(Gc, nodelist=None).keys())): print x,\n",[30,1926,1927],{"__ignoreMap":44},[48,1928,1929],{"class":50,"line":51},[48,1930,1924],{},[39,1932,1935],{"className":1933,"code":1934,"language":429},[427],"/r/spacediscussions /r/wtfit.gif /r/space. /r/subreddit_graph /r/vidalia /r/listentothis. /r/history. /r/all. /r/ghostdriver /r/personalfinance. /r/toombscounty /r/gaming /r/science /r/books.\n",[30,1936,1934],{"__ignoreMap":44},[15,1938,1939],{},"Some of the large communities on reddit include /r/books, /r/gaming and /r/science. These subreddits list related subreddits on separate wiki pages since there are many related subreddits for each one. They were most likely all captured in the subsequent levels of the graph, but they also did not link back to /r/science. Here's an example:",[39,1941,1943],{"className":41,"code":1942,"language":43,"meta":44,"style":44},"for x in master_df_u.loc[master_df_u.subreddit=='/r/physics'].related: print x\n",[30,1944,1945],{"__ignoreMap":44},[48,1946,1947],{"class":50,"line":51},[48,1948,1942],{},[39,1950,1953],{"className":1951,"code":1952,"language":429},[427],"['/r/physicsjokes', '/r/gradadmissions', '/r/homeworkhelp', '/r/scienceimages', '/r/askacademia', '/r/physicsgifs', '/r/physicsstudents', '/r/gradschool', '/r/askphysics', '/r/physics']\n",[30,1954,1952],{"__ignoreMap":44},[15,1956,1957,1958,1963],{},"I've got some additional ideas to explore in another post on this topic, such as finding cliques and maximual cliques, and doing graph visualizations with D3.js. If you are interested in playing with the data, you can clone ",[19,1959,1962],{"href":1960,"rel":1961},"https://github.com/briancaffey/reddit-graph-analysis",[161],"my GitHub repo"," and load the pickled DataFrames like this:",[39,1965,1967],{"className":41,"code":1966,"language":43,"meta":44,"style":44},"import pandas as pd\ndf = pd.read_pickle('pickle/master_df.p')\n",[30,1968,1969,1973],{"__ignoreMap":44},[48,1970,1971],{"class":50,"line":51},[48,1972,1058],{},[48,1974,1975],{"class":50,"line":57},[48,1976,1977],{},"df = pd.read_pickle('pickle/master_df.p')\n",[1979,1980,1981],"style",{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":44,"searchDepth":57,"depth":57,"links":1983},[],"2017-03-03","This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from reddit.com with the goal of visualizing the network of related subreddits (forums on specific topics) and related data.",false,"md",null,"/static/subreddits.png",{"layout":1991},"post","/2017/03/03/graph_subreddit",{"title":5,"description":1985},"2017/03/03/graph_subreddit",[1996,43,1997,1998,1999],"reddit","scraping","data","graphs","6iZj-hiDxE2fBFIlIkzBrQQuykMyCBtk3aebWowsoyg",1753130132332]