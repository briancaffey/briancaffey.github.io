<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Flux Plug-in for Project G-Assist</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-05905815]{padding:0}.selected[data-v-05905815]{text-shadow:.25px 0 0 #000}.picker[data-v-05905815]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-05905815]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-1edc4b99]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-de9cb334]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-de9cb334]:hover{transform:scale(1.3)}.centered[data-v-de9cb334]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>span.emoji-mart-emoji[data-v-f26190e6]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-f26190e6]:hover{transform:scale(1.3)}.picker[data-v-f26190e6]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-f26190e6]{background-color:var(--bg)}.localeText[data-v-f26190e6]{color:var(--color-primary)}</style><style>.tag[data-v-09161b57]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-09161b57]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.MFRQdpI0.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2025/07/17/flux-plugin-for-project-g-assist-hackathon/_payload.json?f7a02a46-9c0e-478e-936c-d4b79733d6b2"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/jzUacxre.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDMEWRFj.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CzmcxFEq.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B-mOQKwH.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DPol-5Mm.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DuQfXRjE.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DjIsa8m4.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/rBAQty2o.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C4Sm19mx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CgFg6jAm.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/rgQGHqoJ.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D7hIbxa9.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/lzNerLNA.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CTfEw4GE.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BuB7IBBV.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CE4dwlpx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CfTcFQKu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BgkXKTQS.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C5aKNO_V.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BnNjkjIJ.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BYB8-TAF.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDyDDOxV.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BOjBGTeT.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B2Gcn1Be.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DTfw26--.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D4ysgOa-.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/f7a02a46-9c0e-478e-936c-d4b79733d6b2.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BAz4hE9E.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DPdwgYFM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B9ziLBQK.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DAwWNWwV.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/_7iENaYg.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BVmCdIjn.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/D86XVKAs.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DDRJokCh.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BaoTS1Ec.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B9rTWv16.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CJwV7gYv.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/C9IforG0.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Du9tXjnC.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Bq3ZV0Cc.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BPxGl8VU.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DbwMaSgs.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/zTRos7l-.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BZZoewB1.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BSjOxtvE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CRVSn5jR.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/UiuPncGx.js"><meta name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="Flux Plug-in for Project G-Assist"><meta property="og:description" content="A Plug-in for Project G-Assist that puts the power of AI image generation right at your fingertips"><meta property="og:image" content="https://briancaffey.github.io/static/flux/flux_plugin_for_project_g_assist.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/flux/flux_plugin_for_project_g_assist.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/jzUacxre.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-05905815><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-05905815><div class="justify-left flex-grow flex-cols-4" data-v-05905815><a href="/" class="text-xl" data-v-05905815><span class="hidden sm:inline text-2xl" data-v-05905815>Brian Caffey</span><span class="inline sm:hidden" data-v-05905815>JBC</span></a></div><div class="flex-grow relative" data-v-05905815><nav z-index="10000" data-v-05905815 data-v-1edc4b99><div data-v-1edc4b99><ul class="items-right float-right hidden md:flex" data-v-1edc4b99><li class="px-4 text-lg" data-v-1edc4b99><a href="/blog/1" class="" data-v-1edc4b99>Blog</a></li><li class="px-4 text-lg" data-v-1edc4b99><a href="/contact" class="" data-v-1edc4b99>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-1edc4b99><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-1edc4b99><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-1edc4b99><title data-v-1edc4b99>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-1edc4b99></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-05905815><div class="centered" data-v-05905815 data-v-de9cb334><div class="grid items-center justify-center" data-v-de9cb334><ul class="flex px-4" data-v-de9cb334><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🖥️, desktop_computer" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🌞, sun_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🌚, new_moon_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="☕, coffee" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><div data-v-de9cb334 data-v-f26190e6><ul data-v-f26190e6><li class="md:px-1 px-1 cursor-pointer" data-v-f26190e6><span aria-label="🇺🇸, us, flag-us" class="emoji-mart-emoji" data-v-f26190e6><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-f26190e6><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="A Plug-in for Project G-Assist that puts the power of AI image generation right at your fingertips" data-nuxt-img srcset="/_ipx/f_webp/static/flux/flux_plugin_for_project_g_assist.png 1x, /_ipx/f_webp/static/flux/flux_plugin_for_project_g_assist.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/flux/flux_plugin_for_project_g_assist.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">Flux Plug-in for Project G-Assist</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/nvidia/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>nvidia 🏷️ <!----></div></a><a href="/blog/tags/ai/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>ai 🏷️ <!----></div></a><a href="/blog/tags/rtx/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>rtx 🏷️ <!----></div></a><a href="/blog/tags/nim/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>nim 🏷️ <!----></div></a><a href="/blog/tags/flux/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>flux 🏷️ <!----></div></a><!--]--></div><div class="flex py-2"><!--[--><div class="pr-4 rounded"><a href="https://x.com/briancaffey/status/1946684573095497992"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://x.com/briancaffey/status/1946684573095497992" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/x.png 25w, /_ipx/w_50&amp;f_webp/icons/x.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/x.png"></a></div><!--]--></div><p class="blog-date text-gray-500 mb-4">Last updated July 20, 2025</p><!----><div class="markdown"><p><!--[-->This article will discuss my submission for the Project G-Assist Hackathon: Flux Plug-in for Project G-Assist. This plugin allows RTX AI PC users to tap into their GPU&#39;s image generation capabilities through the Flux.1-dev NVIDIA NIM. G-Assist now generates images from your commands on demand!<!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-theme="dark"><p lang="en" dir="ltr">Flux Plug-in for Project G-Assist<br>⚡️🎨🤖🖼️🖥️⚡️ <br>Generate stunning images on your NVIDIA GeForce RTX AI PC with natural language using G-Assist<a href="https://twitter.com/hashtag/AIonRTXHackathon?src=hash&amp;ref_src=twsrc%5Etfw">#AIonRTXHackathon</a><br><br>🖼️ <a href="https://twitter.com/bfl_ml?ref_src=twsrc%5Etfw">@bfl_ml</a> flux.1-dev &amp; flux-kontext<br>🎶 <a href="https://twitter.com/SunoMusic?ref_src=twsrc%5Etfw">@SunoMusic</a> “vibes” by <a href="https://twitter.com/briancaffey?ref_src=twsrc%5Etfw">@briancaffey</a><br>🗣️ <a href="https://twitter.com/nari_labs?ref_src=twsrc%5Etfw">@nari_labs</a> dia (video narration)<br><br>my… <a href="https://t.co/JyXwkz68Lj">pic.twitter.com/JyXwkz68Lj</a></p>— Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1946684573095497992?ref_src=twsrc%5Etfw">July 19, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->The Flux Plugin for G-Assist is a plugin developed for NVIDIA’s Project G-Assist that brings real-time AI image generation directly to the desktop through natural language commands. It allows users to create high-quality images using the Flux family of models from Black Forest Labs, seamlessly integrated with the G-Assist interface. Users can simply type or speak prompts such as “a futuristic robot painter in a neon-lit workshop”, and the plugin handles the entire process—from submitting the prompt to generating and displaying the image—all without leaving the G-Assist chat window.<!--]--></p><p><!--[-->The plugin supports multiple deployment backends for inference, including Flux NIMs running locally via WSL, on the cloud via build.nvidia.com, or through InvokeAI using the Flux Kontext model for image-to-image and screenshot-based transformations. The plugin includes additional tools for managing the inference service, such as checking status and turning the NIM service on or off, enabling a user-friendly experience for both beginners and power users.<!--]--></p><p><!--[-->The goal of this plugin is to make generative image workflows faster, more accessible, and more fun—leveraging the strengths of NVIDIA’s hardware, AI models, and desktop ecosystem. By combining the power of FLUX with the voice-enabled G-Assist interface, the plugin turns any PC into a hands-free creative studio.<!--]--></p><h2 id="what-can-it-do"><a href="#what-can-it-do"><!--[-->What Can It Do?<!--]--></a></h2><h3 id="image-generation"><a href="#image-generation"><!--[-->Image Generation<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Generate images from text prompts<!--]--></strong> using Flux AI model<!--]--></li><li><!--[--><strong><!--[-->Support for multiple backends<!--]--></strong>: Local NIM servers, NVIDIA hosted services, or InvokeAI<!--]--></li><li><!--[--><strong><!--[-->Automatic desktop background setting<!--]--></strong> - generated images can be set as your wallpaper<!--]--></li><li><!--[--><strong><!--[-->High-quality output<!--]--></strong> with customizable parameters (resolution, steps, CFG scale)<!--]--></li><!--]--></ul><h3 id="nim-server-management"><a href="#nim-server-management"><!--[-->NIM Server Management<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Start/stop local Flux NIM servers<!--]--></strong> using WSL and Podman<!--]--></li><li><!--[--><strong><!--[-->Check NIM server status<!--]--></strong> to see if the service is running<!--]--></li><li><!--[--><strong><!--[-->Health endpoint testing<!--]--></strong> for local servers<!--]--></li><li><!--[--><strong><!--[-->Automatic configuration<!--]--></strong> using NGC API keys and Hugging Face tokens<!--]--></li><!--]--></ul><h3 id="invokeai-integration"><a href="#invokeai-integration"><!--[-->InvokeAI Integration<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Upload screenshots to InvokeAI<!--]--></strong> for image-to-image workflows<!--]--></li><li><!--[--><strong><!--[-->Flux Kontext generation<!--]--></strong> using uploaded images as reference<!--]--></li><li><!--[--><strong><!--[-->Processor control<!--]--></strong> - pause and resume InvokeAI processing queues<!--]--></li><li><!--[--><strong><!--[-->VRAM management<!--]--></strong> - empty model cache to free up memory<!--]--></li><li><!--[--><strong><!--[-->Status monitoring<!--]--></strong> - check InvokeAI service health and version<!--]--></li><!--]--></ul><h3 id="smart-configuration"><a href="#smart-configuration"><!--[-->Smart Configuration<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Flexible URL configuration<!--]--></strong> - works with local servers or NVIDIA hosted endpoints<!--]--></li><li><!--[--><strong><!--[-->Automatic API key validation<!--]--></strong> - ensures proper NVIDIA API key format<!--]--></li><li><!--[--><strong><!--[-->Configurable output directories<!--]--></strong> for generated images<!--]--></li><li><!--[--><strong><!--[-->Board management<!--]--></strong> for InvokeAI gallery organization<!--]--></li><!--]--></ul><h3 id="example-commands"><a href="#example-commands"><!--[-->Example Commands<!--]--></a></h3><ul><!--[--><li><!--[-->&quot;hey flux, generate an image of a cyberpunk city at night&quot;<!--]--></li><li><!--[-->&quot;hey flux, start the Flux NIM server&quot;<!--]--></li><li><!--[-->&quot;hey flux, use kontext to make it a cartoon style&quot; (does image-to-image generation using latest screenshot taken with NVIDIA screenshot shortcut)<!--]--></li><li><!--[-->&quot;hey flux, empty the InvokeAI model cache to free up VRAM&quot;<!--]--></li><li><!--[-->&quot;hey flux, Check if the NIM server is running&quot;<!--]--></li><!--]--></ul><h2 id="before-you-start"><a href="#before-you-start"><!--[-->Before You Start<!--]--></a></h2><p><!--[-->Make sure you have:<!--]--></p><ul><!--[--><li><!--[-->Windows PC<!--]--></li><li><!--[-->Python 3.12 or higher<!--]--></li><li><!--[-->G-Assist installed on your system<!--]--></li><li><!--[-->pywin32 &gt;= 223<!--]--></li><li><!--[-->Basic knowledge of Python<!--]--></li><!--]--></ul><p><!--[-->💡 <strong><!--[-->Tip<!--]--></strong>: Use a virtual environment to keep your plugin dependencies isolated from other Python projects!<!--]--></p><h2 id="installation-guide"><a href="#installation-guide"><!--[-->Installation Guide<!--]--></a></h2><h3 id="step-1-get-the-files"><a href="#step-1-get-the-files"><!--[-->Step 1: Get the Files<!--]--></a></h3><p><!--[-->Clone this repository<!--]--></p><p><!--[-->This downloads the plugin code and all necessary files to your computer.<!--]--></p><h3 id="step-2-set-up-python-environment"><a href="#step-2-set-up-python-environment"><!--[-->Step 2: Set Up Python Environment<!--]--></a></h3><p><!--[-->Run the <code><!--[-->setup.bat<!--]--></code> file. This will take care of creating a Python virtual environment will install project dependencies.<!--]--></p><pre class="language-bash shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="srTi1">python</span><span class="s7F3e"> -m</span><span class="sstjo"> venv</span><span class="sstjo"> .venv
</span></span><span class="line" line="2"><span class="srTi1">.venv\Scripts\activate
</span></span><span class="line" line="3"><span class="srTi1">python</span><span class="s7F3e"> -m</span><span class="sstjo"> pip</span><span class="sstjo"> install</span><span class="s7F3e"> -r</span><span class="sstjo"> requirements.txt
</span></span></code><!--]--></pre><h3 id="step-3-build-the-project"><a href="#step-3-build-the-project"><!--[-->Step 3: Build the project<!--]--></a></h3><p><!--[-->Run <code><!--[-->build.bat<!--]--></code> to build the project. This script will also place the <code><!--[-->g-assist-plugin-flux.exe<!--]--></code> and <code><!--[-->manifest.json<!--]--></code> files in <code><!--[-->%PROGRAMDATA%\NVIDIA Corporation\nvtopps\rise\plugins\flux<!--]--></code>.<!--]--></p><h3 id="step-4-configuration"><a href="#step-4-configuration"><!--[-->Step 4: Configuration<!--]--></a></h3><p><!--[-->Copy the <code><!--[-->config.example.json<!--]--></code> file and rename it as <code><!--[-->config.json<!--]--></code> and place it in <code><!--[-->%PROGRAMDATA%\NVIDIA Corporation\nvtopps\rise\plugins\flux<!--]--></code>. Customize it with the appropriate configuration values (see below for configuration details). Here is an example configuration:<!--]--></p><pre class="language-json shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">{
</span></span><span class="line" line="2"><span class="s-m8C">    &quot;GALLERY_DIRECTORY&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;E:</span><span class="s7F3e">\\</span><span class="sCZoN">NVIDIA&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="3"><span class="s-m8C">    &quot;FLUX_NIM_URL&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;http://localhost:8000&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="4"><span class="s-m8C">    &quot;NGC_API_KEY&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;xxxxxxxx&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="5"><span class="s-m8C">    &quot;HF_TOKEN&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;hf_xxxxxxxx&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="6"><span class="s-m8C">    &quot;LOCAL_NIM_CACHE&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;~/.cache/nim&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="7"><span class="s-m8C">    &quot;INVOKEAI_URL&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;http://localhost:9090&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="8"><span class="s-m8C">    &quot;OUTPUT_DIRECTORY&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;E:</span><span class="s7F3e">\\</span><span class="sCZoN">Flux&quot;
</span></span><span class="line" line="9"><span class="sMOD_">}
</span></span></code><!--]--></pre><h3 id="step-5-set-up-flux"><a href="#step-5-set-up-flux"><!--[-->Step 5: Set up Flux<!--]--></a></h3><p><!--[-->Follow instructions <a href="https://build.nvidia.com/black-forest-labs/flux_1-dev/deploy?environment=wsl2.md" rel="nofollow"><!--[-->here<!--]--></a> for installing the FLUX.1-dev model from Black Forest Labs using NVIDIA NIM on WSL. Be sure to do the following:<!--]--></p><ul><!--[--><li><!--[-->follow instructions here: <a href="https://docs.nvidia.com/nim/wsl2/latest/getting-started.html" rel="nofollow"><!--[-->https://docs.nvidia.com/nim/wsl2/latest/getting-started.html<!--]--></a> for getting started with WSL<!--]--></li><li><!--[-->use the <a href="https://docs.nvidia.com/nim/wsl2/latest/getting-started.html#use-the-nvidia-nim-wsl2-installer-recommended" rel="nofollow"><!--[-->NVIDIA NIM WSL2 Installer<!--]--></a> for configuring a new WSL environment configured with all of the required NVIDIA dependencies<!--]--></li><li><!--[-->In your Hugging Face account read and accept FLUX.1-dev, FLUX.1-Canny-dev, FLUX.1-Depth-dev and FLUX.1-dev-onnx License Agreements and Acceptable Use Policy. You must accept the agreements/policies for all of the models even though this plugin does not directly use the Canny or Depth modes.<!--]--></li><li><!--[-->Make sure to map port <code><!--[-->8000<!--]--></code> in the NIM to port <code><!--[-->8000<!--]--></code> on the WSL host as shown in the setup link above.<!--]--></li><!--]--></ul><h3 id="step-6-install-invokeai-optional"><a href="#step-6-install-invokeai-optional"><!--[-->Step 6: Install InvokeAI (optional)<!--]--></a></h3><p><!--[-->InvokeAI has a Windows installer that can be found here: <a href="https://invoke-ai.github.io/InvokeAI/installation/quick_start/#step-2-download" rel="nofollow"><!--[-->https://invoke-ai.github.io/InvokeAI/installation/quick_start/#step-2-download<!--]--></a>. Download the Flux models including the <strong><!--[-->FLUX.1 Kontext dev (Quantized)<!--]--></strong> model for using Flux Kontext in the Flux Plug-in for G-Assist. By default InvokeAI runs on <code><!--[-->http://localhost:9090<!--]--></code>.<!--]--></p><h3 id="step-7-start-the-nvidia-nim"><a href="#step-7-start-the-nvidia-nim"><!--[-->Step 7: Start the NVIDIA NIM<!--]--></a></h3><p><!--[-->Ask flux if this NIM is running. If it is not running, ask flux to start the NIM. This will run a command to star the NIM container in WSL using podman:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">podman_cmd </span><span class="sC2Qs">=</span><span class="sMOD_"> [
</span></span><span class="line" line="2"><span class="sstjo">    &#39;wsl&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;-d&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;NVIDIA-Workbench&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="3"><span class="sstjo">    &#39;podman&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;run&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;-d&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;--rm&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;--name=nim-server&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="4"><span class="sstjo">    &#39;--device&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;nvidia.com/gpu=all&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="5"><span class="sstjo">    &#39;-e&#39;</span><span class="sMOD_">, </span><span class="sq6CD">f</span><span class="sstjo">&#39;NGC_API_KEY=</span><span class="s7F3e">{NGC_API_KEY}</span><span class="sstjo">&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="6"><span class="sstjo">    &#39;-e&#39;</span><span class="sMOD_">, </span><span class="sq6CD">f</span><span class="sstjo">&#39;HF_TOKEN=</span><span class="s7F3e">{HF_TOKEN}</span><span class="sstjo">&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="7"><span class="sstjo">    &#39;-p&#39;</span><span class="sMOD_">, </span><span class="sstjo">&#39;8000:8000&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="8"><span class="sstjo">    &#39;-v&#39;</span><span class="sMOD_">, </span><span class="sq6CD">f</span><span class="sstjo">&#39;</span><span class="s7F3e">{LOCAL_NIM_CACHE}</span><span class="sstjo">:/opt/nim/.cache/&#39;</span><span class="sMOD_">,
</span></span><span class="line" line="9"><span class="sstjo">    &#39;nvcr.io/nim/black-forest-labs/flux.1-dev:1.0.0&#39;
</span></span><span class="line" line="10"><span class="sMOD_">]
</span></span></code><!--]--></pre><p><!--[-->Then ask if the NIM is ready. This will check the <code><!--[-->/v1/health/live<!--]--></code> and <code><!--[-->/v1/health/ready<!--]--></code> endpoints of the Flux NIM.<!--]--></p><h3 id="step-8-generete-ai-images-using-the-flux-plug-in-in-the-g-assist-chat-window"><a href="#step-8-generete-ai-images-using-the-flux-plug-in-in-the-g-assist-chat-window"><!--[-->Step 8: Generete AI images using the Flux Plug-in in the G-Assist chat window<!--]--></a></h3><p><!--[-->Send a message to G-Assist:<!--]--></p><p><!--[-->&quot;hey flux, generate a cat piloting a spaceship&quot;<!--]--></p><p><!--[-->The Flux Plug-in will respond with:<!--]--></p><p><!--[-->flux&gt; Your image generation request is in progress! Prompt: &quot;a cat piloting a spaceship&quot;<!--]--></p><p><!--[-->When the image generation is complete you will find the image on your Desktop background, and the image will be saved to the output directory specified in your configuration file.<!--]--></p><h3 id="step-9-transform-a-screenshot-with-flux-kontext"><a href="#step-9-transform-a-screenshot-with-flux-kontext"><!--[-->Step 9: Transform a screenshot with Flux Kontext<!--]--></a></h3><p><!--[-->Take a screenshot using the NVIDIA Screenshot hotkey (usually <code><!--[-->Alt + F1<!--]--></code>), and then ask the Flux Plug-in to transform it to any style using Kontext.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Cat piloting spaceship" data-nuxt-img srcset="/_ipx/_/static/flux/cat_spaceship.png 1x, /_ipx/_/static/flux/cat_spaceship.png 2x" src="/_ipx/_/static/flux/cat_spaceship.png"><!--]--></p><p><!--[-->hey flux, use kontext with the prompt: cartoon style<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Cat Spaceship Cartoon style with Kontext" data-nuxt-img srcset="/_ipx/_/static/flux/cat_spaceship_cartoon.png 1x, /_ipx/_/static/flux/cat_spaceship_cartoon.png 2x" src="/_ipx/_/static/flux/cat_spaceship_cartoon.png"><!--]--></p><p><!--[-->Flux does this by triggering an InvokeAI graph workflow. The generated image and the workflow can both be viewed in the InvokeAI UI:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="InvokeAI Flux Kontext Workflow" data-nuxt-img srcset="/_ipx/_/static/flux/invokeai_workflow.png 1x, /_ipx/_/static/flux/invokeai_workflow.png 2x" src="/_ipx/_/static/flux/invokeai_workflow.png"><!--]--></p><p><!--[-->You can ask the Flux Plug-in to pause/resume InvokeAI processing to avoid running Flux Kontext image generation while your GPU is busy with other tasks. Also you can ask flux to empty the model cache in order to free up VRAM on your GPU.<!--]--></p><h2 id="configuration"><a href="#configuration"><!--[-->Configuration<!--]--></a></h2><p><!--[-->The Flux plugin uses a <code><!--[-->config.json<!--]--></code> file to manage all settings. Copy <code><!--[-->config.example.json<!--]--></code> to <code><!--[-->config.json<!--]--></code> and customize the values for your setup.<!--]--></p><h3 id="configuration-options"><a href="#configuration-options"><!--[-->Configuration Options<!--]--></a></h3><table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Option<!--]--></th><th><!--[-->Example Values<!--]--></th><th><!--[-->Required<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[--><code><!--[-->GALLERY_DIRECTORY<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;D:\\Screenshots&quot;<!--]--></code>, <code><!--[-->&quot;C:\\NVIDIA&quot;<!--]--></code><!--]--></td><td><!--[-->No<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->NVIDIA_API_KEY<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;nvapi-your-key-here&quot;<!--]--></code><!--]--></td><td><!--[-->No*<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->FLUX_NIM_URL<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;http://localhost:8000&quot;<!--]--></code>, <code><!--[-->&quot;http://192.168.1.100:8000&quot;<!--]--></code><!--]--></td><td><!--[-->Yes<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->NGC_API_KEY<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;your-ngc-key&quot;<!--]--></code><!--]--></td><td><!--[-->Yes**<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->HF_TOKEN<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;hf_your-token&quot;<!--]--></code><!--]--></td><td><!--[-->Yes**<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->LOCAL_NIM_CACHE<!--]--></code><!--]--></td><td><!--[--><code><!--[-->~/.cache/nim<!--]--></code><!--]--></td><td><!--[-->Yes**<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->INVOKEAI_URL<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;http://localhost:9090&quot;<!--]--></code>, <code><!--[-->&quot;http://192.168.1.100:9090&quot;<!--]--></code><!--]--></td><td><!--[-->No<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->BOARD_ID<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;my-gallery-board&quot;<!--]--></code>, <code><!--[-->&quot;flux-gallery&quot;<!--]--></code><!--]--></td><td><!--[-->No<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->OUTPUT_DIRECTORY<!--]--></code><!--]--></td><td><!--[--><code><!--[-->&quot;C:\\GeneratedImages&quot;<!--]--></code>, <code><!--[-->&quot;D:\\flux-output&quot;<!--]--></code><!--]--></td><td><!--[-->No<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table><p><!--[-->*Required only when using NVIDIA hosted Flux service (<code><!--[-->FLUX_NIM_URL<!--]--></code> starts with &quot;<a href="https://ai.api.nvidia.com" rel="nofollow"><!--[-->https://ai.api.nvidia.com<!--]--></a>&quot;)
**Required only when using local NIM server<!--]--></p><h2 id="using-the-flux1-dev-nvidia-nim-for-text-to-image-generation"><a href="#using-the-flux1-dev-nvidia-nim-for-text-to-image-generation"><!--[-->Using the Flux.1-dev NVIDIA NIM for text-to-image generation<!--]--></a></h2><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Desert Nomad" data-nuxt-img srcset="/_ipx/_/static/flux/desert_nomad.png 1x, /_ipx/_/static/flux/desert_nomad.png 2x" src="/_ipx/_/static/flux/desert_nomad.png"><!--]--></p><p><!--[-->On NVIDIA GeForce RTX AI PCs, the best way to do AI image inference is by using NVIDIA NIMs. Windows currently has beta support for running NVIDIA NIMs in WSL with Podman (a program for running containers, similar to Docker).<!--]--></p><p><!--[-->NVIDIA provides an installer that installs a WSL distribution with all dependencies installed. You can find those resources here: <a href="https://docs.nvidia.com/nim/wsl2/latest/getting-started.html" rel="nofollow"><!--[-->https://docs.nvidia.com/nim/wsl2/latest/getting-started.html<!--]--></a> (WSL2 is required for hosting any NIM. Refer to the official NVIDIA NIM on WSL2 documentation for setup instructions.)<!--]--></p><h3 id="how-it-works"><a href="#how-it-works"><!--[-->How It Works<!--]--></a></h3><p><!--[-->You can request an image to be generated by simply saying something like:<!--]--></p><p><!--[-->&quot;hey flux, generate a cat piloting a spaceship&quot;<!--]--></p><p><!--[-->The Flux Plugin will make an API request to the Flux NIM URL (configured in your config.json, defaults to <a href="http://localhost:8000" rel="nofollow"><!--[-->http://localhost:8000<!--]--></a>).<!--]--></p><h3 id="asynchronous-processing"><a href="#asynchronous-processing"><!--[-->Asynchronous Processing<!--]--></a></h3><p><!--[-->The G-Assist chat assistant has a timeout of 10 seconds, so the chat assistant returns immediately with:<!--]--></p><pre class="language-text"><!--[--><code>flux&gt; Your image generation request is in progress! Prompt: &quot;a cat piloting a spaceship&quot;
</code><!--]--></pre><p><!--[-->The image generation request runs on a separate thread, since the image generation process with the NVIDIA NIM can take up to 30 seconds (depending on the number of steps, 50 steps is used in the plugin for best results). The plugin then creates an image file from the base64 encoded image in the response from the Flux NIM server, and it sets this image as your desktop background image.<!--]--></p><h3 id="nim-management"><a href="#nim-management"><!--[-->NIM Management<!--]--></a></h3><p><!--[-->There are also commands for starting and stopping the Flux NIM, which runs Podman commands inside of the NVIDIA-Workbench WSL distribution:<!--]--></p><ul><!--[--><li><!--[--><strong><!--[-->Start NIM<!--]--></strong>: &quot;hey flux, start the Flux NIM server&quot;<!--]--></li><li><!--[--><strong><!--[-->Stop NIM<!--]--></strong>: &quot;hey flux, stop the Flux NIM server&quot;<!--]--></li><li><!--[--><strong><!--[-->Check Status<!--]--></strong>: &quot;hey flux, check if the NIM server is running&quot;<!--]--></li><!--]--></ul><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Flux Plug-in controls" data-nuxt-img srcset="/_ipx/_/static/flux/controls.png 1x, /_ipx/_/static/flux/controls.png 2x" src="/_ipx/_/static/flux/controls.png"><!--]--></p><h3 id="configuration-1"><a href="#configuration-1"><!--[-->Configuration<!--]--></a></h3><p><!--[-->Make sure your <code><!--[-->config.json<!--]--></code> includes the necessary credentials:<!--]--></p><ul><!--[--><li><!--[--><code><!--[-->NGC_API_KEY<!--]--></code>: Your NVIDIA NGC API key for downloading models<!--]--></li><li><!--[--><code><!--[-->HF_TOKEN<!--]--></code>: Your Hugging Face token for model access<!--]--></li><li><!--[--><code><!--[-->LOCAL_NIM_CACHE<!--]--></code>: Path to your local NIM cache directory<!--]--></li><!--]--></ul><h2 id="image-to-image-generation-with-flux-kontext-and-invokeai"><a href="#image-to-image-generation-with-flux-kontext-and-invokeai"><!--[-->Image-to-image generation with Flux Kontext and InvokeAI<!--]--></a></h2><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Helicopter over NYC" data-nuxt-img srcset="/_ipx/_/static/flux/nyc_heli.png 1x, /_ipx/_/static/flux/nyc_heli.png 2x" src="/_ipx/_/static/flux/nyc_heli.png"><!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Helicopter over NYC" data-nuxt-img srcset="/_ipx/_/static/flux/nyc_heli_watercolor.png 1x, /_ipx/_/static/flux/nyc_heli_watercolor.png 2x" src="/_ipx/_/static/flux/nyc_heli_watercolor.png"><!--]--></p><p><!--[-->The Flux Plug-in supports image-to-image generation using an open source image generation tool called InvokeAI. This tool is similar to ComfyUI and it has solid API support. Currently there is no NVIDIA NIM for Flux Kontext but the NVIDIA blog mentioned that this might be released as soon as May 2025.<!--]--></p><p><!--[-->You can interact with the InvokeAI program in a few different ways:<!--]--></p><h3 id="screenshot-based-image-generation"><a href="#screenshot-based-image-generation"><!--[-->Screenshot-based Image Generation<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Upload your latest screenshot<!--]--></strong> and perform image-to-image generation using Flux Kontext. This allows you to apply any type of manipulation to your screenshot (for example, you can say &quot;hey flux, use kontext to make it in the style of a cartoon&quot;)<!--]--></li><li><!--[-->The plugin automatically finds your most recent screenshot and uploads it to InvokeAI<!--]--></li><li><!--[-->You can provide custom prompts to guide the transformation process<!--]--></li><!--]--></ul><h3 id="processing-control"><a href="#processing-control"><!--[-->Processing Control<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Pause or resume processing<!--]--></strong>: This is useful if you are playing a GPU intensive game. You can pause processing, but still submit image-to-image generation tasks using Flux Kontext. The tasks will be queued and they can be resumed later when your GPU is not busy with other tasks.<!--]--></li><li><!--[-->Monitor the processing queue status and control when generation happens<!--]--></li><!--]--></ul><h3 id="memory-management"><a href="#memory-management"><!--[-->Memory Management<!--]--></a></h3><ul><!--[--><li><!--[--><strong><!--[-->Empty the model cache<!--]--></strong>: InvokeAI keeps models cached between generation, but you can empty the model cache by simply telling it to do so<!--]--></li><li><!--[-->This helps free up VRAM when you&#39;re not actively using InvokeAI<!--]--></li><li><!--[-->Useful for switching between different AI workloads or when playing games<!--]--></li><!--]--></ul><h3 id="setup-requirements"><a href="#setup-requirements"><!--[-->Setup Requirements<!--]--></a></h3><p><!--[-->To use the image-to-image features, you&#39;ll need:<!--]--></p><ul><!--[--><li><!--[-->InvokeAI installed and running locally (typically on <code><!--[-->http://localhost:9090<!--]--></code>)<!--]--></li><li><!--[-->Flux Kontext model loaded in InvokeAI<!--]--></li><li><!--[-->Proper configuration in your <code><!--[-->config.json<!--]--></code> file<!--]--></li><!--]--></ul><h2 id="supported-commands"><a href="#supported-commands"><!--[-->Supported Commands<!--]--></a></h2><p><!--[-->The Flux Plug-in for G-Assist supports the following commands:<!--]--></p><table><!--[--><thead><!--[--><tr><!--[--><th><!--[-->Function<!--]--></th><th><!--[-->Description<!--]--></th><th><!--[-->Example<!--]--></th><!--]--></tr><!--]--></thead><tbody><!--[--><tr><!--[--><td><!--[--><code><!--[-->flux_nim_ready_check<!--]--></code><!--]--></td><td><!--[-->Tests health endpoints of the Flux NIM server<!--]--></td><td><!--[-->&quot;hey flux, check if the flux nim server is ready&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->check_nim_status<!--]--></code><!--]--></td><td><!--[-->Checks if the Flux NIM server is running<!--]--></td><td><!--[-->&quot;hey flux, check if the nim server is running&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->stop_nim<!--]--></code><!--]--></td><td><!--[-->Stops the Flux NIM server<!--]--></td><td><!--[-->&quot;hey flux, stop the flux nim server&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->start_nim<!--]--></code><!--]--></td><td><!--[-->Starts the Flux NIM server<!--]--></td><td><!--[-->&quot;hey flux, start the flux nim server&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->generate_image<!--]--></code><!--]--></td><td><!--[-->Generates an image from text prompt using Flux<!--]--></td><td><!--[-->&quot;hey flux, generate an image of a cyberpunk city&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->generate_image_using_kontext<!--]--></code><!--]--></td><td><!--[-->Performs image-to-image generation using Flux Kontext<!--]--></td><td><!--[-->&quot;hey flux, use kontext to make it a cartoon style&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->invokeai_status<!--]--></code><!--]--></td><td><!--[-->Checks the status of the InvokeAI service<!--]--></td><td><!--[-->&quot;hey flux, check invokeai status&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->pause_invokeai_processor<!--]--></code><!--]--></td><td><!--[-->Pauses the InvokeAI processing queue<!--]--></td><td><!--[-->&quot;hey flux, pause the invokeai processor&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->resume_invokeai_processor<!--]--></code><!--]--></td><td><!--[-->Resumes the InvokeAI processing queue<!--]--></td><td><!--[-->&quot;hey flux, resume the invokeai processor&quot;<!--]--></td><!--]--></tr><tr><!--[--><td><!--[--><code><!--[-->invokeai_empty_model_cache<!--]--></code><!--]--></td><td><!--[-->Empties the InvokeAI model cache to free VRAM<!--]--></td><td><!--[-->&quot;hey flux, empty the invokeai model cache&quot;<!--]--></td><!--]--></tr><!--]--></tbody><!--]--></table><h2 id="logging"><a href="#logging"><!--[-->Logging<!--]--></a></h2><p><!--[-->Your plugin automatically logs to <code><!--[-->flux_plugin.log<!--]--></code> in your user&#39;s profile directory. It tracks:<!--]--></p><ul><!--[--><li><!--[-->Plugin startup and shutdown<!--]--></li><li><!--[-->Command reception and processing<!--]--></li><li><!--[-->Error conditions<!--]--></li><li><!--[-->Function execution details<!--]--></li><!--]--></ul><h2 id="troubleshooting-tips"><a href="#troubleshooting-tips"><!--[-->Troubleshooting Tips<!--]--></a></h2><ul><!--[--><li><!--[--><strong><!--[-->Plugin not starting?<!--]--></strong> Check if Python 3.12+ is installed and in PATH<!--]--></li><li><!--[--><strong><!--[-->Communication errors?<!--]--></strong> Verify pywin32 is installed correctly<!--]--></li><li><!--[--><strong><!--[-->Commands not working?<!--]--></strong> Double-check your command registration<!--]--></li><li><!--[--><strong><!--[-->Missing logs?<!--]--></strong> Ensure write permissions in user profile directory<!--]--></li><!--]--></ul><h2 id="want-to-contribute"><a href="#want-to-contribute"><!--[-->Want to Contribute?<!--]--></a></h2><p><!--[-->We&#39;d love your help making this template even better! Check out <code><!--[-->CONTRIBUTING.md<!--]--></code> for guidelines on how to contribute.<!--]--></p><h2 id="license"><a href="#license"><!--[-->License<!--]--></a></h2><p><!--[-->This project is licensed under the Apache License 2.0 - see the <code><!--[-->LICENSE<!--]--></code> file for details.<!--]--></p><h2 id="hardware"><a href="#hardware"><!--[-->Hardware<!--]--></a></h2><p><!--[-->The Flux Plug-in for G-Assist was developed and tested on a PC with a GeForce RTX 4090 GPU.<!--]--></p><style>html pre.shiki code .srTi1, html code.shiki .srTi1{--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E}html pre.shiki code .s7F3e, html code.shiki .s7F3e{--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#AE81FF}html pre.shiki code .sstjo, html code.shiki .sstjo{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#E6DB74}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html.sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html pre.shiki code .sMOD_, html code.shiki .sMOD_{--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#F8F8F2}html pre.shiki code .s-m8C, html code.shiki .s-m8C{--shiki-default:#005CC5;--shiki-default-font-style:inherit;--shiki-dark:#79B8FF;--shiki-dark-font-style:inherit;--shiki-sepia:#66D9EF;--shiki-sepia-font-style:italic}html pre.shiki code .sCZoN, html code.shiki .sCZoN{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#CFCFC2}html pre.shiki code .sC2Qs, html code.shiki .sC2Qs{--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#F92672}html pre.shiki code .sq6CD, html code.shiki .sq6CD{--shiki-default:#D73A49;--shiki-default-font-style:inherit;--shiki-dark:#F97583;--shiki-dark-font-style:inherit;--shiki-sepia:#66D9EF;--shiki-sepia-font-style:italic}</style></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments 💬 </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> © 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2025/07/17/flux-plugin-for-project-g-assist-hackathon/_payload.json?f7a02a46-9c0e-478e-936c-d4b79733d6b2">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"flux-plugin-for-project-g-assist-hackathon":-1},"/2025/07/17/flux-plugin-for-project-g-assist-hackathon",{},1757463806473]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"🇺🇸",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Français",flag:"🇫🇷",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"简体中文",flag:"🇨🇳",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"Русский",flag:"🇷🇺",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"日本語",flag:"🇯🇵",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"हिंदी",flag:"🇮🇳",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"f7a02a46-9c0e-478e-936c-d4b79733d6b2",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>