<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA&#x27;s Generative AI on RTX PCs Developer Contest</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-05905815]{padding:0}.selected[data-v-05905815]{text-shadow:.25px 0 0 #000}.picker[data-v-05905815]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-05905815]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-1edc4b99]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-de9cb334]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-de9cb334]:hover{transform:scale(1.3)}.centered[data-v-de9cb334]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>span.emoji-mart-emoji[data-v-f26190e6]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-f26190e6]:hover{transform:scale(1.3)}.picker[data-v-f26190e6]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-f26190e6]{background-color:var(--bg)}.localeText[data-v-f26190e6]{color:var(--color-primary)}</style><style>.tag[data-v-09161b57]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-09161b57]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.MFRQdpI0.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest/_payload.json?20ba086c-11db-4b8f-94df-37732bc03fbf"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DTOJyF60.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/gVxYxcS_.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/frsG0wpz.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/yzSkLFeN.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/COAx7XzY.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BJBLwbpI.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/XBFQQbJV.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DtHt2UUL.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Qp6U59fi.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/JB-1fz56.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DnrkkvNu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Df7t3zvh.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CMRTWK2q.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/cMi4VfMl.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CzazSYNa.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/YodgojWX.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cxjqv_Ul.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Bm7EKwoT.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BVBxd0nE.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Bt9I_Efx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CQs7lR2Q.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/20ba086c-11db-4b8f-94df-37732bc03fbf.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BAz4hE9E.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/C6M3awII.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/l4GCCFy-.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Ct1sy6hk.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/OBo_gbKo.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B2C1PNz9.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Bt1fpBmg.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/D3ihcdeA.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DFbNQjaE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BJdiGWz8.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DmShVml6.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DZ6dNqb9.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/C5Po-Rxp.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B75Uq4DY.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DJ54Mi4c.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Bdhy8oLr.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/eCyQPqzC.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BKXkvZfM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BTcBYm-t.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BWTGlsIW.js"><meta name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA's Generative AI on RTX PCs Developer Contest"><meta property="og:description" content="This article discusses my entry for NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat"><meta property="og:image" content="https://briancaffey.github.io/static/rlbc/cover.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/rlbc/cover.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/DTOJyF60.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-05905815><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-05905815><div class="justify-left flex-grow flex-cols-4" data-v-05905815><a href="/" class="text-xl" data-v-05905815><span class="hidden sm:inline text-2xl" data-v-05905815>Brian Caffey</span><span class="inline sm:hidden" data-v-05905815>JBC</span></a></div><div class="flex-grow relative" data-v-05905815><nav z-index="10000" data-v-05905815 data-v-1edc4b99><div data-v-1edc4b99><ul class="items-right float-right hidden md:flex" data-v-1edc4b99><li class="px-4 text-lg" data-v-1edc4b99><a href="/blog/1" class="" data-v-1edc4b99>Blog</a></li><li class="px-4 text-lg" data-v-1edc4b99><a href="/contact" class="" data-v-1edc4b99>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-1edc4b99><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-1edc4b99><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-1edc4b99><title data-v-1edc4b99>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-1edc4b99></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-05905815><div class="centered" data-v-05905815 data-v-de9cb334><div class="grid items-center justify-center" data-v-de9cb334><ul class="flex px-4" data-v-de9cb334><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🖥️, desktop_computer" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🌞, sun_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="🌚, new_moon_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="☕, coffee" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><div data-v-de9cb334 data-v-f26190e6><ul data-v-f26190e6><li class="md:px-1 px-1 cursor-pointer" data-v-f26190e6><span aria-label="🇺🇸, us, flag-us" class="emoji-mart-emoji" data-v-f26190e6><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-f26190e6><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="This article discusses my entry for NVIDIA&#39;s Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat" data-nuxt-img srcset="/_ipx/f_webp/static/rlbc/cover.png 1x, /_ipx/f_webp/static/rlbc/cover.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/rlbc/cover.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA&#39;s Generative AI on RTX PCs Developer Contest</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/nvidia/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>nvidia 🏷️ <!----></div></a><a href="/blog/tags/rtx/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>rtx 🏷️ <!----></div></a><a href="/blog/tags/gpu/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>gpu 🏷️ <!----></div></a><a href="/blog/tags/tensorrt-llm/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>tensorrt-llm 🏷️ <!----></div></a><a href="/blog/tags/ai/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>ai 🏷️ <!----></div></a><a href="/blog/tags/llm/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>llm 🏷️ <!----></div></a><a href="/blog/tags/llama/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>llama 🏷️ <!----></div></a><a href="/blog/tags/rocket-league/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>rocket-league 🏷️ <!----></div></a><a href="/blog/tags/gaming/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>gaming 🏷️ <!----></div></a><a href="/blog/tags/windows/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>windows 🏷️ <!----></div></a><!--]--></div><div class="flex py-2"><!--[--><div class="pr-4 rounded"><a href="https://twitter.com/briancaffey/status/1760529251072118901"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://twitter.com/briancaffey/status/1760529251072118901" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/x.png 25w, /_ipx/w_50&amp;f_webp/icons/x.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/x.png"></a></div><div class="pr-4 rounded"><a href="https://www.reddit.com/r/RocketLeague/comments/1au0po3/rocket_league_botchat_an_llmpowered_bakkesmod/"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://www.reddit.com/r/RocketLeague/comments/1au0po3/rocket_league_botchat_an_llmpowered_bakkesmod/" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/reddit.png 25w, /_ipx/w_50&amp;f_webp/icons/reddit.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/reddit.png"></a></div><div class="pr-4 rounded"><a href="https://dev.to/briancaffey/rocket-league-botchat-powered-by-tensorrt-llm-my-submission-for-nvidias-generative-ai-on-rtx-pcs-developer-contest-2oao"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://dev.to/briancaffey/rocket-league-botchat-powered-by-tensorrt-llm-my-submission-for-nvidias-generative-ai-on-rtx-pcs-developer-contest-2oao" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/dev.png 25w, /_ipx/w_50&amp;f_webp/icons/dev.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/dev.png"></a></div><!--]--></div><p class="blog-date text-gray-500 mb-4">Last updated February 17, 2024</p><!----><div class="markdown"><h2 id="tldr"><a href="#tldr"><!--[-->tl;dr<!--]--></a></h2><p><!--[-->This article is about my submission to NVIDIA&#39;s Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA&#39;s TensorRT-LLM library.<!--]--></p><p><!--[-->Here&#39;s my project submission post on 𝕏:<!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-theme="dark"><p lang="en" dir="ltr">Rocket League BotChat - powered by TensorRT-LLM<br>⚽️🚗⚡️🤖💬<br>My submission for NVIDIA&#39;s Gen AI on RTX PCs Developer Contest!<a href="https://twitter.com/hashtag/GenAIonRTX?src=hash&amp;ref_src=twsrc%5Etfw">#GenAIonRTX</a> <a href="https://twitter.com/hashtag/DevContest?src=hash&amp;ref_src=twsrc%5Etfw">#DevContest</a> <a href="https://twitter.com/hashtag/GTC24?src=hash&amp;ref_src=twsrc%5Etfw">#GTC24</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://twitter.com/hashtag/RocketLeague?src=hash&amp;ref_src=twsrc%5Etfw">#RocketLeague</a> <a href="https://twitter.com/hashtag/LLM?src=hash&amp;ref_src=twsrc%5Etfw">#LLM</a> <a href="https://twitter.com/hashtag/Llama?src=hash&amp;ref_src=twsrc%5Etfw">#Llama</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Windows11?src=hash&amp;ref_src=twsrc%5Etfw">#Windows11</a> <a href="https://t.co/4H8u3KpQ6G">pic.twitter.com/4H8u3KpQ6G</a></p>— Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1760529251072118901?ref_src=twsrc%5Etfw">February 22, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->Here&#39;s a link to the <a href="https://github.com/briancaffey/RocketLeagueBotChat" rel="nofollow"><!--[-->Rocket League BotChat GitHub repository<!--]--></a>.<!--]--></p><h2 id="nvidias-gen-ai-developer-contest"><a href="#nvidias-gen-ai-developer-contest"><!--[-->NVIDIA&#39;s Gen AI Developer Contest<!--]--></a></h2><p><!--[-->The following email caught my attention last month:<!--]--></p><blockquote><!--[--><p><!--[-->Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems<!--]--></p><!--]--></blockquote><p><!--[-->The part about “on Windows PC” made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had also never used any of the Windows-specific development frameworks like .NET or Visual Studio.<!--]--></p><p><!--[-->My experience with Windows goes back to 2016 when I built my fist PC with an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives in order to dual boot into both Windows and Ubuntu operating systems. Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer.<!--]--></p><p><!--[-->Last year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my PC&#39;s Ubuntu operating system, and frequently had to switch between using Ubuntu for working with stable diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems pushed me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy.<!--]--></p><p><!--[-->I learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently compared ot Linux-based operating systems. And just like that, I became a Windows developer! This experience got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Windows winfetch screenshot" data-nuxt-img srcset="/_ipx/_/static/rlbc/winfetch.png 1x, /_ipx/_/static/rlbc/winfetch.png 2x" src="/_ipx/_/static/rlbc/winfetch.png"><!--]--></p><h2 id="coming-up-with-an-idea"><a href="#coming-up-with-an-idea"><!--[-->Coming up with an Idea<!--]--></a></h2><p><!--[-->The contest description and some related NVIDIA articles about the contest helped me with brainstorming:<!--]--></p><blockquote><!--[--><p><!--[-->Whether it’s a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless.<!--]--></p><!--]--></blockquote><blockquote><!--[--><p><!--[-->Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.<!--]--></p><!--]--></blockquote><p><!--[-->This contest is focused on NVIDIA&#39;s consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software, creative tools and new AI techniques including image generation and LLM (Large Language Model) inference.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="A stacked bar chart showing the composition of Nvidia&#39;s revenue each quarter going back to fiscal 2019." data-nuxt-img srcset="https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&amp;op=resize&amp;w=700 1x, https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&amp;op=resize&amp;w=700 2x" src="https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&amp;op=resize&amp;w=700"><!--]--></p><p><!--[-->Gaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don&#39;t play these types of games and it didn&#39;t seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know.<!--]--></p><p><!--[-->Rocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and all got hooked. We also came to learn that Rocket League&#39;s in-game is varies from entertaining, annoying, toxic and in some cases, sportsmanlike.<!--]--></p><p><!--[-->One other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas &quot;bakkes&quot; Bakke, a Norwegian software engineer). Rocket League&#39;s in-game chat inspired the idea for my submission to NVIDIA&#39;s Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)<!--]--></p><p><!--[-->Bots are built into the Rocket League game and you can play with or against them in offline matches. However, the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow and PyTorch. These bots are very good, but they are not infallible. My contest project idea was now clear: develop a plugin for Rocket League capable of sending messages from bot players. This idea seemed to check the boxes for the large language model category of NVIDIA&#39;s developer contest: develop a project in a Windows environment for a Windows-specific program, and use an LLM powered by TensorRT-LLM.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RLBot Ascii Art" data-nuxt-img srcset="/_ipx/_/static/rlbc/bot.png 1x, /_ipx/_/static/rlbc/bot.png 2x" src="/_ipx/_/static/rlbc/bot.png"><!--]--></p><h2 id="putting-together-the-puzzle-pieces"><a href="#putting-together-the-puzzle-pieces"><!--[-->Putting together the puzzle pieces<!--]--></a></h2><p><!--[-->With this idea in mind, I looked into the project&#39;s feasibility. I really had no idea if this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept.<!--]--></p><ul><!--[--><li><!--[-->The Bakkesmod Plugin Wiki <a href="https://wiki.bakkesplugins.com/" rel="nofollow"><!--[-->https://wiki.bakkesplugins.com/<!--]--></a><ul><!--[--><li><!--[--><a href="https://wiki.bakkesplugins.com/code_snippets/using_http_wrapper/" rel="nofollow"><!--[--><code><!--[-->HttpWrapper<!--]--></code><!--]--></a> for sending HTTP requests from Bakkesmod<!--]--></li><li><!--[--><a href="https://wiki.bakkesplugins.com/functions/stat_events/" rel="nofollow"><!--[--><code><!--[-->StatEvents<!--]--></code><!--]--></a> that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save).<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->The Bakkesmod plugin template: <a href="https://github.com/Martinii89/BakkesmodPluginTemplate" rel="nofollow"><!--[-->https://github.com/Martinii89/BakkesmodPluginTemplate<!--]--></a><ul><!--[--><li><!--[-->This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly<!--]--></li><!--]--></ul><!--]--></li><li><!--[-->Plugin Tutorial: <a href="https://wiki.bakkesplugins.com/plugin_tutorial/getting_started/" rel="nofollow"><!--[-->https://wiki.bakkesplugins.com/plugin_tutorial/getting_started/<!--]--></a><!--]--></li><li><!--[-->Open-source chat-related Bakkesmod plugins on GitHub
<ul><!--[--><li><!--[-->BetterChat: <a href="https://github.com/JulienML/BetterChat" rel="nofollow"><!--[-->https://github.com/JulienML/BetterChat<!--]--></a><!--]--></li><li><!--[-->Translate: <a href="https://github.com/0xleft/trnslt" rel="nofollow"><!--[-->https://github.com/0xleft/trnslt<!--]--></a><!--]--></li><!--]--></ul><!--]--></li><!--]--></ul><p><!--[-->Starting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to <code><!--[-->localhost:8000/hello<!--]--></code>. I set up a Hello World Flask app running on <code><!--[-->localhost:8000<!--]--></code> and I was able to get a response from my Hello World server! There didn&#39;t seem to be any network or permission errors that would prevent my game code from communicating with other applications on my PC.<!--]--></p><p><!--[-->Next I started looking into how to build and run optimized LLMs with NVIDIA&#39;s TensorRT-LLM library, the software that this contest is promoting. The contest announcement included an interesting building block that I thought could be very useful: an example repo showing how to run <code><!--[-->CodeLlama-13b-instruct-hf<!--]--></code> optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev).<!--]--></p><ul><!--[--><li><!--[--><code><!--[-->CodeLlama-13b-instruct-hf<!--]--></code> is an open source model from Meta that is trained on code and can help with code generation tasks<!--]--></li><li><!--[-->TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like <code><!--[-->CodeLlama-13b-instruct-hf<!--]--></code> and generates an engine that can be used for doing inference<!--]--></li><li><!--[-->VSCode is an open source code editor developed by Microsoft with an large number of community plugins<!--]--></li><li><!--[-->Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT<!--]--></li><!--]--></ul><p><!--[-->To get the coding assistant project working I needed to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:<!--]--></p><ul><!--[--><li><!--[-->using a &quot;bare-metal&quot; virtual environment on Windows (with PowerShell)<!--]--></li><li><!--[-->using WSL<!--]--></li><!--]--></ul><p><!--[-->At the time of writing, building a TensorRT-LLM engine on Windows can only be done with version <code><!--[-->v0.6.1<!--]--></code> of the TensorRT-LLM repo and version <code><!--[-->v0.7.1<!--]--></code> of the <code><!--[-->tensorrt_llm<!--]--></code> Python package.<!--]--></p><p><!--[-->With WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system.<!--]--></p><p><!--[-->Once the engines are built you can use them to run the example from the <code><!--[-->trt-llm-as-openai-windows<!--]--></code> repo.<!--]--></p><p><!--[-->The example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:<!--]--></p><pre class="language-json shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">{
</span></span><span class="line" line="2"><span class="s-m8C">  &quot;title&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;CodeLlama-13b-instruct-hf&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="3"><span class="s-m8C">  &quot;apiBase&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;http://192.168.5.96:5000/&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="4"><span class="s-m8C">  &quot;provider&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;openai&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="5"><span class="s-m8C">  &quot;apiKey&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;None&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="6"><span class="s-m8C">  &quot;model&quot;</span><span class="sMOD_">: </span><span class="sCZoN">&quot;gpt-4&quot;
</span></span><span class="line" line="7"><span class="sMOD_">}
</span></span></code><!--]--></pre><p><!--[-->The Continue.dev extension using <code><!--[-->CodeLlama-13b-instruct-hf<!--]--></code> accelerated and optimized by TensorRT-LLM is very fast. According to <a href="https://blog.continue.dev/programming-languages/" rel="nofollow"><!--[-->this post on Continue.dev&#39;s blog<!--]--></a>, C++ is a &quot;first tier&quot; language:<!--]--></p><blockquote><!--[--><p><!--[-->C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode / TP3, MBXP / Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don’t believe that LLMs are as helpful for C++ as some of the other languages in this tier.<!--]--></p><!--]--></blockquote><p><!--[-->Most of the time I&#39;m working with either Python and TypeScript. I&#39;ve read about C++ but haven&#39;t used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with the Continue.dev autopilot extension was helpful for tackling smaller problems in a REPL-like environment. For example, I used Continue.dev in VSCode to figure out how to handle JSON. Coming from Python and JavaScript languages, I found the <code><!--[-->nlohmann/json<!--]--></code> JSON library syntax to be somewhat different. For example, here is how to add a message to <code><!--[-->messages<!--]--></code> in the body of an OpenAI API request:<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">messages.</span><span class="srTi1">push_back</span><span class="sMOD_">({ {</span><span class="sstjo">&quot;role&quot;</span><span class="sMOD_">, role}, {</span><span class="sstjo">&quot;content&quot;</span><span class="sMOD_">, content } });
</span></span></code><!--]--></pre><p><!--[-->In Python the code for appending a message to a list of messages would be written differently:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">messages.append({</span><span class="sstjo">&quot;role&quot;</span><span class="sMOD_">: role, </span><span class="sstjo">&quot;content&quot;</span><span class="sMOD_">: content})
</span></span></code><!--]--></pre><h2 id="development-environment"><a href="#development-environment"><!--[-->Development environment<!--]--></a></h2><p><!--[-->While working on different projects using web technologies and frameworks in the Python and JavaScript ecosystems, I developed an appreciation for well-structured development environments that are easy to use. Development environment refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running on a local environment. The local environment (the developer&#39;s computer) should be a close proxy for the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality, offering a tight feedback loop. I really like the development environment for this project. Here&#39;s a screenshot that shows the different parts of the development environment I used for working on Rocket League BotChat:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Screenshot of Rocket League BotChat development environment" data-nuxt-img srcset="/_ipx/_/static/rlbc/devenv2.png 1x, /_ipx/_/static/rlbc/devenv2.png 2x" src="/_ipx/_/static/rlbc/devenv2.png"><!--]--></p><ul><!--[--><li><!--[-->Rocket League (running with the <code><!--[-->-dev<!--]--></code> flag turned on). The console is helpful for viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine<!--]--></li><li><!--[-->Visual Studio for working on the plugin code. <code><!--[-->Control<!--]--></code>+<code><!--[-->Shift<!--]--></code>+<code><!--[-->B<!--]--></code> rebuilds the code and automatically reloads the plugin in the game<!--]--></li><li><!--[-->OpenAI-compatible LLM server powered by TensorRT-LLM (using <code><!--[-->Llama-2-13b-chat-hf<!--]--></code> with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL<!--]--></li><li><!--[-->VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using <code><!--[-->CodeLlama-13b-instruct-hf<!--]--></code> with AWQ INT4 quantization) running in a virtual environment on Windows<!--]--></li><!--]--></ul><h3 id="building-the-tensorrt-llm-engines"><a href="#building-the-tensorrt-llm-engines"><!--[-->Building the TensorRT-LLM engines<!--]--></a></h3><p><!--[-->I was able to build and run the TensorRT LLM engines for my game plugin&#39;s inference and the Continue.dev extension&#39;s inference both in Python virtual environments on Windows and on Ubuntu in WSL. For building the <code><!--[-->Llama-2-13b-chat-hf<!--]--></code> model with INT4 AWQ quantization on Windows 11 I used this command:<!--]--></p><pre class="language-powershell shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">(.venv) PS C:\Users\My PC\GitHub\TensorRT</span><span class="sC2Qs">-</span><span class="sMOD_">LLM\examples\llama</span><span class="sC2Qs">&gt;</span><span class="sMOD_"> python build.py </span><span class="sC2Qs">--</span><span class="sMOD_">model_dir D:\llama\Llama</span><span class="sC2Qs">-</span><span class="s7F3e">2</span><span class="sC2Qs">-</span><span class="sMOD_">13b</span><span class="sC2Qs">-</span><span class="sMOD_">chat</span><span class="sC2Qs">-</span><span class="sMOD_">hf\ </span><span class="sC2Qs">--</span><span class="sMOD_">quant_ckpt_path D:\llama\Llama</span><span class="sC2Qs">-</span><span class="s7F3e">2</span><span class="sC2Qs">-</span><span class="sMOD_">13b</span><span class="sC2Qs">-</span><span class="sMOD_">chat</span><span class="sC2Qs">-</span><span class="sMOD_">hf\llama_tp1_rank0.npz </span><span class="sC2Qs">--</span><span class="sMOD_">dtype float16 </span><span class="sC2Qs">--</span><span class="sMOD_">use_gpt_attention_plugin float16 </span><span class="sC2Qs">--</span><span class="sMOD_">use_gemm_plugin float16 </span><span class="sC2Qs">--</span><span class="sMOD_">use_weight_only </span><span class="sC2Qs">--</span><span class="sMOD_">weight_only_precision int4_awq </span><span class="sC2Qs">--</span><span class="sMOD_">per_group </span><span class="sC2Qs">--</span><span class="sMOD_">enable_context_fmha </span><span class="sC2Qs">--</span><span class="sMOD_">max_batch_size </span><span class="s7F3e">1</span><span class="sC2Qs"> --</span><span class="sMOD_">max_input_len </span><span class="s7F3e">3500</span><span class="sC2Qs"> --</span><span class="sMOD_">max_output_len </span><span class="s7F3e">1024</span><span class="sC2Qs"> --</span><span class="sMOD_">output_dir D:\llama\Llama</span><span class="sC2Qs">-</span><span class="s7F3e">2</span><span class="sC2Qs">-</span><span class="sMOD_">13b</span><span class="sC2Qs">-</span><span class="sMOD_">chat</span><span class="sC2Qs">-</span><span class="sMOD_">hf\single</span><span class="sC2Qs">-</span><span class="sMOD_">gpu\ </span><span class="sC2Qs">--</span><span class="sMOD_">vocab_size </span><span class="s7F3e">32064
</span></span></code><!--]--></pre><h3 id="running-the-tensorrt-llm-engines"><a href="#running-the-tensorrt-llm-engines"><!--[-->Running the TensorRT-LLM engines<!--]--></a></h3><p><!--[-->Using Windows PowerShell to start the CodeLlama server for Continue.dev:<!--]--></p><pre class="language-powershell shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">(.venv) PS C:\Users\My PC\GitHub\trt</span><span class="sC2Qs">-</span><span class="sMOD_">llm</span><span class="sC2Qs">-</span><span class="sMOD_">as</span><span class="sC2Qs">-</span><span class="sMOD_">openai</span><span class="sC2Qs">-</span><span class="sMOD_">windows</span><span class="sC2Qs">&gt;</span><span class="sMOD_"> python .\app.py </span><span class="sC2Qs">--</span><span class="sMOD_">trt_engine_path </span><span class="sstjo">&quot;D:\llama\CodeLlama-13b-Instruct-hf\trt_engines\1-gpu\&quot;</span><span class="sC2Qs"> --</span><span class="sMOD_">trt_engine_name llama_float16_tp1_rank0.engine </span><span class="sC2Qs">--</span><span class="sMOD_">tokenizer_dir_path </span><span class="sstjo">&quot;D:\llama\CodeLlama-13b-Instruct-hf\&quot;</span><span class="sC2Qs"> --</span><span class="sMOD_">port </span><span class="s7F3e">5000</span><span class="sC2Qs"> --</span><span class="sMOD_">host </span><span class="s7F3e">0.0</span><span class="sMOD_">.</span><span class="s7F3e">0.0
</span></span></code><!--]--></pre><p><!--[-->Tip: Adding <code><!--[-->--host 0.0.0.0<!--]--></code> isn&#39;t required here, but it allows me to use the CodeLlama/TensorRT-LLM server with VSCode any computer on my local network using my PC&#39;s local IP address in the Continue.dev configuration.<!--]--></p><p><!--[-->Using docker in WSL to start the Llama-2-13b-chat-hf LLM server:<!--]--></p><pre class="language-text"><!--[--><code>root@0a5b5b75f079:/code/git/TensorRT-LLM/examples/server/flask# python3 app.py --trt_engine_path /llama/Llama-2-13b-chat-hf/trt_engines/1-gpu/ --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path /llama/Llama-2-13b-chat-hf/ --port 5001 --host 0.0.0.0
</code><!--]--></pre><p><!--[-->Note: Here I also add <code><!--[-->--host 0.0.0.0<!--]--></code>, but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows.<!--]--></p><p><!--[-->BakkesMod includes a console window that came in handy for debugging errors during development.<!--]--></p><p><!--[-->At the beginning of this developer contest on January 9, NVIDIA announced Chat with RTX. This is a demo program for Windows that automates a lots of the processes needed to set up a TensorRT-LLM-powered LLM running on your PC. Keep an eye on this project as it may become the best way to install and manage large language models on Windows PCs.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Chat with RTX image" data-nuxt-img srcset="/_ipx/_/static/rlbc/chat_with_rtx.jpeg 1x, /_ipx/_/static/rlbc/chat_with_rtx.jpeg 2x" src="/_ipx/_/static/rlbc/chat_with_rtx.jpeg"><!--]--></p><h2 id="how-it-works"><a href="#how-it-works"><!--[-->How it works<!--]--></a></h2><p><!--[-->Here&#39;s a quick look at key parts of the plugin source code (<a href="https://github.com/briancaffey/RocketLeagueBotChat" rel="nofollow"><!--[-->https://github.com/briancaffey/RocketLeagueBotChat<!--]--></a>).<!--]--></p><h3 id="hooking-events"><a href="#hooking-events"><!--[-->Hooking events<!--]--></a></h3><p><!--[-->Hooking events is the core of how this plugin works. <code><!--[-->StatTickerMessage<!--]--></code> events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car.<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="s8-w5">    // Hooks different types of events that are handled in onStatTickerMessage
</span></span><span class="line" line="2"><span class="s8-w5">    // See https://wiki.bakkesplugins.com/functions/stat_events/
</span></span><span class="line" line="3"><span class="sMOD_">    gameWrapper-&gt;HookEventWithCallerPost</span><span class="sC2Qs">&lt;</span><span class="sMOD_">ServerWrapper</span><span class="sC2Qs">&gt;</span><span class="sMOD_">(</span><span class="sstjo">&quot;Function TAGame.GFxHUD_TA.HandleStatTickerMessage&quot;</span><span class="sMOD_">,
</span></span><span class="line" line="4"><span class="sMOD_">        [</span><span class="sP7S_">this</span><span class="sMOD_">](</span><span class="sz2Vg">ServerWrapper</span><span class="sTHNf"> caller</span><span class="sMOD_">, </span><span class="sq6CD">void</span><span class="sC2Qs">*</span><span class="sTHNf"> params</span><span class="sMOD_">, </span><span class="sz2Vg">std</span><span class="sMOD_">::</span><span class="sz2Vg">string</span><span class="sTHNf"> eventname</span><span class="sMOD_">) {
</span></span><span class="line" line="5"><span class="srTi1">            onStatTickerMessage</span><span class="sMOD_">(params);
</span></span><span class="line" line="6"><span class="sMOD_">        });
</span></span></code><!--]--></pre><h3 id="handling-events-and-building-the-prompt"><a href="#handling-events-and-building-the-prompt"><!--[-->Handling events and building the prompt<!--]--></a></h3><p><!--[-->We can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the <code><!--[-->appendToPrompt<!--]--></code> method.<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="s8-w5">    // handle different events like scoring a goal or making a save
</span></span><span class="line" line="2"><span class="sC2Qs">    if</span><span class="sMOD_"> (statEvent.</span><span class="srTi1">GetEventName</span><span class="sMOD_">() </span><span class="sC2Qs">==</span><span class="sstjo"> &quot;Goal&quot;</span><span class="sMOD_">) {
</span></span><span class="line" line="3"><span emptylineplaceholder="true">
</span></span><span class="line" line="4"><span class="s8-w5">        // was the goal scored by the human player or the bot?
</span></span><span class="line" line="5"><span class="sC2Qs">        if</span><span class="sMOD_"> (playerPRI.memory_address </span><span class="sC2Qs">==</span><span class="sMOD_"> receiver.memory_address) {
</span></span><span class="line" line="6"><span class="srTi1">            appendToPrompt</span><span class="sMOD_">(</span><span class="sstjo">&quot;Your human opponent just scored a goal against you! &quot;</span><span class="sC2Qs"> +</span><span class="sMOD_"> score_sentence, </span><span class="sstjo">&quot;user&quot;</span><span class="sMOD_">);
</span></span><span class="line" line="7"><span class="sMOD_">        }
</span></span><span class="line" line="8"><span class="sC2Qs">        else</span><span class="sMOD_"> {
</span></span><span class="line" line="9"><span class="srTi1">            appendToPrompt</span><span class="sMOD_">(</span><span class="sstjo">&quot;You just scored a goal against the human player! &quot;</span><span class="sC2Qs"> +</span><span class="sMOD_"> score_sentence, </span><span class="sstjo">&quot;user&quot;</span><span class="sMOD_">);
</span></span><span class="line" line="10"><span class="sMOD_">        }
</span></span><span class="line" line="11"><span class="sMOD_">    }
</span></span></code><!--]--></pre><h3 id="making-requests-and-handling-responses"><a href="#making-requests-and-handling-responses"><!--[-->Making requests and handling responses<!--]--></a></h3><p><!--[-->The last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI&#39;s API.<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sz2Vg">std</span><span class="sMOD_">::string message </span><span class="sC2Qs">=</span><span class="sMOD_"> response_json[</span><span class="sstjo">&quot;choices&quot;</span><span class="sMOD_">][</span><span class="s7F3e">0</span><span class="sMOD_">][</span><span class="sstjo">&quot;message&quot;</span><span class="sMOD_">][</span><span class="sstjo">&quot;content&quot;</span><span class="sMOD_">];
</span></span></code><!--]--></pre><p><!--[-->The <code><!--[-->LogToChatbox<!--]--></code> method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time.<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sMOD_">gameWrapper-&gt;</span><span class="srTi1">LogToChatbox</span><span class="sMOD_">(messages[i], </span><span class="sP7S_">this</span><span class="sMOD_">-&gt;bot_name);
</span></span></code><!--]--></pre><p><!--[-->That&#39;s it! The code isn&#39;t that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages (<code><!--[-->&lt;/s&gt;<!--]--></code>). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt.<!--]--></p><h2 id="rocket-league-botchat-ui"><a href="#rocket-league-botchat-ui"><!--[-->Rocket League BotChat UI<!--]--></a></h2><p><!--[-->Most BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here&#39;s what the UI for Rocket League BotChat looks like:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Rocket League BotChat Plugin UI" data-nuxt-img srcset="/_ipx/_/static/rlbc/rlbcui.png 1x, /_ipx/_/static/rlbc/rlbcui.png 2x" src="/_ipx/_/static/rlbc/rlbcui.png"><!--]--></p><h3 id="system-prompt"><a href="#system-prompt"><!--[-->System prompt<!--]--></a></h3><p><!--[-->The system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here&#39;s the default prompt that I used:<!--]--></p><pre class="language-cpp shiki shiki-themes github-light github-dark monokai" style=""><!--[--><code><span class="line" line="1"><span class="sz2Vg">    std</span><span class="sMOD_">::string ai_player </span><span class="sC2Qs">=</span><span class="sstjo"> &quot;You are an elite AI player in the car soccer game Rocket League. &quot;</span><span class="sMOD_">;
</span></span><span class="line" line="2"><span class="sz2Vg">    std</span><span class="sMOD_">::string one_v_one </span><span class="sC2Qs">=</span><span class="sstjo"> &quot;You are playing a 1v1 match against a human player. &quot;</span><span class="sMOD_">;
</span></span><span class="line" line="3"><span class="sz2Vg">    std</span><span class="sMOD_">::string instructions </span><span class="sC2Qs">=</span><span class="sstjo"> &quot;You will send short chat messages to your human opponent in response to what happens in the game. &quot;</span><span class="sMOD_">;
</span></span><span class="line" line="4"><span class="sz2Vg">    std</span><span class="sMOD_">::string details </span><span class="sC2Qs">=</span><span class="sstjo"> &quot;Respond to the human player with brief messages no more than 12 words long.&quot;</span><span class="sMOD_">;
</span></span><span class="line" line="5"><span class="s8-w5">    // initial system prompt
</span></span><span class="line" line="6"><span class="sz2Vg">    std</span><span class="sMOD_">::string initial_system_prompt </span><span class="sC2Qs">=</span><span class="sMOD_"> ai_player </span><span class="sC2Qs">+</span><span class="sMOD_"> one_v_one </span><span class="sC2Qs">+</span><span class="sMOD_"> instructions </span><span class="sC2Qs">+</span><span class="sMOD_"> details;
</span></span></code><!--]--></pre><p><!--[-->The last part about <code><!--[-->no more than 12 words long<!--]--></code> was the most effective way of controlling the length responses from the LLM. I tried changing the <code><!--[-->max_output_len<!--]--></code> when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user. Changing the system prompt was a lot of fun to expirment with!<!--]--></p><h3 id="temperature-and-seed"><a href="#temperature-and-seed"><!--[-->Temperature and Seed<!--]--></a></h3><p><!--[-->These values are included in the body of the request to the LLM, but I didn&#39;t have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn&#39;t really work.<!--]--></p><h3 id="messages"><a href="#messages"><!--[-->Messages<!--]--></a></h3><p><!--[-->This section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the &quot;user&quot; (which are messages about game events) and the &quot;assistant&quot; (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt.<!--]--></p><h2 id="demo-video-for-contest-submission"><a href="#demo-video-for-contest-submission"><!--[-->Demo Video for Contest Submission<!--]--></a></h2><!--[--><blockquote class="twitter-tweet tw-align-center" data-media-max-width="560"><p lang="en" dir="ltr">Rocket League BotChat - powered by TensorRT-LLM<br>⚽️🚗⚡️🤖💬<br>My submission for NVIDIA&#39;s Gen AI on RTX PCs Developer Contest!<a href="https://twitter.com/hashtag/GenAIonRTX?src=hash&amp;ref_src=twsrc%5Etfw">#GenAIonRTX</a> <a href="https://twitter.com/hashtag/DevContest?src=hash&amp;ref_src=twsrc%5Etfw">#DevContest</a> <a href="https://twitter.com/hashtag/GTC24?src=hash&amp;ref_src=twsrc%5Etfw">#GTC24</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://twitter.com/hashtag/RocketLeague?src=hash&amp;ref_src=twsrc%5Etfw">#RocketLeague</a> <a href="https://twitter.com/hashtag/LLM?src=hash&amp;ref_src=twsrc%5Etfw">#LLM</a> <a href="https://twitter.com/hashtag/Llama?src=hash&amp;ref_src=twsrc%5Etfw">#Llama</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Windows11?src=hash&amp;ref_src=twsrc%5Etfw">#Windows11</a> <a href="https://t.co/4H8u3KpQ6G">pic.twitter.com/4H8u3KpQ6G</a></p>— Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1760529251072118901?ref_src=twsrc%5Etfw">February 22, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->I used Blender&#39;s sequence editor to create a demo video for my contest submission. I don&#39;t edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here&#39;s how I approached creating the demo video for my project.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Blender video sequence editor UI used to create my project video" data-nuxt-img srcset="/_ipx/_/static/rlbc/blender.png 1x, /_ipx/_/static/rlbc/blender.png 2x" src="/_ipx/_/static/rlbc/blender.png"><!--]--></p><ul><!--[--><li><!--[-->Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action<!--]--></li><li><!--[-->Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used <a href="https://open.spotify.com/track/68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51" rel="nofollow"><!--[-->&quot;Dads in Space&quot; by Steven Walking<!--]--></a><!--]--></li><li><!--[-->Get stock Rocket League footage from YouTube with <code><!--[-->youtube-dl<!--]--></code> (this is an amazing tool!). I mostly used footage from the <a href="https://www.youtube.com/watch?v=e1tqWldCYOI&amp;pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D" rel="nofollow"><!--[-->RLCS 2023 Winter Major Trailer<!--]--></a>. This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps.<!--]--></li><li><!--[-->Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!<!--]--></li><li><!--[-->Use ComfyUI to animate some of the images used in the contest description and use these in my video<!--]--></li><!--]--></ul><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="ComfyUI workflow for animating images using img2vid model" data-nuxt-img srcset="/_ipx/_/static/rlbc/comfyui.png 1x, /_ipx/_/static/rlbc/comfyui.png 2x" src="/_ipx/_/static/rlbc/comfyui.png"><!--]--></p><ul><!--[--><li><!--[-->Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice.<!--]--></li><!--]--></ul><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Eleven Labs Voice Generation Web UI" data-nuxt-img srcset="/_ipx/_/static/rlbc/elevenlabs.png 1x, /_ipx/_/static/rlbc/elevenlabs.png 2x" src="/_ipx/_/static/rlbc/elevenlabs.png"><!--]--></p><p><!--[--><a href="#"><!--[-->Embed twitter video here<!--]--></a><!--]--></p><h2 id="shortcomings-of-my-project"><a href="#shortcomings-of-my-project"><!--[-->Shortcomings of my project<!--]--></a></h2><p><!--[-->This plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in &quot;user&quot; and &quot;assistant&quot; prompts getting out of order which breaks assertions on the <code><!--[-->trt-llm-as-openai-windows<!--]--></code> repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM.<!--]--></p><p><!--[-->There are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like &quot;shot on goal&quot; to avoid prompt messages getting out of order. This could also be addressed with more code logic.<!--]--></p><p><!--[-->Prompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example.<!--]--></p><p><!--[-->I faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like <code><!--[-->SendMessage Good shot, bot!<!--]--></code>, for example.<!--]--></p><h2 id="whats-next"><a href="#whats-next"><!--[-->What&#39;s next?<!--]--></a></h2><p><!--[-->Participating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a &quot;Rocket League Coach&quot; plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc.<!--]--></p><p><!--[-->I think the gaming industry&#39;s adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I&#39;m excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM.<!--]--></p><p><!--[-->Thanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!<!--]--></p><style>html pre.shiki code .sMOD_, html code.shiki .sMOD_{--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#F8F8F2}html pre.shiki code .s-m8C, html code.shiki .s-m8C{--shiki-default:#005CC5;--shiki-default-font-style:inherit;--shiki-dark:#79B8FF;--shiki-dark-font-style:inherit;--shiki-sepia:#66D9EF;--shiki-sepia-font-style:italic}html pre.shiki code .sCZoN, html code.shiki .sCZoN{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#CFCFC2}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html.sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html pre.shiki code .sstjo, html code.shiki .sstjo{--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#E6DB74}html pre.shiki code .srTi1, html code.shiki .srTi1{--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E}html pre.shiki code .sC2Qs, html code.shiki .sC2Qs{--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#F92672}html pre.shiki code .s7F3e, html code.shiki .s7F3e{--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#AE81FF}html pre.shiki code .s8-w5, html code.shiki .s8-w5{--shiki-default:#6A737D;--shiki-dark:#6A737D;--shiki-sepia:#88846F}html pre.shiki code .sP7S_, html code.shiki .sP7S_{--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#FD971F}html pre.shiki code .sz2Vg, html code.shiki .sz2Vg{--shiki-default:#6F42C1;--shiki-default-text-decoration:inherit;--shiki-dark:#B392F0;--shiki-dark-text-decoration:inherit;--shiki-sepia:#A6E22E;--shiki-sepia-text-decoration:underline}html pre.shiki code .sTHNf, html code.shiki .sTHNf{--shiki-default:#E36209;--shiki-default-font-style:inherit;--shiki-dark:#FFAB70;--shiki-dark-font-style:inherit;--shiki-sepia:#FD971F;--shiki-sepia-font-style:italic}html pre.shiki code .sq6CD, html code.shiki .sq6CD{--shiki-default:#D73A49;--shiki-default-font-style:inherit;--shiki-dark:#F97583;--shiki-dark-font-style:inherit;--shiki-sepia:#66D9EF;--shiki-sepia-font-style:italic}</style></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments 💬 </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> © 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest/_payload.json?20ba086c-11db-4b8f-94df-37732bc03fbf">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest":-1},"/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",{},1753140970265]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"🇺🇸",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Français",flag:"🇫🇷",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"简体中文",flag:"🇨🇳",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"Русский",flag:"🇷🇺",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"日本語",flag:"🇯🇵",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"हिंदी",flag:"🇮🇳",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"20ba086c-11db-4b8f-94df-37732bc03fbf",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>