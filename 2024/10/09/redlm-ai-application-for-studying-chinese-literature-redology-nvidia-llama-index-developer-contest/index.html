<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>RedLM: My submission for the NVIDIA and LlamaIndex Developer Contest</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-d2ff5fdf]{padding:0}.selected[data-v-d2ff5fdf]{text-shadow:.25px 0 0 #000}.picker[data-v-d2ff5fdf]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-d2ff5fdf]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-ed0088f5]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-2649ce61]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-2649ce61]:hover{transform:scale(1.3)}.centered[data-v-2649ce61]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>.emoji-mart,.emoji-mart *{-webkit-box-sizing:border-box;box-sizing:border-box;line-height:1.15}.emoji-mart{display:-webkit-box;display:-ms-flexbox;display:flex;font-family:-apple-system,BlinkMacSystemFont,Helvetica Neue,sans-serif;font-size:16px;-webkit-box-orient:vertical;-webkit-box-direction:normal;background:#fff;border:1px solid #d9d9d9;border-radius:5px;color:#222427;-ms-flex-direction:column;flex-direction:column;height:420px}.emoji-mart-emoji{background:none;border:none;-webkit-box-shadow:none;box-shadow:none;padding:6px}.emoji-mart-emoji span{display:inline-block}.emoji-mart-preview-emoji .emoji-mart-emoji span{font-size:32px;height:38px;width:38px}.emoji-type-native{font-family:Segoe UI Emoji,Segoe UI Symbol,Segoe UI,Apple Color Emoji,Twemoji Mozilla,Noto Color Emoji,EmojiOne Color,Android Emoji;word-break:keep-all}.emoji-type-image{background-size:6100%}.emoji-type-image.emoji-set-apple{background-image:url(https://unpkg.com/emoji-datasource-apple@15.0.1/img/apple/sheets-256/64.png)}.emoji-type-image.emoji-set-facebook{background-image:url(https://unpkg.com/emoji-datasource-facebook@15.0.1/img/facebook/sheets-256/64.png)}.emoji-type-image.emoji-set-google{background-image:url(https://unpkg.com/emoji-datasource-google@15.0.1/img/google/sheets-256/64.png)}.emoji-type-image.emoji-set-twitter{background-image:url(https://unpkg.com/emoji-datasource-twitter@15.0.1/img/twitter/sheets-256/64.png)}.emoji-mart-bar{border:0 solid #d9d9d9}.emoji-mart-bar:first-child{border-bottom-width:1px;border-top-left-radius:5px;border-top-right-radius:5px}.emoji-mart-bar:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-top-width:1px}.emoji-mart-scroll{overflow-y:scroll;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-anchors{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;color:#858585;justify-content:space-between;line-height:0;padding:0 6px}.emoji-mart-anchor{display:block;position:relative;-webkit-box-flex:1;background:none;border:none;-webkit-box-shadow:none;box-shadow:none;-ms-flex:1 1 auto;flex:1 1 auto;overflow:hidden;padding:12px 4px;text-align:center;-webkit-transition:color .1s ease-out;transition:color .1s ease-out}.emoji-mart-anchor-selected,.emoji-mart-anchor:hover{color:#464646}.emoji-mart-anchor-selected .emoji-mart-anchor-bar{bottom:0}.emoji-mart-anchor-bar{background-color:#464646;bottom:-3px;height:3px;left:0;position:absolute;width:100%}.emoji-mart-anchors i{display:inline-block;max-width:22px;width:100%}.emoji-mart-anchors svg{fill:currentColor;max-height:18px}.emoji-mart .scroller{height:250px;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-search{margin-top:6px;padding:0 6px}.emoji-mart-search input{border:1px solid #d9d9d9;border-radius:25px;display:block;font-size:16px;outline:0;padding:.2em .6em;width:100%}.emoji-mart-search-results{height:250px;overflow-y:scroll}.emoji-mart-category{position:relative}.emoji-mart-category .emoji-mart-emoji span{cursor:default;position:relative;text-align:center;z-index:1}.emoji-mart-category .emoji-mart-emoji:hover:before,.emoji-mart-emoji-selected:before{background-color:#f4f4f4;border-radius:100%;content:"";height:100%;left:0;opacity:1;position:absolute;top:0;width:100%;z-index:0}.emoji-mart-category-label{position:-webkit-sticky;position:sticky;top:0}.emoji-mart-static .emoji-mart-category-label{position:relative;z-index:2}.emoji-mart-category-label h3{background-color:#fff;background-color:#fffffff2;display:block;font-size:16px;font-weight:500;padding:5px 6px;width:100%}.emoji-mart-emoji{display:inline-block;font-size:0;position:relative}.emoji-mart-no-results{color:#858585;font-size:14px;padding-top:70px;text-align:center}.emoji-mart-no-results .emoji-mart-category-label{display:none}.emoji-mart-no-results .emoji-mart-no-results-label{margin-top:.2em}.emoji-mart-no-results .emoji-mart-emoji:hover:before{content:none}.emoji-mart-preview{height:70px;position:relative}.emoji-mart-preview-data,.emoji-mart-preview-emoji,.emoji-mart-preview-skins{position:absolute;top:50%;-webkit-transform:translateY(-50%);-ms-transform:translateY(-50%);transform:translateY(-50%)}.emoji-mart-preview-emoji{left:12px}.emoji-mart-preview-data{left:68px;right:12px;word-break:break-all}.emoji-mart-preview-skins{right:30px;text-align:right}.emoji-mart-preview-name{font-size:14px}.emoji-mart-preview-shortname{color:#888;font-size:12px}.emoji-mart-preview-emoticon+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-shortname{margin-left:.5em}.emoji-mart-preview-emoticon{color:#bbb;font-size:11px}.emoji-mart-title span{display:inline-block;vertical-align:middle}.emoji-mart-title .emoji-mart-emoji{padding:0}.emoji-mart-title-label{color:#999a9c;font-size:21px;font-weight:300}.emoji-mart-skin-swatches{background-color:#fff;border:1px solid #d9d9d9;border-radius:12px;font-size:0;padding:2px 0}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch{padding:0 2px;width:16px}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch-selected:after{opacity:.75}.emoji-mart-skin-swatch{display:inline-block;-webkit-transition-duration:.125s;transition-duration:.125s;-webkit-transition-property:width,padding;transition-property:width,padding;-webkit-transition-timing-function:ease-out;transition-timing-function:ease-out;vertical-align:middle;width:0}.emoji-mart-skin-swatch:first-child{-webkit-transition-delay:0s;transition-delay:0s}.emoji-mart-skin-swatch:nth-child(2){-webkit-transition-delay:.03s;transition-delay:.03s}.emoji-mart-skin-swatch:nth-child(3){-webkit-transition-delay:.06s;transition-delay:.06s}.emoji-mart-skin-swatch:nth-child(4){-webkit-transition-delay:.09s;transition-delay:.09s}.emoji-mart-skin-swatch:nth-child(5){-webkit-transition-delay:.12s;transition-delay:.12s}.emoji-mart-skin-swatch:nth-child(6){-webkit-transition-delay:.15s;transition-delay:.15s}.emoji-mart-skin-swatch-selected{padding:0 2px;position:relative;width:16px}.emoji-mart-skin-swatch-selected:after{background-color:#fff;border-radius:100%;content:"";height:4px;left:50%;margin:-2px 0 0 -2px;opacity:0;pointer-events:none;position:absolute;top:50%;-webkit-transition:opacity .2s ease-out;transition:opacity .2s ease-out;width:4px}.emoji-mart-skin{border-radius:100%;display:inline-block;max-width:12px;padding-top:100%;width:100%}.emoji-mart-skin-tone-1{background-color:#ffc93a}.emoji-mart-skin-tone-2{background-color:#fadcbc}.emoji-mart-skin-tone-3{background-color:#e0bb95}.emoji-mart-skin-tone-4{background-color:#bf8f68}.emoji-mart-skin-tone-5{background-color:#9b643d}.emoji-mart-skin-tone-6{background-color:#594539}.emoji-mart .vue-recycle-scroller{position:relative}.emoji-mart .vue-recycle-scroller.direction-vertical:not(.page-mode){overflow-y:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal:not(.page-mode){overflow-x:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal{display:-webkit-box;display:-ms-flexbox;display:flex}.emoji-mart .vue-recycle-scroller__slot{-webkit-box-flex:1;-ms-flex:auto 0 0px;flex:auto 0 0}.emoji-mart .vue-recycle-scroller__item-wrapper{-webkit-box-flex:1;-webkit-box-sizing:border-box;box-sizing:border-box;-ms-flex:1;flex:1;overflow:hidden;position:relative}.emoji-mart .vue-recycle-scroller.ready .vue-recycle-scroller__item-view{left:0;position:absolute;top:0;will-change:transform}.emoji-mart .vue-recycle-scroller.direction-vertical .vue-recycle-scroller__item-wrapper{width:100%}.emoji-mart .vue-recycle-scroller.direction-horizontal .vue-recycle-scroller__item-wrapper{height:100%}.emoji-mart .vue-recycle-scroller.ready.direction-vertical .vue-recycle-scroller__item-view{width:100%}.emoji-mart .vue-recycle-scroller.ready.direction-horizontal .vue-recycle-scroller__item-view{height:100%}.emoji-mart .resize-observer[data-v-b329ee4c]{background-color:transparent;border:none;opacity:0}.emoji-mart .resize-observer[data-v-b329ee4c],.emoji-mart .resize-observer[data-v-b329ee4c] object{display:block;height:100%;left:0;overflow:hidden;pointer-events:none;position:absolute;top:0;width:100%;z-index:-1}.emoji-mart-search .hidden{display:none;visibility:hidden}</style><style>span.emoji-mart-emoji[data-v-03afe4fa]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-03afe4fa]:hover{transform:scale(1.3)}.picker[data-v-03afe4fa]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-03afe4fa]{background-color:var(--bg)}.localeText[data-v-03afe4fa]{color:var(--color-primary)}</style><style>.tag[data-v-eb2063c1]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-eb2063c1]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.BAXwoxDU.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/U2gz5u3s.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D3Q7tgbv.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CAEbC5FA.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BE4wn3fs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ivQeT1ix.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BwRoW_yn.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cz9IHiFo.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_-EP1mu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CeeZmN6j.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BWfwsB7c.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDdUvEIO.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D7kgLHFI.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DnLIRtEt.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/H6MlkUZ9.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CaawH2jc.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/PV-tEdEs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Fo6dHlhp.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/4gN8t08k.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BiTVazLu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/iyVKi4R7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CZYsndi7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BV4pdvGF.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CE5h_3Zy.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C94jHEjt.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CZsi5-5V.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/711989cd-b755-45e7-b273-f0754c0f755f.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dk67likk.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BU1UZLbD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CZvWFDQc.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/yEdly4JE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CvvIFhEj.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B5fv-1Xq.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DFI9gCi9.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DE3BAxce.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/uYY1jy-2.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BDjFqGfp.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Cebmboq7.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CRRfaG9P.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/fV3BeRRD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/6PKCu2pi.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/HSA0rOnf.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dbk39hGM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DlT8tiJO.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/b04OUO_Q.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DPR8_a9D.js"><meta hid="description" name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="RedLM: My submission for the NVIDIA and LlamaIndex Developer Contest"><meta property="og:description" content="RedLM is an AI-powered application for the study of China's greatest classical novel: Dream of the Red Chamber"><meta property="og:image" content="https://briancaffey.github.io/static/redlm/title.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/redlm/title.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/U2gz5u3s.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-d2ff5fdf><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-d2ff5fdf><div class="justify-left flex-grow flex-cols-4" data-v-d2ff5fdf><a href="/" class="text-xl" data-v-d2ff5fdf><span class="hidden sm:inline text-2xl" data-v-d2ff5fdf>Brian Caffey</span><span class="inline sm:hidden" data-v-d2ff5fdf>JBC</span></a></div><div class="flex-grow relative" data-v-d2ff5fdf><nav z-index="10000" data-v-d2ff5fdf data-v-ed0088f5><div data-v-ed0088f5><ul class="items-right float-right hidden md:flex" data-v-ed0088f5><li class="px-4 text-lg" data-v-ed0088f5><a href="/blog/1" class="" data-v-ed0088f5>Blog</a></li><li class="px-4 text-lg" data-v-ed0088f5><a href="/contact" class="" data-v-ed0088f5>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-ed0088f5><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-ed0088f5><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-ed0088f5><title data-v-ed0088f5>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-ed0088f5></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-d2ff5fdf><div class="centered" data-v-d2ff5fdf data-v-2649ce61><div class="grid items-center justify-center" data-v-2649ce61><ul class="flex px-4" data-v-2649ce61><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üñ•Ô∏è, desktop_computer" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåû, sun_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåö, new_moon_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="‚òï, coffee" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><div data-v-2649ce61 data-v-03afe4fa><ul data-v-03afe4fa><li class="md:px-1 px-1 cursor-pointer" data-v-03afe4fa><span aria-label="üá∫üá∏, us, flag-us" class="emoji-mart-emoji" data-v-03afe4fa><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-03afe4fa><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RedLM is an AI-powered application for the study of China&#39;s greatest classical novel: Dream of the Red Chamber" data-nuxt-img srcset="/_ipx/f_webp/static/redlm/title.png 1x, /_ipx/f_webp/static/redlm/title.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/redlm/title.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">RedLM: My submission for the NVIDIA and LlamaIndex Developer Contest</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/nvidia/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>nvidia üè∑Ô∏è <!----></div></a><a href="/blog/tags/llama-index/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>llama-index üè∑Ô∏è <!----></div></a><a href="/blog/tags/ai/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>ai üè∑Ô∏è <!----></div></a><a href="/blog/tags/llm/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/rag/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>rag üè∑Ô∏è <!----></div></a><a href="/blog/tags/tensorrt-llm/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>tensorrt-llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/chinese/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>chinese üè∑Ô∏è <!----></div></a><a href="/blog/tags/redology/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>redology üè∑Ô∏è <!----></div></a><!--]--></div><div class="flex py-2"><!--[--><div class="pr-4 rounded"><a href="https://x.com/briancaffey/status/1855186768452321330"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://x.com/briancaffey/status/1855186768452321330" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/x.png 25w, /_ipx/w_50&amp;f_webp/icons/x.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/x.png"></a></div><div class="pr-4 rounded"><a href="https://dev.to/briancaffey/redlm-my-submission-for-the-nvidia-and-llamaindex-developer-contest-1c3k"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://dev.to/briancaffey/redlm-my-submission-for-the-nvidia-and-llamaindex-developer-contest-1c3k" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/dev.png 25w, /_ipx/w_50&amp;f_webp/icons/dev.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/dev.png"></a></div><!--]--></div><p class="blog-date text-gray-500 mb-4">Last updated November 9, 2024</p><!----><div class="markdown"><h2 id="tddr"><a href="#tddr"><!--[-->td;dr<!--]--></a></h2><p><!--[-->RedLM is a new way to study art and literature powered by artificial intelligence. It is an application that applies LLMs to the study of one of China‚Äôs most famous literary works: Dream of the Red Chamber. It uses leading language and vision models from Chinese AI groups including Alibaba‚Äôs Qwen, Baichuan Intelligence Technology and 01.AI. RedLM uses tools, techniques and services from NVIDIA and LlamaIndex including NVIDIA NIMs, Retrieval Augmented Generation and Multi-Modal RAG with vision language models. This project is my submission for the NVIDIA and LlamaIndex Developer Contest.<!--]--></p><p><!--[-->This article will cover how I built the project, challenges I faced and some of the lessons I learned while working with NVIDIA and LlamaIndex technologies.<!--]--></p><h3 id="links"><a href="#links"><!--[-->Links<!--]--></a></h3><!--[--><blockquote class="twitter-tweet tw-align-center" data-theme="dark"><p lang="en" dir="ltr">RedLM: An AI-powered application for Redology<br>ü™®‚ú®üìúüîéüñºÔ∏è<br>A project for the <a href="https://twitter.com/hashtag/NVIDIADevContest?src=hash&amp;ref_src=twsrc%5Etfw">#NVIDIADevContest</a> <a href="https://twitter.com/hashtag/LlamaIndex?src=hash&amp;ref_src=twsrc%5Etfw">#LlamaIndex</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://twitter.com/llama_index?ref_src=twsrc%5Etfw">@llama_index</a> <a href="https://t.co/BUaBBbsLGl">pic.twitter.com/BUaBBbsLGl</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1855186768452321330?ref_src=twsrc%5Etfw">November 9, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>/&gt; <!--]--><ul><!--[--><li><!--[--><a href="https://x.com/briancaffey/status/1855186768452321330" rel="nofollow"><!--[-->ùïè Post<!--]--></a><!--]--></li><li><!--[--><a href="https://github.com/briancaffey/RedLM" rel="nofollow"><!--[-->RedLM GitHub repository<!--]--></a><!--]--></li><!--]--></ul><h2 id="what-is-redlm"><a href="#what-is-redlm"><!--[-->What is RedLM?<!--]--></a></h2><p><!--[-->RedLM is a combination of the word ‚ÄúRed‚Äù and LM, an abbreviation for ‚Äúlanguage model‚Äù. Dream of the Red Chamber is such an important book in Chinese literature that it has its own field of study called Á∫¢Â≠¶ (literally ‚Äúthe study of red‚Äù), or Redology. So, RedLM is an application that uses language models for the study of Redology.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RedLM" data-nuxt-img srcset="/_ipx/_/static/redlm/title.png 1x, /_ipx/_/static/redlm/title.png 2x" src="/_ipx/_/static/redlm/title.png"><!--]--></p><p><!--[-->In this project I focused on three applications of language models:<!--]--></p><ol><!--[--><li><!--[-->Summary and translation of the source text<!--]--></li><li><!--[-->A Q&amp;A bot that can answer questions about the book providing references to the specific paragraphs used to give answers<!--]--></li><li><!--[-->An image-based Q&amp;A bot that can answer questions about sections of paintings that depict scenes from each of the book‚Äôs chapters.<!--]--></li><!--]--></ol><h2 id="notebooklm"><a href="#notebooklm"><!--[-->NotebookLM<!--]--></a></h2><p><!--[-->I used this article to create a &quot;Deep Dive&quot; podcast episode for RedLM using Google&#39;s NotebookLM.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="NotebookLM" data-nuxt-img srcset="/_ipx/_/static/redlm/notebooklm.png 1x, /_ipx/_/static/redlm/notebooklm.png 2x" src="/_ipx/_/static/redlm/notebooklm.png"><!--]--></p><p><!--[-->You can <a href="https://x.com/briancaffey/status/1855186771409244491" rel="nofollow"><!--[-->listen to this podcast episode here on ùïè<!--]--></a>.<!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-media-max-width="560"><p lang="en" dir="ltr">üéôÔ∏è a &quot;deep dive&quot; episode all about RedLM<br><br>made with <a href="https://twitter.com/hashtag/NotebookLM?src=hash&amp;ref_src=twsrc%5Etfw">#NotebookLM</a>, whisper, pyannote and Blender <a href="https://t.co/7BC4SIWs2E">pic.twitter.com/7BC4SIWs2E</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1855186771409244491?ref_src=twsrc%5Etfw">November 9, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><h2 id="how-i-built-redlm"><a href="#how-i-built-redlm"><!--[-->How I built RedLM<!--]--></a></h2><p><!--[-->RedLM consists of two parts: a web UI built with Vue 3 using the Nuxt Framework and a backend API built with Python, FastAPI and LlamaIndex. There are lots of great tools for building full-stack AI applications such as Gradio and Streamlit, but I wanted to build with the web tools that I‚Äôm most familiar with and that provide the most flexibility. These frameworks (Nuxt and FastAPI) are simple and effective and they allowed me to develop quickly.<!--]--></p><p><!--[-->Most of the code for this project was written by AI. I used OpenAI‚Äôs ChatGPT 4o, Anthropic‚Äôs Claude 3.5 Sonnet and 01.AI‚Äôs Yi-1.5-9B-Chat model. In my development process with AI, I prompted for one logical piece of the application at a time, such as one API route, one Vue component, one pinia store or one utility function, for example. In this article I&#39;ll share some of the prompts I used in my development workflow.<!--]--></p><p><!--[-->This project embraces a hybrid AI inference model, meaning that the AI inference can be done either on local RTX PCs or using NVIDIA‚Äôs Cloud APIs from <code><!--[-->build.nvidia.com<!--]--></code> depending on configuration via environment variables. I used PCs with NVIDIA GeForce RTX 4090 GPUs to do inference with language and vision models, and with a change of configuration, I was able to do similar inference using NVIDIA‚Äôs API endpoints. This allowed me to develop the project both on powerful RTX desktop workstations and Mac laptops.<!--]--></p><h2 id="translating-dream-of-the-red-chamber-with-tensorrt-llm"><a href="#translating-dream-of-the-red-chamber-with-tensorrt-llm"><!--[-->Translating Dream of the Red Chamber with TensorRT-LLM<!--]--></a></h2><p><!--[-->Translation is often mentioned as one of the capabilities of bilingual LLMs from China. I wanted to try translating this book from Chinese to English, but I also wanted to better understand the meaning of the original text written in vernacular Chinese. Written vernacular Chinese is essentially a form of Chinese that closely resembles the way Chinese was spoken in imperial China by common people. The use of vernacular Chinese (Baihua) in literary works marked a significant cultural shift that started to make literature and education more accessible. Before the emergence of written vernacular Chinese, Chinese literature was dominated by Classical Chinese (Wenyanwen) which is a more concise, ambiguous and specialized for of languages that assumes an understanding of ancient texts and Confucian classics. The difference between vernacular Chinese and modern Mandarin Chinese is somewhat analogous to the different between Shakespearian English (Early Modern English) and Modern English.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Baihua, Mandarin and English" data-nuxt-img srcset="/_ipx/_/static/redlm/translations.png 1x, /_ipx/_/static/redlm/translations.png 2x" src="/_ipx/_/static/redlm/translations.png"><!--]--></p><p><!--[-->Chinese large language models are well versed in Classical Chinese, written Chinese vernacular and modern Mandarin Chinese. I decided to rewrite the original vernacular text in simple, modern Mandarin Chinese and then using this new modern Mandarin version, translate the story into English.<!--]--></p><p><!--[-->Dream of the Red Chamber is a large book. It is composed of over 800,000 Chinese characters, using 4303 unique Chinese characters. It has 120 chapters and a total of 3996 paragraphs. Here is a histogram showing the number of characters per paragraph.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Paragraph lengths" data-nuxt-img srcset="/_ipx/_/static/redlm/paragraphs.png 1x, /_ipx/_/static/redlm/paragraphs.png 2x" src="/_ipx/_/static/redlm/paragraphs.png"><!--]--></p><p><!--[-->I rented a large multi-GPU instance from AWS using some of the credits I get as a member of the AWS Community Builders program. The g5.12xlarge instance I selected has 4 A10G Tensor Core GPUs. The TensorRT-LLM LLM API is a relatively new part of the TensorRT-LLM library. It provides a very simple, high-level interface for doing inference. Following the <a href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_inference_distributed.html" rel="nofollow"><!--[-->LLM Generate Distributed example<!--]--></a> from the TensorRT-LLM documentation, I was able to translate the entire book into simple Mandarin and then from Mandarin into English in about an hour and 15 minutes. The <code><!--[-->tensor_parallel_size<!--]--></code> option in the LLM API allows for distributed inference, this meant that up to 4 paragraphs could be translated at the same time on different GPUs on the same EC2 instance.<!--]--></p><pre class="language-text"><!--[--><code>Translating: data/book/22.json
Processed requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:15&lt;00:00,  2.41it/s]
Processed requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:24&lt;00:00,  1.54it/s]
Translated: data/book/22.json
Translating: data/book/114.json
Processed requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11&lt;00:00,  1.81it/s]
Processed requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12&lt;00:00,  1.58it/s]
Translated: data/book/114.json
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session
[TensorRT-LLM][INFO] Refreshed the MPI local session

real    74m1.578s
user    0m45.936s
sys 0m36.283s
</code><!--]--></pre><p><!--[-->Getting good results required a bit of experimentation with parameters. The LLM API makes this very easy. The following code configures settings and builds the inference engine that can be used for doing completions:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=256)
</span></span><span class="line" line="2"><span>build_config = BuildConfig(max_seq_len=2048)
</span></span><span class="line" line="3"><span>llm = LLM(model=MODEL, build_config=build_config, tensor_parallel_size=4)
</span></span></code><!--]--></pre><p><!--[-->I used the following prompts to rewrite each paragraph of the original text in simple, modern Mandarin Chinese:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>bai_prompts = [
</span></span><span class="line" line="2"><span>    # Here are examples of how to rewrite Chinese vernacular into simple modern Mandarin.\n\nChinese vernacular:\n\n{p}\n\nSimple modern Mandarin
</span></span><span class="line" line="3"><span>    f&quot;‰ª•‰∏ãÊòØÂ¶Ç‰ΩïÂ∞Ü‰∏≠ÂõΩÁôΩËØùÊîπÂÜô‰∏∫ÁÆÄÂçïÁöÑÁé∞‰ª£ÊôÆÈÄöËØùÁöÑÁ§∫‰æã„ÄÇ\n\n‰∏≠ÊñáÁôΩËØùÔºö\n\n{p}\n\nÁÆÄÂçïÁöÑÁé∞‰ª£ÊôÆÈÄöËØùÔºö\n\n&quot;
</span></span><span class="line" line="4"><span>    for p in flat_bai
</span></span><span class="line" line="5"><span>]
</span></span></code><!--]--></pre><p><!--[-->It was difficult to get good results consistently. Here are some observations I had:<!--]--></p><ul><!--[--><li><!--[-->Some of the translated paragraphs were perfect<!--]--></li><li><!--[-->some translated paragraphs would randomly hallucinate the same phrase over and over again<!--]--></li><li><!--[-->Some requests to translate text to English would reply in Mandarin Chinese rather than in English<!--]--></li><li><!--[-->Sometimes I would even see computer code generated when asking for a translation<!--]--></li><li><!--[-->The names of characters were sometimes translated inconsistently, sometimes literally and sometimes using differing versions of pinyin, the Romanization system for transcribing the sounds of Mandarin Chinese<!--]--></li><!--]--></ul><p><!--[-->I found that ChatGPT 4o could handle any Chinese translation task flawlessly, but the <code><!--[-->Qwen2-7B<!--]--></code> model I used had mixed results! The change that I made that seemed to have the biggest impact on translation quality was setting <code><!--[-->*max_tokens*=256<!--]--></code> in <code><!--[-->SamplingParams<!--]--></code>. I probably could have used a dynamic value for <code><!--[-->max_tokens<!--]--></code> based on the size of the current paragraph being translated. I also would have like to set up side-by-side comparisons of translations using different sized models, but rather than spend time and AWS credits on optimizing translation with TensorRT-LLM, I wanted to focus on the main part of this project: retrieval augmented generation (RAG) with LlamaIndex.<!--]--></p><h2 id="building-qa-bots-with-rag-using-llamaindex"><a href="#building-qa-bots-with-rag-using-llamaindex"><!--[-->Building Q&amp;A bots with RAG using LlamaIndex<!--]--></a></h2><p><!--[-->My primary objective with this project was to implement a simple chat bot that responds to questions about the book with references to the specific paragraphs used in the response. The following shows images of the UI I built with one of the examples I included in the video I made for this project.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RAG Example" data-nuxt-img srcset="/_ipx/_/static/redlm/rag_example.png 1x, /_ipx/_/static/redlm/rag_example.png 2x" src="/_ipx/_/static/redlm/rag_example.png"><!--]--></p><p><!--[-->I haven&#39;t read that much of the book before working on this project, but I have read a lot <em><!--[-->about<!--]--></em> this book&#39;s characters, major themes and plot. This Q&amp;A bot was a very interesting entrypoint to explore specific passages of the book starting with questions coming from my knowledge about the book. The question in the screenshots above is: ‚ÄúWhat does Jia Baoyu‚Äôs father think about him?‚Äù The response includes references to paragraphs where Jia Zheng (Baoyu‚Äôs father) is discussing his son. I was pretty amazed that the RAG query was able to pull out these two paragraphs.<!--]--></p><p><!--[--><em><!--[-->In Dream of the Red Chamber, the relationship between protagonist Jia Baoyu and his father, Jia Zheng, is complex and fraught with tension. Jia Zheng, a strict, traditional Confucian patriarch, embodies values of discipline, scholarly rigor, and duty. He expects his son to excel in his studies and uphold the family‚Äôs honor by pursuing an official career in government. Baoyu, however, is sensitive, imaginative, and inclined toward poetry and the company of women, especially his cousins Lin Daiyu and Xue Baochai. This preference clashes with Jia Zheng‚Äôs expectations, leading to frequent misunderstandings and disappointment.<!--]--></em><!--]--></p><p><!--[-->By default, LlamaIndex uses cosine similarity as the distance metric for finding the vectors representing the documents (paragraphs) that are ‚Äúclosest‚Äù to the vector representing the user query. This is the central mechanism by which RAG works. LlamaIndex provides an abstraction of this process, hiding the implementation details and allowing rapid development of retrieval systems.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Cosine Similarity" data-nuxt-img srcset="/_ipx/_/static/redlm/cosine_similarity.png 1x, /_ipx/_/static/redlm/cosine_similarity.png 2x" src="/_ipx/_/static/redlm/cosine_similarity.png"><!--]--></p><p><!--[-->Source: <a href="https://medium.com/@kbdhunga/a-beginners-guide-to-similarity-search-vector-indexing-part-one-9cf5e9171976" rel="nofollow"><!--[-->https://medium.com/@kbdhunga/a-beginners-guide-to-similarity-search-vector-indexing-part-one-9cf5e9171976<!--]--></a><!--]--></p><p><!--[-->Here is some of the code I wrote for the text-based Q&amp;A bot using LlamaIndex‚Äôs <code><!--[-->CustomQueryEngine<!--]--></code> class to fetch the nodes from which I get the referenced paragraph text, chapter number and paragraph number.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>class QAndAQueryEngine(CustomQueryEngine):
</span></span><span class="line" line="2"><span>    &quot;&quot;&quot;RAG Completion Query Engine optimized for Q&amp;A&quot;&quot;&quot;
</span></span><span class="line" line="3"><span emptylineplaceholder="true">
</span></span><span class="line" line="4"><span>    retriever: BaseRetriever
</span></span><span class="line" line="5"><span>    response_synthesizer: BaseSynthesizer
</span></span><span class="line" line="6"><span>    llm: OpenAILike
</span></span><span class="line" line="7"><span>    qa_prompt: PromptTemplate
</span></span><span class="line" line="8"><span emptylineplaceholder="true">
</span></span><span class="line" line="9"><span>    def custom_query(self, query_str: str):
</span></span><span class="line" line="10"><span>        nodes = self.retriever.retrieve(query_str)
</span></span><span class="line" line="11"><span>        metadata = []
</span></span><span class="line" line="12"><span>        # Collect the metadata into a list of dicts so that it can be sent to UI for references
</span></span><span class="line" line="13"><span>        for node in nodes:
</span></span><span class="line" line="14"><span>            metadata_dict = {}
</span></span><span class="line" line="15"><span>            node_metadata = node.node.metadata
</span></span><span class="line" line="16"><span>            metadata_dict[&quot;content&quot;] = node.node.text
</span></span><span class="line" line="17"><span>            metadata_dict[&quot;chapter&quot;] = int(node_metadata.get(&quot;chapter&quot;))
</span></span><span class="line" line="18"><span>            metadata_dict[&quot;paragraph&quot;] = int(node_metadata.get(&quot;paragraph&quot;))
</span></span><span class="line" line="19"><span emptylineplaceholder="true">
</span></span><span class="line" line="20"><span>            metadata.append(metadata_dict)
</span></span><span class="line" line="21"><span emptylineplaceholder="true">
</span></span><span class="line" line="22"><span>        context_str = &quot;\n\n&quot;.join([n.node.get_content() for n in nodes])
</span></span><span class="line" line="23"><span>        response = self.llm.chat(
</span></span><span class="line" line="24"><span>            [
</span></span><span class="line" line="25"><span>                ChatMessage(
</span></span><span class="line" line="26"><span>                    role=&quot;user&quot;,
</span></span><span class="line" line="27"><span>                    content=q_and_a_prompt.format( # the English and Chinese prompt templates are discussed below
</span></span><span class="line" line="28"><span>                        context_str=context_str, query_str=query_str
</span></span><span class="line" line="29"><span>                    ),
</span></span><span class="line" line="30"><span>                )
</span></span><span class="line" line="31"><span>            ]
</span></span><span class="line" line="32"><span>        )
</span></span><span class="line" line="33"><span emptylineplaceholder="true">
</span></span><span class="line" line="34"><span>        return response, metadata
</span></span></code><!--]--></pre><h3 id="indexing-the-book-data"><a href="#indexing-the-book-data"><!--[-->Indexing the book data<!--]--></a></h3><p><!--[-->In the indexing process, embedding models are used to translate chunks of text (paragraphs) into high-dimensional vectors that represent the relationships between the tokens in a chunk of text. These are the vectors stored in the &quot;Vector Database&quot; used by LlamaIndex. The chapter number, paragraph number and version (original, Mandarin Chinese and English) of each paragraph are added to the database entry as metadata during the indexing step which runs via a script before starting the FastAPI server. Here&#39;s how I indexed the original text and translations with LlamaIndex:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>from llama_index.core import Document, VectorStoreIndex
</span></span><span class="line" line="2"><span>from llama_index.embeddings.huggingface import HuggingFaceEmbedding
</span></span><span class="line" line="3"><span emptylineplaceholder="true">
</span></span><span class="line" line="4"><span>en_embedding_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)
</span></span><span class="line" line="5"><span>zh_embedding_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-zh-v1.5&quot;)
</span></span><span class="line" line="6"><span emptylineplaceholder="true">
</span></span><span class="line" line="7"><span>def persist_index():
</span></span><span class="line" line="8"><span>    documents = []
</span></span><span class="line" line="9"><span>    for chapter in range(1, 121):
</span></span><span class="line" line="10"><span>        with open(f&quot;data/book/{chapter}.json&quot;, &quot;r&quot;) as f:
</span></span><span class="line" line="11"><span>            data = json.load(f)
</span></span><span class="line" line="12"><span>            paragraphs = data[&quot;paragraphs&quot;]
</span></span><span class="line" line="13"><span emptylineplaceholder="true">
</span></span><span class="line" line="14"><span>        for i, p in enumerate(paragraphs):
</span></span><span class="line" line="15"><span>            for lang in [&quot;original&quot;, &quot;chinese&quot;, &quot;english&quot;]:
</span></span><span class="line" line="16"><span>                document = Document(
</span></span><span class="line" line="17"><span>                    text=p[lang],
</span></span><span class="line" line="18"><span>                    metadata={
</span></span><span class="line" line="19"><span>                        &quot;chapter&quot;: str(chapter),
</span></span><span class="line" line="20"><span>                        &quot;paragraph&quot;: str(i),
</span></span><span class="line" line="21"><span>                        &quot;language&quot;: lang,
</span></span><span class="line" line="22"><span>                    },
</span></span><span class="line" line="23"><span>                    metadata_seperator=&quot;::&quot;,
</span></span><span class="line" line="24"><span>                    metadata_template=&quot;{key}=&gt;{value}&quot;,
</span></span><span class="line" line="25"><span>                    text_template=&quot;Metadata: {metadata_str}\n-----\nContent: {content}&quot;,
</span></span><span class="line" line="26"><span>                    embed_model=(
</span></span><span class="line" line="27"><span>                        en_embedding_model if lang == &quot;english&quot; else zh_embedding_model
</span></span><span class="line" line="28"><span>                    ),
</span></span><span class="line" line="29"><span>                )
</span></span><span class="line" line="30"><span>                documents.append(document)
</span></span><span class="line" line="31"><span emptylineplaceholder="true">
</span></span><span class="line" line="32"><span>    index = VectorStoreIndex.from_documents(documents)
</span></span><span class="line" line="33"><span>    index.storage_context.persist(persist_dir=&quot;storage&quot;)
</span></span><span class="line" line="34"><span emptylineplaceholder="true">
</span></span><span class="line" line="35"><span>if __name__ == &quot;__main__&quot;:
</span></span><span class="line" line="36"><span>    persist_index()
</span></span></code><!--]--></pre><p><!--[-->For the embedding models, I used the small BAAI General Embedding models (BGE) for English and Chinese. BAAI is the Beijing Academy of Artificial Intelligence, and I learned about this organization through some of the examples on the LlamaIndex site that use BAAI embeddings. There are multi-lingual embedding models (e.g. <code><!--[-->BAAI/bge-m3<!--]--></code>), but setting the embedding model on a per-document basis is possible and in some cases it might be preferable to using a single embedding model for all documents.<!--]--></p><h3 id="milvus-vector-database"><a href="#milvus-vector-database"><!--[-->Milvus Vector Database<!--]--></a></h3><p><!--[-->I did most of the development for this project using the in-memory VectorIndexStore provided by LlamaIndex. This worked well, but making any changes to the FastAPI server required the data to be reloaded into memory which took several seconds each time. This can really hinder a good development flow, so I looked into using an external service for the vector database instead of running it in memory.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Vector Database Options" data-nuxt-img srcset="/_ipx/_/static/redlm/vectordbs.png 1x, /_ipx/_/static/redlm/vectordbs.png 2x" src="/_ipx/_/static/redlm/vectordbs.png"><!--]--></p><p><!--[-->There are a LOT of options to consider when picking a vector database for a RAG application. Milvus has a highly decoupled architecture, it is fully open source and I had seen it in some examples in the <a href="https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/examples/advanced_rag/multimodal_rag" rel="nofollow"><!--[--><code><!--[-->NVIDIA/GenerativeAIExamples<!--]--></code><!--]--></a> repo, so I decided to give it a try.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Milvus Vector Database Architecture" data-nuxt-img srcset="/_ipx/_/static/redlm/milvus.png 1x, /_ipx/_/static/redlm/milvus.png 2x" src="/_ipx/_/static/redlm/milvus.png"><!--]--></p><p><!--[-->Using the <a href="https://milvus.io/docs/v2.0.x/install_standalone-docker.md" rel="nofollow"><!--[-->Milvus docker compose example<!--]--></a> I was able to set up an external vector database based on etcd and minio. Milvus also provides a Helm chart for running their vector database, this would be helpful if I was going to be running everything in Kubernetes (inference, vector database and application containers).<!--]--></p><h3 id="other-examples-of-rag-with-english-questions"><a href="#other-examples-of-rag-with-english-questions"><!--[-->Other examples of RAG with English questions<!--]--></a></h3><p><!--[-->One interesting design question I faced was how to support answering questions in both English and Chinese. I initially built the Q&amp;A bot with only Chinese language support. Later, I added a simple helper function to determine if the input text is Chinese:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>def is_chinese_text(text: str) -&gt; bool:
</span></span><span class="line" line="2"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="3"><span>    This is a simple helper function that is used to determine which prompt to use
</span></span><span class="line" line="4"><span>    depending on the language of the original user query
</span></span><span class="line" line="5"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="6"><span>    chinese_count = sum(1 for char in text if &#39;\u4e00&#39; &lt;= char &lt;= &#39;\u9fff&#39;)
</span></span><span class="line" line="7"><span>    english_count = sum(1 for char in text if &#39;a&#39; &lt;= char.lower() &lt;= &#39;z&#39;)
</span></span><span class="line" line="8"><span emptylineplaceholder="true">
</span></span><span class="line" line="9"><span>    return chinese_count &gt; english_count
</span></span></code><!--]--></pre><p><!--[-->This boolean value would then be used in the <code><!--[-->CustomQueryEngine<!--]--></code> to use either the Chinese or English <code><!--[-->PromptTemplate<!--]--></code>. This allowed the Q&amp;A bot to answer questions in either Chinese or English, and it does not require translating back and forth between Chinese and English. However, this method relies on high-quality translations, so I don&#39;t expect English language questions to be answered as accurately as Chinese language questions. Here are the Chinese and English prompts that I used for the text-based Q&amp;A bot, as well as some examples of the Q&amp;A bot answering questions in English. The referenced materials include paragraphs from the English translation.<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span># Chinese prompt for text-based Q&amp;A bot
</span></span><span class="line" line="2"><span>q_and_a_prompt = PromptTemplate(
</span></span><span class="line" line="3"><span>    &quot;ËøôÊòØÁõ∏ÂÖ≥ÁöÑÂèÇËÄÉËµÑÊñôÔºö\n&quot;
</span></span><span class="line" line="4"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="5"><span>    &quot;{context_str}\n&quot; # context_str contains Chinese paragraphs retrieved via RAG query
</span></span><span class="line" line="6"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="7"><span>    &quot;Ê†πÊçÆ‰∏äËø∞ÁöÑÂèÇËÄÉËµÑÊñôÔºåÂõûÁ≠î‰∏ãÈù¢ÁöÑÈóÆÈ¢ò\n&quot;
</span></span><span class="line" line="8"><span>    &quot;ÈóÆÈ¢òÔºö{user_question}\n&quot;
</span></span><span class="line" line="9"><span>)
</span></span><span class="line" line="10"><span emptylineplaceholder="true">
</span></span><span class="line" line="11"><span># English prompt for text-based Q&amp;A bot
</span></span><span class="line" line="12"><span>q_and_a_prompt_english = PromptTemplate(
</span></span><span class="line" line="13"><span>    &quot;This is some related reference material:\n&quot;
</span></span><span class="line" line="14"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="15"><span>    &quot;{context_str}\n&quot; # context_str contains English paragraphs retrieved via RAG query
</span></span><span class="line" line="16"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="17"><span>    &quot;Based on the above material, answer the following question:\n&quot;
</span></span><span class="line" line="18"><span>    &quot;Question: {user_question}\n&quot;
</span></span><span class="line" line="19"><span>)
</span></span></code><!--]--></pre><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Multi-modal Q&amp;A example 1" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_01.png 1x, /_ipx/_/static/redlm/qa_example_01.png 2x" src="/_ipx/_/static/redlm/qa_example_01.png"><!--]--></p><p><!--[-->Asking random questions like this one is a fun way to explore the many scenes of Dream of the Red Chamber.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RAG Flower Pedal Example" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_flower_pedals.png 1x, /_ipx/_/static/redlm/qa_example_flower_pedals.png 2x" src="/_ipx/_/static/redlm/qa_example_flower_pedals.png"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RAG Flower Pedal Example with Reference" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_flower_pedals_a.png 1x, /_ipx/_/static/redlm/qa_example_flower_pedals_a.png 2x" src="/_ipx/_/static/redlm/qa_example_flower_pedals_a.png"><!--]--></p><h2 id="redlm-rag-evaluation"><a href="#redlm-rag-evaluation"><!--[-->RedLM RAG Evaluation<!--]--></a></h2><p><!--[-->Examinations have long been a cornerstone of Chinese society, shaping individual aspirations, cultural values, and even government structures. This legacy began with the imperial civil service exams, kƒìj«î (Áßë‰∏æ), established during the Sui and Tang dynasties, and carries through in Modern China with the gaokao (È´òËÄÉ) college entrance examination, both of which have allowed for unprecedented meritocratic routes to power and prestige. Given how widely this novel is studied in China, I was not surprised to find a wealth of examination questions written for students studying Dream of the Red Chamber.<!--]--></p><p><!--[-->I used <a href="https://www.examcoo.com/editor/do/view/id/246401" rel="nofollow"><!--[-->a set of 1000 multiple choice questions about Dream of the Red Chamber on examcoo.com<!--]--></a> to evaluate the effectiveness of the RAG system I built with LlamaIndex. I wrote a script to parse the questions from the website HTML using ChatGPT (parsing HTML is one of my favorite use cases of LLMs!) I filtered the list of 1000 questions down to 877 questions based on the following criteria:<!--]--></p><ul><!--[--><li><!--[--><strong><!--[-->Four answer choices<!--]--></strong>: some of the questions had more than four answer choices. I filtered questions with more than four answer choices to keep the evaluation simple. This would allow me to assume that random answer choices would have a 25% chance of being correct.<!--]--></li><li><!--[--><strong><!--[-->Only one answer<!--]--></strong>: For some questions the correct answer required selecting multiple answer choices. This would also help keep the evaluation logic simple.<!--]--></li><!--]--></ul><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Multiple Choice Questions from Dream of the Red Chamber Test" data-nuxt-img srcset="/_ipx/_/static/redlm/hlm_mcq.png 1x, /_ipx/_/static/redlm/hlm_mcq.png 2x" src="/_ipx/_/static/redlm/hlm_mcq.png"><!--]--></p><p><!--[-->Multiple choice questions from a Dream of the Red Chamber test (examcoo.com)<!--]--></p><p><!--[-->To run the evaluation I set up two scripts. The first script would prompt the LLM to answer the question without any additional information from the RAG system. This served as a baseline to see how well the LLM could do at answering multiple choice questions about the book. The script simply checks to see if the LLM response contains the letter (A, B, C or D) of the correct answer and keeps track of the number of questions answered correctly.<!--]--></p><p><!--[-->Another script was used to take the test using large language models with RAG. In this script, the prompt sent to the LLM included relevant paragraphs from the book based on how similar the query is to each paragraph in the book based on the cosine similarity metric mentioned earlier.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RAG evaluation" data-nuxt-img srcset="/_ipx/_/static/redlm/rag_eval.png 1x, /_ipx/_/static/redlm/rag_eval.png 2x" src="/_ipx/_/static/redlm/rag_eval.png"><!--]--></p><p><!--[-->Here are some results and other observations from this experiment:<!--]--></p><ul><!--[--><li><!--[-->LLMs alone scored in the mid 30% range (36%)<!--]--></li><li><!--[-->LLMs using retrieval augmented generation with the set of questions score in the mid 40% range (44%)<!--]--></li><li><!--[-->I used the completion API rather than the chat API and set the <code><!--[-->max_tokens<!--]--></code> to 16. This was done to ensure that the LLM only gave a short response with a valid answer choice rather than giving a long response with an explanation.<!--]--></li><li><!--[-->The evaluation took longer for LLM + RAG test because of the time required for making the RAG query and the longer prompt (including both the original multiple-choice question and the referenced paragraphs).<!--]--></li><li><!--[-->I used the <code><!--[-->01-ai/Yi-1.5-9B-Chat<!--]--></code> model for this test, but I probably should have used the base model rather than the chat model.<!--]--></li><li><!--[-->Some questions would not be capable of being answered by RAG. For example, some of the questions are about film renditions of the novel. Most of the questions seemed relevant to the content of the book, so I didn‚Äôt bother to filter out the questions that were not directly related to the book‚Äôs content.<!--]--></li><!--]--></ul><p><!--[-->Here is an example of a question that the LLM test script answered <em><!--[-->incorrectly<!--]--></em> and the LLM + RAG test script answered <strong><!--[-->correctly<!--]--></strong>.<!--]--></p><blockquote><!--[--><p><!--[-->Áß¶ÈíüÁöÑÁà∂‰∫≤ÊòØÂ¶Ç‰ΩïÊ≠ªÁöÑÔºü<!--]--></p><p><!--[-->A„ÄÅÂ§ñÊÑüÈ£éÂØí„ÄÅÈ£éÊØí‰πãÁóá<!--]--></p><p><!--[-->B„ÄÅË¢´Êô∫ËÉΩÂÑøÊ∞îÊ≠ªÁöÑ<!--]--></p><p><!--[-->C„ÄÅÁîüÊ∞îÂºïÂèëÊóßÁóÖÂä†Èáç<!--]--></p><p><!--[-->D„ÄÅÁîüÊ∞îËÄåËØ±Âèë‰∏≠È£éËÄåÊ≠ª<!--]--></p><!--]--></blockquote><p><!--[-->Translation:<!--]--></p><blockquote><!--[--><p><!--[-->How did Qin Zhong&#39;s father die?<!--]--></p><p><!--[--><strong><!--[-->A.<!--]--></strong> He caught a cold and developed wind-related illnesses.<!--]--></p><p><!--[--><strong><!--[-->B.<!--]--></strong> He was angered to death by Zhineng&#39;er (a character).<!--]--></p><p><!--[--><strong><!--[-->C.<!--]--></strong> His old illness worsened due to anger.<!--]--></p><p><!--[--><strong><!--[-->D.<!--]--></strong> He had a stroke induced by anger and died.<!--]--></p><!--]--></blockquote><p><!--[-->Here is the paragraphs that the RAG query returned along with the English translation:<!--]--></p><p><!--[-->Original<!--]--></p><blockquote><!--[--><p><!--[-->Ëç£‰∏§Â§Ñ‰∏ä‰∏ãÂÜÖÂ§ñ‰∫∫Á≠âËé´‰∏çÊ¨¢Â§©ÂñúÂú∞ÔºåÁã¨ÊúâÂÆùÁéâÁΩÆËã•ÁΩîÈóª„ÄÇ‰Ω†ÈÅì‰ªÄ‰πàÁºòÊïÖÔºüÂéüÊù•ËøëÊó•Ê∞¥ÊúàÂ∫µÁöÑÊô∫ËÉΩÁßÅÈÄÉÂÖ•ÂüéÔºåÊù•ÊâæÁß¶ÈíüÔºå‰∏çÊÑèË¢´Áß¶ÈÇ¶‰∏öÁü•ËßâÔºåÂ∞ÜÊô∫ËÉΩÈÄêÂá∫ÔºåÂ∞ÜÁß¶ÈíüÊâì‰∫Ü‰∏ÄÈ°øÔºåËá™Â∑±Ê∞îÁöÑËÄÅÁóÖÂèë‰∫ÜÔºå‰∏â‰∫îÊó•‰æøÂëúÂëºÂìÄÂìâ‰∫Ü„ÄÇÁß¶ÈíüÊú¨Ëá™ÊÄØÂº±ÔºåÂèàÂ∏¶ÁóÖÊú™ÁóäÔºåÂèó‰∫ÜÁ¨ûÊùñÔºå‰ªäËßÅËÄÅÁà∂Ê∞îÊ≠ªÔºåÊÇîÁóõÊó†ÂèäÔºåÂèàÊ∑ª‰∫ÜËÆ∏Â§öÁóÖÁóá„ÄÇÂõ†Ê≠§ÔºåÂÆùÁéâÂøÉ‰∏≠ÊÄÖÊÄÖ‰∏ç‰πê„ÄÇËôΩÊúâÂÖÉÊò•ÊôãÂ∞Å‰πã‰∫ãÔºåÈÇ£Ëß£Âæó‰ªñÁöÑÊÑÅÈó∑ÔºüË¥æÊØçÁ≠âÂ¶Ç‰ΩïË∞¢ÊÅ©ÔºåÂ¶Ç‰ΩïÂõûÂÆ∂Ôºå‰∫≤ÂèãÂ¶Ç‰ΩïÊù•Â∫ÜË¥∫ÔºåÂÆÅËç£‰∏§Â∫úËøëÊó•Â¶Ç‰ΩïÁÉ≠ÈóπÔºå‰ºó‰∫∫Â¶Ç‰ΩïÂæóÊÑèÔºåÁã¨‰ªñ‰∏Ä‰∏™ÁöÜËßÜÊúâÂ¶ÇÊó†ÔºåÊØ´‰∏ç‰ªãÊÑè„ÄÇÂõ†Ê≠§Ôºå‰ºó‰∫∫Âò≤‰ªñË∂äÂèëÂëÜ‰∫Ü„ÄÇ<!--]--></p><!--]--></blockquote><p><!--[-->English<!--]--></p><blockquote><!--[--><p><!--[-->Everyone in the Rong and Ning households, both inside and outside, were extremely happy, except for Baoyu, who seemed indifferent. Do you want to know why? It turns out that recently, the nun Zhineng from Shuiyue Temple secretly ran into the city to find Qin Zhong. Unexpectedly, she was discovered by Qin Zhong&#39;s father, Qin Banger. Qin Banger not only drove Zhineng away but also gave Qin Zhong a beating. This made Qin Banger so angry that his old illness relapsed, and within three to five days, he passed away. Qin Zhong had always been weak and hadn&#39;t fully recovered from a previous illness. After being beaten and seeing his father die in anger, he was overwhelmed with regret and sorrow, which worsened his condition. As a result, Baoyu felt very melancholic. Although the promotion of Yuan Chun to imperial concubine was a joyful event, it couldn&#39;t alleviate the gloom in his heart. While Grandmother Jia and others were busy expressing their gratitude and returning home, and relatives and friends came to celebrate, and the Rong and Ning households were bustling with excitement, Baoyu alone remained completely indifferent to it all. Consequently, everyone started to mock him for becoming more and more absent-minded.<!--]--></p><!--]--></blockquote><p><!--[-->The correct answer for this question is C.<!--]--></p><h2 id="multi-modal-rag-for-visual-reasoning"><a href="#multi-modal-rag-for-visual-reasoning"><!--[-->Multi-modal RAG for visual reasoning<!--]--></a></h2><p><!--[-->Qwen2-VL is a new AI model that was released in late August 2024. Qwen is the name of Alibaba‚Äôs AI Lab, and it is an abbreviation of the Chinese characters: ÂçÉÈóÆ (&quot;qian wen&quot;, meaning 1000 questions). VL stands for vision-language, meaning that the model is capable of understanding both text and images. I had tested out the previous version of Qwen‚Äôs vision-language model and was very impressed by how it could accurately describe the contents of images and also answer general questions about images.<!--]--></p><p><!--[-->Sun Wen was a Qing-era painter who spent 36 years of his life creating a series of 230 paintings capturing scenes from Dream of the Red Chamber. The paintings are incredibly detailed and often contain repeated figures in a temporal sequence. If you asked a Qwen-VL model to describe one of the images, it might return lengthy description that doesn&#39;t fully capture the full detail of the scene. It might also be difficult for a language model to &quot;focus&quot; on a portion of the whole image.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Dream of the Red Chamber Painting 131" data-nuxt-img srcset="/_ipx/_/static/redlm/painting_131.png 1x, /_ipx/_/static/redlm/painting_131.png 2x" src="/_ipx/_/static/redlm/painting_131.png"><!--]--></p><p><!--[-->This sparked the idea to create a feature where users can click and drag over an image to select part of a painting, then ask questions specifically about the selected portion. I knew that while this could be achieved with tools like HTML canvas, writing it on my own would be quite time-consuming. It took me just a few minutes to write out the prompt, and Claude 3.5 Sonnet generated a perfect prototype of this feature in under a minute. Here‚Äôs the prompt I used:<!--]--></p><blockquote><!--[--><p><!--[-->I&#39;m going to describe a Vue component and I want you to write it using Vue 3 to the best of your ability.<!--]--></p><p><!--[-->write a simple single-file vue component using Vue 3 that does the following:<!--]--></p><ul><!--[--><li><!--[-->displays an image<!--]--></li><li><!--[-->allows the users to click and drag to select a subsection of the image<!--]--></li><li><!--[-->the subsection of the image is saved as a base64-encoded data url to a variable that is displayed below the image<!--]--></li><!--]--></ul><p><!--[-->The solution should make use of HTML canvas. When you click down on the image you begin selecting the subsection. You then move the mouse to make your subsection on the image, and when you mouse-up the subsection is selected and the data url is updated. Then the subsection is displayed at the very bottom of the page as a &quot;preview&quot; image using the base 64 image string as the image source.<!--]--></p><p><!--[-->The selection box should be a dashed red line<!--]--></p><!--]--></blockquote><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RedLM Image Q&amp;A" data-nuxt-img srcset="/_ipx/_/static/redlm/image-qa.png 1x, /_ipx/_/static/redlm/image-qa.png 2x" src="/_ipx/_/static/redlm/image-qa.png"><!--]--></p><p><!--[-->This shows the final result of the UI I built for the image Q&amp;A feature in RedLM. It uses a similar chat layout that the text-based Q&amp;A feature uses, with the addition of the image preview included in the chat log. The user query in this example just says ‚ÄúPlease describe the contents of the image‚Äù. This was the first image that I tested when building the image Q&amp;A feature to see if the correct passage can be referenced based on the description of an image. This pulled the exact passage and the answer provides details about what happened (a fire broke out) where it happened (at the Gourd Temple) and why it happened (a Monk accidentally set an oil pot on fire).<!--]--></p><p><!--[-->Here is a diagram showing the overall flow of data in the image Q&amp;A feature:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Diagram of RedLM Image Q&amp;A with RAG and Vision Language Models" data-nuxt-img srcset="/_ipx/_/static/redlm/redlm.drawio.png 1x, /_ipx/_/static/redlm/redlm.drawio.png 2x" src="/_ipx/_/static/redlm/redlm.drawio.png"><!--]--></p><p><!--[-->This flow chart shows how the image Q&amp;A feature works.<!--]--></p><ol><!--[--><li><!--[-->The user selects part of an image and writes a question. This data is then sent to the RedLM API as a post request to the <code><!--[-->/mm-q-and-a<!--]--></code> endpoint (multi-modal Q&amp;A).<!--]--></li><li><!--[-->Vision language models are used to get a description of the image. Depending on the application configuration, this query can use models such as <code><!--[-->Qwen/Qwen2-VL-2B-Instruct<!--]--></code> on RTX PCs or using the NVIDIA API Catalog using larger models such as <code><!--[-->meta/llama-3.2-90b-vision-instruct<!--]--></code>. Not all vision language models have the same interface, so I added some logic to handle different model formats.<!--]--></li><li><!--[-->The image description is used to fetch relevant documents from the Vector Database<!--]--></li><li><!--[-->The full prompt with the image description and relevant documents is sent to the LLM. Again, inference for this step is done either with RTX PCs or using models from the <code><!--[-->build.nvidia.com<!--]--></code> API catalog.<!--]--></li><li><!--[-->The response from the LLM is sent back to the browser and is displayed to the user as a chat message.<!--]--></li><!--]--></ol><p><!--[-->Here is the prompt I used for the image Q&amp;A feature:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span># Chinese prompt for image-based Q&amp;A bot
</span></span><span class="line" line="2"><span>mm_q_and_a_prompt = PromptTemplate(
</span></span><span class="line" line="3"><span>    &quot;ËøôÊòØ‰π¶‰∏≠Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπÔºö\n&quot;
</span></span><span class="line" line="4"><span>    &quot;{context_str}\n&quot;
</span></span><span class="line" line="5"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="6"><span>    &quot;‰∏ãÈù¢ÊòØÂú∫ÊôØÁöÑÊèèËø∞Ôºö\n&quot;
</span></span><span class="line" line="7"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="8"><span>    &quot;{image_description}\n&quot;
</span></span><span class="line" line="9"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="10"><span>    &quot;Ê†πÊçÆ‰∏äËø∞ÁöÑ‰ø°ÊÅØÔºåÂ∞ΩÈáèËß£Èáä‰∏äËØ¥ÁöÑÂú∫ÊôØÂíå‰π¶ÁöÑÂÖ≥Á≥ª„ÄÇ&quot;
</span></span><span class="line" line="11"><span>)
</span></span><span class="line" line="12"><span emptylineplaceholder="true">
</span></span><span class="line" line="13"><span># English prompt for image-based Q&amp;A bot
</span></span><span class="line" line="14"><span>mm_q_and_a_prompt_english = PromptTemplate(
</span></span><span class="line" line="15"><span>    &quot;Here is relevant content from the book:\n&quot;
</span></span><span class="line" line="16"><span>    &quot;{context_str}\n&quot;
</span></span><span class="line" line="17"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="18"><span>    &quot;Below is the description of a scene:\n&quot;
</span></span><span class="line" line="19"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="20"><span>    &quot;{image_description}\n&quot;
</span></span><span class="line" line="21"><span>    &quot;---------------------\n&quot;
</span></span><span class="line" line="22"><span>    &quot;Based on the information provided above, try to explain the relationship between the described scene and the book content.&quot;
</span></span><span class="line" line="23"><span>)
</span></span></code><!--]--></pre><p><!--[-->The prompt engineering for this feature was tricky. I was able to get some awesome results that would give me detailed and accurate responses, and then sometimes the LLM would seem confused about my query and tell me that there was no relationship between the scene description and the book content. Sometimes it would give me an accurate description of the scene, but then proceed to tell me that the book content is not related to the scene at all.<!--]--></p><p><!--[-->There is another important concept from LlamaIndex that I used to build the image Q&amp;A feature: metadata filtering. Metadata filtering is an important concept in RAG systems  because it helps you focus your query on relevant documents in a precise way. A very simple example might be a RAG system that indexes news articles and stores the associated date as metadata. You could allow a user to set a date range for their query and only include articles that match the given date range.<!--]--></p><p><!--[-->For my image Q&amp;A system, I have a mapping between the paintings and their associated chapters. When I ask a question about a painting, I want to use the description of the image to find similar paragraphs, but only the paragraphs that occur in the painting‚Äôs associated chapter. What I ended up doing was filtering the entire index before making the query. The alternative would be filtering the returned nodes after making the query, but this would have the possibility of not returning any nodes.<!--]--></p><p><!--[-->Here‚Äôs what some of the metadata filtering code looks like:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span># main.py
</span></span><span class="line" line="2"><span># filter by chapters associated with the queried image
</span></span><span class="line" line="3"><span>filters = MetadataFilters(
</span></span><span class="line" line="4"><span>    filters=[ExactMatchFilter(key=&quot;chapter&quot;, value=str(req_data.chapter))]
</span></span><span class="line" line="5"><span>)
</span></span><span class="line" line="6"><span>query_engine = get_query_engine_for_multi_modal(filters)
</span></span><span class="line" line="7"><span emptylineplaceholder="true">
</span></span><span class="line" line="8"><span># rag.py
</span></span><span class="line" line="9"><span># utility function that returns the query engine use for image Q&amp;A
</span></span><span class="line" line="10"><span># the index is filtered to only include nodes associated with the image being queried
</span></span><span class="line" line="11"><span>def get_query_engine_for_multi_modal(filters):
</span></span><span class="line" line="12"><span>    retriever = index.as_retriever(filters=filters)
</span></span><span class="line" line="13"><span>    synthesizer = get_response_synthesizer(response_mode=&quot;compact&quot;)
</span></span><span class="line" line="14"><span>    try:
</span></span><span class="line" line="15"><span>        query_engine = QAndAQueryEngine(
</span></span><span class="line" line="16"><span>            retriever=retriever,
</span></span><span class="line" line="17"><span>            response_synthesizer=synthesizer,
</span></span><span class="line" line="18"><span>            llm=model,
</span></span><span class="line" line="19"><span>            qa_prompt=mm_q_and_a_prompt,
</span></span><span class="line" line="20"><span>        )
</span></span><span class="line" line="21"><span>    except Exception as e:
</span></span><span class="line" line="22"><span>        print(e)
</span></span><span class="line" line="23"><span>    return query_engine
</span></span></code><!--]--></pre><p><!--[-->This seemed to work well for my use case, but it might not be a best practice, and it might not be efficient at a bigger scale.<!--]--></p><h3 id="multi-modal-qa-examples"><a href="#multi-modal-qa-examples"><!--[-->Multi-modal Q&amp;A examples<!--]--></a></h3><p><!--[-->Here are some more examples of results from different types of questions from the multi-modal Q&amp;A bot.<!--]--></p><p><!--[-->The response to the following query did a good job of combining information gathered from the image description and image from related passages.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Multi-modal Q&amp;A example 2" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_02.png 1x, /_ipx/_/static/redlm/qa_example_02.png 2x" src="/_ipx/_/static/redlm/qa_example_02.png"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Multi-modal Q&amp;A example 3" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_03.png 1x, /_ipx/_/static/redlm/qa_example_03.png 2x" src="/_ipx/_/static/redlm/qa_example_03.png"><!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Q&amp;A Example with Carriage" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_carriage.png 1x, /_ipx/_/static/redlm/qa_example_carriage.png 2x" src="/_ipx/_/static/redlm/qa_example_carriage.png"><!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Ou Guan Example" data-nuxt-img srcset="/_ipx/_/static/redlm/qa_example_ou_guan.png 1x, /_ipx/_/static/redlm/qa_example_ou_guan.png 2x" src="/_ipx/_/static/redlm/qa_example_ou_guan.png"><!--]--></p><p><!--[-->This is one of my favorite examples of the RedLM image Q&amp;A bot in action. The query here in Chinese says: &quot;What are these two people doing&quot;? The answer incorporates a description of what is happening in the story (Jia Baoyu comes across Ou Guan while visiting a temple) and also describes the significance of what is happening (<a href="https://en.wikipedia.org/wiki/Joss_paper" rel="nofollow"><!--[-->burning paper money as a form of Chinese ancestral worship<!--]--></a>). But this answer is not perfect and it demonstrates some of the difficulties I had with the prompt engineering for this project. The first part makes a lot of sense:<!--]--></p><blockquote><!--[--><p><!--[-->ËøôÊÆµÂèÇËÄÉËµÑÊñôÊù•Ëá™‰∏≠ÂõΩÂè§ÂÖ∏Â∞èËØ¥„ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÔºåËÆ≤Ëø∞‰∫ÜË¥æÂÆùÁéâÂú®Êé¢ËÆøÂØ∫Â∫ôÊó∂ÂèëÁé∞ËóïÂÆòÂú®Ê≤≥ËæπÁÉßÁ∫∏Èí±ÁöÑÊÉÖÊôØ„ÄÇËóïÂÆòÊòØË¥æÂ∫ú‰∏≠ÁöÑ‰∏Ä‰∏™ÊàèÂ≠êÔºåÂ•πÁöÑË°å‰∏∫ÂèØËÉΩ‰ª£Ë°®‰∫ÜÂØπÂéª‰∏ñ‰∫≤‰∫∫ÁöÑÂìÄÊÇºÊàñÁ•≠Â••„ÄÇË¥æÂÆùÁéâËØ¢ÈóÆÂ•πÁÉßÁ∫∏ÁöÑÂéüÂõ†ÔºåËóïÂÆòÈÄèÈú≤‰∫ÜËá™Â∑±ÊúâÁßÅ‰∏ãÁöÑÊÉÖÊÑüÈóÆÈ¢òÔºåÂπ∂‰∏îÊèêÂà∞‰∫ÜËä≥ÂÆòÂíåËïäÂÆòÔºåÊöóÁ§∫‰∫ÜÂ•π‰ª¨‰πãÈó¥ÂèØËÉΩÂ≠òÂú®ÊüêÁßçÁßòÂØÜÊàñÊÉÖÊÑüËÅîÁ≥ª„ÄÇ<!--]--></p><!--]--></blockquote><blockquote><!--[--><p><!--[-->This reference is from the classical Chinese novel Dream of the Red Chamber, describing a scene where Jia Baoyu discovers Ou Guan burning paper money by the river during his visit to a temple. Ou Guan is a performer in the Jia household, and her actions might represent mourning or honoring deceased loved ones. Jia Baoyu asks her why she is burning the paper, and Ou Guan reveals that she has private emotional issues, mentioning Fang Guan and Rui Guan, hinting that there might be some kind of secret or emotional connection among them.<!--]--></p><!--]--></blockquote><p><!--[-->But the second part seems to contradict the first part of the response:<!--]--></p><blockquote><!--[--><p><!--[-->Áõ∏ÊØî‰πã‰∏ãÔºå‰Ω†ÊèèËø∞ÁöÑÁîª‰ΩúÂú∫ÊôØ‰∏é„ÄäÁ∫¢Ê•ºÊ¢¶„Äã‰∏≠ÁöÑÊèèËø∞‰∏çÂêå„ÄÇÁîª‰ΩúÊèèÁªòÁöÑÊòØ‰∏§‰∏™‰∫∫Âú®Ê≤≥ËæπÁöÑÂú∫ÊôØÔºåÂ∑¶ËæπÁöÑ‰∫∫Ë∑™Âú®Âú∞‰∏äÔºå‰ºº‰πéÂú®ÁÉß‰∏úË•øÔºåÂè≥ËæπÁöÑ‰∫∫Á´ôÂú®ÊóÅËæπÔºå‰ºº‰πéÂú®ËßÇÂØüÊàñÊåáÂØº„ÄÇÊ†πÊçÆ‰Ω†Êèê‰æõÁöÑÊèèËø∞ÔºåÊó†Ê≥ïÁ°ÆÂÆöÁîª‰∏≠‰∫∫Áâ©ÁöÑÂÖ∑‰ΩìË°å‰∏∫ÔºåÂ¶ÇÂèØËÉΩÊòØÁÉßÊ∞¥„ÄÅÁÖÆÈ•≠„ÄÅÁÉßËçØÁ≠â„ÄÇÂõ†Ê≠§ÔºåËøôÂπÖÁîªÂèØËÉΩÊòØ‰∏ÄÂπÖÁé∞ÂÆû‰∏ª‰πâ‰ΩúÂìÅÔºåÊèèÁªò‰∫ÜÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑ‰∏ÄÂπï„ÄÇÂÆÉÂèØËÉΩÊ≤°ÊúâÂÉè„ÄäÁ∫¢Ê•ºÊ¢¶„Äã‰∏≠ÁöÑÊÉÖËäÇÈÇ£Ê†∑Ê∑±ÂàªÁöÑÊÉÖÊÑüÊàñÁßòÂØÜËÉåÊôØÔºåËÄåÊòØÂçïÁ∫ØÂú∞Ë°®Áé∞‰∫Ü‰∫∫‰∏é‰∫∫‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂíåË°å‰∏∫„ÄÇÊ≠§Â§ñÔºåÁîª‰∏≠ÁöÑËÉåÊôØ„ÄÅ‰∫∫Áâ©ÁöÑË°®ÊÉÖÂíåÂßøÊÄÅÁ≠âÁªÜËäÇ‰πü‰ºöÂØπÁîªÈù¢ÁöÑÊÑè‰πâ‰∫ßÁîüÂΩ±ÂìçÔºåÈúÄË¶ÅÁªìÂêàÂÖ∑‰ΩìÁöÑÁîª‰ΩúËøõË°åÂàÜÊûê„ÄÇ<!--]--></p><!--]--></blockquote><blockquote><!--[--><p><!--[-->In contrast, the scene you described in the painting differs from the depiction in Dream of the Red Chamber. The painting shows two people by the river: the person on the left is kneeling on the ground, seemingly burning something, while the person on the right stands nearby, possibly observing or guiding. Based on your description, it‚Äôs unclear what specific action the figures are engaged in, such as possibly boiling water, cooking, or burning herbs. Therefore, this painting might be a realist work, portraying a moment from everyday life. It may not have the profound emotional or secretive background found in the storyline of Dream of the Red Chamber, instead simply highlighting the relationships and actions between individuals. Additionally, details such as the background, expressions, and postures of the figures in the painting would also influence the scene‚Äôs meaning and would require analysis based on the specific artwork.<!--]--></p><!--]--></blockquote><h2 id="llamaindex-developer-experience"><a href="#llamaindex-developer-experience"><!--[-->LlamaIndex Developer Experience<!--]--></a></h2><p><!--[-->Overall, I found the LlamaIndex documentation to be very helpful. Before using LlamaIndex for this project I had used LangChain to build a RAG POC, but I didn‚Äôt get very good results. I love how the LlamaIndex documentation has a 5-line starter example for building a RAG system:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
</span></span><span class="line" line="2"><span emptylineplaceholder="true">
</span></span><span class="line" line="3"><span>documents = SimpleDirectoryReader(&quot;data&quot;).load_data()
</span></span><span class="line" line="4"><span>index = VectorStoreIndex.from_documents(documents)
</span></span><span class="line" line="5"><span>query_engine = index.as_query_engine()
</span></span><span class="line" line="6"><span>response = query_engine.query(&quot;Some question about the data should go here&quot;)
</span></span><span class="line" line="7"><span>print(response)
</span></span></code><!--]--></pre><p><!--[-->Source: <a href="https://docs.llamaindex.ai/en/stable/#getting-started" rel="nofollow"><!--[-->https://docs.llamaindex.ai/en/stable/#getting-started<!--]--></a><!--]--></p><p><!--[-->I was able to expand this simple example to implement the text and image Q&amp;A bots for RedLM fairly easily. The application I built is somewhat similar to the <a href="https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_app_guide/" rel="nofollow"><!--[-->Full-Stack Web App with LLamaIndex<!--]--></a> included in their documentation.<!--]--></p><p><!--[-->Most of the early development I did on this project used the <code><!--[-->CustomQueryEngine<!--]--></code>. Later I tried using <a href="https://docs.llamaindex.ai/en/stable/module_guides/workflow/" rel="nofollow"><!--[-->LlamaIndex Workflows<!--]--></a> to better organize the logic in the text and image-based Q&amp;A bots. The same workflow <code><!--[-->RAGWorkflow<!--]--></code> is used to handle requests for both the text and image Q&amp;A bot queries. Workflows also work seamlessly with asynchronous Python frameworks like FastAPI. Here&#39;s the API endpoint for the multimodal image-Q&amp;A bot using a LlamaIndex Workflow:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>@app.post(&quot;/mm-q-and-a&quot;)
</span></span><span class="line" line="2"><span>async def mm_q_and_a_workflow(req_data: MultiModalRequest):
</span></span><span class="line" line="3"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="4"><span>    This function handles Multimodal Q&amp;A bot requests using a LlamaIndex workflow
</span></span><span class="line" line="5"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="6"><span>    try:
</span></span><span class="line" line="7"><span>        # parse data from request object
</span></span><span class="line" line="8"><span>        image_b64 = req_data.image
</span></span><span class="line" line="9"><span>        prompt = req_data.prompt
</span></span><span class="line" line="10"><span>        chapter = req_data.chapter
</span></span><span class="line" line="11"><span emptylineplaceholder="true">
</span></span><span class="line" line="12"><span>        # setup LlamaIndex Workflow and run it with data from request
</span></span><span class="line" line="13"><span>        w = RAGWorkflow(timeout=None)
</span></span><span class="line" line="14"><span>        result = await w.run(query=prompt, image_data=image_b64, chapter_number=chapter)
</span></span><span class="line" line="15"><span emptylineplaceholder="true">
</span></span><span class="line" line="16"><span>        # return response
</span></span><span class="line" line="17"><span>        return QAQueryResponse(
</span></span><span class="line" line="18"><span>            response=result[&quot;response&quot;].message.content,
</span></span><span class="line" line="19"><span>            metadata=result[&quot;metadata&quot;],
</span></span><span class="line" line="20"><span>            image_desc=result[&quot;image_description&quot;],
</span></span><span class="line" line="21"><span>        )
</span></span><span class="line" line="22"><span>    except Exception as e:
</span></span><span class="line" line="23"><span>        raise HTTPException(status_code=500, detail=str(e))
</span></span></code><!--]--></pre><p><!--[-->Using LlamaIndex Workflows also helped me add additional logic in a maintainable and standardized way. For example, I expanded the <code><!--[-->RAGWorkflow<!--]--></code> logic to include LLM-based re-ranking in order to ensure retrieval of the most relevant documents for my chatbot queries. This technique increases request latency, but this is an acceptable tradeoff for an application like RedLM.<!--]--></p><h3 id="llmrerank"><a href="#llmrerank"><!--[-->LLMRerank<!--]--></a></h3><p><!--[-->LLM Rerank was an interesting technique to try out, and LlamaIndex provides <code><!--[-->LLMRerank<!--]--></code> to make the implementation as simple as possible. Here&#39;s my understanding of how it works:<!--]--></p><ul><!--[--><li><!--[-->LLMRerank searches in the vector database for a high number of documents that are relevant to your query. This is done using cosine similarity, which essentially compares the vectors that represent the query and the documents.<!--]--></li><li><!--[-->Next, LLMRerank goes through a process of assigning a numerical to each document to score relevancy. It does this via a special prompt that requests relevancy score for each document in batches.<!--]--></li><li><!--[-->For example, I configured <code><!--[-->LLMRerank<!--]--></code> to initially fetch 4 documents from the vector database based on cosine similarity. Then in batches of 2, relevancy scores are assigned. Finally, the top 2 most relevant documents based on the LLM-give scores are used to make the RAG query.<!--]--></li><li><!--[-->Adding LLMRerank can require a number of additional queries based on how you configure the batch size and the number of documents you would like to compare. This will increase latency for your application and use more resources to make the extra calls.<!--]--></li><!--]--></ul><p><!--[-->Here&#39;s an example LLM query that <code><!--[-->LLMRerank<!--]--></code> uses to do assign scores:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="LLMRerank Prompt" data-nuxt-img srcset="/_ipx/_/static/redlm/llmrerank_prompt.png 1x, /_ipx/_/static/redlm/llmrerank_prompt.png 2x" src="/_ipx/_/static/redlm/llmrerank_prompt.png"><!--]--></p><p><!--[-->Here are logs from my application showing what happens inside the workflow.<!--]--></p><p><!--[-->Application for text-base Q&amp;A query:<!--]--></p><pre class="language-text"><!--[--><code>INFO:     üí¨Request for Q&amp;A chatbot: query=&#39;ÂÆùÁéâÂíåË∞ÅÊâìÊû∂Ôºü&#39;
INFO:     üîÄRouting Workflow to next step
INFO:     üí¨Routing to QueryEvent
INFO:     üßÆQuery the vector database with: ÂÆùÁéâÂíåË∞ÅÊâìÊû∂Ôºü
INFO:     üñ•Ô∏èUsing in-memory embedding database
INFO:     ‚è≥Loading index from storage directory...
INFO:     ‚úÖFinished loading index.
INFO:     üìêRetrieved 4 nodes.
INFO:     üîÄDoing LLMRerank
INFO:     ‚ÑπÔ∏è Chat Model Info:
INFO:     üü©Using NVIDIA Cloud API for inference
INFO:     üîòChat Model: baichuan-inc/baichuan2-13b-chat
INFO:     üî¢Reranked nodes to 2
INFO:     ü§ñDoing inference step
INFO:     ‚öôÔ∏è Getting query engine..
INFO:     üîéGetting response from custom query engine
INFO:     üí¨Text-based Q&amp;A query
INFO:     üÄÑText is Chinese
INFO:     Using nodes from workflow...
INFO:     üîèFormatting prompt
INFO:     Prompt is

ËøôÊòØÁõ∏ÂÖ≥ÁöÑÂèÇËÄÉËµÑÊñôÔºö
---------------------
ÂÆùÁéâ‰ªéÊù•Ê≤°ÊúâÁªèÂéÜËøáËøôÊ†∑ÁöÑÁóõËã¶„ÄÇËµ∑ÂàùÔºå‰ªñËßâÂæóË¢´ÊâìÂæóÂæàÁóõÔºå‰π±Âñä‰π±Âè´„ÄÇÂêéÊù•Ôºå‰ªñÁöÑÊ∞îÂèòÂæóËôöÂº±ÔºåÂ£∞Èü≥ÂèòÂæóÂò∂ÂìëÔºåÊó†Ê≥ïËØ¥ËØù„ÄÇ‰ºóÈó®ÂÆ¢ËßÅ‰ªñË¢´ÊâìÂæóÂæàÊÉ®ÔºåËµ∂‰∏äÊù•ÊÅ≥Ê±Ç‰ªñÂÅú‰∏ãÊù•„ÄÇË¥æÊîø‰∏çËÇØÂê¨ÔºåËØ¥Ôºö‚Äú‰Ω†‰ª¨Áü•ÈÅì‰ªñÂπ≤‰∫Ü‰ªÄ‰πàÂùè‰∫ãÔºåËøòËÉΩÈ•∂‰ªñÂêóÔºüÂπ≥Êó∂ÈÉΩÊòØ‰Ω†‰ª¨Ëøô‰∫õ‰∫∫Êää‰ªñÂ∏¶Âùè‰∫ÜÔºåÁé∞Âú®Âà∞‰∫ÜËøôÊ≠•Áî∞Âú∞Ôºå‰Ω†‰ª¨ËøòÊù•Âäù‰ªñ„ÄÇÊòéÂ§©ÔºåÂ¶ÇÊûú‰ªñÊùÄÁà∂ÂºëÂêõÔºå‰Ω†‰ª¨Êâç‰∏çÂäùÂêóÔºü‚Äù

ÂÆùÁéâ‰ªéÊù•Ê≤°ÊúâÁªèÂéÜËøáËøôÊ†∑ÁöÑÁóõËã¶„ÄÇËµ∑ÂàùÔºå‰ªñËßâÂæóÊâìÂæóÂæàÁóõÔºå‰π±Âñä‰π±Âè´„ÄÇÂêéÊù•Ôºå‰ªñÁöÑÊ∞îÂèòÂæóËôöÂº±ÔºåÂ£∞Èü≥ÂèòÂæóÂò∂ÂìëÔºåÊó†Ê≥ïËØ¥ËØù„ÄÇ‰ºóÈó®ÂÆ¢ËßÅ‰ªñË¢´ÊâìÂæóÂæàÊÉ®ÔºåËµ∂‰∏äÊù•ÊÅ≥Ê±Ç‰ªñÂÅú‰∏ãÊù•„ÄÇË¥æÊîø‰∏çËÇØÂê¨ÔºåËØ¥Ôºö‚Äú‰Ω†‰ª¨Áü•ÈÅì‰ªñÂπ≤‰∫Ü‰ªÄ‰πàÂùè‰∫ãÔºåËøòËÉΩÈ•∂‰ªñÂêóÔºüÂπ≥Êó∂ÈÉΩÊòØ‰Ω†‰ª¨Ëøô‰∫õ‰∫∫Êää‰ªñÂ∏¶Âùè‰∫ÜÔºåÁé∞Âú®Âà∞‰∫ÜËøôÊ≠•Áî∞Âú∞Ôºå‰Ω†‰ª¨ËøòÊù•Âäù‰ªñ„ÄÇÊòéÂ§©ÔºåÂ¶ÇÊûú‰ªñÊùÄÁà∂ÂºëÂêõÔºå‰Ω†‰ª¨Êâç‰∏çÂäùÂêóÔºü‚Äù
---------------------
Ê†πÊçÆ‰∏äËø∞ÁöÑÂèÇËÄÉËµÑÊñôÔºåÂõûÁ≠î‰∏ãÈù¢ÁöÑÈóÆÈ¢ò
ÈóÆÈ¢òÔºöÂÆùÁéâÂíåË∞ÅÊâìÊû∂Ôºü

Response...
ÂÆùÁéâÂíåË¥æÊîøÊâìÊû∂„ÄÇ
</code><!--]--></pre><p><!--[-->My question here was basically asking &quot;Who gets in a fight with Baoyu?&quot; The reply says that his father, Jiazheng, gets in a fight with Baoyu, and the documents that are used here very similar, differing by only one character. One of the documents is supposed to be and English translation, but in fact there was a failure in the translation for this paragraph and it &quot;translated&quot; the Chinese by simply repeating it. A translation of this paragraph using GPT 4o describes a tense scene between protagonist Jia Baoyu and his father Jia Zheng:<!--]--></p><blockquote><!--[--><p><!--[-->Baoyu had never endured such agony before. At first, he felt the pain intensely and cried out loudly. Later, his breath grew weak, his voice turned hoarse, and he couldn‚Äôt speak. The attendants, seeing how severely he was being beaten, rushed forward to plead for him to stop. Jia Zheng refused to listen, saying, ‚ÄúDo you know the misdeeds he‚Äôs committed, and still you want to spare him? Normally, it‚Äôs you people who lead him astray, and now that it‚Äôs come to this, you still try to persuade him? Tomorrow, if he were to commit patricide or treason, would you still not advise him?‚Äù<!--]--></p><!--]--></blockquote><p><!--[-->Another benefit of LlamaIndex workflows is the ability to create visualizations of each step, the branches between them and the overall flow of events and the functions that accept/emit them as arguments/return values. It took a little bit of getting used to the patterns used to create workflows, but the documentation for Workflows provided a good starting point that I could adapt for my application. Here&#39;s a visualization of the LlamaIndex Workflow that is used by the image and text-based Q&amp;A bots:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RedLM RAG Workflow" data-nuxt-img srcset="/_ipx/_/static/redlm/rag_workflow.png 1x, /_ipx/_/static/redlm/rag_workflow.png 2x" src="/_ipx/_/static/redlm/rag_workflow.png"><!--]--></p><h3 id="observability-and-tracing-with-langfuse"><a href="#observability-and-tracing-with-langfuse"><!--[-->Observability and Tracing with Langfuse<!--]--></a></h3><p><!--[-->It is never too soon to add observability and tracing to a RAG application! I learned this the hard way after doing some refactoring of prompts and <code><!--[-->CustomQueryEngine<!--]--></code> logic.<!--]--></p><blockquote><!--[--><p><!--[-->Langfuse is an open source LLM engineering platform to help teams collaboratively debug, analyze and iterate on their LLM Applications. With the Langfuse integration, you can seamlessly track and monitor performance, traces, and metrics of your LlamaIndex application. Detailed traces of the LlamaIndex context augmentation and the LLM querying processes are captured and can be inspected directly in the Langfuse UI.<!--]--></p><!--]--></blockquote><p><!--[-->LlamaIndex supports lots of different observability and tracing solutions. I tried using <a href="https://langfuse.com/" rel="nofollow"><!--[-->Langfuse<!--]--></a> (YC W23) which is an open-source option that has a self hosted option.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Langfuse tracing for RedLM" data-nuxt-img srcset="/_ipx/_/static/redlm/langfuse.png 1x, /_ipx/_/static/redlm/langfuse.png 2x" src="/_ipx/_/static/redlm/langfuse.png"><!--]--></p><p><!--[-->Langfuse came in handy when debugging the prompts for the image-based Q&amp;A bot. This screenshot shows a trace of a multi-modal Q&amp;A bot query about the fire at the Gourd Temple that occurs in Chapter 1 of the book.<!--]--></p><h2 id="nvidia-inference-stack-tensorrt-llm-and-buildnvidiacom"><a href="#nvidia-inference-stack-tensorrt-llm-and-buildnvidiacom"><!--[-->NVIDIA inference stack (TensorRT-LLM and build.nvidia.com)<!--]--></a></h2><p><!--[-->The LLM API for TensorRT-LLM is a very nice developer experience compared with my earlier attempts with manually building inference engines. The roadmap for TensorRT-LLM looks promising, I‚Äôm looking forward to support for an OpenAI Compatible API and more models. NVIDIA NIMs using TensorRT-LLM are an easy way to run models as OpenAI compatible API servers, but the selection of models is still pretty limited. vLLM provides a strong alternative with a wide range of support models. NVIDIA NIMs for LLMs build on vLLM libraries and the TensorRT-LLM library, so it is helpful to have an understanding of both of these libraries to stay on the bleeding edge of performant inference engines.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="trt-llm-roadmap" data-nuxt-img srcset="/_ipx/_/static/redlm/trt-llm-roadmap.png 1x, /_ipx/_/static/redlm/trt-llm-roadmap.png 2x" src="/_ipx/_/static/redlm/trt-llm-roadmap.png"><!--]--></p><p><!--[-->The NVIDIA API catalog is a great way to test a variety of different models, especially large models that cannot fit into consumer hardware like RTX PCs or high-end MacBooks. I got to try out the new meta/llama-3.2-90b-vision-instruct model in my project by simply changing a value in my .env file, this is a great developer experience!<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="build.nvidia.com" data-nuxt-img srcset="/_ipx/_/static/redlm/build.nvidia.com.png 1x, /_ipx/_/static/redlm/build.nvidia.com.png 2x" src="/_ipx/_/static/redlm/build.nvidia.com.png"><!--]--></p><p><!--[-->The NVIDIA API catalog doesn‚Äôt have every model in every size, however. For example, it has the qwen/qwen2-7b-instruct model, but doesn‚Äôt have the qwen/qwen2-7b-instruct model. Also, only some of the models are labeled as ‚ÄúRun Anywhere‚Äù; a lot of the models say ‚ÄúSelf-Hosted API Coming Soon‚Äù meaning that they can‚Äôt be downloaded an run locally as a container. To get around this, I ran inferences services locally using both vLLM‚Äôs vllm/vllm-openai container and my own container running Qwen and other services.<!--]--></p><h2 id="my-local-inference-stack-rtx"><a href="#my-local-inference-stack-rtx"><!--[-->My local inference stack (RTX)<!--]--></a></h2><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RTX PCs" data-nuxt-img srcset="/_ipx/_/static/redlm/rtxpcs.png 1x, /_ipx/_/static/redlm/rtxpcs.png 2x" src="/_ipx/_/static/redlm/rtxpcs.png"><!--]--></p><p><!--[-->Two of the RTX PCs in my home network: <code><!--[-->a1<!--]--></code> and <code><!--[-->a3<!--]--></code>. <code><!--[-->a1<!--]--></code> was the first PC I built by myself and was the beginning of my GeForce journey. Luckily I built it with an over-provisioned PSU, so it can use a 4090 FE card! The front panel doesn&#39;t fit, however.<!--]--></p><p><!--[-->One limitation of the NVIDIA API catalog is the number of free credits given for a trial account. Using 1 credit per API call, I would use up the 1000 credits very quickly when running scripts like translation or the RAG evaluation. The same would be true with rate limits of the OpenAI API. That‚Äôs why running LLMs locally is still an important part of the development cycle for this type of project.<!--]--></p><p><!--[-->This project primarily uses two models: a large language model and a vision language models. Running the Yi-1.5-9B-Chat model from <a href="http://01.AI" rel="nofollow"><!--[-->01.AI<!--]--></a> takes up just about all of the GPU memory on one of my RTX 4090 PCs, so I had to run the vision model on another PC. In a previous project, I used Kubernetes to manage lots of different inference services: LLMs, ComfyUI, ChatTTS and MusicGen for making AI videos and I found it to a nice way to manage different containerized inference services.<!--]--></p><pre class="language-text"><!--[--><code>brian@a3:~$ microk8s kubectl get no -o wide
NAME   STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
a1     Ready    &lt;none&gt;   4d4h   v1.30.5   192.168.5.182   &lt;none&gt;        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28
a2     Ready    &lt;none&gt;   11d    v1.30.5   192.168.5.96    &lt;none&gt;        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28
a3     Ready    &lt;none&gt;   11d    v1.30.5   192.168.5.173   &lt;none&gt;        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28
</code><!--]--></pre><p><!--[-->In the RedLM GitHub repo I included kubernetes manifests that show how to run the LLM and VLM across two different computers. I used Kustomize as a way to replace dynamic values in the YAML files for different resources. The kubernetes set up is experimental; the LLM and VLM can more reliably be run with <code><!--[-->docker run<!--]--></code> commands.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="k8s dashboard for local inference services" data-nuxt-img srcset="/_ipx/_/static/redlm/k8s-dashboard.png 1x, /_ipx/_/static/redlm/k8s-dashboard.png 2x" src="/_ipx/_/static/redlm/k8s-dashboard.png"><!--]--></p><p><!--[-->I had a lot of driver issues when trying to get kubernetes to run the vLLM container for the Yi LLM. I struggled with the following error message when trying to run the <code><!--[-->vllm<!--]--></code> LLM service:<!--]--></p><blockquote><!--[--><p><!--[-->RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW<!--]--></p><!--]--></blockquote><p><!--[-->I tried uninstalling and reinstalling different versions of the NVIDIA drivers and CUDA but kept seeing the same message once the server would try to start up in the vLLM container logs. Rebooting my PC didn&#39;t work either. I saw a recommendation to turn off secure boot in BIOS. I didn&#39;t turn it on, but having nothing else to try I went into the BIOS settings and found that there were some keys configured in the secure boot section. After I deleted these keys and reboot, everything seemed to work normally. I&#39;m not sure why my PC was in secure boot mode, though!<!--]--></p><h2 id="ai-models-used-in-this-project"><a href="#ai-models-used-in-this-project"><!--[-->AI Models used in this project<!--]--></a></h2><p><!--[-->I selected LLMs that run efficiently on RTX PCs, are available in the NVIDIA API catalog, and offer strong bilingual support in Chinese and English, ensuring compatibility, performance, and linguistic flexibility. Here are the models that I ended up using with RedLM:<!--]--></p><h3 id="_01-aiyi-15-9b-chat-and-nvidiayi-large"><a href="#_01-aiyi-15-9b-chat-and-nvidiayi-large"><!--[--><code><!--[-->01-ai/Yi-1.5-9B-Chat<!--]--></code> and <code><!--[-->nvidia/yi-large<!--]--></code><!--]--></a></h3><p><!--[-->I used <code><!--[-->01-ai/Yi-1.5-9B-Chat<!--]--></code> for most of the LLM inference while developing RedLM on my RTX PCs. <a href="https://github.com/01-ai/Yi" rel="nofollow"><!--[-->This model family<!--]--></a> performs well on both Chinese and English benchmarks, and has a variety of model sizes. I was able to try using the <code><!--[-->01-ai/yi-large<!--]--></code> model from the NVIDIA API catalog when using remote cloud inference. I used the <code><!--[-->vllm/vllm-openai:latest<!--]--></code> container to run this locally.<!--]--></p><p><!--[-->There are also vision models in the Yi series, such as <a href="https://huggingface.co/01-ai/Yi-VL-34B" rel="nofollow"><!--[-->01-ai/Yi-VL-34B<!--]--></a>, but I didn&#39;t use these models in my project.<!--]--></p><h3 id="baichuan-incbaichuan2-13b-chat"><a href="#baichuan-incbaichuan2-13b-chat"><!--[--><code><!--[-->baichuan-inc/baichuan2-13b-chat<!--]--></code><!--]--></a></h3><p><!--[-->This model is available in the NVIDIA API catalog, and it was the main model I used when testing remote inference. It performs well in a variety of tasks and scores highly on the the Chinese Massive Multitask Language Understanding (CMMLU) benchmark.<!--]--></p><h3 id="qwenqwen2-7b"><a href="#qwenqwen2-7b"><!--[--><code><!--[-->Qwen/Qwen2-7B<!--]--></code><!--]--></a></h3><p><!--[-->This model was used for summary and translation of the source text. It was supported by the TensorRT-LLM LLM API and I didn&#39;t have any issues building the TensorRT-LLM model with it on the EC2 instance used to do the completion inference for translations.<!--]--></p><h3 id="qwenqwen2-vl-2b-instruct"><a href="#qwenqwen2-vl-2b-instruct"><!--[--><code><!--[-->Qwen/Qwen2-VL-2B-Instruct<!--]--></code><!--]--></a></h3><p><!--[-->This was the vision language model (VLM) that I used locally when developing on RTX. I was impressed at how well it could describe images given the small parameter count of the model (2 billion parameters). The small size of this model made it easy to run in my RTX PC cluster.<!--]--></p><p><!--[-->There is <a href="https://github.com/NVIDIA/TensorRT-LLM/issues/2183" rel="nofollow"><!--[-->an open GitHub issue for TensorRT-LLM support for Qwen2-VL<!--]--></a> at the time of writing.<!--]--></p><p><!--[-->I wrote a simple FastAPI server using the Hugging Face <code><!--[-->transformers<!--]--></code> library based on example code from this model&#39;s documentation (see <code><!--[-->services/qwen2-vl<!--]--></code> in the RedLM GitHub repo for more details). I packaged this service into a container in order to run it in my local kubernetes cluster along with other inference services.<!--]--></p><h3 id="metallama-32-90b-vision-instruct"><a href="#metallama-32-90b-vision-instruct"><!--[--><code><!--[-->meta/llama-3.2-90b-vision-instruct<!--]--></code><!--]--></a></h3><p><!--[-->This model came out while I was working on the project, and I decided to use it instead of the <code><!--[-->adept/fuyu-8b<!--]--></code> model that was previously one of the only vision language models in the NVIDIA API catalog. The <code><!--[-->meta/llama-3.2-90b-vision-instruct<!--]--></code> model has strong Chinese language skills, so it was a good model to use when doing remote inference for the image Q&amp;A bot.<!--]--></p><h3 id="nvidianvlm-d-72b"><a href="#nvidianvlm-d-72b"><!--[--><a href="https://huggingface.co/nvidia/NVLM-D-72B" rel="nofollow"><!--[--><code><!--[-->nvidia/NVLM-D-72B<!--]--></code><!--]--></a><!--]--></a></h3><p><!--[-->I didn&#39;t use this model in my project, but it came out recently and looks awesome! Hopefully this model will be available on the NVIDIA API catalog soon. It is trained on the <code><!--[-->Qwen2-72B-Instruct<!--]--></code> text-only model, so it likely also has very strong support for Chinese language.<!--]--></p><blockquote><!--[--><p><!--[-->Today (September 17th, 2024), we introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.<!--]--></p><!--]--></blockquote><h2 id="the-success-of-black-myth-wukong"><a href="#the-success-of-black-myth-wukong"><!--[-->The success of Black Myth: Wukong<!--]--></a></h2><p><!--[-->I originally got the idea to build this project after seeing the release of Black Myth: Wukong. This game is a blockbuster success from a Chinese developer that tells the story of the Monkey King‚Äôs adventure in the Journey West universe. Journey West (Ë•øÊ∏∏ËÆ∞) is another one of the ‚ÄúFour Great Works‚Äù of Chinese literature. It tells the story of the legendary pilgrimage of the monk Xuanzang (also known as Tang Sanzang) to India, accompanied by his three disciples‚ÄîSun Wukong (the Monkey King), Zhu Bajie (Pigsy), and Sha Wujing (Sandy). The group travels from China to India to retrieve sacred Buddhist scriptures, facing numerous challenges, demons, and supernatural beings along the way.<!--]--></p><p><!--[-->The novel blends elements of adventure, mythology, and spiritual allegory, with Sun Wukong&#39;s mischievous nature and extraordinary powers adding humor and excitement. Through their journey, the characters grow and overcome personal flaws, ultimately achieving enlightenment and spiritual success. The video game adaptation has set world records for numbers of concurrent players, and it has rewritten the narrative around what is possible with single-player, offline games in the gaming industry.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Black Myth: Wukong" data-nuxt-img srcset="/_ipx/_/static/redlm/wukong.png 1x, /_ipx/_/static/redlm/wukong.png 2x" src="/_ipx/_/static/redlm/wukong.png"><!--]--></p><p><!--[-->Three renditions of Journey West: Songok≈´ (The Monkey King) polychrome woodblock (surimono) (1824) by Yashima Gakutei (1786‚Äì1868), Black Myth: Wukong video game by Game Science (2024), Journey to the West TV series by CCTV (1982-2000)<!--]--></p><h2 id="redlm-video"><a href="#redlm-video"><!--[-->RedLM video<!--]--></a></h2><p><!--[--><a href="https://x.com/briancaffey/status/1855186768452321330" rel="nofollow"><!--[-->Watch the RedLM video on ùïè<!--]--></a><!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-media-max-width="560"><p lang="en" dir="ltr">RedLM: An AI-powered application for Redology<br>ü™®‚ú®üìúüîéüñºÔ∏è<br>A project for the <a href="https://twitter.com/hashtag/NVIDIADevContest?src=hash&amp;ref_src=twsrc%5Etfw">#NVIDIADevContest</a> <a href="https://twitter.com/hashtag/LlamaIndex?src=hash&amp;ref_src=twsrc%5Etfw">#LlamaIndex</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://twitter.com/llama_index?ref_src=twsrc%5Etfw">@llama_index</a> <a href="https://t.co/BUaBBbsLGl">pic.twitter.com/BUaBBbsLGl</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1855186768452321330?ref_src=twsrc%5Etfw">November 9, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->I created the video for this project using Blender.The Blender sequencer editor is a great non-linear video editing tool for simple video projects like this one. I used the following formula to create the project video for RedLM:<!--]--></p><ol><!--[--><li><!--[-->Background music: I used the AI music generation service called Suno with the prompt ‚Äúmystical strange traditional Chinese music from the Qing Dynasty‚Äù. Here‚Äôs the link to my Suno playlist called ‚ÄúQing Dynasty Music‚Äù where you can find the original song and some other good songs that I generated using this prompt. My <a href="https://suno.com/playlist/863ea0dd-1921-467c-8b69-16dbd126d966" rel="nofollow"><!--[-->Qing Dynasty Music Playlist on Suno<!--]--></a><!--]--></li><li><!--[-->Outline: For this project, the main sections are the introduction, then explaining each part with a short demo: translation, text-based Q&amp;A, evaluation for text-based Q&amp;A, image-based Q&amp;A, and finally a short outro. I wrote an outline and then ChatGPT helped with filling out the content.<!--]--></li><li><!--[-->Narration: I used ElevenLabs to narrate the main part of the video using a clone of my voice using the ElevenLabs Voice Lab. The Chinese voices were generated on my computer with an open-source text-to-speech model called ChatTTS.<!--]--></li><li><!--[-->Images and videos: I gathered images and screen captures of different parts of the project including code snippets, paintings of the book, flow diagrams and screen recordings of the application.<!--]--></li><!--]--></ol><p><!--[-->The video is composed of different ‚Äústrips‚Äù. The green strips represent the music and voice clips. Red strips are images and yellow strips are videos. Here is what the final cut of the video looks like in Blender‚Äôs Sequencer view:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Blender Sequence Editor" data-nuxt-img srcset="/_ipx/_/static/redlm/blender_sequence_editor.png 1x, /_ipx/_/static/redlm/blender_sequence_editor.png 2x" src="/_ipx/_/static/redlm/blender_sequence_editor.png"><!--]--></p><p><!--[-->ChatTTS is one of the most impressive open-source models I have seen for generating conversational speech with prosodic elements (pausing, laughter, etc.) It is developed by a Chinese company called 2noise. Earlier this year I made a small contribution to this project with an API example using FastAPI to show how to run a standalone API using the model. Another example in this project provides a comprehensive example application built with gradio:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="ChatTTS UI" data-nuxt-img srcset="/_ipx/_/static/redlm/chattts_ui.png 1x, /_ipx/_/static/redlm/chattts_ui.png 2x" src="/_ipx/_/static/redlm/chattts_ui.png"><!--]--></p><p><!--[-->I was planning on streaming the narration audio for Q&amp;A answers using my ChatTTS API service, but I didn‚Äôt get around to doing this. Instead, I just used the Gradio application to generate the Chinese narration for Q&amp;A and image Q&amp;A examples included in the video.<!--]--></p><h3 id="redlm-deep-dive-video-with-notebooklm"><a href="#redlm-deep-dive-video-with-notebooklm"><!--[-->RedLM Deep Dive video with NotebookLM<!--]--></a></h3><p><!--[-->NotebookLM is a new application from Google that is a truly magical application of retrieval augmented generation.<!--]--></p><blockquote><!--[--><p><!--[-->NotebookLM is a research and note-taking online tool developed by Google Labs that uses artificial intelligence, specifically Google Gemini, to assist users in interacting with their documents. It can generate summaries, explanations, and answers based on content uploaded by users.<!--]--></p><!--]--></blockquote><p><!--[-->I used NotebookLM to generate a &quot;Deep Dive&quot; podcast episode using only this article. I was pretty impressed with what it was able to produce, and I wanted to share it as part of this project, so I used Blender and some Python scripts to put together a simple and engaging visualization.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Deep Dive video in Blender" data-nuxt-img srcset="/_ipx/_/static/redlm/deep_dive_blender.png 1x, /_ipx/_/static/redlm/deep_dive_blender.png 2x" src="/_ipx/_/static/redlm/deep_dive_blender.png"><!--]--></p><p><!--[-->The <code><!--[-->openai/whisper-base<!--]--></code> model was used to get time stamps for the start and end of each spoken word using Automated Speech Recognition (ASR). A speaker segmentation library called <a href="https://github.com/pyannote/pyannote-audio" rel="nofollow"><!--[--><code><!--[-->pyannote/audio<!--]--></code><!--]--></a> was used to perform speaker diarization. This is an interesting algorithm that can segment any number of distinct speakers in an audio recording using a series of models and a discrete-time stochastic process known as the <a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process" rel="nofollow"><!--[-->Chinese restaurant process<!--]--></a>. This gave a list of time intervals with a speaker ID, and I used the intervals to attribute a speaker ID to each word. Then I segmented the audio into two files using this data and used the files to generate audio waveforms using Blender&#39;s geometry nodes. Another script was used to animate each word of as it is spoken in one of two positions for each speaker.<!--]--></p><h2 id="final-thoughts"><a href="#final-thoughts"><!--[-->Final thoughts<!--]--></a></h2><p><!--[-->I‚Äôm glad to have had the opportunity to join three NVIDIA developer contests this year. I like the idea of a ‚Äúdeveloper contest‚Äù that takes place over several weeks compared to hackathons that take place over just a few days. Having more time allows you to learn about a new tool or framework at a deeper level and think about how to apply it in a creative project.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="NVIDIA and LlamaIndex Contest" data-nuxt-img srcset="/_ipx/_/static/redlm/llama-contest-og.jpg 1x, /_ipx/_/static/redlm/llama-contest-og.jpg 2x" src="/_ipx/_/static/redlm/llama-contest-og.jpg"><!--]--></p><p><!--[-->I also like how this contest is not team based. Working on this project I was able to do a lot of high-level thinking, write out features as detailed prompts, and then delegate the code writing to LLMs as if I was giving tasks to teammates.<!--]--></p><p><!--[-->NVIDIA‚Äôs contests are ‚Äúglobal developer contests‚Äù, but the contests so far are not open to developers in India and China. This is probably due to local rules and regulations governing how contests, prizes and taxes work. It is too bad; I would love to see what types of applications would come from participants in these countries. Also, there are also a lot of really interesting developments happening in the LLM space in both China and India!<!--]--></p><p><!--[-->The LLMs I used in this project were developed by leading Chinese AI companies, and they are competitive with LLMs from Western countries on LLM benchmarks despite having access to fewer GPU resources. <a href="https://qwenlm.github.io/blog/qwen2.5-coder-family/" rel="nofollow"><!--[-->Qwen recently released a new model called <code><!--[-->Qwen2.5-Coder-32B<!--]--></code><!--]--></a> that has outperfomed leading models at coding tasks.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Qwen coder model" data-nuxt-img srcset="/_ipx/_/static/redlm/qwen_coder.png 1x, /_ipx/_/static/redlm/qwen_coder.png 2x" src="/_ipx/_/static/redlm/qwen_coder.png"><!--]--></p><p><!--[--><a href="https://www.youtube.com/watch?v=UitJxc9LE60" rel="nofollow"><!--[-->Kaifu Lee mentioned in a Bloomberg interview<!--]--></a> that the scarcity of GPU resources in China will force Chinese engineers to innovate in new ways to gain an advantage. One example of this we saw recently was when Chinese hardware hackers doubled the usable memory of the RTX 4090D (a variant of the RTX 4090 card with lower processing power to comply with US export regulations for China - the D stands for Dragon, apparently!)<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="RTX 4090D 48GB" data-nuxt-img srcset="/_ipx/_/static/redlm/RTX4090D.jpg 1x, /_ipx/_/static/redlm/RTX4090D.jpg 2x" src="/_ipx/_/static/redlm/RTX4090D.jpg"><!--]--></p><p><!--[-->NVIDIA recently concluded it&#39;s AI Summit in Mumbai. I was intrigued by the fact that Hindi has unique challenges that have have limited the development of Hindi LLMs compared to the development of English and Chinese LLMs. In a conversation with Jensen Huang, Indian industrial titan and CEO of Reliance Industries Mukesh Ambani spoke about his aspirations and ambition for India to overcome these challenges and develop a Hindi LLM. In a viral moment Mukesh Ambani shared that through devotion to attaining knowledge through the Hindu Goddess of knowledge Sarawati, India will be met by the Goddess of prosperity, Lakshmi.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Mukesh Ambani" data-nuxt-img srcset="/_ipx/_/static/redlm/mukesh_ambani.png 1x, /_ipx/_/static/redlm/mukesh_ambani.png 2x" src="/_ipx/_/static/redlm/mukesh_ambani.png"><!--]--></p><p><!--[-->NVIDIA recently released a small language model for Hindi at the AI Summit in Mumbai called  <a href="https://indiaai.gov.in/article/nvidia-unveils-nemotron-4-mini-hindi-4b-ai-for-india-s-500-million-hindi-speakers" rel="nofollow"><!--[-->Nemotron-4-Mini-Hindi-4B<!--]--></a>. Hindi LLMs could enable applications to explore important works of literature from India. I don&#39;t know that much about India literature, but a comparable work of literature in size and cultural significance might be the Ramayana.<!--]--></p><p><!--[--><em><!--[-->The Ramayana is an ancient Indian epic that tells the story of Prince Rama&#39;s heroic quest to rescue his wife, Sita, who has been kidnapped by the demon king Ravana. Set in a world of gods, demons, and celestial beings, the story explores themes of duty, loyalty, and the triumph of good over evil. Guided by wisdom, strength, and the support of devoted allies like Hanuman, the monkey god, and his brother Lakshmana, Rama&#39;s journey is a deeply spiritual tale, celebrated for its poetic beauty and moral depth. The Ramayana continues to inspire and captivate audiences across cultures.<!--]--></em><!--]--></p><p><!--[-->The Ramayana story journeyed to Thailand centuries ago, transforming into the Ramakien, a Thai adaptation that retains the essence of the original Indian epic while adding distinctive Thai cultural elements. Introduced through trade, diplomacy, and cultural exchange between India and Southeast Asia, the story became deeply woven into Thailand‚Äôs art, literature, and performance traditions. Thai kings, particularly King Rama I, adapted and documented the Ramakien, giving it a prominent place in Thai history. Lavishly detailed murals surrounding the Temple of the Emerald Buddha in Bangkok‚Äôs Grand Palace depict the Ramakien in over 178 panels that totaling over 2 kilometers in length. On a recent visit to the Grand Palace, I imagined having an application that could link the detailed murals to elements of the story in Hindi, Thai, English, Chinese or any language.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Ramakien murals surrounding Temple of the Emerald Buddha" data-nuxt-img srcset="/_ipx/_/static/redlm/ramakien.png 1x, /_ipx/_/static/redlm/ramakien.png 2x" src="/_ipx/_/static/redlm/ramakien.png"><!--]--></p><p><!--[-->The Dream of the Red Chamber, originally titled The Story of the Stone, is one of China‚Äôs greatest literary works and a masterpiece of world literature. The novel begins with a frame story centered on a magical stone, left over from the Chinese creation myth where the goddess Nuwa mends the heavens. Longing to experience the human world, the sentient stone persuades a Buddhist monk and a Taoist priest to reincarnate it as a boy. This boy, Baoyu, is born into a wealthy and influential family‚Äîa character partly based on the author, Cao Xueqin, and his own aristocratic upbringing. Through Baoyu&#39;s life, friendships, and romantic relationships, the novel delves into his family‚Äôs gradual decline, mirroring the instability of China‚Äôs own noble families in the late Qing dynasty. The story also portrays the era&#39;s customs, social structures, and beliefs, offering readers a richly detailed exploration of life in Qing China.<!--]--></p><p><!--[-->It was a lot of fun to work on this project with tools from LlamaIndex and NVIDIA. With AI technology, GPUs are now essentially sentient stones, and I was able to share this important touchstone of the human experience with my computers using LlamaIndex and open source language models. In turn, RedLM shared with me delightful insights into world of Dream of the Red Chamber.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Story of a Stone" data-nuxt-img srcset="/_ipx/_/static/redlm/stone_story.png 1x, /_ipx/_/static/redlm/stone_story.png 2x" src="/_ipx/_/static/redlm/stone_story.png"><!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Story of a Stone Analysis" data-nuxt-img srcset="/_ipx/_/static/redlm/stone_story_analysis.png 1x, /_ipx/_/static/redlm/stone_story_analysis.png 2x" src="/_ipx/_/static/redlm/stone_story_analysis.png"><!--]--></p><blockquote><!--[--><p><!--[-->This scene describes a piece of traditional Chinese painting, depicting two elderly figures conversing amidst mountains and rivers. The painting likely visually represents the scene from the book where a monk and a Taoist are chatting at the foot of Qinggeng Peak. The two elderly figures in the painting may represent the monk and Taoist from the book, discussing their discovery of a bright and pristine stone, and planning to take it to a bustling, splendid place for a happy life. The painting‚Äôs elements‚Äîmountains, peaks, flowing water, trees, and rocks‚Äîmight echo the book&#39;s descriptions, illustrating the natural environment at the base of Qinggeng Peak where the monk and Taoist reside. The painting‚Äôs tranquil and harmonious atmosphere may also align with the storyline, expressing the monk and Taoist&#39;s care for the stone and their wish for it to live a happy life. In summary, this painted scene might be an artistic portrayal of the story between the monk, the Taoist, and the stone from the book, using visual elements and ambiance to convey the narrative and themes within the story.<!--]--></p><!--]--></blockquote><style>html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}</style></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments üí¨ </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> ¬© 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest":-1},"/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest",{},1753130128174]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"üá∫üá∏",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Fran√ßais",flag:"üá´üá∑",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"ÁÆÄ‰Ωì‰∏≠Êñá",flag:"üá®üá≥",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"–†—É—Å—Å–∫–∏–π",flag:"üá∑üá∫",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"Êó•Êú¨Ë™û",flag:"üáØüáµ",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"‡§π‡§ø‡§Ç‡§¶‡•Ä",flag:"üáÆüá≥",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"711989cd-b755-45e7-b273-f0754c0f755f",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>