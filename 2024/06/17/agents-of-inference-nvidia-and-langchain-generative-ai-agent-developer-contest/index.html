<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Agents of Inference: My submission for NVIDIA&#x27;s Generative AI Agents Developer Contest by NVIDIA and LangChain</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-d2ff5fdf]{padding:0}.selected[data-v-d2ff5fdf]{text-shadow:.25px 0 0 #000}.picker[data-v-d2ff5fdf]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-d2ff5fdf]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-ed0088f5]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-2649ce61]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-2649ce61]:hover{transform:scale(1.3)}.centered[data-v-2649ce61]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>.emoji-mart,.emoji-mart *{-webkit-box-sizing:border-box;box-sizing:border-box;line-height:1.15}.emoji-mart{display:-webkit-box;display:-ms-flexbox;display:flex;font-family:-apple-system,BlinkMacSystemFont,Helvetica Neue,sans-serif;font-size:16px;-webkit-box-orient:vertical;-webkit-box-direction:normal;background:#fff;border:1px solid #d9d9d9;border-radius:5px;color:#222427;-ms-flex-direction:column;flex-direction:column;height:420px}.emoji-mart-emoji{background:none;border:none;-webkit-box-shadow:none;box-shadow:none;padding:6px}.emoji-mart-emoji span{display:inline-block}.emoji-mart-preview-emoji .emoji-mart-emoji span{font-size:32px;height:38px;width:38px}.emoji-type-native{font-family:Segoe UI Emoji,Segoe UI Symbol,Segoe UI,Apple Color Emoji,Twemoji Mozilla,Noto Color Emoji,EmojiOne Color,Android Emoji;word-break:keep-all}.emoji-type-image{background-size:6100%}.emoji-type-image.emoji-set-apple{background-image:url(https://unpkg.com/emoji-datasource-apple@15.0.1/img/apple/sheets-256/64.png)}.emoji-type-image.emoji-set-facebook{background-image:url(https://unpkg.com/emoji-datasource-facebook@15.0.1/img/facebook/sheets-256/64.png)}.emoji-type-image.emoji-set-google{background-image:url(https://unpkg.com/emoji-datasource-google@15.0.1/img/google/sheets-256/64.png)}.emoji-type-image.emoji-set-twitter{background-image:url(https://unpkg.com/emoji-datasource-twitter@15.0.1/img/twitter/sheets-256/64.png)}.emoji-mart-bar{border:0 solid #d9d9d9}.emoji-mart-bar:first-child{border-bottom-width:1px;border-top-left-radius:5px;border-top-right-radius:5px}.emoji-mart-bar:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-top-width:1px}.emoji-mart-scroll{overflow-y:scroll;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-anchors{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;color:#858585;justify-content:space-between;line-height:0;padding:0 6px}.emoji-mart-anchor{display:block;position:relative;-webkit-box-flex:1;background:none;border:none;-webkit-box-shadow:none;box-shadow:none;-ms-flex:1 1 auto;flex:1 1 auto;overflow:hidden;padding:12px 4px;text-align:center;-webkit-transition:color .1s ease-out;transition:color .1s ease-out}.emoji-mart-anchor-selected,.emoji-mart-anchor:hover{color:#464646}.emoji-mart-anchor-selected .emoji-mart-anchor-bar{bottom:0}.emoji-mart-anchor-bar{background-color:#464646;bottom:-3px;height:3px;left:0;position:absolute;width:100%}.emoji-mart-anchors i{display:inline-block;max-width:22px;width:100%}.emoji-mart-anchors svg{fill:currentColor;max-height:18px}.emoji-mart .scroller{height:250px;position:relative;-webkit-box-flex:1;-ms-flex:1;flex:1;padding:0 6px 6px;will-change:transform;z-index:0;-webkit-overflow-scrolling:touch}.emoji-mart-search{margin-top:6px;padding:0 6px}.emoji-mart-search input{border:1px solid #d9d9d9;border-radius:25px;display:block;font-size:16px;outline:0;padding:.2em .6em;width:100%}.emoji-mart-search-results{height:250px;overflow-y:scroll}.emoji-mart-category{position:relative}.emoji-mart-category .emoji-mart-emoji span{cursor:default;position:relative;text-align:center;z-index:1}.emoji-mart-category .emoji-mart-emoji:hover:before,.emoji-mart-emoji-selected:before{background-color:#f4f4f4;border-radius:100%;content:"";height:100%;left:0;opacity:1;position:absolute;top:0;width:100%;z-index:0}.emoji-mart-category-label{position:-webkit-sticky;position:sticky;top:0}.emoji-mart-static .emoji-mart-category-label{position:relative;z-index:2}.emoji-mart-category-label h3{background-color:#fff;background-color:#fffffff2;display:block;font-size:16px;font-weight:500;padding:5px 6px;width:100%}.emoji-mart-emoji{display:inline-block;font-size:0;position:relative}.emoji-mart-no-results{color:#858585;font-size:14px;padding-top:70px;text-align:center}.emoji-mart-no-results .emoji-mart-category-label{display:none}.emoji-mart-no-results .emoji-mart-no-results-label{margin-top:.2em}.emoji-mart-no-results .emoji-mart-emoji:hover:before{content:none}.emoji-mart-preview{height:70px;position:relative}.emoji-mart-preview-data,.emoji-mart-preview-emoji,.emoji-mart-preview-skins{position:absolute;top:50%;-webkit-transform:translateY(-50%);-ms-transform:translateY(-50%);transform:translateY(-50%)}.emoji-mart-preview-emoji{left:12px}.emoji-mart-preview-data{left:68px;right:12px;word-break:break-all}.emoji-mart-preview-skins{right:30px;text-align:right}.emoji-mart-preview-name{font-size:14px}.emoji-mart-preview-shortname{color:#888;font-size:12px}.emoji-mart-preview-emoticon+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-emoticon,.emoji-mart-preview-shortname+.emoji-mart-preview-shortname{margin-left:.5em}.emoji-mart-preview-emoticon{color:#bbb;font-size:11px}.emoji-mart-title span{display:inline-block;vertical-align:middle}.emoji-mart-title .emoji-mart-emoji{padding:0}.emoji-mart-title-label{color:#999a9c;font-size:21px;font-weight:300}.emoji-mart-skin-swatches{background-color:#fff;border:1px solid #d9d9d9;border-radius:12px;font-size:0;padding:2px 0}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch{padding:0 2px;width:16px}.emoji-mart-skin-swatches-opened .emoji-mart-skin-swatch-selected:after{opacity:.75}.emoji-mart-skin-swatch{display:inline-block;-webkit-transition-duration:.125s;transition-duration:.125s;-webkit-transition-property:width,padding;transition-property:width,padding;-webkit-transition-timing-function:ease-out;transition-timing-function:ease-out;vertical-align:middle;width:0}.emoji-mart-skin-swatch:first-child{-webkit-transition-delay:0s;transition-delay:0s}.emoji-mart-skin-swatch:nth-child(2){-webkit-transition-delay:.03s;transition-delay:.03s}.emoji-mart-skin-swatch:nth-child(3){-webkit-transition-delay:.06s;transition-delay:.06s}.emoji-mart-skin-swatch:nth-child(4){-webkit-transition-delay:.09s;transition-delay:.09s}.emoji-mart-skin-swatch:nth-child(5){-webkit-transition-delay:.12s;transition-delay:.12s}.emoji-mart-skin-swatch:nth-child(6){-webkit-transition-delay:.15s;transition-delay:.15s}.emoji-mart-skin-swatch-selected{padding:0 2px;position:relative;width:16px}.emoji-mart-skin-swatch-selected:after{background-color:#fff;border-radius:100%;content:"";height:4px;left:50%;margin:-2px 0 0 -2px;opacity:0;pointer-events:none;position:absolute;top:50%;-webkit-transition:opacity .2s ease-out;transition:opacity .2s ease-out;width:4px}.emoji-mart-skin{border-radius:100%;display:inline-block;max-width:12px;padding-top:100%;width:100%}.emoji-mart-skin-tone-1{background-color:#ffc93a}.emoji-mart-skin-tone-2{background-color:#fadcbc}.emoji-mart-skin-tone-3{background-color:#e0bb95}.emoji-mart-skin-tone-4{background-color:#bf8f68}.emoji-mart-skin-tone-5{background-color:#9b643d}.emoji-mart-skin-tone-6{background-color:#594539}.emoji-mart .vue-recycle-scroller{position:relative}.emoji-mart .vue-recycle-scroller.direction-vertical:not(.page-mode){overflow-y:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal:not(.page-mode){overflow-x:auto}.emoji-mart .vue-recycle-scroller.direction-horizontal{display:-webkit-box;display:-ms-flexbox;display:flex}.emoji-mart .vue-recycle-scroller__slot{-webkit-box-flex:1;-ms-flex:auto 0 0px;flex:auto 0 0}.emoji-mart .vue-recycle-scroller__item-wrapper{-webkit-box-flex:1;-webkit-box-sizing:border-box;box-sizing:border-box;-ms-flex:1;flex:1;overflow:hidden;position:relative}.emoji-mart .vue-recycle-scroller.ready .vue-recycle-scroller__item-view{left:0;position:absolute;top:0;will-change:transform}.emoji-mart .vue-recycle-scroller.direction-vertical .vue-recycle-scroller__item-wrapper{width:100%}.emoji-mart .vue-recycle-scroller.direction-horizontal .vue-recycle-scroller__item-wrapper{height:100%}.emoji-mart .vue-recycle-scroller.ready.direction-vertical .vue-recycle-scroller__item-view{width:100%}.emoji-mart .vue-recycle-scroller.ready.direction-horizontal .vue-recycle-scroller__item-view{height:100%}.emoji-mart .resize-observer[data-v-b329ee4c]{background-color:transparent;border:none;opacity:0}.emoji-mart .resize-observer[data-v-b329ee4c],.emoji-mart .resize-observer[data-v-b329ee4c] object{display:block;height:100%;left:0;overflow:hidden;pointer-events:none;position:absolute;top:0;width:100%;z-index:-1}.emoji-mart-search .hidden{display:none;visibility:hidden}</style><style>span.emoji-mart-emoji[data-v-03afe4fa]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-03afe4fa]:hover{transform:scale(1.3)}.picker[data-v-03afe4fa]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-03afe4fa]{background-color:var(--bg)}.localeText[data-v-03afe4fa]{color:var(--color-primary)}</style><style>.tag[data-v-eb2063c1]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-eb2063c1]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.BAXwoxDU.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/U2gz5u3s.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D3Q7tgbv.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CAEbC5FA.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BE4wn3fs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/ivQeT1ix.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BwRoW_yn.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Cz9IHiFo.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C_-EP1mu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CeeZmN6j.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BWfwsB7c.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDdUvEIO.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/PV-tEdEs.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B5fv-1Xq.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/H6MlkUZ9.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CaawH2jc.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/Fo6dHlhp.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D7kgLHFI.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C94jHEjt.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CE5h_3Zy.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CZYsndi7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/iyVKi4R7.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DFI9gCi9.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/711989cd-b755-45e7-b273-f0754c0f755f.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dk67likk.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BU1UZLbD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CZvWFDQc.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/yEdly4JE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CvvIFhEj.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DE3BAxce.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/uYY1jy-2.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BiTVazLu.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DnLIRtEt.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CZsi5-5V.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BDjFqGfp.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Cebmboq7.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CRRfaG9P.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/fV3BeRRD.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/6PKCu2pi.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/HSA0rOnf.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Dbk39hGM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DlT8tiJO.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/b04OUO_Q.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DPR8_a9D.js"><meta hid="description" name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="Agents of Inference: My submission for NVIDIA's Generative AI Agents Developer Contest by NVIDIA and LangChain"><meta property="og:description" content="This article discusses my entry for NVIDIA's Generative AI Agents Developer Contest entry: Agents of Inference"><meta property="og:image" content="https://briancaffey.github.io/static/aoi/aoi_title.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/aoi/aoi_title.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/U2gz5u3s.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-d2ff5fdf><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-d2ff5fdf><div class="justify-left flex-grow flex-cols-4" data-v-d2ff5fdf><a href="/" class="text-xl" data-v-d2ff5fdf><span class="hidden sm:inline text-2xl" data-v-d2ff5fdf>Brian Caffey</span><span class="inline sm:hidden" data-v-d2ff5fdf>JBC</span></a></div><div class="flex-grow relative" data-v-d2ff5fdf><nav z-index="10000" data-v-d2ff5fdf data-v-ed0088f5><div data-v-ed0088f5><ul class="items-right float-right hidden md:flex" data-v-ed0088f5><li class="px-4 text-lg" data-v-ed0088f5><a href="/blog/1" class="" data-v-ed0088f5>Blog</a></li><li class="px-4 text-lg" data-v-ed0088f5><a href="/contact" class="" data-v-ed0088f5>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-ed0088f5><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-ed0088f5><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-ed0088f5><title data-v-ed0088f5>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-ed0088f5></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-d2ff5fdf><div class="centered" data-v-d2ff5fdf data-v-2649ce61><div class="grid items-center justify-center" data-v-2649ce61><ul class="flex px-4" data-v-2649ce61><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üñ•Ô∏è, desktop_computer" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåû, sun_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="üåö, new_moon_with_face" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><span aria-label="‚òï, coffee" class="emoji-mart-emoji" data-v-2649ce61><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-2649ce61><div data-v-2649ce61 data-v-03afe4fa><ul data-v-03afe4fa><li class="md:px-1 px-1 cursor-pointer" data-v-03afe4fa><span aria-label="üá∫üá∏, us, flag-us" class="emoji-mart-emoji" data-v-03afe4fa><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-03afe4fa><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="This article discusses my entry for NVIDIA&#39;s Generative AI Agents Developer Contest entry: Agents of Inference" data-nuxt-img srcset="/_ipx/f_webp/static/aoi/aoi_title.png 1x, /_ipx/f_webp/static/aoi/aoi_title.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/aoi/aoi_title.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">Agents of Inference: My submission for NVIDIA&#39;s Generative AI Agents Developer Contest by NVIDIA and LangChain</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/nvidia/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>nvidia üè∑Ô∏è <!----></div></a><a href="/blog/tags/langchain/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>langchain üè∑Ô∏è <!----></div></a><a href="/blog/tags/agents/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>agents üè∑Ô∏è <!----></div></a><a href="/blog/tags/rtx/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>rtx üè∑Ô∏è <!----></div></a><a href="/blog/tags/gpu/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>gpu üè∑Ô∏è <!----></div></a><a href="/blog/tags/tensorrt/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>tensorrt üè∑Ô∏è <!----></div></a><a href="/blog/tags/tensorrt-llm/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>tensorrt-llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/ai/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>ai üè∑Ô∏è <!----></div></a><a href="/blog/tags/llm/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/llama/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>llama üè∑Ô∏è <!----></div></a><a href="/blog/tags/007/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>007 üè∑Ô∏è <!----></div></a><a href="/blog/tags/stable-diffusion/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>stable-diffusion üè∑Ô∏è <!----></div></a><a href="/blog/tags/stable-video-diffusion/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>stable-video-diffusion üè∑Ô∏è <!----></div></a><a href="/blog/tags/comfyui/" class="" data-v-eb2063c1><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-eb2063c1>comfyui üè∑Ô∏è <!----></div></a><!--]--></div><div class="flex py-2"><!--[--><div class="pr-4 rounded"><a href="https://x.com/briancaffey/status/1802754703207583886"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://x.com/briancaffey/status/1802754703207583886" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/x.png 25w, /_ipx/w_50&amp;f_webp/icons/x.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/x.png"></a></div><!--]--></div><p class="blog-date text-gray-500 mb-4">Last updated June 17, 2024</p><!----><div class="markdown"><h2 id="update"><a href="#update"><!--[-->Update<!--]--></a></h2><p><!--[-->I recently posted another article about optimizing this project with TensorRT and TensorRT-LLM running on local NVIDIA NIM inference microservices, please have a look here: <a href="https://briancaffey.github.io/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update" rel="nofollow"><!--[-->https://briancaffey.github.io/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update<!--]--></a><!--]--></p><h2 id="tldr"><a href="#tldr"><!--[-->tl;dr<!--]--></a></h2><p><!--[-->‚ÄúAgents of Inference‚Äù is my entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This project aims to integrate techniques for generating text, images and video to create an application capable of producing short thematic films. In this article, I will detail how I developed the project leveraging LangGraph‚Äîa library for building stateful, multi-actor applications with LLMs--and hybrid AI workflows using NVIDIA AI-powered tools and technologies running on RTX PCs and in the cloud.<!--]--></p><p><!--[-->Here&#39;s my project submission post on ùïè:<!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-theme="dark"><p lang="en" dir="ltr">Agents of Inference<br>üç∏ü§µüèº‚Äç‚ôÇÔ∏è‚ö°Ô∏èüé•üé¨<a href="https://twitter.com/hashtag/NVIDIADevContest?src=hash&amp;ref_src=twsrc%5Etfw">#NVIDIADevContest</a> <a href="https://twitter.com/hashtag/LangChain?src=hash&amp;ref_src=twsrc%5Etfw">#LangChain</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://t.co/VT3rgzFbD6">pic.twitter.com/VT3rgzFbD6</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1802754703207583886?ref_src=twsrc%5Etfw">June 17, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->Here&#39;s a link to the <a href="https://github.com/briancaffey/agents-of-inference" rel="nofollow"><!--[-->Agents of Inference code repository on GitHub<!--]--></a>.<!--]--></p><h2 id="nvidias-generative-ai-agents-developer-contest"><a href="#nvidias-generative-ai-agents-developer-contest"><!--[-->NVIDIA&#39;s Generative AI Agents Developer Contest<!--]--></a></h2><p><!--[-->AI agents are having a moment. They are the building blocks for building &quot;applications that reason&quot;, and LangChain is a company that provides a comprehensive set of tools for developing, deploying and monitoring AI agents. I have struggled to understand how I can build or use agents in my own projects, and with the contest I have been able to just scratch the surface of what is possible with AI agents--but I think it is a promising paradigm for developing AI-driven applications.<!--]--></p><h2 id="coming-up-with-an-idea"><a href="#coming-up-with-an-idea"><!--[-->Coming up with an idea<!--]--></a></h2><p><!--[-->I love stable diffusion. I closely follow the development of the three leading applications for generating images with stable dissuion models: Stable Diffusion WebUI, InvokeAI and ComfyUI. Write a prompt, instantly see the result, tweak the prompt and generate again. This is the basic process by which I have previously used stable diffusion. It is a satisfying mental exercise that feeds the creative and imaginative part of my brain. My idea for this project came from wanting to automate this process: use large language models to build cohesive scenes and detailed prompts and then feed them into my stable diffusion programs via API. Using LangChain and LangGraph allowed me to rapidly prototype the idea and start generating short feature films in the style of my favorite British Secret Agent: 007.<!--]--></p><h2 id="putting-together-the-puzzle-pieces"><a href="#putting-together-the-puzzle-pieces"><!--[-->Putting together the puzzle pieces<!--]--></a></h2><p><!--[-->Here&#39;s how I set up an MVP for my project project to get started. I set up a simple graph (a linked list, really) that included the following nodes. *Important: in this context, a node is an agent, and that agent is a simple Python function. It takes one parameter which is the state, a Python dictionary, that holds the output of LLM calls that the agents make. Not all nodes make LLM calls, some just run basic functions like initializing directories or calling external stable diffusion APIs.<!--]--></p><ul><!--[--><li><!--[-->Casting Agent ‚Üí come up with some characters<!--]--></li><li><!--[-->Location Agent ‚Üí come up with some locations<!--]--></li><li><!--[-->Synopsis Agent ‚Üí write a synopsis based on the characters and locations<!--]--></li><li><!--[-->Scene Agent ‚Üí write some number of scenes based on the synopsis based on the synopsis<!--]--></li><li><!--[-->Shot agent ‚Üí describe some number of camera shots for each scene based on the scene<!--]--></li><li><!--[-->Photography agent ‚Üí take each shot description and generate and image<!--]--></li><li><!--[-->Videography agent ‚Üí take each image generated by the photography agent and convert it to a 4 second clip using stable video diffusion<!--]--></li><li><!--[-->Editor agent ‚Üí compile the movie clips together<!--]--></li><!--]--></ul><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="simple graph of agents of inference" data-nuxt-img srcset="/_ipx/_/static/aoi/graph.png 1x, /_ipx/_/static/aoi/graph.png 2x" src="/_ipx/_/static/aoi/graph.png"><!--]--></p><p><!--[-->It may look simple, but there is a lot going on in this graph.<!--]--></p><h3 id="casting-and-location"><a href="#casting-and-location"><!--[-->Casting and Location<!--]--></a></h3><p><!--[-->The first two agents in my graph are tasked with generating characters and locations that would appear in a British secret agent film. The prompts used for these agents are as follows:<!--]--></p><blockquote><!--[--><p><!--[--><strong><!--[-->casting<!--]--></strong>: &quot;Come up with four to five characters who will appear in an upcoming British spy movie. The list should include the main character who is male, the villain, an attractive female actress who eventually falls in love with the main character, and some other characters as well.&quot;<!--]--></p><!--]--></blockquote><blockquote><!--[--><p><!--[--><strong><!--[-->locations<!--]--></strong>: &quot;Provide three main locations that can be used in an international British Spy movie. The locations should include a variety of cities, remote environments, iconic landmarks, etc. The locations should make for good background scenes for an action movie with lots of stunts, chases, explosions, fights, etc. and other things you would find in an action movie. Be sure to include the country and a description of the environment where these places are.&quot;<!--]--></p><!--]--></blockquote><p><!--[-->These agents leverage the LangChain Expression Language (LCEL) to generate <strong><!--[-->structured output<!--]--></strong> based on Pydantic models. For<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>class Character(BaseModel):
</span></span><span class="line" line="2"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="3"><span>    The type for character that the casting agent casts for a role in the movie
</span></span><span class="line" line="4"><span>    &quot;&quot;&quot;
</span></span><span class="line" line="5"><span>    full_name: str = Field(description=&quot;The character&#39;s name&quot;)
</span></span><span class="line" line="6"><span>    short_name: str = Field(description=&quot;The character&#39;s short name&quot;)
</span></span><span class="line" line="7"><span>    background: str = Field(description=&quot;The character&#39;s background&quot;)
</span></span><span class="line" line="8"><span>    physical_traits: str = Field(description=&quot;The physical traits of the character&quot;)
</span></span><span class="line" line="9"><span>    ethnicity: str = Field(description=&quot;The character&#39;s ethnicity&quot;)
</span></span><span class="line" line="10"><span>    gender: str = Field(description=&quot;The character&#39;s gender, either male of female&quot;)
</span></span><span class="line" line="11"><span>    nationality: str = Field(description=&quot;The character&#39;s nationality&quot;)
</span></span><span class="line" line="12"><span>    main_character: bool = Field(description=&quot;If the character is or is not the main character&quot;)
</span></span></code><!--]--></pre><p><!--[-->LCEL offers wonderful syntactic sugar, I can use this model in a parse and pip that into the output from the mode:<!--]--></p><pre class="language-python shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span>chain = prompt | model | parser
</span></span></code><!--]--></pre><p><!--[-->This results in our structured data:<!--]--></p><pre class="language-yml shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span class="s9eBZ">cast</span><span class="sVt8B">:
</span></span><span class="line" line="2"><span class="sVt8B">- </span><span class="s9eBZ">background</span><span class="sVt8B">: </span><span class="sZZnC">Former MI6 agent
</span></span><span class="line" line="3"><span class="s9eBZ">  ethnicity</span><span class="sVt8B">: </span><span class="sZZnC">British
</span></span><span class="line" line="4"><span class="s9eBZ">  full_name</span><span class="sVt8B">: </span><span class="sZZnC">James Alexander
</span></span><span class="line" line="5"><span class="s9eBZ">  gender</span><span class="sVt8B">: </span><span class="sZZnC">Male
</span></span><span class="line" line="6"><span class="s9eBZ">  main_character</span><span class="sVt8B">: </span><span class="sj4cs">true
</span></span><span class="line" line="7"><span class="s9eBZ">  nationality</span><span class="sVt8B">: </span><span class="sZZnC">British
</span></span><span class="line" line="8"><span class="s9eBZ">  physical_traits</span><span class="sVt8B">: </span><span class="sZZnC">Tall, dark hair, blue eyes
</span></span><span class="line" line="9"><span class="s9eBZ">  short_name</span><span class="sVt8B">: </span><span class="sZZnC">Jamie
</span></span></code><!--]--></pre><p><!--[-->I saved the state for all &quot;Agents of Inference&quot; invocations in the <code><!--[-->output<!--]--></code> directory of my <a href="https://github.com/briancaffey/agents-of-inference/tree/main/output" rel="nofollow"><!--[--><code><!--[-->agents-of-inference<!--]--></code><!--]--></a> GitHub repo. I didn&#39;t commit the images and videos, but you can follow @AgentInference on X to see more of the results from my development process and future improvements, as well!<!--]--></p><h3 id="synopsis-agent"><a href="#synopsis-agent"><!--[-->Synopsis Agent<!--]--></a></h3><p><!--[-->With a cast of characters and locations selected, we need a synopsis to determine what happens. Here&#39;s the prompt:<!--]--></p><pre class="language-yaml shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span class="s9eBZ">synopsis</span><span class="sVt8B">: </span><span class="szBVR">|
</span></span><span class="line" line="2"><span class="sZZnC">  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:
</span></span><span class="line" line="3"><span class="sZZnC">  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., &quot;Agent X&quot;) who works for a top-secret government agency (e.g., MI6).
</span></span><span class="line" line="4"><span class="sZZnC">  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.
</span></span><span class="line" line="5"><span class="sZZnC">  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist‚Äôs plan.
</span></span><span class="line" line="6"><span class="sZZnC">  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.
</span></span><span class="line" line="7"><span class="sZZnC">  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.
</span></span><span class="line" line="8"><span class="sZZnC">  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.
</span></span><span class="line" line="9"><span class="sZZnC">  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.
</span></span><span class="line" line="10"><span class="sZZnC">  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.
</span></span><span class="line" line="11"><span class="sZZnC">  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.
</span></span></code><!--]--></pre><p><!--[-->The synopsis to any good film is key, so I decided to use a feature of LangGraph that would allow a <code><!--[-->synopsis_review_agent<!--]--></code> to provide multiple rounds of feedback to the <code><!--[-->synopsis_agent<!--]--></code> to make it even better. Here&#39;s what the new graph look like after implementing the <code><!--[-->synopsis_review_agent<!--]--></code> using conditional graph edges:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="synopsis_review_agent" data-nuxt-img srcset="/_ipx/_/static/aoi/graph_with_cycle.png 1x, /_ipx/_/static/aoi/graph_with_cycle.png 2x" src="/_ipx/_/static/aoi/graph_with_cycle.png"><!--]--></p><p><!--[-->Conditional edges are a very powerful feature and I just used it in one part of my graph. Other parts of the graph could benefit from this as well, and they can allow for &quot;human-in-the-loop&quot; interactions which are becoming very popular in AI-powered applications.<!--]--></p><h3 id="scene-and-shot-agents"><a href="#scene-and-shot-agents"><!--[-->Scene and shot agents<!--]--></a></h3><p><!--[-->With our perfected synopsis, we are ready to put more agents to work. The scene agent builds out the basic structure of the storyline. It provides a structured list of the main sections of the movie. The shot agent then loops over the scenes and creates a number of different shots for the given scene. This was an effective way to have consistent thematic content for shots within a scene. Here are the prompts I used for the scene and shot agents:<!--]--></p><pre class="language-yaml shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span class="s9eBZ">scenes</span><span class="sVt8B">: </span><span class="szBVR">|
</span></span><span class="line" line="2"><span class="sZZnC">  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:
</span></span><span class="line" line="3"><span class="sZZnC">  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.
</span></span><span class="line" line="4"><span class="sZZnC">  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.
</span></span><span class="line" line="5"><span class="sZZnC">  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.
</span></span><span class="line" line="6"><span class="s9eBZ">shot</span><span class="sVt8B">: </span><span class="szBVR">|
</span></span><span class="line" line="7"><span class="sZZnC">  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don&#39;t use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.
</span></span></code><!--]--></pre><h3 id="stable-diffusion-and-stable-video-diffusion-agents"><a href="#stable-diffusion-and-stable-video-diffusion-agents"><!--[-->Stable Diffusion and Stable Video Diffusion agents<!--]--></a></h3><p><!--[-->The stable diffusion agent makes an API call to a local instance of the Stable Diffusion WebUI API, saves the generated image and saves a reference to that image in the state:<!--]--></p><pre class="language-yaml shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span class="sVt8B">- </span><span class="s9eBZ">description</span><span class="sVt8B">: </span><span class="sZZnC">A medium close-up shot of Ethan Jameson&#39;s face, with a concerned expression,
</span></span><span class="line" line="2"><span class="sZZnC">    as he reads the message from Natalie Jackson. The lighting is dim, with only a
</span></span><span class="line" line="3"><span class="sZZnC">    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow
</span></span><span class="line" line="4"><span class="sZZnC">    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.
</span></span><span class="line" line="5"><span class="s9eBZ">  image</span><span class="sVt8B">: </span><span class="sZZnC">000.png
</span></span><span class="line" line="6"><span class="s9eBZ">  title</span><span class="sVt8B">: </span><span class="sZZnC">Ethan&#39;s Concerned Expression
</span></span><span class="line" line="7"><span class="s9eBZ">  video</span><span class="sVt8B">: </span><span class="sZZnC">000.mp4
</span></span></code><!--]--></pre><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="A medium close-up shot of Ethan Jameson&#39;s face" data-nuxt-img srcset="/_ipx/_/static/aoi/ethan.png 1x, /_ipx/_/static/aoi/ethan.png 2x" src="/_ipx/_/static/aoi/ethan.png"><!--]--></p><p><!--[-->With the perfectly prompted image in hand, we can use Stable Video Diffusion to bring it to life. I prompted phind to come up with a FastAPI service that would accept an image in a post request and return a short video created with stable video diffusion using the diffusers library.<!--]--></p><p><!--[-->Stable video diffusion can generate about 4 seconds of text at 7 frames per second. This isn&#39;t great, but I was able to use ffmpeg to do frame interpolation bringing the frame rate to a much smoother 14 fps using motion compensated interpolation (MCI):<!--]--></p><pre class="language-bash shiki shiki-themes github-light github-dark" style=""><!--[--><code><span class="line" line="1"><span class="sScJk">ffmpeg</span><span class="sj4cs"> -i</span><span class="sZZnC"> output/1718453390/final.mp4</span><span class="sj4cs"> -crf</span><span class="sj4cs"> 10</span><span class="sj4cs"> -vf</span><span class="sZZnC"> &quot;minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1&quot;</span><span class="sZZnC"> output/1718453390/final.14fps.mp4
</span></span></code><!--]--></pre><p><!--[-->Finally, the <code><!--[-->editor_agent<!--]--></code> uses <code><!--[-->moviepy<!--]--></code> to join the clips together into a single video.<!--]--></p><h2 id="development-environment"><a href="#development-environment"><!--[-->Development environment<!--]--></a></h2><p><!--[-->I struggled to optimize the <code><!--[-->meta-llama/Meta-Llama-3-8B-Instruct<!--]--></code> with TensorRT-LLM, so I ran LLM inference on a combination of older Llama2 TensorRT-LLM models, and <code><!--[-->Meta-Llama-3-8B-Instruct<!--]--></code> on LM Studio (which I found to be painfully slow compared to TensorRT-LLM).<!--]--></p><p><!--[-->If you provide an <code><!--[-->NVIDIA_API_KEY<!--]--></code> in the <code><!--[-->.env<!--]--></code> file, LLM calls will be made using the <code><!--[-->meta/llam3-70b-instruct<!--]--></code> model on <a href="https://build.nvidia.com/meta/llama3-70b" rel="nofollow"><!--[-->build.nvidia.com/meta/llama3-70b<!--]--></a>. In fact, <code><!--[-->build.nvidia.com<!--]--></code> also provides stable diffusion and stable video diffusion inference via API. This would be very convenient in the event that my RTX PCs become compromised.<!--]--></p><p><!--[-->My RTX 4090 GPU with 24 GB of memory was able to run lots of different inference servers concurrently (LLM, Stable Diffusion WebUI, ComfyUI, InvokeAI, Stable Video Diffusion FastAPI service), but I generally stuck to doing one type of inference at a time, otherwise things would grind to a hault or crash. I also experimented with ChatTTS, a new text-to-speech model.<!--]--></p><p><!--[-->I developed this project on a MacBook Pro, and I used my RTX PC as if it were a remote service providing inference for text, images and video. This is a helpful mindset when working with hybrid AI workflows that leverage inference services both on local machines and in the cloud.<!--]--></p><h2 id="how-it-works"><a href="#how-it-works"><!--[-->How it works<!--]--></a></h2><p><!--[-->To run the program, you need to install python dependencies and then run an OpenAI compatible LLM and Stable Duffsion WebUI server with the <code><!--[-->--api<!--]--></code> flag. You also need to run the Stable Video Diffusion service. Apologies for any hardcoded local IP address in the source code. Deadlines, you know! With everything configured, you can run the following command:<!--]--></p><pre class="language-text"><!--[--><code>~/git/github/agents-of-inference$ poetry run python agents_of_inference/main.py
## üìÄ Using local models üìÄ ##
## üé≠ Generating Cast üé≠ ##
## üó∫Ô∏è Generating Locations üó∫Ô∏è ##
## ‚úçÔ∏è Generating Synopsis ‚úçÔ∏è ##
## going to synopsis_review_agent ##
## üìë Reviewing Synopsis üìë ##
## ‚úçÔ∏è Generating Synopsis ‚úçÔ∏è ##
## going to synopsis_review_agent ##
## üìë Reviewing Synopsis üìë ##
## ‚úçÔ∏è Generating Synopsis ‚úçÔ∏è ##
## going to scene_agent ##
## üìí Generating Scenes üìí ##
## üé¨ Generating Shots üé¨ ##
## Generated 5 shots for scene 1/5 ##
## Generated 5 shots for scene 2/5 ##
## Generated 5 shots for scene 3/5 ##
## Generated 5 shots for scene 4/5 ##
## Generated 5 shots for scene 5/5 ##

000/0025
A medium shot of a bustling Tokyo street, with neon lights reflecting off wet pavement. Jim Thompson, dressed in a black leather jacket and dark jeans, walks purposefully through the crowd, his piercing blue eyes scanning the area. The sound design features the hum of traffic and chatter of pedestrians.
Generated image output/1718426686/images/000.png

001/0025
A tight close-up shot of Emily Chen&#39;s face, her piercing brown eyes intense as she briefs Jim on the situation. Her short black hair is styled neatly, and she wears a crisp white blouse with a silver necklace. The camera lingers on her lips as she speaks, emphasizing the importance of the information.
Generated image output/1718426686/images/001.png

Generated video output/1718426686/videos/000.mp4
== stable video diffusion generation complete ==
Generated video output/1718426686/videos/001.mp4
== stable video diffusion generation complete ==
</code><!--]--></pre><h2 id="demo-video-for-contest-submission"><a href="#demo-video-for-contest-submission"><!--[-->Demo Video for Contest Submission<!--]--></a></h2><!--[--><blockquote class="twitter-tweet tw-align-center" data-media-max-width="560"><p lang="en" dir="ltr">Agents of Inference<br>üç∏ü§µüèº‚Äç‚ôÇÔ∏è‚ö°Ô∏èüé•üé¨<a href="https://twitter.com/hashtag/NVIDIADevContest?src=hash&amp;ref_src=twsrc%5Etfw">#NVIDIADevContest</a> <a href="https://twitter.com/hashtag/LangChain?src=hash&amp;ref_src=twsrc%5Etfw">#LangChain</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://t.co/VT3rgzFbD6">pic.twitter.com/VT3rgzFbD6</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1802754703207583886?ref_src=twsrc%5Etfw">June 17, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->Making this video was a lot of fun. The &quot;Agents of Inference&quot; highlight reel includes some of the most interesting, exciting and fun clips that I found in the dozens of short films it created. It is important to note that a lot of the content is not very good. Misunderstood prompts, color confusion (prompt includes green eyes, but other things in the scene are also conspicuously green), unrealistic or noisy motion from Stable Video Diffusion--these are some of the issues you will find in the films. Generating AI images sometimes feels like panning for gold: you go through a lot of sediment to get a few good flakes.<!--]--></p><p><!--[-->Also, I added a few short animations that I made with Blender. The final scene shows the NVIDIA Omniverse orange humanoid from the barrel of a pistol. I think we are rapidly approaching a future where agents can generate full-scale theatrical movies by generating OpenUSD code, directly or indirectly. Maybe for the next NVIDIA Developer contest!<!--]--></p><h2 id="shortcomings-of-my-project"><a href="#shortcomings-of-my-project"><!--[-->Shortcomings of my project<!--]--></a></h2><p><!--[-->My goodness, how embarrasing. There are quite a few shortcomings that can be easily identified looking over the output and the source code. Here are a few:<!--]--></p><h3 id="character-variety"><a href="#character-variety"><!--[-->Character variety<!--]--></a></h3><p><!--[-->When generating characters I would frequently see one named Dr. Sophia Patel who is apparently a brilliant cryptologist. Other characters would often have different names or backgrounds, but a saw Dr. Sophia Patel more often than not.<!--]--></p><h3 id="character-consistency"><a href="#character-consistency"><!--[-->Character consistency<!--]--></a></h3><p><!--[-->The characters are not consistent. This is a notoriously difficult problem to solve, but I made a lot of progress on it during this contest. I experimented with calling the ComfyUI API to run a custom workflow built with the ComfyUI graph-based workflow tool for face transfer:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="Dr. Sophia Patel" data-nuxt-img srcset="/_ipx/_/static/aoi/sophia.png 1x, /_ipx/_/static/aoi/sophia.png 2x" src="/_ipx/_/static/aoi/sophia.png"><!--]--></p><p><!--[-->Using ComfyUI would be nice, but it wouldn&#39;t be as easy to tap into cloud APIs if my workflow heavily relied on ComfyUI server with custom models.<!--]--></p><h3 id="understanding-langchain"><a href="#understanding-langchain"><!--[-->Understanding LangChain<!--]--></a></h3><p><!--[-->I started out with the idea I would store all LLM calls to a local JSON to serve as a cache, allowing me to avoid regenerating responses from early in the workflow. This worked well, until I tried to serialize an Annotated list (required for cycles such as the one used with <code><!--[-->synopsis_review_agent<!--]--></code>). I ended up wasting a lot of time trying to figure this out, and I came across some built-in LangChain features for storing state in memory and in Sqlite. I&#39;m sure there are other areas where I used the wrong pattern, but I turned over a lot of stones and look forward to continuing development with LangChain.<!--]--></p><h2 id="whats-next"><a href="#whats-next"><!--[-->What&#39;s next?<!--]--></a></h2><p><!--[-->Thank you to NVIDIA and LangChain for organizing this contest. It was a great way to explore a powerful toolset for automated content generation using AI agents.<!--]--></p><p><!--[-->Video models like Dream Machine and Sora have made some big splashes on the internet and the results are remarkable. However, I&#39;m still almost more interested in finding the limitations of quality content using open-source models on consumer hardware like RTX GPUs.<!--]--></p><p><!--[-->I would also have loved to generate my own music for these films. I am a Suno poweruser and love the songs I have generated on that site. Will the gap between video and music generation on private, payed services and local machines? Or does it just need time to catch up? Hopefully a future installment of &quot;Agents of Inference&quot; will integrate music and voice, and can&#39;t wait to hear it!<!--]--></p><style>html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}</style></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments üí¨ </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> ¬© 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest/_payload.json?711989cd-b755-45e7-b273-f0754c0f755f">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest":-1},"/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",{},1753130128213]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"üá∫üá∏",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Fran√ßais",flag:"üá´üá∑",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"ÁÆÄ‰Ωì‰∏≠Êñá",flag:"üá®üá≥",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"–†—É—Å—Å–∫–∏–π",flag:"üá∑üá∫",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"Êó•Êú¨Ë™û",flag:"üáØüáµ",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"‡§π‡§ø‡§Ç‡§¶‡•Ä",flag:"üáÆüá≥",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"711989cd-b755-45e7-b273-f0754c0f755f",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>