<!DOCTYPE html><html  lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM</title><style>html{font-family:Montserrat,Arial,Sans Serif;font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box}*,:after,:before{box-sizing:border-box;margin:0}.button--green{border:1px solid #3b8070;border-radius:4px;color:#3b8070;display:inline-block;padding:10px 30px;text-decoration:none}.button--green:hover{background-color:#3b8070;color:#fff}.button--grey{border:1px solid #35495e;border-radius:4px;color:#35495e;display:inline-block;margin-left:15px;padding:10px 30px;text-decoration:none}.button--grey:hover{background-color:#35495e;color:#fff}</style><style>span.emoji-mart-emoji[data-v-05905815]{padding:0}.selected[data-v-05905815]{text-shadow:.25px 0 0 #000}.picker[data-v-05905815]{margin-left:auto;margin-right:auto;position:absolute;top:10px;transform:translate(50%,50%)}.top[data-v-05905815]{background-color:var(--color-primary);height:3px;width:100%}</style><style>.selected[data-v-1edc4b99]{text-shadow:.9px 0 0}</style><style>span.emoji-mart-emoji[data-v-de9cb334]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-de9cb334]:hover{transform:scale(1.3)}.centered[data-v-de9cb334]{left:50vw;margin-left:auto;margin-right:auto;position:absolute;right:50vw}</style><style>span.emoji-mart-emoji[data-v-f26190e6]{padding:0;transition:transform .2s}span.emoji-mart-emoji[data-v-f26190e6]:hover{transform:scale(1.3)}.picker[data-v-f26190e6]{left:0;margin-left:auto;margin-right:auto;position:absolute;right:0;z-index:10000000000}.localepicker[data-v-f26190e6]{background-color:var(--bg)}.localeText[data-v-f26190e6]{color:var(--color-primary)}</style><style>.tag[data-v-09161b57]{background-color:var(--color-tag);transition:transform .2s}.tag[data-v-09161b57]:hover{transform:scale(1.05)}</style><style>pre code .line{display:block}</style><link rel="stylesheet" href="/_nuxt/entry.C_qR6n1r.css" crossorigin><link rel="stylesheet" href="/_nuxt/app.MFRQdpI0.css" crossorigin><link rel="preload" as="fetch" crossorigin="anonymous" href="/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update/_payload.json?f7a02a46-9c0e-478e-936c-d4b79733d6b2"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/jzUacxre.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CDMEWRFj.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CzmcxFEq.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/B-mOQKwH.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DPol-5Mm.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DuQfXRjE.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DjIsa8m4.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/rBAQty2o.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C4Sm19mx.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/D7hIbxa9.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CgFg6jAm.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/C5aKNO_V.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BVmCdIjn.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/CfTcFQKu.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BnNjkjIJ.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/BgkXKTQS.js"><link rel="modulepreload" as="script" crossorigin href="/_nuxt/DidSn7PP.js"><link rel="preload" as="fetch" fetchpriority="low" crossorigin="anonymous" href="/_nuxt/builds/meta/f7a02a46-9c0e-478e-936c-d4b79733d6b2.json"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BAz4hE9E.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DPdwgYFM.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B9ziLBQK.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DAwWNWwV.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/_7iENaYg.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/D86XVKAs.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/rgQGHqoJ.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DDRJokCh.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BaoTS1Ec.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/B9rTWv16.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CJwV7gYv.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/C9IforG0.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Du9tXjnC.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/Bq3ZV0Cc.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BPxGl8VU.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/DbwMaSgs.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/zTRos7l-.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BZZoewB1.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/BSjOxtvE.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/CRVSn5jR.js"><link rel="prefetch" as="script" crossorigin href="/_nuxt/UiuPncGx.js"><meta name="description" content="Brian Caffey's personal website"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="robots" content="all"><meta property="twitter:creator" content="@briancaffey"><meta property="twitter:site" content="@briancaffey"><meta property="og:title" content="Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM"><meta property="og:description" content="This article is a brief discusion on recent updates to my project for the Generative AI Agents Developer Contest by NVIDIA and LangChain"><meta property="og:image" content="https://briancaffey.github.io/static/aoi/aoi_title.png"><meta property="twitter:image" content="https://briancaffey.github.io/static/aoi/aoi_title.png"><meta property="twitter:card" content="summary_large_image"><script type="module" src="/_nuxt/jzUacxre.js" crossorigin></script><script>"use strict";(()=>{const t=window,e=document.documentElement,c=["dark","light"],n=getStorageValue("localStorage","nuxt-color-mode")||"system";let i=n==="system"?u():n;const r=e.getAttribute("data-color-mode-forced");r&&(i=r),l(i),t["__NUXT_COLOR_MODE__"]={preference:n,value:i,getColorScheme:u,addColorScheme:l,removeColorScheme:d};function l(o){const s=""+o+"-mode",a="";e.classList?e.classList.add(s):e.className+=" "+s,a&&e.setAttribute("data-"+a,o)}function d(o){const s=""+o+"-mode",a="";e.classList?e.classList.remove(s):e.className=e.className.replace(new RegExp(s,"g"),""),a&&e.removeAttribute("data-"+a)}function f(o){return t.matchMedia("(prefers-color-scheme"+o+")")}function u(){if(t.matchMedia&&f("").media!=="not all"){for(const o of c)if(f(":"+o).matches)return o}return"light"}})();function getStorageValue(t,e){switch(t){case"localStorage":return window.localStorage.getItem(e);case"sessionStorage":return window.sessionStorage.getItem(e);case"cookie":return getCookie(e);default:return null}}function getCookie(t){const c=("; "+window.document.cookie).split("; "+t+"=");if(c.length===2)return c.pop()?.split(";").shift()}</script></head><body><div id="__nuxt"><div><div><div data-v-05905815><div class="mx-auto flex py-2 px-2 sm:px-4 items-center max-w-6xl justify-center" data-v-05905815><div class="justify-left flex-grow flex-cols-4" data-v-05905815><a href="/" class="text-xl" data-v-05905815><span class="hidden sm:inline text-2xl" data-v-05905815>Brian Caffey</span><span class="inline sm:hidden" data-v-05905815>JBC</span></a></div><div class="flex-grow relative" data-v-05905815><nav z-index="10000" data-v-05905815 data-v-1edc4b99><div data-v-1edc4b99><ul class="items-right float-right hidden md:flex" data-v-1edc4b99><li class="px-4 text-lg" data-v-1edc4b99><a href="/blog/1" class="" data-v-1edc4b99>Blog</a></li><li class="px-4 text-lg" data-v-1edc4b99><a href="/contact" class="" data-v-1edc4b99>Contact</a></li></ul><div class="flex justify-end md:hidden z-1000" data-v-1edc4b99><button class="flex items-center px-3 py-2 border rounded menu-icon" data-v-1edc4b99><svg class="fill-current h-3 w-3" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" data-v-1edc4b99><title data-v-1edc4b99>Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0v-2z" data-v-1edc4b99></path></svg></button></div><!----></div></nav></div></div><div class="picker" data-v-05905815><div class="centered" data-v-05905815 data-v-de9cb334><div class="grid items-center justify-center" data-v-de9cb334><ul class="flex px-4" data-v-de9cb334><!--[--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="üñ•Ô∏è, desktop_computer" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:51.67% 95%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="üåû, sun_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 48.33%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="üåö, new_moon_with_face" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:8.33% 41.67%;width:32px;height:32px;"></span></span></li><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><span aria-label="‚òï, coffee" class="emoji-mart-emoji" data-v-de9cb334><span class="emoji-set-apple emoji-type-image" style="background-position:95% 30%;width:32px;height:32px;"></span></span></li><!--]--><li class="md:px-1 px-1 cursor-pointer" data-v-de9cb334><div data-v-de9cb334 data-v-f26190e6><ul data-v-f26190e6><li class="md:px-1 px-1 cursor-pointer" data-v-f26190e6><span aria-label="üá∫üá∏, us, flag-us" class="emoji-mart-emoji" data-v-f26190e6><span class="emoji-set-apple emoji-type-image" style="background-position:6.67% 45%;width:32px;height:32px;"></span></span></li></ul><div class="rounded-md z-10 picker" data-v-f26190e6><!----></div></div></li></ul></div></div></div></div><!--[--><article><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="This article is a brief discusion on recent updates to my project for the Generative AI Agents Developer Contest by NVIDIA and LangChain" data-nuxt-img srcset="/_ipx/f_webp/static/aoi/aoi_title.png 1x, /_ipx/f_webp/static/aoi/aoi_title.png 2x" class="pt-2 w-full object-cover" style="height:32rem;" src="/_ipx/f_webp/static/aoi/aoi_title.png"><div class="mx-auto max-w-5xl px-2 sm:px-4 md:px-4 lg:px-16 mt-2"><h1 class="prose text-4xl leading-9 py-4 font-bold">Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM</h1><div class="flex flex-wrap -ml-1 py-2"><!--[--><a href="/blog/tags/nvidia/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>nvidia üè∑Ô∏è <!----></div></a><a href="/blog/tags/langchain/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>langchain üè∑Ô∏è <!----></div></a><a href="/blog/tags/agents/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>agents üè∑Ô∏è <!----></div></a><a href="/blog/tags/rtx/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>rtx üè∑Ô∏è <!----></div></a><a href="/blog/tags/gpu/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>gpu üè∑Ô∏è <!----></div></a><a href="/blog/tags/tensorrt/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>tensorrt üè∑Ô∏è <!----></div></a><a href="/blog/tags/tensorrt-llm/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>tensorrt-llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/ai/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>ai üè∑Ô∏è <!----></div></a><a href="/blog/tags/llm/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>llm üè∑Ô∏è <!----></div></a><a href="/blog/tags/llama/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>llama üè∑Ô∏è <!----></div></a><a href="/blog/tags/007/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>007 üè∑Ô∏è <!----></div></a><a href="/blog/tags/stable-diffusion/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>stable-diffusion üè∑Ô∏è <!----></div></a><a href="/blog/tags/stable-video-diffusion/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>stable-video-diffusion üè∑Ô∏è <!----></div></a><a href="/blog/tags/comfyui/" class="" data-v-09161b57><div class="px-2 text-lg text-white shadow rounded-lg bg-black mx-1 mb-1 uppercase cursor-pointer tag" data-v-09161b57>comfyui üè∑Ô∏è <!----></div></a><!--]--></div><div class="flex py-2"><!--[--><div class="pr-4 rounded"><a href="https://x.com/briancaffey/status/1802754703207583886"><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" width="25" alt="https://x.com/briancaffey/status/1802754703207583886" data-nuxt-img sizes="25px" srcset="/_ipx/w_25&amp;f_webp/icons/x.png 25w, /_ipx/w_50&amp;f_webp/icons/x.png 50w" class="rounded" src="/_ipx/w_50&amp;f_webp/icons/x.png"></a></div><!--]--></div><p class="blog-date text-gray-500 mb-4">Last updated June 24, 2024</p><!----><div class="markdown"><h2 id="tldr"><a href="#tldr"><!--[-->tl;dr<!--]--></a></h2><p><!--[-->&quot;Agents of Inference: Speed of Light&quot; is an update to my original entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This update focuses on how I accelerated local text, image and video generation using TensorRT, TensorRT-LLM and NVIDIA NIMs. You can read the original article about &quot;Agents of Inference&quot; <a href="https://briancaffey.github.io/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest" rel="nofollow"><!--[-->here<!--]--></a>.<!--]--></p><p><!--[-->Here&#39;s my original project submission post on ùïè that introduces the idea of generating short 007-style films using agents, LLMs and stable diffusion:<!--]--></p><!--[--><blockquote class="twitter-tweet tw-align-center" data-theme="dark"><p lang="en" dir="ltr">Agents of Inference<br>üç∏ü§µüèº‚Äç‚ôÇÔ∏è‚ö°Ô∏èüé•üé¨<a href="https://twitter.com/hashtag/NVIDIADevContest?src=hash&amp;ref_src=twsrc%5Etfw">#NVIDIADevContest</a> <a href="https://twitter.com/hashtag/LangChain?src=hash&amp;ref_src=twsrc%5Etfw">#LangChain</a> <a href="https://twitter.com/NVIDIAAIDev?ref_src=twsrc%5Etfw">@NVIDIAAIDev</a> <a href="https://t.co/VT3rgzFbD6">pic.twitter.com/VT3rgzFbD6</a></p>‚Äî Brian Caffey (@briancaffey) <a href="https://twitter.com/briancaffey/status/1802754703207583886?ref_src=twsrc%5Etfw">June 17, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><!--]--><p><!--[-->Here&#39;s a link to the <a href="https://github.com/briancaffey/agents-of-inference" rel="nofollow"><!--[-->Agents of Inference code repository on GitHub<!--]--></a>.<!--]--></p><h2 id="nvidia-nim-inference-microservices"><a href="#nvidia-nim-inference-microservices"><!--[-->NVIDIA NIM inference microservices<!--]--></a></h2><p><!--[-->I thought NVIDIA NIMs was one of the most exciting announcements from GTC 2024. I&#39;m a big fan of using docker containers everywhere, and the idea of standardizing NVIDIA tools and dependencies seemed to make a lot of sense. I had previously struggled to get TensorRT-LLM installed on Windows using example repos provided by NVIDIA.<!--]--></p><p><!--[-->A few weeks ago NVIDIA announced that NVIDIA NIMs can be downloaded and run anywhere. I was able to download this NIM for the <code><!--[-->meta/llama3-8b-instruct<!--]--></code> model:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="llama3 nim" data-nuxt-img srcset="/_ipx/_/static/aoi/meta-llama3-nim.png 1x, /_ipx/_/static/aoi/meta-llama3-nim.png 2x" src="/_ipx/_/static/aoi/meta-llama3-nim.png"><!--]--></p><p><!--[-->Here are the logs for my NVIDIA NIM <code><!--[-->Meta/Llama-3-8B-Instruct<!--]--></code> running in docker container on Windows Subsystem for Linux on my NVIDIA GeForce RTX 4090 GPU-powered PC. Notice that it generates over 50 tokens per second!<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="trt llama3 local" data-nuxt-img srcset="/_ipx/_/static/aoi/trt-llama3.png 1x, /_ipx/_/static/aoi/trt-llama3.png 2x" src="/_ipx/_/static/aoi/trt-llama3.png"><!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="token factory" data-nuxt-img srcset="/_ipx/_/static/aoi/token-factory.png 1x, /_ipx/_/static/aoi/token-factory.png 2x" src="/_ipx/_/static/aoi/token-factory.png"><!--]--></p><p><!--[-->The one main hurdle I faced when running the NIM local was an error about no runnable profiles being available:<!--]--></p><pre class="language-text"><!--[--><code>ERROR 06-23 15:41:21.19 utils.py:21] Could not find a profile that is currently runnable with the detected hardware. Please check the system information below and make sure you have enough free GPUs.
SYSTEM INFO
- Free GPUs: &lt;None&gt;
- Non-free GPUs:
  -  [2684:10de] (0) NVIDIA GeForce RTX 4090 [current utilization: 7%]
</code><!--]--></pre><p><!--[-->This seemed odd, and I found another user with the same issue on the NVIDIA Developer Forum. I was able to get around this by going into the EUFI/BIOS of my PC and switch to integrated graphics:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="bios" data-nuxt-img srcset="/_ipx/_/static/aoi/bios.jpg 1x, /_ipx/_/static/aoi/bios.jpg 2x" src="/_ipx/_/static/aoi/bios.jpg"><!--]--></p><p><!--[-->It was great to be able to run &quot;Agents of Inference&quot; using NVIDIA NIM because it is just as simple as running a docker container:<!--]--></p><pre class="language-text"><!--[--><code>export CONTAINER_NAME=llama3-8b-instruct
export IMG_NAME=&quot;nvcr.io/nim/meta/${CONTAINER_NAME}:1.0.0&quot;
export LOCAL_NIM_CACHE=~/.cache/nim
mkdir -p &quot;$LOCAL_NIM_CACHE&quot;
docker run -it --rm --name=$CONTAINER_NAME \
  --runtime=nvidia \
  --gpus all \
  --shm-size=16GB \
  -e NGC_API_KEY \
  -v &quot;$LOCAL_NIM_CACHE:/opt/nim/.cache&quot; \
  -u $(id -u) \
  -p 8000:8000 \
  $IMG_NAME
</code><!--]--></pre><p><!--[-->Before getting this to work, I was able to get a <code><!--[-->/chat/completions<!--]--></code> endpoint working with the Llama3 model on my fork of the <a href="https://github.com/briancaffey/trt-llm-as-openai-windows/commit/edaa15fd026fe95e645e3d4ae9718dc3ecc3bb65" rel="nofollow"><!--[-->trt-llm-as-openai-windows<!--]--></a>. I borrowed code for the <code><!--[-->TrtLlmAPI<!--]--></code> from the <a href="https://github.com/NVIDIA/ChatRTX" rel="nofollow"><!--[--><code><!--[-->NVIDIA/ChatRTX<!--]--></code><!--]--></a> repo and a function from <code><!--[-->llama-index<!--]--></code> called <code><!--[-->messages_to_prompt_v3_instruct<!--]--></code> which encodes messages with special tokens for chat. This was an interesting exercise and it taught me a lot about how LLMs do chat. I would like to continue working on this fork and see how to implement streaming endpoints for the Llama 3 model.<!--]--></p><p><!--[-->Here is how Llama 3 does the instruct prompting:<!--]--></p><pre class="language-text"><!--[--><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

You are a helpful AI assistant for travel tips and recommendations&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

What can you help me with?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
</code><!--]--></pre><p><!--[-->Compare this with how it was done with Llama2 chat:<!--]--></p><pre class="language-text"><!--[--><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;
{{ system_prompt }}
&lt;&lt;/SYS&gt;&gt;

{{ user_message_1 }} [/INST] {{ model_answer_1 }} &lt;/s&gt;
&lt;s&gt;[INST] {{ user_message_2 }} [/INST]
</code><!--]--></pre><p><!--[-->You can read more about the difference between Llama 2 and 3 on the <a href="https://llama.meta.com/docs/model-cards-and-prompt-formats" rel="nofollow"><!--[-->Model Card &amp; Prompt formats<!--]--></a> page on Meta&#39;s Llama website.<!--]--></p><h2 id="langsmith"><a href="#langsmith"><!--[-->LangSmith<!--]--></a></h2><p><!--[-->I recently started using LangSmith. It is an awesome product and it ties in really well to doing prototype work like in my project &quot;Agents of Inference&quot;. I wish I had started using it earlier in my development cycle! All you need to do is add an API key to your environment and your application automatically starts tracing LLM calls. It also works well with LangGraph and allows you to trace the execution path of your graph. Also it is good to be aware that there are other products similar to LangSmith like LangFuse. I also saw a really neat demo from Datadog at GTC showing an alpha version of their LLM tracing and observability product.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="langsmith screenshot" data-nuxt-img srcset="/_ipx/_/static/aoi/langsmith.png 1x, /_ipx/_/static/aoi/langsmith.png 2x" src="/_ipx/_/static/aoi/langsmith.png"><!--]--></p><p><!--[-->LangSmith can also be helpful when the wrong JSON shape is parsed. I had a lot of difficulty with this in my project. When I used the Q4_K_M gguf quantized <code><!--[-->Meta-Llama-3 8B-Instruct<!--]--></code> model I had no issues with output parsing. Switching to the TensorRT-LLM model provided by the NIM resulted in some parsing errors. The application would report that JSON could not be parsed because the result contained text like: &quot;Here is the JSON that you requested&quot;. I was able to get around this by changing the prompt template from:<!--]--></p><pre class="language-text"><!--[--><code>Answer the user query.
</code><!--]--></pre><p><!--[-->to<!--]--></p><pre class="language-text"><!--[--><code>Don&#39;t include ANYTHING except for valid JSON in your response. Answer the user query.
</code><!--]--></pre><p><!--[-->This was the most frustrating part of development, and I&#39;m still getting occasional errors that I just skip over. I&#39;m also probably have not exhausted all of the tools that LangChain provides to avoid these types of errors. Don&#39;t assume that output parsing that works with one model will work with another! This is another good reason to use something like LangSmith when developing LLM-based applications.<!--]--></p><h2 id="comfyui-tensorrt"><a href="#comfyui-tensorrt"><!--[-->ComfyUI TensorRT<!--]--></a></h2><p><!--[-->My goal with &quot;Agents of Inference&quot; was to be able to test out how small upstream prompt changes can impact the quality and consistency of a series of generated images and videos. Iteration speed is very important! I was able to significantly speed up image and video generation by using the <a href="https://github.com/comfyanonymous/ComfyUI_TensorRT" rel="nofollow"><!--[-->ComfyUI TensorRT custom nodes<!--]--></a>. These nodes allow you to build engines with specifications for parameters that can be either static or dynamic. I had better luck with building dynamic engines. I was able to build and use engines for Stable Diffusion SDXL and Stable Video Diffusion XT.<!--]--></p><p><!--[-->Building a TensorRT engine for ComfyUI can be done using the following workflow:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="trt comfyUI build process" data-nuxt-img srcset="/_ipx/_/static/aoi/comfyui-trt-svd-xt.png 1x, /_ipx/_/static/aoi/comfyui-trt-svd-xt.png 2x" src="/_ipx/_/static/aoi/comfyui-trt-svd-xt.png"><!--]--></p><p><!--[-->The engines can then be used in custom workflows like the following:<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="trt comfyui workflow" data-nuxt-img srcset="/_ipx/_/static/aoi/svd-workflow-trt.png 1x, /_ipx/_/static/aoi/svd-workflow-trt.png 2x" src="/_ipx/_/static/aoi/svd-workflow-trt.png"><!--]--></p><p><!--[-->Once these workflows are configured and are working as expected, you can export them in API format (JSON) and use them to make API calls to the ComfyUI backend. The agents for stable diffusion and stable video diffusion made API calls in this way and it worked pretty well.<!--]--></p><p><!--[--><img onerror="this.setAttribute(&#39;data-error&#39;, 1)" alt="comfy its" data-nuxt-img srcset="/_ipx/_/static/aoi/comfy-its.png 1x, /_ipx/_/static/aoi/comfy-its.png 2x" src="/_ipx/_/static/aoi/comfy-its.png"><!--]--></p><p><!--[-->Using 50 iterations, I was able to generate 1024x576 images in 3 seconds or about 19 iterations per second (it/s). Videos<!--]--></p><p><!--[-->ComfyUI is still early in development and it refers to itself as &quot;alpha software&quot; even though it has a large adoption by a very active community already. I&#39;m excited to see what is next from the developers of ComfyUI.<!--]--></p><h2 id="speed-of-light"><a href="#speed-of-light"><!--[-->Speed of Light<!--]--></a></h2><p><!--[-->&quot;Speed of Light&quot; is a term that I learned from a stable diffusion talk at GTC.<!--]--></p><blockquote><!--[--><p><!--[-->SOL analysis reveals how your code performs, and device utilization compared to relevant maximums.<!--]--></p><!--]--></blockquote><p><!--[-->Adding TensorRT and TensorRT-LLM to inference services on my RTX PC helped increase the throughput of text, image and video generation for my &quot;Agents of Inference&quot; project. I&#39;m looking forward to learning more about profiling and optimization techniques for both LLMs and Stable Diffusion workloads.<!--]--></p><p><!--[-->Thanks again to NVIDIA and LangChain for organizing this contest! It was a lot of fun to learn about builing agents with LangChain and LangGraph and the latest developments from NVIDIA in Generative AI.<!--]--></p></div><div class="text-center pb-4 pt-8"><button class="mc-btn rounded py-1 px-2"> Show Disqus Comments üí¨ </button></div><!----><h1></h1></div></article><!--]--><div class="mx-auto max-w-6xl p-4 lg:px-16 text-center"><hr class="mt-4"><div class="align-center py-4"><div class="pb-4">Join my mailing list to get updated whenever I publish a new article.</div><div class="flex align-center justify-center"><div id="mc_embed_signup" class="w-full md:w-1/2 flex-shrink justify-center"><form id="mc-embedded-subscribe-form" action="https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5" method="post" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll" class="grid grid-cols-1 sm:grid-cols-2 gap-4"><input id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="Enter your email address" class="rounded mc text-center" autocomplete="on"><div style="position:absolute;left:-5000px;" aria-hidden="true"><input type="text" name="b_43a795784ca963e25903a0da6_9937fe4fc5" tabindex="-1" value=""></div><div class="text-right" style="width:100%;"><input id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe" class="mc-btn rounded px-2 py-1 w-full"></div></div></form></div></div></div><hr><div class="py-4">Thanks for checking out my site!</div><div class="pb-4"> ¬© 2025 Brian Caffey </div></div></div></div></div><div id="teleports"></div><script type="application/json" data-nuxt-data="nuxt-app" data-ssr="true" id="__NUXT_DATA__" data-src="/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update/_payload.json?f7a02a46-9c0e-478e-936c-d4b79733d6b2">[{"state":1,"once":18,"_errors":19,"serverRendered":5,"path":21,"pinia":22,"prerenderedAt":23},["Reactive",2],{"$scolor-mode":3,"$si18n:cached-locale-configs":7,"$si18n:resolved-locale":8,"$ssite-config":9},{"preference":4,"value":4,"unknown":5,"forced":6},"system",true,false,{},"",{"_priority":10,"currentLocale":14,"defaultLocale":14,"env":15,"name":16,"url":17},{"name":11,"env":12,"url":11,"defaultLocale":13,"currentLocale":13},-3,-15,-2,"en-US","production","briancaffey.github.io","https://briancaffey.github.io",["Set"],["ShallowReactive",20],{"agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update":-1},"/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",{},1757463790432]</script><script>window.__NUXT__={};window.__NUXT__.config={public:{url:"https://briancaffey.github.io",content:{wsUrl:""},mdc:{components:{prose:true,map:{}},headings:{anchorLinks:{h1:false,h2:true,h3:true,h4:true,h5:false,h6:false}}},gtag:{enabled:true,initMode:"auto",id:"G-S8TVBBMW66",initCommands:[],config:{},tags:[],loadingStrategy:"defer",url:"https://www.googletagmanager.com/gtag/js"},i18n:{baseUrl:"",defaultLocale:"en",rootRedirect:"",redirectStatusCode:302,skipSettingLocaleOnNavigate:false,locales:[{code:"en",emoji:"flag-us",iso:"en-US",name:"English",flag:"üá∫üá∏",language:"en-US",_hreflang:"en-US",_sitemap:"en-US"},{code:"fr",emoji:"flag-fr",iso:"fr-FR",name:"Fran√ßais",flag:"üá´üá∑",language:"fr-FR",_hreflang:"fr-FR",_sitemap:"fr-FR"},{code:"zh",emoji:"flag-cn",iso:"zh-ZH",name:"ÁÆÄ‰Ωì‰∏≠Êñá",flag:"üá®üá≥",language:"zh-ZH",_hreflang:"zh-ZH",_sitemap:"zh-ZH"},{code:"ru",emoji:"flag-ru",iso:"ru-RU",name:"–†—É—Å—Å–∫–∏–π",flag:"üá∑üá∫",language:"ru-RU",_hreflang:"ru-RU",_sitemap:"ru-RU"},{code:"ja",emoji:"flag-jp",iso:"ja-JP",name:"Êó•Êú¨Ë™û",flag:"üáØüáµ",language:"ja-JP",_hreflang:"ja-JP",_sitemap:"ja-JP"},{code:"in",emoji:"flag-in",iso:"hi-IN",name:"‡§π‡§ø‡§Ç‡§¶‡•Ä",flag:"üáÆüá≥",language:"hi-IN",_hreflang:"hi-IN",_sitemap:"hi-IN"}],detectBrowserLanguage:{alwaysRedirect:false,cookieCrossOrigin:false,cookieDomain:"",cookieKey:"i18n_redirected",cookieSecure:false,fallbackLocale:"",redirectOn:"root",useCookie:true},experimental:{localeDetector:"",typedPages:true,typedOptionsAndMessages:false,alternateLinkCanonicalQueries:true,devCache:false,cacheLifetime:"",stripMessagesPayload:false,preload:false,strictSeo:false,nitroContextDetection:true},domainLocales:{en:{domain:""},fr:{domain:""},zh:{domain:""},ru:{domain:""},ja:{domain:""},in:{domain:""}}}},app:{baseURL:"/",buildId:"f7a02a46-9c0e-478e-936c-d4b79733d6b2",buildAssetsDir:"/_nuxt/",cdnURL:""}}</script></body></html>