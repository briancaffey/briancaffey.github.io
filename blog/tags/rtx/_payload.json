[{"data":1,"prerenderedAt":6562},["ShallowReactive",2],{"/blog/tags/rtx/":3},[4,2397,2827,4316],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":9,"description":10,"date":11,"image":12,"tags":13,"draft":7,"external":19,"comments":23,"body":24,"_type":2391,"_id":2392,"_source":2393,"_file":2394,"_stem":2395,"_extension":2396},"/2025/07/17/flux-plugin-for-project-g-assist-hackathon","17",false,"","Flux Plug-in for Project G-Assist","A Plug-in for Project G-Assist that puts the power of AI image generation right at your fingertips","2025-07-20","/static/flux/flux_plugin_for_project_g_assist.png",[14,15,16,17,18],"nvidia","ai","rtx","nim","flux",[20],{"link":21,"site":22},"https://x.com/briancaffey/status/1946684573095497992","x",true,{"type":25,"children":26,"toc":2349},"root",[27,35,39,44,49,54,61,68,114,120,163,169,222,228,271,277,305,311,316,344,356,362,368,373,378,384,398,478,484,521,527,555,752,758,774,829,835,862,868,873,1141,1161,1167,1172,1177,1182,1187,1192,1198,1211,1220,1225,1233,1238,1246,1251,1257,1283,1289,1584,1604,1610,1618,1623,1635,1641,1646,1650,1663,1669,1674,1682,1687,1693,1698,1731,1739,1744,1756,1789,1795,1803,1810,1815,1820,1826,1849,1855,1873,1879,1902,1908,1913,1945,1951,1956,2203,2209,2222,2245,2251,2294,2300,2313,2319,2332,2338,2343],{"type":28,"tag":29,"props":30,"children":31},"element","p",{},[32],{"type":33,"value":34},"text","This article will discuss my submission for the Project G-Assist Hackathon: Flux Plug-in for Project G-Assist. This plugin allows RTX AI PC users to tap into their GPU's image generation capabilities through the Flux.1-dev NVIDIA NIM. G-Assist now generates images from your commands on demand!",{"type":28,"tag":36,"props":37,"children":38},"flux-plugin-tweet",{},[],{"type":28,"tag":29,"props":40,"children":41},{},[42],{"type":33,"value":43},"The Flux Plugin for G-Assist is a plugin developed for NVIDIA’s Project G-Assist that brings real-time AI image generation directly to the desktop through natural language commands. It allows users to create high-quality images using the Flux family of models from Black Forest Labs, seamlessly integrated with the G-Assist interface. Users can simply type or speak prompts such as “a futuristic robot painter in a neon-lit workshop”, and the plugin handles the entire process—from submitting the prompt to generating and displaying the image—all without leaving the G-Assist chat window.",{"type":28,"tag":29,"props":45,"children":46},{},[47],{"type":33,"value":48},"The plugin supports multiple deployment backends for inference, including Flux NIMs running locally via WSL, on the cloud via build.nvidia.com, or through InvokeAI using the Flux Kontext model for image-to-image and screenshot-based transformations. The plugin includes additional tools for managing the inference service, such as checking status and turning the NIM service on or off, enabling a user-friendly experience for both beginners and power users.",{"type":28,"tag":29,"props":50,"children":51},{},[52],{"type":33,"value":53},"The goal of this plugin is to make generative image workflows faster, more accessible, and more fun—leveraging the strengths of NVIDIA’s hardware, AI models, and desktop ecosystem. By combining the power of FLUX with the voice-enabled G-Assist interface, the plugin turns any PC into a hands-free creative studio.",{"type":28,"tag":55,"props":56,"children":58},"h2",{"id":57},"what-can-it-do",[59],{"type":33,"value":60},"What Can It Do?",{"type":28,"tag":62,"props":63,"children":65},"h3",{"id":64},"image-generation",[66],{"type":33,"value":67},"Image Generation",{"type":28,"tag":69,"props":70,"children":71},"ul",{},[72,84,94,104],{"type":28,"tag":73,"props":74,"children":75},"li",{},[76,82],{"type":28,"tag":77,"props":78,"children":79},"strong",{},[80],{"type":33,"value":81},"Generate images from text prompts",{"type":33,"value":83}," using Flux AI model",{"type":28,"tag":73,"props":85,"children":86},{},[87,92],{"type":28,"tag":77,"props":88,"children":89},{},[90],{"type":33,"value":91},"Support for multiple backends",{"type":33,"value":93},": Local NIM servers, NVIDIA hosted services, or InvokeAI",{"type":28,"tag":73,"props":95,"children":96},{},[97,102],{"type":28,"tag":77,"props":98,"children":99},{},[100],{"type":33,"value":101},"Automatic desktop background setting",{"type":33,"value":103}," - generated images can be set as your wallpaper",{"type":28,"tag":73,"props":105,"children":106},{},[107,112],{"type":28,"tag":77,"props":108,"children":109},{},[110],{"type":33,"value":111},"High-quality output",{"type":33,"value":113}," with customizable parameters (resolution, steps, CFG scale)",{"type":28,"tag":62,"props":115,"children":117},{"id":116},"nim-server-management",[118],{"type":33,"value":119},"NIM Server Management",{"type":28,"tag":69,"props":121,"children":122},{},[123,133,143,153],{"type":28,"tag":73,"props":124,"children":125},{},[126,131],{"type":28,"tag":77,"props":127,"children":128},{},[129],{"type":33,"value":130},"Start/stop local Flux NIM servers",{"type":33,"value":132}," using WSL and Podman",{"type":28,"tag":73,"props":134,"children":135},{},[136,141],{"type":28,"tag":77,"props":137,"children":138},{},[139],{"type":33,"value":140},"Check NIM server status",{"type":33,"value":142}," to see if the service is running",{"type":28,"tag":73,"props":144,"children":145},{},[146,151],{"type":28,"tag":77,"props":147,"children":148},{},[149],{"type":33,"value":150},"Health endpoint testing",{"type":33,"value":152}," for local servers",{"type":28,"tag":73,"props":154,"children":155},{},[156,161],{"type":28,"tag":77,"props":157,"children":158},{},[159],{"type":33,"value":160},"Automatic configuration",{"type":33,"value":162}," using NGC API keys and Hugging Face tokens",{"type":28,"tag":62,"props":164,"children":166},{"id":165},"invokeai-integration",[167],{"type":33,"value":168},"InvokeAI Integration",{"type":28,"tag":69,"props":170,"children":171},{},[172,182,192,202,212],{"type":28,"tag":73,"props":173,"children":174},{},[175,180],{"type":28,"tag":77,"props":176,"children":177},{},[178],{"type":33,"value":179},"Upload screenshots to InvokeAI",{"type":33,"value":181}," for image-to-image workflows",{"type":28,"tag":73,"props":183,"children":184},{},[185,190],{"type":28,"tag":77,"props":186,"children":187},{},[188],{"type":33,"value":189},"Flux Kontext generation",{"type":33,"value":191}," using uploaded images as reference",{"type":28,"tag":73,"props":193,"children":194},{},[195,200],{"type":28,"tag":77,"props":196,"children":197},{},[198],{"type":33,"value":199},"Processor control",{"type":33,"value":201}," - pause and resume InvokeAI processing queues",{"type":28,"tag":73,"props":203,"children":204},{},[205,210],{"type":28,"tag":77,"props":206,"children":207},{},[208],{"type":33,"value":209},"VRAM management",{"type":33,"value":211}," - empty model cache to free up memory",{"type":28,"tag":73,"props":213,"children":214},{},[215,220],{"type":28,"tag":77,"props":216,"children":217},{},[218],{"type":33,"value":219},"Status monitoring",{"type":33,"value":221}," - check InvokeAI service health and version",{"type":28,"tag":62,"props":223,"children":225},{"id":224},"smart-configuration",[226],{"type":33,"value":227},"Smart Configuration",{"type":28,"tag":69,"props":229,"children":230},{},[231,241,251,261],{"type":28,"tag":73,"props":232,"children":233},{},[234,239],{"type":28,"tag":77,"props":235,"children":236},{},[237],{"type":33,"value":238},"Flexible URL configuration",{"type":33,"value":240}," - works with local servers or NVIDIA hosted endpoints",{"type":28,"tag":73,"props":242,"children":243},{},[244,249],{"type":28,"tag":77,"props":245,"children":246},{},[247],{"type":33,"value":248},"Automatic API key validation",{"type":33,"value":250}," - ensures proper NVIDIA API key format",{"type":28,"tag":73,"props":252,"children":253},{},[254,259],{"type":28,"tag":77,"props":255,"children":256},{},[257],{"type":33,"value":258},"Configurable output directories",{"type":33,"value":260}," for generated images",{"type":28,"tag":73,"props":262,"children":263},{},[264,269],{"type":28,"tag":77,"props":265,"children":266},{},[267],{"type":33,"value":268},"Board management",{"type":33,"value":270}," for InvokeAI gallery organization",{"type":28,"tag":62,"props":272,"children":274},{"id":273},"example-commands",[275],{"type":33,"value":276},"Example Commands",{"type":28,"tag":69,"props":278,"children":279},{},[280,285,290,295,300],{"type":28,"tag":73,"props":281,"children":282},{},[283],{"type":33,"value":284},"\"hey flux, generate an image of a cyberpunk city at night\"",{"type":28,"tag":73,"props":286,"children":287},{},[288],{"type":33,"value":289},"\"hey flux, start the Flux NIM server\"",{"type":28,"tag":73,"props":291,"children":292},{},[293],{"type":33,"value":294},"\"hey flux, use kontext to make it a cartoon style\" (does image-to-image generation using latest screenshot taken with NVIDIA screenshot shortcut)",{"type":28,"tag":73,"props":296,"children":297},{},[298],{"type":33,"value":299},"\"hey flux, empty the InvokeAI model cache to free up VRAM\"",{"type":28,"tag":73,"props":301,"children":302},{},[303],{"type":33,"value":304},"\"hey flux, Check if the NIM server is running\"",{"type":28,"tag":55,"props":306,"children":308},{"id":307},"before-you-start",[309],{"type":33,"value":310},"Before You Start",{"type":28,"tag":29,"props":312,"children":313},{},[314],{"type":33,"value":315},"Make sure you have:",{"type":28,"tag":69,"props":317,"children":318},{},[319,324,329,334,339],{"type":28,"tag":73,"props":320,"children":321},{},[322],{"type":33,"value":323},"Windows PC",{"type":28,"tag":73,"props":325,"children":326},{},[327],{"type":33,"value":328},"Python 3.12 or higher",{"type":28,"tag":73,"props":330,"children":331},{},[332],{"type":33,"value":333},"G-Assist installed on your system",{"type":28,"tag":73,"props":335,"children":336},{},[337],{"type":33,"value":338},"pywin32 >= 223",{"type":28,"tag":73,"props":340,"children":341},{},[342],{"type":33,"value":343},"Basic knowledge of Python",{"type":28,"tag":29,"props":345,"children":346},{},[347,349,354],{"type":33,"value":348},"💡 ",{"type":28,"tag":77,"props":350,"children":351},{},[352],{"type":33,"value":353},"Tip",{"type":33,"value":355},": Use a virtual environment to keep your plugin dependencies isolated from other Python projects!",{"type":28,"tag":55,"props":357,"children":359},{"id":358},"installation-guide",[360],{"type":33,"value":361},"Installation Guide",{"type":28,"tag":62,"props":363,"children":365},{"id":364},"step-1-get-the-files",[366],{"type":33,"value":367},"Step 1: Get the Files",{"type":28,"tag":29,"props":369,"children":370},{},[371],{"type":33,"value":372},"Clone this repository",{"type":28,"tag":29,"props":374,"children":375},{},[376],{"type":33,"value":377},"This downloads the plugin code and all necessary files to your computer.",{"type":28,"tag":62,"props":379,"children":381},{"id":380},"step-2-set-up-python-environment",[382],{"type":33,"value":383},"Step 2: Set Up Python Environment",{"type":28,"tag":29,"props":385,"children":386},{},[387,389,396],{"type":33,"value":388},"Run the ",{"type":28,"tag":390,"props":391,"children":393},"code",{"className":392},[],[394],{"type":33,"value":395},"setup.bat",{"type":33,"value":397}," file. This will take care of creating a Python virtual environment will install project dependencies.",{"type":28,"tag":399,"props":400,"children":404},"pre",{"code":401,"language":402,"meta":8,"className":403,"style":8},"python -m venv .venv\n.venv\\Scripts\\activate\npython -m pip install -r requirements.txt\n","bash","language-bash shiki shiki-themes github-light github-dark monokai",[405],{"type":28,"tag":390,"props":406,"children":407},{"__ignoreMap":8},[408,437,446],{"type":28,"tag":409,"props":410,"children":413},"span",{"class":411,"line":412},"line",1,[414,420,426,432],{"type":28,"tag":409,"props":415,"children":417},{"style":416},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E",[418],{"type":33,"value":419},"python",{"type":28,"tag":409,"props":421,"children":423},{"style":422},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#AE81FF",[424],{"type":33,"value":425}," -m",{"type":28,"tag":409,"props":427,"children":429},{"style":428},"--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#E6DB74",[430],{"type":33,"value":431}," venv",{"type":28,"tag":409,"props":433,"children":434},{"style":428},[435],{"type":33,"value":436}," .venv\n",{"type":28,"tag":409,"props":438,"children":440},{"class":411,"line":439},2,[441],{"type":28,"tag":409,"props":442,"children":443},{"style":416},[444],{"type":33,"value":445},".venv\\Scripts\\activate\n",{"type":28,"tag":409,"props":447,"children":449},{"class":411,"line":448},3,[450,454,458,463,468,473],{"type":28,"tag":409,"props":451,"children":452},{"style":416},[453],{"type":33,"value":419},{"type":28,"tag":409,"props":455,"children":456},{"style":422},[457],{"type":33,"value":425},{"type":28,"tag":409,"props":459,"children":460},{"style":428},[461],{"type":33,"value":462}," pip",{"type":28,"tag":409,"props":464,"children":465},{"style":428},[466],{"type":33,"value":467}," install",{"type":28,"tag":409,"props":469,"children":470},{"style":422},[471],{"type":33,"value":472}," -r",{"type":28,"tag":409,"props":474,"children":475},{"style":428},[476],{"type":33,"value":477}," requirements.txt\n",{"type":28,"tag":62,"props":479,"children":481},{"id":480},"step-3-build-the-project",[482],{"type":33,"value":483},"Step 3: Build the project",{"type":28,"tag":29,"props":485,"children":486},{},[487,489,495,497,503,505,511,513,519],{"type":33,"value":488},"Run ",{"type":28,"tag":390,"props":490,"children":492},{"className":491},[],[493],{"type":33,"value":494},"build.bat",{"type":33,"value":496}," to build the project. This script will also place the ",{"type":28,"tag":390,"props":498,"children":500},{"className":499},[],[501],{"type":33,"value":502},"g-assist-plugin-flux.exe",{"type":33,"value":504}," and ",{"type":28,"tag":390,"props":506,"children":508},{"className":507},[],[509],{"type":33,"value":510},"manifest.json",{"type":33,"value":512}," files in ",{"type":28,"tag":390,"props":514,"children":516},{"className":515},[],[517],{"type":33,"value":518},"%PROGRAMDATA%\\NVIDIA Corporation\\nvtopps\\rise\\plugins\\flux",{"type":33,"value":520},".",{"type":28,"tag":62,"props":522,"children":524},{"id":523},"step-4-configuration",[525],{"type":33,"value":526},"Step 4: Configuration",{"type":28,"tag":29,"props":528,"children":529},{},[530,532,538,540,546,548,553],{"type":33,"value":531},"Copy the ",{"type":28,"tag":390,"props":533,"children":535},{"className":534},[],[536],{"type":33,"value":537},"config.example.json",{"type":33,"value":539}," file and rename it as ",{"type":28,"tag":390,"props":541,"children":543},{"className":542},[],[544],{"type":33,"value":545},"config.json",{"type":33,"value":547}," and place it in ",{"type":28,"tag":390,"props":549,"children":551},{"className":550},[],[552],{"type":33,"value":518},{"type":33,"value":554},". Customize it with the appropriate configuration values (see below for configuration details). Here is an example configuration:",{"type":28,"tag":399,"props":556,"children":560},{"code":557,"language":558,"meta":8,"className":559,"style":8},"{\n    \"GALLERY_DIRECTORY\": \"E:\\\\NVIDIA\",\n    \"FLUX_NIM_URL\": \"http://localhost:8000\",\n    \"NGC_API_KEY\": \"xxxxxxxx\",\n    \"HF_TOKEN\": \"hf_xxxxxxxx\",\n    \"LOCAL_NIM_CACHE\": \"~/.cache/nim\",\n    \"INVOKEAI_URL\": \"http://localhost:9090\",\n    \"OUTPUT_DIRECTORY\": \"E:\\\\Flux\"\n}\n","json","language-json shiki shiki-themes github-light github-dark monokai",[561],{"type":28,"tag":390,"props":562,"children":563},{"__ignoreMap":8},[564,573,608,629,651,673,695,717,743],{"type":28,"tag":409,"props":565,"children":566},{"class":411,"line":412},[567],{"type":28,"tag":409,"props":568,"children":570},{"style":569},"--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#F8F8F2",[571],{"type":33,"value":572},"{\n",{"type":28,"tag":409,"props":574,"children":575},{"class":411,"line":439},[576,582,587,593,598,603],{"type":28,"tag":409,"props":577,"children":579},{"style":578},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#66D9EF;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[580],{"type":33,"value":581},"    \"GALLERY_DIRECTORY\"",{"type":28,"tag":409,"props":583,"children":584},{"style":569},[585],{"type":33,"value":586},": ",{"type":28,"tag":409,"props":588,"children":590},{"style":589},"--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#CFCFC2",[591],{"type":33,"value":592},"\"E:",{"type":28,"tag":409,"props":594,"children":595},{"style":422},[596],{"type":33,"value":597},"\\\\",{"type":28,"tag":409,"props":599,"children":600},{"style":589},[601],{"type":33,"value":602},"NVIDIA\"",{"type":28,"tag":409,"props":604,"children":605},{"style":569},[606],{"type":33,"value":607},",\n",{"type":28,"tag":409,"props":609,"children":610},{"class":411,"line":448},[611,616,620,625],{"type":28,"tag":409,"props":612,"children":613},{"style":578},[614],{"type":33,"value":615},"    \"FLUX_NIM_URL\"",{"type":28,"tag":409,"props":617,"children":618},{"style":569},[619],{"type":33,"value":586},{"type":28,"tag":409,"props":621,"children":622},{"style":589},[623],{"type":33,"value":624},"\"http://localhost:8000\"",{"type":28,"tag":409,"props":626,"children":627},{"style":569},[628],{"type":33,"value":607},{"type":28,"tag":409,"props":630,"children":632},{"class":411,"line":631},4,[633,638,642,647],{"type":28,"tag":409,"props":634,"children":635},{"style":578},[636],{"type":33,"value":637},"    \"NGC_API_KEY\"",{"type":28,"tag":409,"props":639,"children":640},{"style":569},[641],{"type":33,"value":586},{"type":28,"tag":409,"props":643,"children":644},{"style":589},[645],{"type":33,"value":646},"\"xxxxxxxx\"",{"type":28,"tag":409,"props":648,"children":649},{"style":569},[650],{"type":33,"value":607},{"type":28,"tag":409,"props":652,"children":654},{"class":411,"line":653},5,[655,660,664,669],{"type":28,"tag":409,"props":656,"children":657},{"style":578},[658],{"type":33,"value":659},"    \"HF_TOKEN\"",{"type":28,"tag":409,"props":661,"children":662},{"style":569},[663],{"type":33,"value":586},{"type":28,"tag":409,"props":665,"children":666},{"style":589},[667],{"type":33,"value":668},"\"hf_xxxxxxxx\"",{"type":28,"tag":409,"props":670,"children":671},{"style":569},[672],{"type":33,"value":607},{"type":28,"tag":409,"props":674,"children":676},{"class":411,"line":675},6,[677,682,686,691],{"type":28,"tag":409,"props":678,"children":679},{"style":578},[680],{"type":33,"value":681},"    \"LOCAL_NIM_CACHE\"",{"type":28,"tag":409,"props":683,"children":684},{"style":569},[685],{"type":33,"value":586},{"type":28,"tag":409,"props":687,"children":688},{"style":589},[689],{"type":33,"value":690},"\"~/.cache/nim\"",{"type":28,"tag":409,"props":692,"children":693},{"style":569},[694],{"type":33,"value":607},{"type":28,"tag":409,"props":696,"children":698},{"class":411,"line":697},7,[699,704,708,713],{"type":28,"tag":409,"props":700,"children":701},{"style":578},[702],{"type":33,"value":703},"    \"INVOKEAI_URL\"",{"type":28,"tag":409,"props":705,"children":706},{"style":569},[707],{"type":33,"value":586},{"type":28,"tag":409,"props":709,"children":710},{"style":589},[711],{"type":33,"value":712},"\"http://localhost:9090\"",{"type":28,"tag":409,"props":714,"children":715},{"style":569},[716],{"type":33,"value":607},{"type":28,"tag":409,"props":718,"children":720},{"class":411,"line":719},8,[721,726,730,734,738],{"type":28,"tag":409,"props":722,"children":723},{"style":578},[724],{"type":33,"value":725},"    \"OUTPUT_DIRECTORY\"",{"type":28,"tag":409,"props":727,"children":728},{"style":569},[729],{"type":33,"value":586},{"type":28,"tag":409,"props":731,"children":732},{"style":589},[733],{"type":33,"value":592},{"type":28,"tag":409,"props":735,"children":736},{"style":422},[737],{"type":33,"value":597},{"type":28,"tag":409,"props":739,"children":740},{"style":589},[741],{"type":33,"value":742},"Flux\"\n",{"type":28,"tag":409,"props":744,"children":746},{"class":411,"line":745},9,[747],{"type":28,"tag":409,"props":748,"children":749},{"style":569},[750],{"type":33,"value":751},"}\n",{"type":28,"tag":62,"props":753,"children":755},{"id":754},"step-5-set-up-flux",[756],{"type":33,"value":757},"Step 5: Set up Flux",{"type":28,"tag":29,"props":759,"children":760},{},[761,763,772],{"type":33,"value":762},"Follow instructions ",{"type":28,"tag":764,"props":765,"children":769},"a",{"href":766,"rel":767},"https://build.nvidia.com/black-forest-labs/flux_1-dev/deploy?environment=wsl2.md",[768],"nofollow",[770],{"type":33,"value":771},"here",{"type":33,"value":773}," for installing the FLUX.1-dev model from Black Forest Labs using NVIDIA NIM on WSL. Be sure to do the following:",{"type":28,"tag":69,"props":775,"children":776},{},[777,790,804,809],{"type":28,"tag":73,"props":778,"children":779},{},[780,782,788],{"type":33,"value":781},"follow instructions here: ",{"type":28,"tag":764,"props":783,"children":786},{"href":784,"rel":785},"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html",[768],[787],{"type":33,"value":784},{"type":33,"value":789}," for getting started with WSL",{"type":28,"tag":73,"props":791,"children":792},{},[793,795,802],{"type":33,"value":794},"use the ",{"type":28,"tag":764,"props":796,"children":799},{"href":797,"rel":798},"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html#use-the-nvidia-nim-wsl2-installer-recommended",[768],[800],{"type":33,"value":801},"NVIDIA NIM WSL2 Installer",{"type":33,"value":803}," for configuring a new WSL environment configured with all of the required NVIDIA dependencies",{"type":28,"tag":73,"props":805,"children":806},{},[807],{"type":33,"value":808},"In your Hugging Face account read and accept FLUX.1-dev, FLUX.1-Canny-dev, FLUX.1-Depth-dev and FLUX.1-dev-onnx License Agreements and Acceptable Use Policy. You must accept the agreements/policies for all of the models even though this plugin does not directly use the Canny or Depth modes.",{"type":28,"tag":73,"props":810,"children":811},{},[812,814,820,822,827],{"type":33,"value":813},"Make sure to map port ",{"type":28,"tag":390,"props":815,"children":817},{"className":816},[],[818],{"type":33,"value":819},"8000",{"type":33,"value":821}," in the NIM to port ",{"type":28,"tag":390,"props":823,"children":825},{"className":824},[],[826],{"type":33,"value":819},{"type":33,"value":828}," on the WSL host as shown in the setup link above.",{"type":28,"tag":62,"props":830,"children":832},{"id":831},"step-6-install-invokeai-optional",[833],{"type":33,"value":834},"Step 6: Install InvokeAI (optional)",{"type":28,"tag":29,"props":836,"children":837},{},[838,840,846,848,853,855,861],{"type":33,"value":839},"InvokeAI has a Windows installer that can be found here: ",{"type":28,"tag":764,"props":841,"children":844},{"href":842,"rel":843},"https://invoke-ai.github.io/InvokeAI/installation/quick_start/#step-2-download",[768],[845],{"type":33,"value":842},{"type":33,"value":847},". Download the Flux models including the ",{"type":28,"tag":77,"props":849,"children":850},{},[851],{"type":33,"value":852},"FLUX.1 Kontext dev (Quantized)",{"type":33,"value":854}," model for using Flux Kontext in the Flux Plug-in for G-Assist. By default InvokeAI runs on ",{"type":28,"tag":390,"props":856,"children":858},{"className":857},[],[859],{"type":33,"value":860},"http://localhost:9090",{"type":33,"value":520},{"type":28,"tag":62,"props":863,"children":865},{"id":864},"step-7-start-the-nvidia-nim",[866],{"type":33,"value":867},"Step 7: Start the NVIDIA NIM",{"type":28,"tag":29,"props":869,"children":870},{},[871],{"type":33,"value":872},"Ask flux if this NIM is running. If it is not running, ask flux to start the NIM. This will run a command to star the NIM container in WSL using podman:",{"type":28,"tag":399,"props":874,"children":877},{"code":875,"language":419,"meta":8,"className":876,"style":8},"podman_cmd = [\n    'wsl', '-d', 'NVIDIA-Workbench',\n    'podman', 'run', '-d', '--rm', '--name=nim-server',\n    '--device', 'nvidia.com/gpu=all',\n    '-e', f'NGC_API_KEY={NGC_API_KEY}',\n    '-e', f'HF_TOKEN={HF_TOKEN}',\n    '-p', '8000:8000',\n    '-v', f'{LOCAL_NIM_CACHE}:/opt/nim/.cache/',\n    'nvcr.io/nim/black-forest-labs/flux.1-dev:1.0.0'\n]\n","language-python shiki shiki-themes github-light github-dark monokai",[878],{"type":28,"tag":390,"props":879,"children":880},{"__ignoreMap":8},[881,900,931,978,999,1036,1069,1090,1124,1132],{"type":28,"tag":409,"props":882,"children":883},{"class":411,"line":412},[884,889,895],{"type":28,"tag":409,"props":885,"children":886},{"style":569},[887],{"type":33,"value":888},"podman_cmd ",{"type":28,"tag":409,"props":890,"children":892},{"style":891},"--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#F92672",[893],{"type":33,"value":894},"=",{"type":28,"tag":409,"props":896,"children":897},{"style":569},[898],{"type":33,"value":899}," [\n",{"type":28,"tag":409,"props":901,"children":902},{"class":411,"line":439},[903,908,913,918,922,927],{"type":28,"tag":409,"props":904,"children":905},{"style":428},[906],{"type":33,"value":907},"    'wsl'",{"type":28,"tag":409,"props":909,"children":910},{"style":569},[911],{"type":33,"value":912},", ",{"type":28,"tag":409,"props":914,"children":915},{"style":428},[916],{"type":33,"value":917},"'-d'",{"type":28,"tag":409,"props":919,"children":920},{"style":569},[921],{"type":33,"value":912},{"type":28,"tag":409,"props":923,"children":924},{"style":428},[925],{"type":33,"value":926},"'NVIDIA-Workbench'",{"type":28,"tag":409,"props":928,"children":929},{"style":569},[930],{"type":33,"value":607},{"type":28,"tag":409,"props":932,"children":933},{"class":411,"line":448},[934,939,943,948,952,956,960,965,969,974],{"type":28,"tag":409,"props":935,"children":936},{"style":428},[937],{"type":33,"value":938},"    'podman'",{"type":28,"tag":409,"props":940,"children":941},{"style":569},[942],{"type":33,"value":912},{"type":28,"tag":409,"props":944,"children":945},{"style":428},[946],{"type":33,"value":947},"'run'",{"type":28,"tag":409,"props":949,"children":950},{"style":569},[951],{"type":33,"value":912},{"type":28,"tag":409,"props":953,"children":954},{"style":428},[955],{"type":33,"value":917},{"type":28,"tag":409,"props":957,"children":958},{"style":569},[959],{"type":33,"value":912},{"type":28,"tag":409,"props":961,"children":962},{"style":428},[963],{"type":33,"value":964},"'--rm'",{"type":28,"tag":409,"props":966,"children":967},{"style":569},[968],{"type":33,"value":912},{"type":28,"tag":409,"props":970,"children":971},{"style":428},[972],{"type":33,"value":973},"'--name=nim-server'",{"type":28,"tag":409,"props":975,"children":976},{"style":569},[977],{"type":33,"value":607},{"type":28,"tag":409,"props":979,"children":980},{"class":411,"line":631},[981,986,990,995],{"type":28,"tag":409,"props":982,"children":983},{"style":428},[984],{"type":33,"value":985},"    '--device'",{"type":28,"tag":409,"props":987,"children":988},{"style":569},[989],{"type":33,"value":912},{"type":28,"tag":409,"props":991,"children":992},{"style":428},[993],{"type":33,"value":994},"'nvidia.com/gpu=all'",{"type":28,"tag":409,"props":996,"children":997},{"style":569},[998],{"type":33,"value":607},{"type":28,"tag":409,"props":1000,"children":1001},{"class":411,"line":653},[1002,1007,1011,1017,1022,1027,1032],{"type":28,"tag":409,"props":1003,"children":1004},{"style":428},[1005],{"type":33,"value":1006},"    '-e'",{"type":28,"tag":409,"props":1008,"children":1009},{"style":569},[1010],{"type":33,"value":912},{"type":28,"tag":409,"props":1012,"children":1014},{"style":1013},"--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#66D9EF;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[1015],{"type":33,"value":1016},"f",{"type":28,"tag":409,"props":1018,"children":1019},{"style":428},[1020],{"type":33,"value":1021},"'NGC_API_KEY=",{"type":28,"tag":409,"props":1023,"children":1024},{"style":422},[1025],{"type":33,"value":1026},"{NGC_API_KEY}",{"type":28,"tag":409,"props":1028,"children":1029},{"style":428},[1030],{"type":33,"value":1031},"'",{"type":28,"tag":409,"props":1033,"children":1034},{"style":569},[1035],{"type":33,"value":607},{"type":28,"tag":409,"props":1037,"children":1038},{"class":411,"line":675},[1039,1043,1047,1051,1056,1061,1065],{"type":28,"tag":409,"props":1040,"children":1041},{"style":428},[1042],{"type":33,"value":1006},{"type":28,"tag":409,"props":1044,"children":1045},{"style":569},[1046],{"type":33,"value":912},{"type":28,"tag":409,"props":1048,"children":1049},{"style":1013},[1050],{"type":33,"value":1016},{"type":28,"tag":409,"props":1052,"children":1053},{"style":428},[1054],{"type":33,"value":1055},"'HF_TOKEN=",{"type":28,"tag":409,"props":1057,"children":1058},{"style":422},[1059],{"type":33,"value":1060},"{HF_TOKEN}",{"type":28,"tag":409,"props":1062,"children":1063},{"style":428},[1064],{"type":33,"value":1031},{"type":28,"tag":409,"props":1066,"children":1067},{"style":569},[1068],{"type":33,"value":607},{"type":28,"tag":409,"props":1070,"children":1071},{"class":411,"line":697},[1072,1077,1081,1086],{"type":28,"tag":409,"props":1073,"children":1074},{"style":428},[1075],{"type":33,"value":1076},"    '-p'",{"type":28,"tag":409,"props":1078,"children":1079},{"style":569},[1080],{"type":33,"value":912},{"type":28,"tag":409,"props":1082,"children":1083},{"style":428},[1084],{"type":33,"value":1085},"'8000:8000'",{"type":28,"tag":409,"props":1087,"children":1088},{"style":569},[1089],{"type":33,"value":607},{"type":28,"tag":409,"props":1091,"children":1092},{"class":411,"line":719},[1093,1098,1102,1106,1110,1115,1120],{"type":28,"tag":409,"props":1094,"children":1095},{"style":428},[1096],{"type":33,"value":1097},"    '-v'",{"type":28,"tag":409,"props":1099,"children":1100},{"style":569},[1101],{"type":33,"value":912},{"type":28,"tag":409,"props":1103,"children":1104},{"style":1013},[1105],{"type":33,"value":1016},{"type":28,"tag":409,"props":1107,"children":1108},{"style":428},[1109],{"type":33,"value":1031},{"type":28,"tag":409,"props":1111,"children":1112},{"style":422},[1113],{"type":33,"value":1114},"{LOCAL_NIM_CACHE}",{"type":28,"tag":409,"props":1116,"children":1117},{"style":428},[1118],{"type":33,"value":1119},":/opt/nim/.cache/'",{"type":28,"tag":409,"props":1121,"children":1122},{"style":569},[1123],{"type":33,"value":607},{"type":28,"tag":409,"props":1125,"children":1126},{"class":411,"line":745},[1127],{"type":28,"tag":409,"props":1128,"children":1129},{"style":428},[1130],{"type":33,"value":1131},"    'nvcr.io/nim/black-forest-labs/flux.1-dev:1.0.0'\n",{"type":28,"tag":409,"props":1133,"children":1135},{"class":411,"line":1134},10,[1136],{"type":28,"tag":409,"props":1137,"children":1138},{"style":569},[1139],{"type":33,"value":1140},"]\n",{"type":28,"tag":29,"props":1142,"children":1143},{},[1144,1146,1152,1153,1159],{"type":33,"value":1145},"Then ask if the NIM is ready. This will check the ",{"type":28,"tag":390,"props":1147,"children":1149},{"className":1148},[],[1150],{"type":33,"value":1151},"/v1/health/live",{"type":33,"value":504},{"type":28,"tag":390,"props":1154,"children":1156},{"className":1155},[],[1157],{"type":33,"value":1158},"/v1/health/ready",{"type":33,"value":1160}," endpoints of the Flux NIM.",{"type":28,"tag":62,"props":1162,"children":1164},{"id":1163},"step-8-generete-ai-images-using-the-flux-plug-in-in-the-g-assist-chat-window",[1165],{"type":33,"value":1166},"Step 8: Generete AI images using the Flux Plug-in in the G-Assist chat window",{"type":28,"tag":29,"props":1168,"children":1169},{},[1170],{"type":33,"value":1171},"Send a message to G-Assist:",{"type":28,"tag":29,"props":1173,"children":1174},{},[1175],{"type":33,"value":1176},"\"hey flux, generate a cat piloting a spaceship\"",{"type":28,"tag":29,"props":1178,"children":1179},{},[1180],{"type":33,"value":1181},"The Flux Plug-in will respond with:",{"type":28,"tag":29,"props":1183,"children":1184},{},[1185],{"type":33,"value":1186},"flux> Your image generation request is in progress! Prompt: \"a cat piloting a spaceship\"",{"type":28,"tag":29,"props":1188,"children":1189},{},[1190],{"type":33,"value":1191},"When the image generation is complete you will find the image on your Desktop background, and the image will be saved to the output directory specified in your configuration file.",{"type":28,"tag":62,"props":1193,"children":1195},{"id":1194},"step-9-transform-a-screenshot-with-flux-kontext",[1196],{"type":33,"value":1197},"Step 9: Transform a screenshot with Flux Kontext",{"type":28,"tag":29,"props":1199,"children":1200},{},[1201,1203,1209],{"type":33,"value":1202},"Take a screenshot using the NVIDIA Screenshot hotkey (usually ",{"type":28,"tag":390,"props":1204,"children":1206},{"className":1205},[],[1207],{"type":33,"value":1208},"Alt + F1",{"type":33,"value":1210},"), and then ask the Flux Plug-in to transform it to any style using Kontext.",{"type":28,"tag":29,"props":1212,"children":1213},{},[1214],{"type":28,"tag":1215,"props":1216,"children":1219},"img",{"alt":1217,"src":1218},"Cat piloting spaceship","/static/flux/cat_spaceship.png",[],{"type":28,"tag":29,"props":1221,"children":1222},{},[1223],{"type":33,"value":1224},"hey flux, use kontext with the prompt: cartoon style",{"type":28,"tag":29,"props":1226,"children":1227},{},[1228],{"type":28,"tag":1215,"props":1229,"children":1232},{"alt":1230,"src":1231},"Cat Spaceship Cartoon style with Kontext","/static/flux/cat_spaceship_cartoon.png",[],{"type":28,"tag":29,"props":1234,"children":1235},{},[1236],{"type":33,"value":1237},"Flux does this by triggering an InvokeAI graph workflow. The generated image and the workflow can both be viewed in the InvokeAI UI:",{"type":28,"tag":29,"props":1239,"children":1240},{},[1241],{"type":28,"tag":1215,"props":1242,"children":1245},{"alt":1243,"src":1244},"InvokeAI Flux Kontext Workflow","/static/flux/invokeai_workflow.png",[],{"type":28,"tag":29,"props":1247,"children":1248},{},[1249],{"type":33,"value":1250},"You can ask the Flux Plug-in to pause/resume InvokeAI processing to avoid running Flux Kontext image generation while your GPU is busy with other tasks. Also you can ask flux to empty the model cache in order to free up VRAM on your GPU.",{"type":28,"tag":55,"props":1252,"children":1254},{"id":1253},"configuration",[1255],{"type":33,"value":1256},"Configuration",{"type":28,"tag":29,"props":1258,"children":1259},{},[1260,1262,1267,1269,1274,1276,1281],{"type":33,"value":1261},"The Flux plugin uses a ",{"type":28,"tag":390,"props":1263,"children":1265},{"className":1264},[],[1266],{"type":33,"value":545},{"type":33,"value":1268}," file to manage all settings. Copy ",{"type":28,"tag":390,"props":1270,"children":1272},{"className":1271},[],[1273],{"type":33,"value":537},{"type":33,"value":1275}," to ",{"type":28,"tag":390,"props":1277,"children":1279},{"className":1278},[],[1280],{"type":33,"value":545},{"type":33,"value":1282}," and customize the values for your setup.",{"type":28,"tag":62,"props":1284,"children":1286},{"id":1285},"configuration-options",[1287],{"type":33,"value":1288},"Configuration Options",{"type":28,"tag":1290,"props":1291,"children":1292},"table",{},[1293,1317],{"type":28,"tag":1294,"props":1295,"children":1296},"thead",{},[1297],{"type":28,"tag":1298,"props":1299,"children":1300},"tr",{},[1301,1307,1312],{"type":28,"tag":1302,"props":1303,"children":1304},"th",{},[1305],{"type":33,"value":1306},"Option",{"type":28,"tag":1302,"props":1308,"children":1309},{},[1310],{"type":33,"value":1311},"Example Values",{"type":28,"tag":1302,"props":1313,"children":1314},{},[1315],{"type":33,"value":1316},"Required",{"type":28,"tag":1318,"props":1319,"children":1320},"tbody",{},[1321,1355,1381,1413,1439,1464,1489,1520,1552],{"type":28,"tag":1298,"props":1322,"children":1323},{},[1324,1334,1350],{"type":28,"tag":1325,"props":1326,"children":1327},"td",{},[1328],{"type":28,"tag":390,"props":1329,"children":1331},{"className":1330},[],[1332],{"type":33,"value":1333},"GALLERY_DIRECTORY",{"type":28,"tag":1325,"props":1335,"children":1336},{},[1337,1343,1344],{"type":28,"tag":390,"props":1338,"children":1340},{"className":1339},[],[1341],{"type":33,"value":1342},"\"D:\\\\Screenshots\"",{"type":33,"value":912},{"type":28,"tag":390,"props":1345,"children":1347},{"className":1346},[],[1348],{"type":33,"value":1349},"\"C:\\\\NVIDIA\"",{"type":28,"tag":1325,"props":1351,"children":1352},{},[1353],{"type":33,"value":1354},"No",{"type":28,"tag":1298,"props":1356,"children":1357},{},[1358,1367,1376],{"type":28,"tag":1325,"props":1359,"children":1360},{},[1361],{"type":28,"tag":390,"props":1362,"children":1364},{"className":1363},[],[1365],{"type":33,"value":1366},"NVIDIA_API_KEY",{"type":28,"tag":1325,"props":1368,"children":1369},{},[1370],{"type":28,"tag":390,"props":1371,"children":1373},{"className":1372},[],[1374],{"type":33,"value":1375},"\"nvapi-your-key-here\"",{"type":28,"tag":1325,"props":1377,"children":1378},{},[1379],{"type":33,"value":1380},"No*",{"type":28,"tag":1298,"props":1382,"children":1383},{},[1384,1393,1408],{"type":28,"tag":1325,"props":1385,"children":1386},{},[1387],{"type":28,"tag":390,"props":1388,"children":1390},{"className":1389},[],[1391],{"type":33,"value":1392},"FLUX_NIM_URL",{"type":28,"tag":1325,"props":1394,"children":1395},{},[1396,1401,1402],{"type":28,"tag":390,"props":1397,"children":1399},{"className":1398},[],[1400],{"type":33,"value":624},{"type":33,"value":912},{"type":28,"tag":390,"props":1403,"children":1405},{"className":1404},[],[1406],{"type":33,"value":1407},"\"http://192.168.1.100:8000\"",{"type":28,"tag":1325,"props":1409,"children":1410},{},[1411],{"type":33,"value":1412},"Yes",{"type":28,"tag":1298,"props":1414,"children":1415},{},[1416,1425,1434],{"type":28,"tag":1325,"props":1417,"children":1418},{},[1419],{"type":28,"tag":390,"props":1420,"children":1422},{"className":1421},[],[1423],{"type":33,"value":1424},"NGC_API_KEY",{"type":28,"tag":1325,"props":1426,"children":1427},{},[1428],{"type":28,"tag":390,"props":1429,"children":1431},{"className":1430},[],[1432],{"type":33,"value":1433},"\"your-ngc-key\"",{"type":28,"tag":1325,"props":1435,"children":1436},{},[1437],{"type":33,"value":1438},"Yes**",{"type":28,"tag":1298,"props":1440,"children":1441},{},[1442,1451,1460],{"type":28,"tag":1325,"props":1443,"children":1444},{},[1445],{"type":28,"tag":390,"props":1446,"children":1448},{"className":1447},[],[1449],{"type":33,"value":1450},"HF_TOKEN",{"type":28,"tag":1325,"props":1452,"children":1453},{},[1454],{"type":28,"tag":390,"props":1455,"children":1457},{"className":1456},[],[1458],{"type":33,"value":1459},"\"hf_your-token\"",{"type":28,"tag":1325,"props":1461,"children":1462},{},[1463],{"type":33,"value":1438},{"type":28,"tag":1298,"props":1465,"children":1466},{},[1467,1476,1485],{"type":28,"tag":1325,"props":1468,"children":1469},{},[1470],{"type":28,"tag":390,"props":1471,"children":1473},{"className":1472},[],[1474],{"type":33,"value":1475},"LOCAL_NIM_CACHE",{"type":28,"tag":1325,"props":1477,"children":1478},{},[1479],{"type":28,"tag":390,"props":1480,"children":1482},{"className":1481},[],[1483],{"type":33,"value":1484},"~/.cache/nim",{"type":28,"tag":1325,"props":1486,"children":1487},{},[1488],{"type":33,"value":1438},{"type":28,"tag":1298,"props":1490,"children":1491},{},[1492,1501,1516],{"type":28,"tag":1325,"props":1493,"children":1494},{},[1495],{"type":28,"tag":390,"props":1496,"children":1498},{"className":1497},[],[1499],{"type":33,"value":1500},"INVOKEAI_URL",{"type":28,"tag":1325,"props":1502,"children":1503},{},[1504,1509,1510],{"type":28,"tag":390,"props":1505,"children":1507},{"className":1506},[],[1508],{"type":33,"value":712},{"type":33,"value":912},{"type":28,"tag":390,"props":1511,"children":1513},{"className":1512},[],[1514],{"type":33,"value":1515},"\"http://192.168.1.100:9090\"",{"type":28,"tag":1325,"props":1517,"children":1518},{},[1519],{"type":33,"value":1354},{"type":28,"tag":1298,"props":1521,"children":1522},{},[1523,1532,1548],{"type":28,"tag":1325,"props":1524,"children":1525},{},[1526],{"type":28,"tag":390,"props":1527,"children":1529},{"className":1528},[],[1530],{"type":33,"value":1531},"BOARD_ID",{"type":28,"tag":1325,"props":1533,"children":1534},{},[1535,1541,1542],{"type":28,"tag":390,"props":1536,"children":1538},{"className":1537},[],[1539],{"type":33,"value":1540},"\"my-gallery-board\"",{"type":33,"value":912},{"type":28,"tag":390,"props":1543,"children":1545},{"className":1544},[],[1546],{"type":33,"value":1547},"\"flux-gallery\"",{"type":28,"tag":1325,"props":1549,"children":1550},{},[1551],{"type":33,"value":1354},{"type":28,"tag":1298,"props":1553,"children":1554},{},[1555,1564,1580],{"type":28,"tag":1325,"props":1556,"children":1557},{},[1558],{"type":28,"tag":390,"props":1559,"children":1561},{"className":1560},[],[1562],{"type":33,"value":1563},"OUTPUT_DIRECTORY",{"type":28,"tag":1325,"props":1565,"children":1566},{},[1567,1573,1574],{"type":28,"tag":390,"props":1568,"children":1570},{"className":1569},[],[1571],{"type":33,"value":1572},"\"C:\\\\GeneratedImages\"",{"type":33,"value":912},{"type":28,"tag":390,"props":1575,"children":1577},{"className":1576},[],[1578],{"type":33,"value":1579},"\"D:\\\\flux-output\"",{"type":28,"tag":1325,"props":1581,"children":1582},{},[1583],{"type":33,"value":1354},{"type":28,"tag":29,"props":1585,"children":1586},{},[1587,1589,1594,1596,1602],{"type":33,"value":1588},"*Required only when using NVIDIA hosted Flux service (",{"type":28,"tag":390,"props":1590,"children":1592},{"className":1591},[],[1593],{"type":33,"value":1392},{"type":33,"value":1595}," starts with \"",{"type":28,"tag":764,"props":1597,"children":1600},{"href":1598,"rel":1599},"https://ai.api.nvidia.com",[768],[1601],{"type":33,"value":1598},{"type":33,"value":1603},"\")\n**Required only when using local NIM server",{"type":28,"tag":55,"props":1605,"children":1607},{"id":1606},"using-the-flux1-dev-nvidia-nim-for-text-to-image-generation",[1608],{"type":33,"value":1609},"Using the Flux.1-dev NVIDIA NIM for text-to-image generation",{"type":28,"tag":29,"props":1611,"children":1612},{},[1613],{"type":28,"tag":1215,"props":1614,"children":1617},{"alt":1615,"src":1616},"Desert Nomad","/static/flux/desert_nomad.png",[],{"type":28,"tag":29,"props":1619,"children":1620},{},[1621],{"type":33,"value":1622},"On NVIDIA GeForce RTX AI PCs, the best way to do AI image inference is by using NVIDIA NIMs. Windows currently has beta support for running NVIDIA NIMs in WSL with Podman (a program for running containers, similar to Docker).",{"type":28,"tag":29,"props":1624,"children":1625},{},[1626,1628,1633],{"type":33,"value":1627},"NVIDIA provides an installer that installs a WSL distribution with all dependencies installed. You can find those resources here: ",{"type":28,"tag":764,"props":1629,"children":1631},{"href":784,"rel":1630},[768],[1632],{"type":33,"value":784},{"type":33,"value":1634}," (WSL2 is required for hosting any NIM. Refer to the official NVIDIA NIM on WSL2 documentation for setup instructions.)",{"type":28,"tag":62,"props":1636,"children":1638},{"id":1637},"how-it-works",[1639],{"type":33,"value":1640},"How It Works",{"type":28,"tag":29,"props":1642,"children":1643},{},[1644],{"type":33,"value":1645},"You can request an image to be generated by simply saying something like:",{"type":28,"tag":29,"props":1647,"children":1648},{},[1649],{"type":33,"value":1176},{"type":28,"tag":29,"props":1651,"children":1652},{},[1653,1655,1661],{"type":33,"value":1654},"The Flux Plugin will make an API request to the Flux NIM URL (configured in your config.json, defaults to ",{"type":28,"tag":764,"props":1656,"children":1659},{"href":1657,"rel":1658},"http://localhost:8000",[768],[1660],{"type":33,"value":1657},{"type":33,"value":1662},").",{"type":28,"tag":62,"props":1664,"children":1666},{"id":1665},"asynchronous-processing",[1667],{"type":33,"value":1668},"Asynchronous Processing",{"type":28,"tag":29,"props":1670,"children":1671},{},[1672],{"type":33,"value":1673},"The G-Assist chat assistant has a timeout of 10 seconds, so the chat assistant returns immediately with:",{"type":28,"tag":399,"props":1675,"children":1677},{"code":1676},"flux> Your image generation request is in progress! Prompt: \"a cat piloting a spaceship\"\n",[1678],{"type":28,"tag":390,"props":1679,"children":1680},{"__ignoreMap":8},[1681],{"type":33,"value":1676},{"type":28,"tag":29,"props":1683,"children":1684},{},[1685],{"type":33,"value":1686},"The image generation request runs on a separate thread, since the image generation process with the NVIDIA NIM can take up to 30 seconds (depending on the number of steps, 50 steps is used in the plugin for best results). The plugin then creates an image file from the base64 encoded image in the response from the Flux NIM server, and it sets this image as your desktop background image.",{"type":28,"tag":62,"props":1688,"children":1690},{"id":1689},"nim-management",[1691],{"type":33,"value":1692},"NIM Management",{"type":28,"tag":29,"props":1694,"children":1695},{},[1696],{"type":33,"value":1697},"There are also commands for starting and stopping the Flux NIM, which runs Podman commands inside of the NVIDIA-Workbench WSL distribution:",{"type":28,"tag":69,"props":1699,"children":1700},{},[1701,1711,1721],{"type":28,"tag":73,"props":1702,"children":1703},{},[1704,1709],{"type":28,"tag":77,"props":1705,"children":1706},{},[1707],{"type":33,"value":1708},"Start NIM",{"type":33,"value":1710},": \"hey flux, start the Flux NIM server\"",{"type":28,"tag":73,"props":1712,"children":1713},{},[1714,1719],{"type":28,"tag":77,"props":1715,"children":1716},{},[1717],{"type":33,"value":1718},"Stop NIM",{"type":33,"value":1720},": \"hey flux, stop the Flux NIM server\"",{"type":28,"tag":73,"props":1722,"children":1723},{},[1724,1729],{"type":28,"tag":77,"props":1725,"children":1726},{},[1727],{"type":33,"value":1728},"Check Status",{"type":33,"value":1730},": \"hey flux, check if the NIM server is running\"",{"type":28,"tag":29,"props":1732,"children":1733},{},[1734],{"type":28,"tag":1215,"props":1735,"children":1738},{"alt":1736,"src":1737},"Flux Plug-in controls","/static/flux/controls.png",[],{"type":28,"tag":62,"props":1740,"children":1742},{"id":1741},"configuration-1",[1743],{"type":33,"value":1256},{"type":28,"tag":29,"props":1745,"children":1746},{},[1747,1749,1754],{"type":33,"value":1748},"Make sure your ",{"type":28,"tag":390,"props":1750,"children":1752},{"className":1751},[],[1753],{"type":33,"value":545},{"type":33,"value":1755}," includes the necessary credentials:",{"type":28,"tag":69,"props":1757,"children":1758},{},[1759,1769,1779],{"type":28,"tag":73,"props":1760,"children":1761},{},[1762,1767],{"type":28,"tag":390,"props":1763,"children":1765},{"className":1764},[],[1766],{"type":33,"value":1424},{"type":33,"value":1768},": Your NVIDIA NGC API key for downloading models",{"type":28,"tag":73,"props":1770,"children":1771},{},[1772,1777],{"type":28,"tag":390,"props":1773,"children":1775},{"className":1774},[],[1776],{"type":33,"value":1450},{"type":33,"value":1778},": Your Hugging Face token for model access",{"type":28,"tag":73,"props":1780,"children":1781},{},[1782,1787],{"type":28,"tag":390,"props":1783,"children":1785},{"className":1784},[],[1786],{"type":33,"value":1475},{"type":33,"value":1788},": Path to your local NIM cache directory",{"type":28,"tag":55,"props":1790,"children":1792},{"id":1791},"image-to-image-generation-with-flux-kontext-and-invokeai",[1793],{"type":33,"value":1794},"Image-to-image generation with Flux Kontext and InvokeAI",{"type":28,"tag":29,"props":1796,"children":1797},{},[1798],{"type":28,"tag":1215,"props":1799,"children":1802},{"alt":1800,"src":1801},"Helicopter over NYC","/static/flux/nyc_heli.png",[],{"type":28,"tag":29,"props":1804,"children":1805},{},[1806],{"type":28,"tag":1215,"props":1807,"children":1809},{"alt":1800,"src":1808},"/static/flux/nyc_heli_watercolor.png",[],{"type":28,"tag":29,"props":1811,"children":1812},{},[1813],{"type":33,"value":1814},"The Flux Plug-in supports image-to-image generation using an open source image generation tool called InvokeAI. This tool is similar to ComfyUI and it has solid API support. Currently there is no NVIDIA NIM for Flux Kontext but the NVIDIA blog mentioned that this might be released as soon as May 2025.",{"type":28,"tag":29,"props":1816,"children":1817},{},[1818],{"type":33,"value":1819},"You can interact with the InvokeAI program in a few different ways:",{"type":28,"tag":62,"props":1821,"children":1823},{"id":1822},"screenshot-based-image-generation",[1824],{"type":33,"value":1825},"Screenshot-based Image Generation",{"type":28,"tag":69,"props":1827,"children":1828},{},[1829,1839,1844],{"type":28,"tag":73,"props":1830,"children":1831},{},[1832,1837],{"type":28,"tag":77,"props":1833,"children":1834},{},[1835],{"type":33,"value":1836},"Upload your latest screenshot",{"type":33,"value":1838}," and perform image-to-image generation using Flux Kontext. This allows you to apply any type of manipulation to your screenshot (for example, you can say \"hey flux, use kontext to make it in the style of a cartoon\")",{"type":28,"tag":73,"props":1840,"children":1841},{},[1842],{"type":33,"value":1843},"The plugin automatically finds your most recent screenshot and uploads it to InvokeAI",{"type":28,"tag":73,"props":1845,"children":1846},{},[1847],{"type":33,"value":1848},"You can provide custom prompts to guide the transformation process",{"type":28,"tag":62,"props":1850,"children":1852},{"id":1851},"processing-control",[1853],{"type":33,"value":1854},"Processing Control",{"type":28,"tag":69,"props":1856,"children":1857},{},[1858,1868],{"type":28,"tag":73,"props":1859,"children":1860},{},[1861,1866],{"type":28,"tag":77,"props":1862,"children":1863},{},[1864],{"type":33,"value":1865},"Pause or resume processing",{"type":33,"value":1867},": This is useful if you are playing a GPU intensive game. You can pause processing, but still submit image-to-image generation tasks using Flux Kontext. The tasks will be queued and they can be resumed later when your GPU is not busy with other tasks.",{"type":28,"tag":73,"props":1869,"children":1870},{},[1871],{"type":33,"value":1872},"Monitor the processing queue status and control when generation happens",{"type":28,"tag":62,"props":1874,"children":1876},{"id":1875},"memory-management",[1877],{"type":33,"value":1878},"Memory Management",{"type":28,"tag":69,"props":1880,"children":1881},{},[1882,1892,1897],{"type":28,"tag":73,"props":1883,"children":1884},{},[1885,1890],{"type":28,"tag":77,"props":1886,"children":1887},{},[1888],{"type":33,"value":1889},"Empty the model cache",{"type":33,"value":1891},": InvokeAI keeps models cached between generation, but you can empty the model cache by simply telling it to do so",{"type":28,"tag":73,"props":1893,"children":1894},{},[1895],{"type":33,"value":1896},"This helps free up VRAM when you're not actively using InvokeAI",{"type":28,"tag":73,"props":1898,"children":1899},{},[1900],{"type":33,"value":1901},"Useful for switching between different AI workloads or when playing games",{"type":28,"tag":62,"props":1903,"children":1905},{"id":1904},"setup-requirements",[1906],{"type":33,"value":1907},"Setup Requirements",{"type":28,"tag":29,"props":1909,"children":1910},{},[1911],{"type":33,"value":1912},"To use the image-to-image features, you'll need:",{"type":28,"tag":69,"props":1914,"children":1915},{},[1916,1928,1933],{"type":28,"tag":73,"props":1917,"children":1918},{},[1919,1921,1926],{"type":33,"value":1920},"InvokeAI installed and running locally (typically on ",{"type":28,"tag":390,"props":1922,"children":1924},{"className":1923},[],[1925],{"type":33,"value":860},{"type":33,"value":1927},")",{"type":28,"tag":73,"props":1929,"children":1930},{},[1931],{"type":33,"value":1932},"Flux Kontext model loaded in InvokeAI",{"type":28,"tag":73,"props":1934,"children":1935},{},[1936,1938,1943],{"type":33,"value":1937},"Proper configuration in your ",{"type":28,"tag":390,"props":1939,"children":1941},{"className":1940},[],[1942],{"type":33,"value":545},{"type":33,"value":1944}," file",{"type":28,"tag":55,"props":1946,"children":1948},{"id":1947},"supported-commands",[1949],{"type":33,"value":1950},"Supported Commands",{"type":28,"tag":29,"props":1952,"children":1953},{},[1954],{"type":33,"value":1955},"The Flux Plug-in for G-Assist supports the following commands:",{"type":28,"tag":1290,"props":1957,"children":1958},{},[1959,1980],{"type":28,"tag":1294,"props":1960,"children":1961},{},[1962],{"type":28,"tag":1298,"props":1963,"children":1964},{},[1965,1970,1975],{"type":28,"tag":1302,"props":1966,"children":1967},{},[1968],{"type":33,"value":1969},"Function",{"type":28,"tag":1302,"props":1971,"children":1972},{},[1973],{"type":33,"value":1974},"Description",{"type":28,"tag":1302,"props":1976,"children":1977},{},[1978],{"type":33,"value":1979},"Example",{"type":28,"tag":1318,"props":1981,"children":1982},{},[1983,2005,2027,2049,2071,2093,2115,2137,2159,2181],{"type":28,"tag":1298,"props":1984,"children":1985},{},[1986,1995,2000],{"type":28,"tag":1325,"props":1987,"children":1988},{},[1989],{"type":28,"tag":390,"props":1990,"children":1992},{"className":1991},[],[1993],{"type":33,"value":1994},"flux_nim_ready_check",{"type":28,"tag":1325,"props":1996,"children":1997},{},[1998],{"type":33,"value":1999},"Tests health endpoints of the Flux NIM server",{"type":28,"tag":1325,"props":2001,"children":2002},{},[2003],{"type":33,"value":2004},"\"hey flux, check if the flux nim server is ready\"",{"type":28,"tag":1298,"props":2006,"children":2007},{},[2008,2017,2022],{"type":28,"tag":1325,"props":2009,"children":2010},{},[2011],{"type":28,"tag":390,"props":2012,"children":2014},{"className":2013},[],[2015],{"type":33,"value":2016},"check_nim_status",{"type":28,"tag":1325,"props":2018,"children":2019},{},[2020],{"type":33,"value":2021},"Checks if the Flux NIM server is running",{"type":28,"tag":1325,"props":2023,"children":2024},{},[2025],{"type":33,"value":2026},"\"hey flux, check if the nim server is running\"",{"type":28,"tag":1298,"props":2028,"children":2029},{},[2030,2039,2044],{"type":28,"tag":1325,"props":2031,"children":2032},{},[2033],{"type":28,"tag":390,"props":2034,"children":2036},{"className":2035},[],[2037],{"type":33,"value":2038},"stop_nim",{"type":28,"tag":1325,"props":2040,"children":2041},{},[2042],{"type":33,"value":2043},"Stops the Flux NIM server",{"type":28,"tag":1325,"props":2045,"children":2046},{},[2047],{"type":33,"value":2048},"\"hey flux, stop the flux nim server\"",{"type":28,"tag":1298,"props":2050,"children":2051},{},[2052,2061,2066],{"type":28,"tag":1325,"props":2053,"children":2054},{},[2055],{"type":28,"tag":390,"props":2056,"children":2058},{"className":2057},[],[2059],{"type":33,"value":2060},"start_nim",{"type":28,"tag":1325,"props":2062,"children":2063},{},[2064],{"type":33,"value":2065},"Starts the Flux NIM server",{"type":28,"tag":1325,"props":2067,"children":2068},{},[2069],{"type":33,"value":2070},"\"hey flux, start the flux nim server\"",{"type":28,"tag":1298,"props":2072,"children":2073},{},[2074,2083,2088],{"type":28,"tag":1325,"props":2075,"children":2076},{},[2077],{"type":28,"tag":390,"props":2078,"children":2080},{"className":2079},[],[2081],{"type":33,"value":2082},"generate_image",{"type":28,"tag":1325,"props":2084,"children":2085},{},[2086],{"type":33,"value":2087},"Generates an image from text prompt using Flux",{"type":28,"tag":1325,"props":2089,"children":2090},{},[2091],{"type":33,"value":2092},"\"hey flux, generate an image of a cyberpunk city\"",{"type":28,"tag":1298,"props":2094,"children":2095},{},[2096,2105,2110],{"type":28,"tag":1325,"props":2097,"children":2098},{},[2099],{"type":28,"tag":390,"props":2100,"children":2102},{"className":2101},[],[2103],{"type":33,"value":2104},"generate_image_using_kontext",{"type":28,"tag":1325,"props":2106,"children":2107},{},[2108],{"type":33,"value":2109},"Performs image-to-image generation using Flux Kontext",{"type":28,"tag":1325,"props":2111,"children":2112},{},[2113],{"type":33,"value":2114},"\"hey flux, use kontext to make it a cartoon style\"",{"type":28,"tag":1298,"props":2116,"children":2117},{},[2118,2127,2132],{"type":28,"tag":1325,"props":2119,"children":2120},{},[2121],{"type":28,"tag":390,"props":2122,"children":2124},{"className":2123},[],[2125],{"type":33,"value":2126},"invokeai_status",{"type":28,"tag":1325,"props":2128,"children":2129},{},[2130],{"type":33,"value":2131},"Checks the status of the InvokeAI service",{"type":28,"tag":1325,"props":2133,"children":2134},{},[2135],{"type":33,"value":2136},"\"hey flux, check invokeai status\"",{"type":28,"tag":1298,"props":2138,"children":2139},{},[2140,2149,2154],{"type":28,"tag":1325,"props":2141,"children":2142},{},[2143],{"type":28,"tag":390,"props":2144,"children":2146},{"className":2145},[],[2147],{"type":33,"value":2148},"pause_invokeai_processor",{"type":28,"tag":1325,"props":2150,"children":2151},{},[2152],{"type":33,"value":2153},"Pauses the InvokeAI processing queue",{"type":28,"tag":1325,"props":2155,"children":2156},{},[2157],{"type":33,"value":2158},"\"hey flux, pause the invokeai processor\"",{"type":28,"tag":1298,"props":2160,"children":2161},{},[2162,2171,2176],{"type":28,"tag":1325,"props":2163,"children":2164},{},[2165],{"type":28,"tag":390,"props":2166,"children":2168},{"className":2167},[],[2169],{"type":33,"value":2170},"resume_invokeai_processor",{"type":28,"tag":1325,"props":2172,"children":2173},{},[2174],{"type":33,"value":2175},"Resumes the InvokeAI processing queue",{"type":28,"tag":1325,"props":2177,"children":2178},{},[2179],{"type":33,"value":2180},"\"hey flux, resume the invokeai processor\"",{"type":28,"tag":1298,"props":2182,"children":2183},{},[2184,2193,2198],{"type":28,"tag":1325,"props":2185,"children":2186},{},[2187],{"type":28,"tag":390,"props":2188,"children":2190},{"className":2189},[],[2191],{"type":33,"value":2192},"invokeai_empty_model_cache",{"type":28,"tag":1325,"props":2194,"children":2195},{},[2196],{"type":33,"value":2197},"Empties the InvokeAI model cache to free VRAM",{"type":28,"tag":1325,"props":2199,"children":2200},{},[2201],{"type":33,"value":2202},"\"hey flux, empty the invokeai model cache\"",{"type":28,"tag":55,"props":2204,"children":2206},{"id":2205},"logging",[2207],{"type":33,"value":2208},"Logging",{"type":28,"tag":29,"props":2210,"children":2211},{},[2212,2214,2220],{"type":33,"value":2213},"Your plugin automatically logs to ",{"type":28,"tag":390,"props":2215,"children":2217},{"className":2216},[],[2218],{"type":33,"value":2219},"flux_plugin.log",{"type":33,"value":2221}," in your user's profile directory. It tracks:",{"type":28,"tag":69,"props":2223,"children":2224},{},[2225,2230,2235,2240],{"type":28,"tag":73,"props":2226,"children":2227},{},[2228],{"type":33,"value":2229},"Plugin startup and shutdown",{"type":28,"tag":73,"props":2231,"children":2232},{},[2233],{"type":33,"value":2234},"Command reception and processing",{"type":28,"tag":73,"props":2236,"children":2237},{},[2238],{"type":33,"value":2239},"Error conditions",{"type":28,"tag":73,"props":2241,"children":2242},{},[2243],{"type":33,"value":2244},"Function execution details",{"type":28,"tag":55,"props":2246,"children":2248},{"id":2247},"troubleshooting-tips",[2249],{"type":33,"value":2250},"Troubleshooting Tips",{"type":28,"tag":69,"props":2252,"children":2253},{},[2254,2264,2274,2284],{"type":28,"tag":73,"props":2255,"children":2256},{},[2257,2262],{"type":28,"tag":77,"props":2258,"children":2259},{},[2260],{"type":33,"value":2261},"Plugin not starting?",{"type":33,"value":2263}," Check if Python 3.12+ is installed and in PATH",{"type":28,"tag":73,"props":2265,"children":2266},{},[2267,2272],{"type":28,"tag":77,"props":2268,"children":2269},{},[2270],{"type":33,"value":2271},"Communication errors?",{"type":33,"value":2273}," Verify pywin32 is installed correctly",{"type":28,"tag":73,"props":2275,"children":2276},{},[2277,2282],{"type":28,"tag":77,"props":2278,"children":2279},{},[2280],{"type":33,"value":2281},"Commands not working?",{"type":33,"value":2283}," Double-check your command registration",{"type":28,"tag":73,"props":2285,"children":2286},{},[2287,2292],{"type":28,"tag":77,"props":2288,"children":2289},{},[2290],{"type":33,"value":2291},"Missing logs?",{"type":33,"value":2293}," Ensure write permissions in user profile directory",{"type":28,"tag":55,"props":2295,"children":2297},{"id":2296},"want-to-contribute",[2298],{"type":33,"value":2299},"Want to Contribute?",{"type":28,"tag":29,"props":2301,"children":2302},{},[2303,2305,2311],{"type":33,"value":2304},"We'd love your help making this template even better! Check out ",{"type":28,"tag":390,"props":2306,"children":2308},{"className":2307},[],[2309],{"type":33,"value":2310},"CONTRIBUTING.md",{"type":33,"value":2312}," for guidelines on how to contribute.",{"type":28,"tag":55,"props":2314,"children":2316},{"id":2315},"license",[2317],{"type":33,"value":2318},"License",{"type":28,"tag":29,"props":2320,"children":2321},{},[2322,2324,2330],{"type":33,"value":2323},"This project is licensed under the Apache License 2.0 - see the ",{"type":28,"tag":390,"props":2325,"children":2327},{"className":2326},[],[2328],{"type":33,"value":2329},"LICENSE",{"type":33,"value":2331}," file for details.",{"type":28,"tag":55,"props":2333,"children":2335},{"id":2334},"hardware",[2336],{"type":33,"value":2337},"Hardware",{"type":28,"tag":29,"props":2339,"children":2340},{},[2341],{"type":33,"value":2342},"The Flux Plug-in for G-Assist was developed and tested on a PC with a GeForce RTX 4090 GPU.",{"type":28,"tag":2344,"props":2345,"children":2346},"style",{},[2347],{"type":33,"value":2348},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html.sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}",{"title":8,"searchDepth":439,"depth":439,"links":2350},[2351,2358,2359,2370,2373,2379,2385,2386,2387,2388,2389,2390],{"id":57,"depth":439,"text":60,"children":2352},[2353,2354,2355,2356,2357],{"id":64,"depth":448,"text":67},{"id":116,"depth":448,"text":119},{"id":165,"depth":448,"text":168},{"id":224,"depth":448,"text":227},{"id":273,"depth":448,"text":276},{"id":307,"depth":439,"text":310},{"id":358,"depth":439,"text":361,"children":2360},[2361,2362,2363,2364,2365,2366,2367,2368,2369],{"id":364,"depth":448,"text":367},{"id":380,"depth":448,"text":383},{"id":480,"depth":448,"text":483},{"id":523,"depth":448,"text":526},{"id":754,"depth":448,"text":757},{"id":831,"depth":448,"text":834},{"id":864,"depth":448,"text":867},{"id":1163,"depth":448,"text":1166},{"id":1194,"depth":448,"text":1197},{"id":1253,"depth":439,"text":1256,"children":2371},[2372],{"id":1285,"depth":448,"text":1288},{"id":1606,"depth":439,"text":1609,"children":2374},[2375,2376,2377,2378],{"id":1637,"depth":448,"text":1640},{"id":1665,"depth":448,"text":1668},{"id":1689,"depth":448,"text":1692},{"id":1741,"depth":448,"text":1256},{"id":1791,"depth":439,"text":1794,"children":2380},[2381,2382,2383,2384],{"id":1822,"depth":448,"text":1825},{"id":1851,"depth":448,"text":1854},{"id":1875,"depth":448,"text":1878},{"id":1904,"depth":448,"text":1907},{"id":1947,"depth":439,"text":1950},{"id":2205,"depth":439,"text":2208},{"id":2247,"depth":439,"text":2250},{"id":2296,"depth":439,"text":2299},{"id":2315,"depth":439,"text":2318},{"id":2334,"depth":439,"text":2337},"markdown","content:2025:07:17:flux-plugin-for-project-g-assist-hackathon.md","content","2025/07/17/flux-plugin-for-project-g-assist-hackathon.md","2025/07/17/flux-plugin-for-project-g-assist-hackathon","md",{"_path":2398,"_dir":2399,"_draft":7,"_partial":7,"_locale":8,"title":2400,"description":2401,"date":2402,"image":2403,"tags":2404,"draft":7,"external":2416,"comments":23,"body":2419,"_type":2391,"_id":2824,"_source":2393,"_file":2825,"_stem":2826,"_extension":2396},"/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update","24","Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM","This article is a brief discusion on recent updates to my project for the Generative AI Agents Developer Contest by NVIDIA and LangChain","2024-06-24","/static/aoi/aoi_title.png",[14,2405,2406,16,2407,2408,2409,15,2410,2411,2412,2413,2414,2415],"langchain","agents","gpu","tensorrt","tensorrt-llm","llm","llama","007","stable-diffusion","stable-video-diffusion","comfyui",[2417],{"link":2418,"site":22},"https://x.com/briancaffey/status/1802754703207583886",{"type":25,"children":2420,"toc":2817},[2421,2427,2439,2444,2448,2461,2467,2472,2485,2493,2506,2514,2522,2527,2535,2540,2548,2553,2561,2620,2625,2633,2638,2646,2660,2666,2671,2679,2692,2700,2705,2713,2718,2724,2738,2743,2751,2756,2764,2769,2777,2782,2787,2793,2798,2807,2812],{"type":28,"tag":55,"props":2422,"children":2424},{"id":2423},"tldr",[2425],{"type":33,"value":2426},"tl;dr",{"type":28,"tag":29,"props":2428,"children":2429},{},[2430,2432,2438],{"type":33,"value":2431},"\"Agents of Inference: Speed of Light\" is an update to my original entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This update focuses on how I accelerated local text, image and video generation using TensorRT, TensorRT-LLM and NVIDIA NIMs. You can read the original article about \"Agents of Inference\" ",{"type":28,"tag":764,"props":2433,"children":2436},{"href":2434,"rel":2435},"https://briancaffey.github.io/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",[768],[2437],{"type":33,"value":771},{"type":33,"value":520},{"type":28,"tag":29,"props":2440,"children":2441},{},[2442],{"type":33,"value":2443},"Here's my original project submission post on 𝕏 that introduces the idea of generating short 007-style films using agents, LLMs and stable diffusion:",{"type":28,"tag":2445,"props":2446,"children":2447},"agents-of-inference-tweet",{},[],{"type":28,"tag":29,"props":2449,"children":2450},{},[2451,2453,2460],{"type":33,"value":2452},"Here's a link to the ",{"type":28,"tag":764,"props":2454,"children":2457},{"href":2455,"rel":2456},"https://github.com/briancaffey/agents-of-inference",[768],[2458],{"type":33,"value":2459},"Agents of Inference code repository on GitHub",{"type":33,"value":520},{"type":28,"tag":55,"props":2462,"children":2464},{"id":2463},"nvidia-nim-inference-microservices",[2465],{"type":33,"value":2466},"NVIDIA NIM inference microservices",{"type":28,"tag":29,"props":2468,"children":2469},{},[2470],{"type":33,"value":2471},"I thought NVIDIA NIMs was one of the most exciting announcements from GTC 2024. I'm a big fan of using docker containers everywhere, and the idea of standardizing NVIDIA tools and dependencies seemed to make a lot of sense. I had previously struggled to get TensorRT-LLM installed on Windows using example repos provided by NVIDIA.",{"type":28,"tag":29,"props":2473,"children":2474},{},[2475,2477,2483],{"type":33,"value":2476},"A few weeks ago NVIDIA announced that NVIDIA NIMs can be downloaded and run anywhere. I was able to download this NIM for the ",{"type":28,"tag":390,"props":2478,"children":2480},{"className":2479},[],[2481],{"type":33,"value":2482},"meta/llama3-8b-instruct",{"type":33,"value":2484}," model:",{"type":28,"tag":29,"props":2486,"children":2487},{},[2488],{"type":28,"tag":1215,"props":2489,"children":2492},{"alt":2490,"src":2491},"llama3 nim","/static/aoi/meta-llama3-nim.png",[],{"type":28,"tag":29,"props":2494,"children":2495},{},[2496,2498,2504],{"type":33,"value":2497},"Here are the logs for my NVIDIA NIM ",{"type":28,"tag":390,"props":2499,"children":2501},{"className":2500},[],[2502],{"type":33,"value":2503},"Meta/Llama-3-8B-Instruct",{"type":33,"value":2505}," running in docker container on Windows Subsystem for Linux on my NVIDIA GeForce RTX 4090 GPU-powered PC. Notice that it generates over 50 tokens per second!",{"type":28,"tag":29,"props":2507,"children":2508},{},[2509],{"type":28,"tag":1215,"props":2510,"children":2513},{"alt":2511,"src":2512},"trt llama3 local","/static/aoi/trt-llama3.png",[],{"type":28,"tag":29,"props":2515,"children":2516},{},[2517],{"type":28,"tag":1215,"props":2518,"children":2521},{"alt":2519,"src":2520},"token factory","/static/aoi/token-factory.png",[],{"type":28,"tag":29,"props":2523,"children":2524},{},[2525],{"type":33,"value":2526},"The one main hurdle I faced when running the NIM local was an error about no runnable profiles being available:",{"type":28,"tag":399,"props":2528,"children":2530},{"code":2529},"ERROR 06-23 15:41:21.19 utils.py:21] Could not find a profile that is currently runnable with the detected hardware. Please check the system information below and make sure you have enough free GPUs.\nSYSTEM INFO\n- Free GPUs: \u003CNone>\n- Non-free GPUs:\n  -  [2684:10de] (0) NVIDIA GeForce RTX 4090 [current utilization: 7%]\n",[2531],{"type":28,"tag":390,"props":2532,"children":2533},{"__ignoreMap":8},[2534],{"type":33,"value":2529},{"type":28,"tag":29,"props":2536,"children":2537},{},[2538],{"type":33,"value":2539},"This seemed odd, and I found another user with the same issue on the NVIDIA Developer Forum. I was able to get around this by going into the EUFI/BIOS of my PC and switch to integrated graphics:",{"type":28,"tag":29,"props":2541,"children":2542},{},[2543],{"type":28,"tag":1215,"props":2544,"children":2547},{"alt":2545,"src":2546},"bios","/static/aoi/bios.jpg",[],{"type":28,"tag":29,"props":2549,"children":2550},{},[2551],{"type":33,"value":2552},"It was great to be able to run \"Agents of Inference\" using NVIDIA NIM because it is just as simple as running a docker container:",{"type":28,"tag":399,"props":2554,"children":2556},{"code":2555},"export CONTAINER_NAME=llama3-8b-instruct\nexport IMG_NAME=\"nvcr.io/nim/meta/${CONTAINER_NAME}:1.0.0\"\nexport LOCAL_NIM_CACHE=~/.cache/nim\nmkdir -p \"$LOCAL_NIM_CACHE\"\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus all \\\n  --shm-size=16GB \\\n  -e NGC_API_KEY \\\n  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n  -u $(id -u) \\\n  -p 8000:8000 \\\n  $IMG_NAME\n",[2557],{"type":28,"tag":390,"props":2558,"children":2559},{"__ignoreMap":8},[2560],{"type":33,"value":2555},{"type":28,"tag":29,"props":2562,"children":2563},{},[2564,2566,2572,2574,2581,2583,2589,2591,2602,2604,2610,2612,2618],{"type":33,"value":2565},"Before getting this to work, I was able to get a ",{"type":28,"tag":390,"props":2567,"children":2569},{"className":2568},[],[2570],{"type":33,"value":2571},"/chat/completions",{"type":33,"value":2573}," endpoint working with the Llama3 model on my fork of the ",{"type":28,"tag":764,"props":2575,"children":2578},{"href":2576,"rel":2577},"https://github.com/briancaffey/trt-llm-as-openai-windows/commit/edaa15fd026fe95e645e3d4ae9718dc3ecc3bb65",[768],[2579],{"type":33,"value":2580},"trt-llm-as-openai-windows",{"type":33,"value":2582},". I borrowed code for the ",{"type":28,"tag":390,"props":2584,"children":2586},{"className":2585},[],[2587],{"type":33,"value":2588},"TrtLlmAPI",{"type":33,"value":2590}," from the ",{"type":28,"tag":764,"props":2592,"children":2595},{"href":2593,"rel":2594},"https://github.com/NVIDIA/ChatRTX",[768],[2596],{"type":28,"tag":390,"props":2597,"children":2599},{"className":2598},[],[2600],{"type":33,"value":2601},"NVIDIA/ChatRTX",{"type":33,"value":2603}," repo and a function from ",{"type":28,"tag":390,"props":2605,"children":2607},{"className":2606},[],[2608],{"type":33,"value":2609},"llama-index",{"type":33,"value":2611}," called ",{"type":28,"tag":390,"props":2613,"children":2615},{"className":2614},[],[2616],{"type":33,"value":2617},"messages_to_prompt_v3_instruct",{"type":33,"value":2619}," which encodes messages with special tokens for chat. This was an interesting exercise and it taught me a lot about how LLMs do chat. I would like to continue working on this fork and see how to implement streaming endpoints for the Llama 3 model.",{"type":28,"tag":29,"props":2621,"children":2622},{},[2623],{"type":33,"value":2624},"Here is how Llama 3 does the instruct prompting:",{"type":28,"tag":399,"props":2626,"children":2628},{"code":2627},"\u003C|begin_of_text|>\u003C|start_header_id|>system\u003C|end_header_id|>\n\nYou are a helpful AI assistant for travel tips and recommendations\u003C|eot_id|>\u003C|start_header_id|>user\u003C|end_header_id|>\n\nWhat can you help me with?\u003C|eot_id|>\u003C|start_header_id|>assistant\u003C|end_header_id|>\n",[2629],{"type":28,"tag":390,"props":2630,"children":2631},{"__ignoreMap":8},[2632],{"type":33,"value":2627},{"type":28,"tag":29,"props":2634,"children":2635},{},[2636],{"type":33,"value":2637},"Compare this with how it was done with Llama2 chat:",{"type":28,"tag":399,"props":2639,"children":2641},{"code":2640},"\u003Cs>[INST] \u003C\u003CSYS>>\n{{ system_prompt }}\n\u003C\u003C/SYS>>\n\n{{ user_message_1 }} [/INST] {{ model_answer_1 }} \u003C/s>\n\u003Cs>[INST] {{ user_message_2 }} [/INST]\n",[2642],{"type":28,"tag":390,"props":2643,"children":2644},{"__ignoreMap":8},[2645],{"type":33,"value":2640},{"type":28,"tag":29,"props":2647,"children":2648},{},[2649,2651,2658],{"type":33,"value":2650},"You can read more about the difference between Llama 2 and 3 on the ",{"type":28,"tag":764,"props":2652,"children":2655},{"href":2653,"rel":2654},"https://llama.meta.com/docs/model-cards-and-prompt-formats",[768],[2656],{"type":33,"value":2657},"Model Card & Prompt formats",{"type":33,"value":2659}," page on Meta's Llama website.",{"type":28,"tag":55,"props":2661,"children":2663},{"id":2662},"langsmith",[2664],{"type":33,"value":2665},"LangSmith",{"type":28,"tag":29,"props":2667,"children":2668},{},[2669],{"type":33,"value":2670},"I recently started using LangSmith. It is an awesome product and it ties in really well to doing prototype work like in my project \"Agents of Inference\". I wish I had started using it earlier in my development cycle! All you need to do is add an API key to your environment and your application automatically starts tracing LLM calls. It also works well with LangGraph and allows you to trace the execution path of your graph. Also it is good to be aware that there are other products similar to LangSmith like LangFuse. I also saw a really neat demo from Datadog at GTC showing an alpha version of their LLM tracing and observability product.",{"type":28,"tag":29,"props":2672,"children":2673},{},[2674],{"type":28,"tag":1215,"props":2675,"children":2678},{"alt":2676,"src":2677},"langsmith screenshot","/static/aoi/langsmith.png",[],{"type":28,"tag":29,"props":2680,"children":2681},{},[2682,2684,2690],{"type":33,"value":2683},"LangSmith can also be helpful when the wrong JSON shape is parsed. I had a lot of difficulty with this in my project. When I used the Q4_K_M gguf quantized ",{"type":28,"tag":390,"props":2685,"children":2687},{"className":2686},[],[2688],{"type":33,"value":2689},"Meta-Llama-3 8B-Instruct",{"type":33,"value":2691}," model I had no issues with output parsing. Switching to the TensorRT-LLM model provided by the NIM resulted in some parsing errors. The application would report that JSON could not be parsed because the result contained text like: \"Here is the JSON that you requested\". I was able to get around this by changing the prompt template from:",{"type":28,"tag":399,"props":2693,"children":2695},{"code":2694},"Answer the user query.\n",[2696],{"type":28,"tag":390,"props":2697,"children":2698},{"__ignoreMap":8},[2699],{"type":33,"value":2694},{"type":28,"tag":29,"props":2701,"children":2702},{},[2703],{"type":33,"value":2704},"to",{"type":28,"tag":399,"props":2706,"children":2708},{"code":2707},"Don't include ANYTHING except for valid JSON in your response. Answer the user query.\n",[2709],{"type":28,"tag":390,"props":2710,"children":2711},{"__ignoreMap":8},[2712],{"type":33,"value":2707},{"type":28,"tag":29,"props":2714,"children":2715},{},[2716],{"type":33,"value":2717},"This was the most frustrating part of development, and I'm still getting occasional errors that I just skip over. I'm also probably have not exhausted all of the tools that LangChain provides to avoid these types of errors. Don't assume that output parsing that works with one model will work with another! This is another good reason to use something like LangSmith when developing LLM-based applications.",{"type":28,"tag":55,"props":2719,"children":2721},{"id":2720},"comfyui-tensorrt",[2722],{"type":33,"value":2723},"ComfyUI TensorRT",{"type":28,"tag":29,"props":2725,"children":2726},{},[2727,2729,2736],{"type":33,"value":2728},"My goal with \"Agents of Inference\" was to be able to test out how small upstream prompt changes can impact the quality and consistency of a series of generated images and videos. Iteration speed is very important! I was able to significantly speed up image and video generation by using the ",{"type":28,"tag":764,"props":2730,"children":2733},{"href":2731,"rel":2732},"https://github.com/comfyanonymous/ComfyUI_TensorRT",[768],[2734],{"type":33,"value":2735},"ComfyUI TensorRT custom nodes",{"type":33,"value":2737},". These nodes allow you to build engines with specifications for parameters that can be either static or dynamic. I had better luck with building dynamic engines. I was able to build and use engines for Stable Diffusion SDXL and Stable Video Diffusion XT.",{"type":28,"tag":29,"props":2739,"children":2740},{},[2741],{"type":33,"value":2742},"Building a TensorRT engine for ComfyUI can be done using the following workflow:",{"type":28,"tag":29,"props":2744,"children":2745},{},[2746],{"type":28,"tag":1215,"props":2747,"children":2750},{"alt":2748,"src":2749},"trt comfyUI build process","/static/aoi/comfyui-trt-svd-xt.png",[],{"type":28,"tag":29,"props":2752,"children":2753},{},[2754],{"type":33,"value":2755},"The engines can then be used in custom workflows like the following:",{"type":28,"tag":29,"props":2757,"children":2758},{},[2759],{"type":28,"tag":1215,"props":2760,"children":2763},{"alt":2761,"src":2762},"trt comfyui workflow","/static/aoi/svd-workflow-trt.png",[],{"type":28,"tag":29,"props":2765,"children":2766},{},[2767],{"type":33,"value":2768},"Once these workflows are configured and are working as expected, you can export them in API format (JSON) and use them to make API calls to the ComfyUI backend. The agents for stable diffusion and stable video diffusion made API calls in this way and it worked pretty well.",{"type":28,"tag":29,"props":2770,"children":2771},{},[2772],{"type":28,"tag":1215,"props":2773,"children":2776},{"alt":2774,"src":2775},"comfy its","/static/aoi/comfy-its.png",[],{"type":28,"tag":29,"props":2778,"children":2779},{},[2780],{"type":33,"value":2781},"Using 50 iterations, I was able to generate 1024x576 images in 3 seconds or about 19 iterations per second (it/s). Videos",{"type":28,"tag":29,"props":2783,"children":2784},{},[2785],{"type":33,"value":2786},"ComfyUI is still early in development and it refers to itself as \"alpha software\" even though it has a large adoption by a very active community already. I'm excited to see what is next from the developers of ComfyUI.",{"type":28,"tag":55,"props":2788,"children":2790},{"id":2789},"speed-of-light",[2791],{"type":33,"value":2792},"Speed of Light",{"type":28,"tag":29,"props":2794,"children":2795},{},[2796],{"type":33,"value":2797},"\"Speed of Light\" is a term that I learned from a stable diffusion talk at GTC.",{"type":28,"tag":2799,"props":2800,"children":2801},"blockquote",{},[2802],{"type":28,"tag":29,"props":2803,"children":2804},{},[2805],{"type":33,"value":2806},"SOL analysis reveals how your code performs, and device utilization compared to relevant maximums.",{"type":28,"tag":29,"props":2808,"children":2809},{},[2810],{"type":33,"value":2811},"Adding TensorRT and TensorRT-LLM to inference services on my RTX PC helped increase the throughput of text, image and video generation for my \"Agents of Inference\" project. I'm looking forward to learning more about profiling and optimization techniques for both LLMs and Stable Diffusion workloads.",{"type":28,"tag":29,"props":2813,"children":2814},{},[2815],{"type":33,"value":2816},"Thanks again to NVIDIA and LangChain for organizing this contest! It was a lot of fun to learn about builing agents with LangChain and LangGraph and the latest developments from NVIDIA in Generative AI.",{"title":8,"searchDepth":439,"depth":439,"links":2818},[2819,2820,2821,2822,2823],{"id":2423,"depth":439,"text":2426},{"id":2463,"depth":439,"text":2466},{"id":2662,"depth":439,"text":2665},{"id":2720,"depth":439,"text":2723},{"id":2789,"depth":439,"text":2792},"content:2024:06:24:agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update.md","2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update.md","2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",{"_path":2828,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":2829,"description":2830,"date":2831,"image":2403,"tags":2832,"draft":7,"external":2833,"comments":23,"body":2835,"_type":2391,"_id":4313,"_source":2393,"_file":4314,"_stem":4315,"_extension":2396},"/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest","Agents of Inference: My submission for NVIDIA's Generative AI Agents Developer Contest by NVIDIA and LangChain","This article discusses my entry for NVIDIA's Generative AI Agents Developer Contest entry: Agents of Inference","2024-06-17",[14,2405,2406,16,2407,2408,2409,15,2410,2411,2412,2413,2414,2415],[2834],{"link":2418,"site":22},{"type":25,"children":2836,"toc":4292},[2837,2843,2854,2858,2863,2868,2871,2881,2887,2892,2898,2903,2909,2914,2957,2965,2970,2976,2981,2994,3007,3019,3384,3389,3432,3437,3600,3626,3632,3637,3743,3771,3778,3783,3789,3794,3873,3879,3884,3986,3994,3999,4004,4054,4075,4081,4102,4147,4152,4157,4162,4175,4183,4189,4193,4198,4203,4209,4214,4220,4225,4231,4236,4244,4249,4255,4267,4273,4278,4283,4288],{"type":28,"tag":55,"props":2838,"children":2840},{"id":2839},"update",[2841],{"type":33,"value":2842},"Update",{"type":28,"tag":29,"props":2844,"children":2845},{},[2846,2848],{"type":33,"value":2847},"I recently posted another article about optimizing this project with TensorRT and TensorRT-LLM running on local NVIDIA NIM inference microservices, please have a look here: ",{"type":28,"tag":764,"props":2849,"children":2852},{"href":2850,"rel":2851},"https://briancaffey.github.io/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",[768],[2853],{"type":33,"value":2850},{"type":28,"tag":55,"props":2855,"children":2856},{"id":2423},[2857],{"type":33,"value":2426},{"type":28,"tag":29,"props":2859,"children":2860},{},[2861],{"type":33,"value":2862},"“Agents of Inference” is my entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This project aims to integrate techniques for generating text, images and video to create an application capable of producing short thematic films. In this article, I will detail how I developed the project leveraging LangGraph—a library for building stateful, multi-actor applications with LLMs--and hybrid AI workflows using NVIDIA AI-powered tools and technologies running on RTX PCs and in the cloud.",{"type":28,"tag":29,"props":2864,"children":2865},{},[2866],{"type":33,"value":2867},"Here's my project submission post on 𝕏:",{"type":28,"tag":2445,"props":2869,"children":2870},{},[],{"type":28,"tag":29,"props":2872,"children":2873},{},[2874,2875,2880],{"type":33,"value":2452},{"type":28,"tag":764,"props":2876,"children":2878},{"href":2455,"rel":2877},[768],[2879],{"type":33,"value":2459},{"type":33,"value":520},{"type":28,"tag":55,"props":2882,"children":2884},{"id":2883},"nvidias-generative-ai-agents-developer-contest",[2885],{"type":33,"value":2886},"NVIDIA's Generative AI Agents Developer Contest",{"type":28,"tag":29,"props":2888,"children":2889},{},[2890],{"type":33,"value":2891},"AI agents are having a moment. They are the building blocks for building \"applications that reason\", and LangChain is a company that provides a comprehensive set of tools for developing, deploying and monitoring AI agents. I have struggled to understand how I can build or use agents in my own projects, and with the contest I have been able to just scratch the surface of what is possible with AI agents--but I think it is a promising paradigm for developing AI-driven applications.",{"type":28,"tag":55,"props":2893,"children":2895},{"id":2894},"coming-up-with-an-idea",[2896],{"type":33,"value":2897},"Coming up with an idea",{"type":28,"tag":29,"props":2899,"children":2900},{},[2901],{"type":33,"value":2902},"I love stable diffusion. I closely follow the development of the three leading applications for generating images with stable dissuion models: Stable Diffusion WebUI, InvokeAI and ComfyUI. Write a prompt, instantly see the result, tweak the prompt and generate again. This is the basic process by which I have previously used stable diffusion. It is a satisfying mental exercise that feeds the creative and imaginative part of my brain. My idea for this project came from wanting to automate this process: use large language models to build cohesive scenes and detailed prompts and then feed them into my stable diffusion programs via API. Using LangChain and LangGraph allowed me to rapidly prototype the idea and start generating short feature films in the style of my favorite British Secret Agent: 007.",{"type":28,"tag":55,"props":2904,"children":2906},{"id":2905},"putting-together-the-puzzle-pieces",[2907],{"type":33,"value":2908},"Putting together the puzzle pieces",{"type":28,"tag":29,"props":2910,"children":2911},{},[2912],{"type":33,"value":2913},"Here's how I set up an MVP for my project project to get started. I set up a simple graph (a linked list, really) that included the following nodes. *Important: in this context, a node is an agent, and that agent is a simple Python function. It takes one parameter which is the state, a Python dictionary, that holds the output of LLM calls that the agents make. Not all nodes make LLM calls, some just run basic functions like initializing directories or calling external stable diffusion APIs.",{"type":28,"tag":69,"props":2915,"children":2916},{},[2917,2922,2927,2932,2937,2942,2947,2952],{"type":28,"tag":73,"props":2918,"children":2919},{},[2920],{"type":33,"value":2921},"Casting Agent → come up with some characters",{"type":28,"tag":73,"props":2923,"children":2924},{},[2925],{"type":33,"value":2926},"Location Agent → come up with some locations",{"type":28,"tag":73,"props":2928,"children":2929},{},[2930],{"type":33,"value":2931},"Synopsis Agent → write a synopsis based on the characters and locations",{"type":28,"tag":73,"props":2933,"children":2934},{},[2935],{"type":33,"value":2936},"Scene Agent → write some number of scenes based on the synopsis based on the synopsis",{"type":28,"tag":73,"props":2938,"children":2939},{},[2940],{"type":33,"value":2941},"Shot agent → describe some number of camera shots for each scene based on the scene",{"type":28,"tag":73,"props":2943,"children":2944},{},[2945],{"type":33,"value":2946},"Photography agent → take each shot description and generate and image",{"type":28,"tag":73,"props":2948,"children":2949},{},[2950],{"type":33,"value":2951},"Videography agent → take each image generated by the photography agent and convert it to a 4 second clip using stable video diffusion",{"type":28,"tag":73,"props":2953,"children":2954},{},[2955],{"type":33,"value":2956},"Editor agent → compile the movie clips together",{"type":28,"tag":29,"props":2958,"children":2959},{},[2960],{"type":28,"tag":1215,"props":2961,"children":2964},{"alt":2962,"src":2963},"simple graph of agents of inference","/static/aoi/graph.png",[],{"type":28,"tag":29,"props":2966,"children":2967},{},[2968],{"type":33,"value":2969},"It may look simple, but there is a lot going on in this graph.",{"type":28,"tag":62,"props":2971,"children":2973},{"id":2972},"casting-and-location",[2974],{"type":33,"value":2975},"Casting and Location",{"type":28,"tag":29,"props":2977,"children":2978},{},[2979],{"type":33,"value":2980},"The first two agents in my graph are tasked with generating characters and locations that would appear in a British secret agent film. The prompts used for these agents are as follows:",{"type":28,"tag":2799,"props":2982,"children":2983},{},[2984],{"type":28,"tag":29,"props":2985,"children":2986},{},[2987,2992],{"type":28,"tag":77,"props":2988,"children":2989},{},[2990],{"type":33,"value":2991},"casting",{"type":33,"value":2993},": \"Come up with four to five characters who will appear in an upcoming British spy movie. The list should include the main character who is male, the villain, an attractive female actress who eventually falls in love with the main character, and some other characters as well.\"",{"type":28,"tag":2799,"props":2995,"children":2996},{},[2997],{"type":28,"tag":29,"props":2998,"children":2999},{},[3000,3005],{"type":28,"tag":77,"props":3001,"children":3002},{},[3003],{"type":33,"value":3004},"locations",{"type":33,"value":3006},": \"Provide three main locations that can be used in an international British Spy movie. The locations should include a variety of cities, remote environments, iconic landmarks, etc. The locations should make for good background scenes for an action movie with lots of stunts, chases, explosions, fights, etc. and other things you would find in an action movie. Be sure to include the country and a description of the environment where these places are.\"",{"type":28,"tag":29,"props":3008,"children":3009},{},[3010,3012,3017],{"type":33,"value":3011},"These agents leverage the LangChain Expression Language (LCEL) to generate ",{"type":28,"tag":77,"props":3013,"children":3014},{},[3015],{"type":33,"value":3016},"structured output",{"type":33,"value":3018}," based on Pydantic models. For",{"type":28,"tag":399,"props":3020,"children":3022},{"code":3021,"language":419,"meta":8,"className":876,"style":8},"class Character(BaseModel):\n    \"\"\"\n    The type for character that the casting agent casts for a role in the movie\n    \"\"\"\n    full_name: str = Field(description=\"The character's name\")\n    short_name: str = Field(description=\"The character's short name\")\n    background: str = Field(description=\"The character's background\")\n    physical_traits: str = Field(description=\"The physical traits of the character\")\n    ethnicity: str = Field(description=\"The character's ethnicity\")\n    gender: str = Field(description=\"The character's gender, either male of female\")\n    nationality: str = Field(description=\"The character's nationality\")\n    main_character: bool = Field(description=\"If the character is or is not the main character\")\n\n",[3023],{"type":28,"tag":390,"props":3024,"children":3025},{"__ignoreMap":8},[3026,3056,3064,3072,3079,3122,3159,3196,3233,3270,3307,3345],{"type":28,"tag":409,"props":3027,"children":3028},{"class":411,"line":412},[3029,3034,3040,3045,3051],{"type":28,"tag":409,"props":3030,"children":3031},{"style":1013},[3032],{"type":33,"value":3033},"class",{"type":28,"tag":409,"props":3035,"children":3037},{"style":3036},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E;--shiki-default-text-decoration:inherit;--shiki-dark-text-decoration:inherit;--shiki-sepia-text-decoration:underline",[3038],{"type":33,"value":3039}," Character",{"type":28,"tag":409,"props":3041,"children":3042},{"style":569},[3043],{"type":33,"value":3044},"(",{"type":28,"tag":409,"props":3046,"children":3048},{"style":3047},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic;--shiki-default-text-decoration:inherit;--shiki-dark-text-decoration:inherit;--shiki-sepia-text-decoration:underline",[3049],{"type":33,"value":3050},"BaseModel",{"type":28,"tag":409,"props":3052,"children":3053},{"style":569},[3054],{"type":33,"value":3055},"):\n",{"type":28,"tag":409,"props":3057,"children":3058},{"class":411,"line":439},[3059],{"type":28,"tag":409,"props":3060,"children":3061},{"style":428},[3062],{"type":33,"value":3063},"    \"\"\"\n",{"type":28,"tag":409,"props":3065,"children":3066},{"class":411,"line":448},[3067],{"type":28,"tag":409,"props":3068,"children":3069},{"style":428},[3070],{"type":33,"value":3071},"    The type for character that the casting agent casts for a role in the movie\n",{"type":28,"tag":409,"props":3073,"children":3074},{"class":411,"line":631},[3075],{"type":28,"tag":409,"props":3076,"children":3077},{"style":428},[3078],{"type":33,"value":3063},{"type":28,"tag":409,"props":3080,"children":3081},{"class":411,"line":653},[3082,3087,3092,3097,3102,3108,3112,3117],{"type":28,"tag":409,"props":3083,"children":3084},{"style":569},[3085],{"type":33,"value":3086},"    full_name: ",{"type":28,"tag":409,"props":3088,"children":3089},{"style":578},[3090],{"type":33,"value":3091},"str",{"type":28,"tag":409,"props":3093,"children":3094},{"style":891},[3095],{"type":33,"value":3096}," =",{"type":28,"tag":409,"props":3098,"children":3099},{"style":569},[3100],{"type":33,"value":3101}," Field(",{"type":28,"tag":409,"props":3103,"children":3105},{"style":3104},"--shiki-default:#E36209;--shiki-dark:#FFAB70;--shiki-sepia:#FD971F;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[3106],{"type":33,"value":3107},"description",{"type":28,"tag":409,"props":3109,"children":3110},{"style":891},[3111],{"type":33,"value":894},{"type":28,"tag":409,"props":3113,"children":3114},{"style":428},[3115],{"type":33,"value":3116},"\"The character's name\"",{"type":28,"tag":409,"props":3118,"children":3119},{"style":569},[3120],{"type":33,"value":3121},")\n",{"type":28,"tag":409,"props":3123,"children":3124},{"class":411,"line":675},[3125,3130,3134,3138,3142,3146,3150,3155],{"type":28,"tag":409,"props":3126,"children":3127},{"style":569},[3128],{"type":33,"value":3129},"    short_name: ",{"type":28,"tag":409,"props":3131,"children":3132},{"style":578},[3133],{"type":33,"value":3091},{"type":28,"tag":409,"props":3135,"children":3136},{"style":891},[3137],{"type":33,"value":3096},{"type":28,"tag":409,"props":3139,"children":3140},{"style":569},[3141],{"type":33,"value":3101},{"type":28,"tag":409,"props":3143,"children":3144},{"style":3104},[3145],{"type":33,"value":3107},{"type":28,"tag":409,"props":3147,"children":3148},{"style":891},[3149],{"type":33,"value":894},{"type":28,"tag":409,"props":3151,"children":3152},{"style":428},[3153],{"type":33,"value":3154},"\"The character's short name\"",{"type":28,"tag":409,"props":3156,"children":3157},{"style":569},[3158],{"type":33,"value":3121},{"type":28,"tag":409,"props":3160,"children":3161},{"class":411,"line":697},[3162,3167,3171,3175,3179,3183,3187,3192],{"type":28,"tag":409,"props":3163,"children":3164},{"style":569},[3165],{"type":33,"value":3166},"    background: ",{"type":28,"tag":409,"props":3168,"children":3169},{"style":578},[3170],{"type":33,"value":3091},{"type":28,"tag":409,"props":3172,"children":3173},{"style":891},[3174],{"type":33,"value":3096},{"type":28,"tag":409,"props":3176,"children":3177},{"style":569},[3178],{"type":33,"value":3101},{"type":28,"tag":409,"props":3180,"children":3181},{"style":3104},[3182],{"type":33,"value":3107},{"type":28,"tag":409,"props":3184,"children":3185},{"style":891},[3186],{"type":33,"value":894},{"type":28,"tag":409,"props":3188,"children":3189},{"style":428},[3190],{"type":33,"value":3191},"\"The character's background\"",{"type":28,"tag":409,"props":3193,"children":3194},{"style":569},[3195],{"type":33,"value":3121},{"type":28,"tag":409,"props":3197,"children":3198},{"class":411,"line":719},[3199,3204,3208,3212,3216,3220,3224,3229],{"type":28,"tag":409,"props":3200,"children":3201},{"style":569},[3202],{"type":33,"value":3203},"    physical_traits: ",{"type":28,"tag":409,"props":3205,"children":3206},{"style":578},[3207],{"type":33,"value":3091},{"type":28,"tag":409,"props":3209,"children":3210},{"style":891},[3211],{"type":33,"value":3096},{"type":28,"tag":409,"props":3213,"children":3214},{"style":569},[3215],{"type":33,"value":3101},{"type":28,"tag":409,"props":3217,"children":3218},{"style":3104},[3219],{"type":33,"value":3107},{"type":28,"tag":409,"props":3221,"children":3222},{"style":891},[3223],{"type":33,"value":894},{"type":28,"tag":409,"props":3225,"children":3226},{"style":428},[3227],{"type":33,"value":3228},"\"The physical traits of the character\"",{"type":28,"tag":409,"props":3230,"children":3231},{"style":569},[3232],{"type":33,"value":3121},{"type":28,"tag":409,"props":3234,"children":3235},{"class":411,"line":745},[3236,3241,3245,3249,3253,3257,3261,3266],{"type":28,"tag":409,"props":3237,"children":3238},{"style":569},[3239],{"type":33,"value":3240},"    ethnicity: ",{"type":28,"tag":409,"props":3242,"children":3243},{"style":578},[3244],{"type":33,"value":3091},{"type":28,"tag":409,"props":3246,"children":3247},{"style":891},[3248],{"type":33,"value":3096},{"type":28,"tag":409,"props":3250,"children":3251},{"style":569},[3252],{"type":33,"value":3101},{"type":28,"tag":409,"props":3254,"children":3255},{"style":3104},[3256],{"type":33,"value":3107},{"type":28,"tag":409,"props":3258,"children":3259},{"style":891},[3260],{"type":33,"value":894},{"type":28,"tag":409,"props":3262,"children":3263},{"style":428},[3264],{"type":33,"value":3265},"\"The character's ethnicity\"",{"type":28,"tag":409,"props":3267,"children":3268},{"style":569},[3269],{"type":33,"value":3121},{"type":28,"tag":409,"props":3271,"children":3272},{"class":411,"line":1134},[3273,3278,3282,3286,3290,3294,3298,3303],{"type":28,"tag":409,"props":3274,"children":3275},{"style":569},[3276],{"type":33,"value":3277},"    gender: ",{"type":28,"tag":409,"props":3279,"children":3280},{"style":578},[3281],{"type":33,"value":3091},{"type":28,"tag":409,"props":3283,"children":3284},{"style":891},[3285],{"type":33,"value":3096},{"type":28,"tag":409,"props":3287,"children":3288},{"style":569},[3289],{"type":33,"value":3101},{"type":28,"tag":409,"props":3291,"children":3292},{"style":3104},[3293],{"type":33,"value":3107},{"type":28,"tag":409,"props":3295,"children":3296},{"style":891},[3297],{"type":33,"value":894},{"type":28,"tag":409,"props":3299,"children":3300},{"style":428},[3301],{"type":33,"value":3302},"\"The character's gender, either male of female\"",{"type":28,"tag":409,"props":3304,"children":3305},{"style":569},[3306],{"type":33,"value":3121},{"type":28,"tag":409,"props":3308,"children":3310},{"class":411,"line":3309},11,[3311,3316,3320,3324,3328,3332,3336,3341],{"type":28,"tag":409,"props":3312,"children":3313},{"style":569},[3314],{"type":33,"value":3315},"    nationality: ",{"type":28,"tag":409,"props":3317,"children":3318},{"style":578},[3319],{"type":33,"value":3091},{"type":28,"tag":409,"props":3321,"children":3322},{"style":891},[3323],{"type":33,"value":3096},{"type":28,"tag":409,"props":3325,"children":3326},{"style":569},[3327],{"type":33,"value":3101},{"type":28,"tag":409,"props":3329,"children":3330},{"style":3104},[3331],{"type":33,"value":3107},{"type":28,"tag":409,"props":3333,"children":3334},{"style":891},[3335],{"type":33,"value":894},{"type":28,"tag":409,"props":3337,"children":3338},{"style":428},[3339],{"type":33,"value":3340},"\"The character's nationality\"",{"type":28,"tag":409,"props":3342,"children":3343},{"style":569},[3344],{"type":33,"value":3121},{"type":28,"tag":409,"props":3346,"children":3348},{"class":411,"line":3347},12,[3349,3354,3359,3363,3367,3371,3375,3380],{"type":28,"tag":409,"props":3350,"children":3351},{"style":569},[3352],{"type":33,"value":3353},"    main_character: ",{"type":28,"tag":409,"props":3355,"children":3356},{"style":578},[3357],{"type":33,"value":3358},"bool",{"type":28,"tag":409,"props":3360,"children":3361},{"style":891},[3362],{"type":33,"value":3096},{"type":28,"tag":409,"props":3364,"children":3365},{"style":569},[3366],{"type":33,"value":3101},{"type":28,"tag":409,"props":3368,"children":3369},{"style":3104},[3370],{"type":33,"value":3107},{"type":28,"tag":409,"props":3372,"children":3373},{"style":891},[3374],{"type":33,"value":894},{"type":28,"tag":409,"props":3376,"children":3377},{"style":428},[3378],{"type":33,"value":3379},"\"If the character is or is not the main character\"",{"type":28,"tag":409,"props":3381,"children":3382},{"style":569},[3383],{"type":33,"value":3121},{"type":28,"tag":29,"props":3385,"children":3386},{},[3387],{"type":33,"value":3388},"LCEL offers wonderful syntactic sugar, I can use this model in a parse and pip that into the output from the mode:",{"type":28,"tag":399,"props":3390,"children":3392},{"code":3391,"language":419,"meta":8,"className":876,"style":8},"chain = prompt | model | parser\n",[3393],{"type":28,"tag":390,"props":3394,"children":3395},{"__ignoreMap":8},[3396],{"type":28,"tag":409,"props":3397,"children":3398},{"class":411,"line":412},[3399,3404,3408,3413,3418,3423,3427],{"type":28,"tag":409,"props":3400,"children":3401},{"style":569},[3402],{"type":33,"value":3403},"chain ",{"type":28,"tag":409,"props":3405,"children":3406},{"style":891},[3407],{"type":33,"value":894},{"type":28,"tag":409,"props":3409,"children":3410},{"style":569},[3411],{"type":33,"value":3412}," prompt ",{"type":28,"tag":409,"props":3414,"children":3415},{"style":891},[3416],{"type":33,"value":3417},"|",{"type":28,"tag":409,"props":3419,"children":3420},{"style":569},[3421],{"type":33,"value":3422}," model ",{"type":28,"tag":409,"props":3424,"children":3425},{"style":891},[3426],{"type":33,"value":3417},{"type":28,"tag":409,"props":3428,"children":3429},{"style":569},[3430],{"type":33,"value":3431}," parser\n",{"type":28,"tag":29,"props":3433,"children":3434},{},[3435],{"type":33,"value":3436},"This results in our structured data:",{"type":28,"tag":399,"props":3438,"children":3442},{"code":3439,"language":3440,"meta":8,"className":3441,"style":8},"cast:\n- background: Former MI6 agent\n  ethnicity: British\n  full_name: James Alexander\n  gender: Male\n  main_character: true\n  nationality: British\n  physical_traits: Tall, dark hair, blue eyes\n  short_name: Jamie\n","yml","language-yml shiki shiki-themes github-light github-dark monokai",[3443],{"type":28,"tag":390,"props":3444,"children":3445},{"__ignoreMap":8},[3446,3460,3482,3499,3516,3533,3550,3566,3583],{"type":28,"tag":409,"props":3447,"children":3448},{"class":411,"line":412},[3449,3455],{"type":28,"tag":409,"props":3450,"children":3452},{"style":3451},"--shiki-default:#22863A;--shiki-dark:#85E89D;--shiki-sepia:#F92672",[3453],{"type":33,"value":3454},"cast",{"type":28,"tag":409,"props":3456,"children":3457},{"style":569},[3458],{"type":33,"value":3459},":\n",{"type":28,"tag":409,"props":3461,"children":3462},{"class":411,"line":439},[3463,3468,3473,3477],{"type":28,"tag":409,"props":3464,"children":3465},{"style":569},[3466],{"type":33,"value":3467},"- ",{"type":28,"tag":409,"props":3469,"children":3470},{"style":3451},[3471],{"type":33,"value":3472},"background",{"type":28,"tag":409,"props":3474,"children":3475},{"style":569},[3476],{"type":33,"value":586},{"type":28,"tag":409,"props":3478,"children":3479},{"style":428},[3480],{"type":33,"value":3481},"Former MI6 agent\n",{"type":28,"tag":409,"props":3483,"children":3484},{"class":411,"line":448},[3485,3490,3494],{"type":28,"tag":409,"props":3486,"children":3487},{"style":3451},[3488],{"type":33,"value":3489},"  ethnicity",{"type":28,"tag":409,"props":3491,"children":3492},{"style":569},[3493],{"type":33,"value":586},{"type":28,"tag":409,"props":3495,"children":3496},{"style":428},[3497],{"type":33,"value":3498},"British\n",{"type":28,"tag":409,"props":3500,"children":3501},{"class":411,"line":631},[3502,3507,3511],{"type":28,"tag":409,"props":3503,"children":3504},{"style":3451},[3505],{"type":33,"value":3506},"  full_name",{"type":28,"tag":409,"props":3508,"children":3509},{"style":569},[3510],{"type":33,"value":586},{"type":28,"tag":409,"props":3512,"children":3513},{"style":428},[3514],{"type":33,"value":3515},"James Alexander\n",{"type":28,"tag":409,"props":3517,"children":3518},{"class":411,"line":653},[3519,3524,3528],{"type":28,"tag":409,"props":3520,"children":3521},{"style":3451},[3522],{"type":33,"value":3523},"  gender",{"type":28,"tag":409,"props":3525,"children":3526},{"style":569},[3527],{"type":33,"value":586},{"type":28,"tag":409,"props":3529,"children":3530},{"style":428},[3531],{"type":33,"value":3532},"Male\n",{"type":28,"tag":409,"props":3534,"children":3535},{"class":411,"line":675},[3536,3541,3545],{"type":28,"tag":409,"props":3537,"children":3538},{"style":3451},[3539],{"type":33,"value":3540},"  main_character",{"type":28,"tag":409,"props":3542,"children":3543},{"style":569},[3544],{"type":33,"value":586},{"type":28,"tag":409,"props":3546,"children":3547},{"style":422},[3548],{"type":33,"value":3549},"true\n",{"type":28,"tag":409,"props":3551,"children":3552},{"class":411,"line":697},[3553,3558,3562],{"type":28,"tag":409,"props":3554,"children":3555},{"style":3451},[3556],{"type":33,"value":3557},"  nationality",{"type":28,"tag":409,"props":3559,"children":3560},{"style":569},[3561],{"type":33,"value":586},{"type":28,"tag":409,"props":3563,"children":3564},{"style":428},[3565],{"type":33,"value":3498},{"type":28,"tag":409,"props":3567,"children":3568},{"class":411,"line":719},[3569,3574,3578],{"type":28,"tag":409,"props":3570,"children":3571},{"style":3451},[3572],{"type":33,"value":3573},"  physical_traits",{"type":28,"tag":409,"props":3575,"children":3576},{"style":569},[3577],{"type":33,"value":586},{"type":28,"tag":409,"props":3579,"children":3580},{"style":428},[3581],{"type":33,"value":3582},"Tall, dark hair, blue eyes\n",{"type":28,"tag":409,"props":3584,"children":3585},{"class":411,"line":745},[3586,3591,3595],{"type":28,"tag":409,"props":3587,"children":3588},{"style":3451},[3589],{"type":33,"value":3590},"  short_name",{"type":28,"tag":409,"props":3592,"children":3593},{"style":569},[3594],{"type":33,"value":586},{"type":28,"tag":409,"props":3596,"children":3597},{"style":428},[3598],{"type":33,"value":3599},"Jamie\n",{"type":28,"tag":29,"props":3601,"children":3602},{},[3603,3605,3611,3613,3624],{"type":33,"value":3604},"I saved the state for all \"Agents of Inference\" invocations in the ",{"type":28,"tag":390,"props":3606,"children":3608},{"className":3607},[],[3609],{"type":33,"value":3610},"output",{"type":33,"value":3612}," directory of my ",{"type":28,"tag":764,"props":3614,"children":3617},{"href":3615,"rel":3616},"https://github.com/briancaffey/agents-of-inference/tree/main/output",[768],[3618],{"type":28,"tag":390,"props":3619,"children":3621},{"className":3620},[],[3622],{"type":33,"value":3623},"agents-of-inference",{"type":33,"value":3625}," GitHub repo. I didn't commit the images and videos, but you can follow @AgentInference on X to see more of the results from my development process and future improvements, as well!",{"type":28,"tag":62,"props":3627,"children":3629},{"id":3628},"synopsis-agent",[3630],{"type":33,"value":3631},"Synopsis Agent",{"type":28,"tag":29,"props":3633,"children":3634},{},[3635],{"type":33,"value":3636},"With a cast of characters and locations selected, we need a synopsis to determine what happens. Here's the prompt:",{"type":28,"tag":399,"props":3638,"children":3642},{"code":3639,"language":3640,"meta":8,"className":3641,"style":8},"synopsis: |\n  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n","yaml","language-yaml shiki shiki-themes github-light github-dark monokai",[3643],{"type":28,"tag":390,"props":3644,"children":3645},{"__ignoreMap":8},[3646,3663,3671,3679,3687,3695,3703,3711,3719,3727,3735],{"type":28,"tag":409,"props":3647,"children":3648},{"class":411,"line":412},[3649,3654,3658],{"type":28,"tag":409,"props":3650,"children":3651},{"style":3451},[3652],{"type":33,"value":3653},"synopsis",{"type":28,"tag":409,"props":3655,"children":3656},{"style":569},[3657],{"type":33,"value":586},{"type":28,"tag":409,"props":3659,"children":3660},{"style":891},[3661],{"type":33,"value":3662},"|\n",{"type":28,"tag":409,"props":3664,"children":3665},{"class":411,"line":439},[3666],{"type":28,"tag":409,"props":3667,"children":3668},{"style":428},[3669],{"type":33,"value":3670},"  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n",{"type":28,"tag":409,"props":3672,"children":3673},{"class":411,"line":448},[3674],{"type":28,"tag":409,"props":3675,"children":3676},{"style":428},[3677],{"type":33,"value":3678},"  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n",{"type":28,"tag":409,"props":3680,"children":3681},{"class":411,"line":631},[3682],{"type":28,"tag":409,"props":3683,"children":3684},{"style":428},[3685],{"type":33,"value":3686},"  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n",{"type":28,"tag":409,"props":3688,"children":3689},{"class":411,"line":653},[3690],{"type":28,"tag":409,"props":3691,"children":3692},{"style":428},[3693],{"type":33,"value":3694},"  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n",{"type":28,"tag":409,"props":3696,"children":3697},{"class":411,"line":675},[3698],{"type":28,"tag":409,"props":3699,"children":3700},{"style":428},[3701],{"type":33,"value":3702},"  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n",{"type":28,"tag":409,"props":3704,"children":3705},{"class":411,"line":697},[3706],{"type":28,"tag":409,"props":3707,"children":3708},{"style":428},[3709],{"type":33,"value":3710},"  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n",{"type":28,"tag":409,"props":3712,"children":3713},{"class":411,"line":719},[3714],{"type":28,"tag":409,"props":3715,"children":3716},{"style":428},[3717],{"type":33,"value":3718},"  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n",{"type":28,"tag":409,"props":3720,"children":3721},{"class":411,"line":745},[3722],{"type":28,"tag":409,"props":3723,"children":3724},{"style":428},[3725],{"type":33,"value":3726},"  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n",{"type":28,"tag":409,"props":3728,"children":3729},{"class":411,"line":1134},[3730],{"type":28,"tag":409,"props":3731,"children":3732},{"style":428},[3733],{"type":33,"value":3734},"  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n",{"type":28,"tag":409,"props":3736,"children":3737},{"class":411,"line":3309},[3738],{"type":28,"tag":409,"props":3739,"children":3740},{"style":428},[3741],{"type":33,"value":3742},"  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n",{"type":28,"tag":29,"props":3744,"children":3745},{},[3746,3748,3754,3756,3762,3764,3769],{"type":33,"value":3747},"The synopsis to any good film is key, so I decided to use a feature of LangGraph that would allow a ",{"type":28,"tag":390,"props":3749,"children":3751},{"className":3750},[],[3752],{"type":33,"value":3753},"synopsis_review_agent",{"type":33,"value":3755}," to provide multiple rounds of feedback to the ",{"type":28,"tag":390,"props":3757,"children":3759},{"className":3758},[],[3760],{"type":33,"value":3761},"synopsis_agent",{"type":33,"value":3763}," to make it even better. Here's what the new graph look like after implementing the ",{"type":28,"tag":390,"props":3765,"children":3767},{"className":3766},[],[3768],{"type":33,"value":3753},{"type":33,"value":3770}," using conditional graph edges:",{"type":28,"tag":29,"props":3772,"children":3773},{},[3774],{"type":28,"tag":1215,"props":3775,"children":3777},{"alt":3753,"src":3776},"/static/aoi/graph_with_cycle.png",[],{"type":28,"tag":29,"props":3779,"children":3780},{},[3781],{"type":33,"value":3782},"Conditional edges are a very powerful feature and I just used it in one part of my graph. Other parts of the graph could benefit from this as well, and they can allow for \"human-in-the-loop\" interactions which are becoming very popular in AI-powered applications.",{"type":28,"tag":62,"props":3784,"children":3786},{"id":3785},"scene-and-shot-agents",[3787],{"type":33,"value":3788},"Scene and shot agents",{"type":28,"tag":29,"props":3790,"children":3791},{},[3792],{"type":33,"value":3793},"With our perfected synopsis, we are ready to put more agents to work. The scene agent builds out the basic structure of the storyline. It provides a structured list of the main sections of the movie. The shot agent then loops over the scenes and creates a number of different shots for the given scene. This was an effective way to have consistent thematic content for shots within a scene. Here are the prompts I used for the scene and shot agents:",{"type":28,"tag":399,"props":3795,"children":3797},{"code":3796,"language":3640,"meta":8,"className":3641,"style":8},"scenes: |\n  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\nshot: |\n  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",[3798],{"type":28,"tag":390,"props":3799,"children":3800},{"__ignoreMap":8},[3801,3817,3825,3833,3841,3849,3865],{"type":28,"tag":409,"props":3802,"children":3803},{"class":411,"line":412},[3804,3809,3813],{"type":28,"tag":409,"props":3805,"children":3806},{"style":3451},[3807],{"type":33,"value":3808},"scenes",{"type":28,"tag":409,"props":3810,"children":3811},{"style":569},[3812],{"type":33,"value":586},{"type":28,"tag":409,"props":3814,"children":3815},{"style":891},[3816],{"type":33,"value":3662},{"type":28,"tag":409,"props":3818,"children":3819},{"class":411,"line":439},[3820],{"type":28,"tag":409,"props":3821,"children":3822},{"style":428},[3823],{"type":33,"value":3824},"  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n",{"type":28,"tag":409,"props":3826,"children":3827},{"class":411,"line":448},[3828],{"type":28,"tag":409,"props":3829,"children":3830},{"style":428},[3831],{"type":33,"value":3832},"  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n",{"type":28,"tag":409,"props":3834,"children":3835},{"class":411,"line":631},[3836],{"type":28,"tag":409,"props":3837,"children":3838},{"style":428},[3839],{"type":33,"value":3840},"  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n",{"type":28,"tag":409,"props":3842,"children":3843},{"class":411,"line":653},[3844],{"type":28,"tag":409,"props":3845,"children":3846},{"style":428},[3847],{"type":33,"value":3848},"  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\n",{"type":28,"tag":409,"props":3850,"children":3851},{"class":411,"line":675},[3852,3857,3861],{"type":28,"tag":409,"props":3853,"children":3854},{"style":3451},[3855],{"type":33,"value":3856},"shot",{"type":28,"tag":409,"props":3858,"children":3859},{"style":569},[3860],{"type":33,"value":586},{"type":28,"tag":409,"props":3862,"children":3863},{"style":891},[3864],{"type":33,"value":3662},{"type":28,"tag":409,"props":3866,"children":3867},{"class":411,"line":697},[3868],{"type":28,"tag":409,"props":3869,"children":3870},{"style":428},[3871],{"type":33,"value":3872},"  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",{"type":28,"tag":62,"props":3874,"children":3876},{"id":3875},"stable-diffusion-and-stable-video-diffusion-agents",[3877],{"type":33,"value":3878},"Stable Diffusion and Stable Video Diffusion agents",{"type":28,"tag":29,"props":3880,"children":3881},{},[3882],{"type":33,"value":3883},"The stable diffusion agent makes an API call to a local instance of the Stable Diffusion WebUI API, saves the generated image and saves a reference to that image in the state:",{"type":28,"tag":399,"props":3885,"children":3887},{"code":3886,"language":3640,"meta":8,"className":3641,"style":8},"- description: A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n  image: 000.png\n  title: Ethan's Concerned Expression\n  video: 000.mp4\n",[3888],{"type":28,"tag":390,"props":3889,"children":3890},{"__ignoreMap":8},[3891,3911,3919,3927,3935,3952,3969],{"type":28,"tag":409,"props":3892,"children":3893},{"class":411,"line":412},[3894,3898,3902,3906],{"type":28,"tag":409,"props":3895,"children":3896},{"style":569},[3897],{"type":33,"value":3467},{"type":28,"tag":409,"props":3899,"children":3900},{"style":3451},[3901],{"type":33,"value":3107},{"type":28,"tag":409,"props":3903,"children":3904},{"style":569},[3905],{"type":33,"value":586},{"type":28,"tag":409,"props":3907,"children":3908},{"style":428},[3909],{"type":33,"value":3910},"A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n",{"type":28,"tag":409,"props":3912,"children":3913},{"class":411,"line":439},[3914],{"type":28,"tag":409,"props":3915,"children":3916},{"style":428},[3917],{"type":33,"value":3918},"    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n",{"type":28,"tag":409,"props":3920,"children":3921},{"class":411,"line":448},[3922],{"type":28,"tag":409,"props":3923,"children":3924},{"style":428},[3925],{"type":33,"value":3926},"    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n",{"type":28,"tag":409,"props":3928,"children":3929},{"class":411,"line":631},[3930],{"type":28,"tag":409,"props":3931,"children":3932},{"style":428},[3933],{"type":33,"value":3934},"    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n",{"type":28,"tag":409,"props":3936,"children":3937},{"class":411,"line":653},[3938,3943,3947],{"type":28,"tag":409,"props":3939,"children":3940},{"style":3451},[3941],{"type":33,"value":3942},"  image",{"type":28,"tag":409,"props":3944,"children":3945},{"style":569},[3946],{"type":33,"value":586},{"type":28,"tag":409,"props":3948,"children":3949},{"style":428},[3950],{"type":33,"value":3951},"000.png\n",{"type":28,"tag":409,"props":3953,"children":3954},{"class":411,"line":675},[3955,3960,3964],{"type":28,"tag":409,"props":3956,"children":3957},{"style":3451},[3958],{"type":33,"value":3959},"  title",{"type":28,"tag":409,"props":3961,"children":3962},{"style":569},[3963],{"type":33,"value":586},{"type":28,"tag":409,"props":3965,"children":3966},{"style":428},[3967],{"type":33,"value":3968},"Ethan's Concerned Expression\n",{"type":28,"tag":409,"props":3970,"children":3971},{"class":411,"line":697},[3972,3977,3981],{"type":28,"tag":409,"props":3973,"children":3974},{"style":3451},[3975],{"type":33,"value":3976},"  video",{"type":28,"tag":409,"props":3978,"children":3979},{"style":569},[3980],{"type":33,"value":586},{"type":28,"tag":409,"props":3982,"children":3983},{"style":428},[3984],{"type":33,"value":3985},"000.mp4\n",{"type":28,"tag":29,"props":3987,"children":3988},{},[3989],{"type":28,"tag":1215,"props":3990,"children":3993},{"alt":3991,"src":3992},"A medium close-up shot of Ethan Jameson's face","/static/aoi/ethan.png",[],{"type":28,"tag":29,"props":3995,"children":3996},{},[3997],{"type":33,"value":3998},"With the perfectly prompted image in hand, we can use Stable Video Diffusion to bring it to life. I prompted phind to come up with a FastAPI service that would accept an image in a post request and return a short video created with stable video diffusion using the diffusers library.",{"type":28,"tag":29,"props":4000,"children":4001},{},[4002],{"type":33,"value":4003},"Stable video diffusion can generate about 4 seconds of text at 7 frames per second. This isn't great, but I was able to use ffmpeg to do frame interpolation bringing the frame rate to a much smoother 14 fps using motion compensated interpolation (MCI):",{"type":28,"tag":399,"props":4005,"children":4007},{"code":4006,"language":402,"meta":8,"className":403,"style":8},"ffmpeg -i output/1718453390/final.mp4 -crf 10 -vf \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\" output/1718453390/final.14fps.mp4\n",[4008],{"type":28,"tag":390,"props":4009,"children":4010},{"__ignoreMap":8},[4011],{"type":28,"tag":409,"props":4012,"children":4013},{"class":411,"line":412},[4014,4019,4024,4029,4034,4039,4044,4049],{"type":28,"tag":409,"props":4015,"children":4016},{"style":416},[4017],{"type":33,"value":4018},"ffmpeg",{"type":28,"tag":409,"props":4020,"children":4021},{"style":422},[4022],{"type":33,"value":4023}," -i",{"type":28,"tag":409,"props":4025,"children":4026},{"style":428},[4027],{"type":33,"value":4028}," output/1718453390/final.mp4",{"type":28,"tag":409,"props":4030,"children":4031},{"style":422},[4032],{"type":33,"value":4033}," -crf",{"type":28,"tag":409,"props":4035,"children":4036},{"style":422},[4037],{"type":33,"value":4038}," 10",{"type":28,"tag":409,"props":4040,"children":4041},{"style":422},[4042],{"type":33,"value":4043}," -vf",{"type":28,"tag":409,"props":4045,"children":4046},{"style":428},[4047],{"type":33,"value":4048}," \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\"",{"type":28,"tag":409,"props":4050,"children":4051},{"style":428},[4052],{"type":33,"value":4053}," output/1718453390/final.14fps.mp4\n",{"type":28,"tag":29,"props":4055,"children":4056},{},[4057,4059,4065,4067,4073],{"type":33,"value":4058},"Finally, the ",{"type":28,"tag":390,"props":4060,"children":4062},{"className":4061},[],[4063],{"type":33,"value":4064},"editor_agent",{"type":33,"value":4066}," uses ",{"type":28,"tag":390,"props":4068,"children":4070},{"className":4069},[],[4071],{"type":33,"value":4072},"moviepy",{"type":33,"value":4074}," to join the clips together into a single video.",{"type":28,"tag":55,"props":4076,"children":4078},{"id":4077},"development-environment",[4079],{"type":33,"value":4080},"Development environment",{"type":28,"tag":29,"props":4082,"children":4083},{},[4084,4086,4092,4094,4100],{"type":33,"value":4085},"I struggled to optimize the ",{"type":28,"tag":390,"props":4087,"children":4089},{"className":4088},[],[4090],{"type":33,"value":4091},"meta-llama/Meta-Llama-3-8B-Instruct",{"type":33,"value":4093}," with TensorRT-LLM, so I ran LLM inference on a combination of older Llama2 TensorRT-LLM models, and ",{"type":28,"tag":390,"props":4095,"children":4097},{"className":4096},[],[4098],{"type":33,"value":4099},"Meta-Llama-3-8B-Instruct",{"type":33,"value":4101}," on LM Studio (which I found to be painfully slow compared to TensorRT-LLM).",{"type":28,"tag":29,"props":4103,"children":4104},{},[4105,4107,4112,4114,4120,4122,4128,4130,4137,4139,4145],{"type":33,"value":4106},"If you provide an ",{"type":28,"tag":390,"props":4108,"children":4110},{"className":4109},[],[4111],{"type":33,"value":1366},{"type":33,"value":4113}," in the ",{"type":28,"tag":390,"props":4115,"children":4117},{"className":4116},[],[4118],{"type":33,"value":4119},".env",{"type":33,"value":4121}," file, LLM calls will be made using the ",{"type":28,"tag":390,"props":4123,"children":4125},{"className":4124},[],[4126],{"type":33,"value":4127},"meta/llam3-70b-instruct",{"type":33,"value":4129}," model on ",{"type":28,"tag":764,"props":4131,"children":4134},{"href":4132,"rel":4133},"https://build.nvidia.com/meta/llama3-70b",[768],[4135],{"type":33,"value":4136},"build.nvidia.com/meta/llama3-70b",{"type":33,"value":4138},". In fact, ",{"type":28,"tag":390,"props":4140,"children":4142},{"className":4141},[],[4143],{"type":33,"value":4144},"build.nvidia.com",{"type":33,"value":4146}," also provides stable diffusion and stable video diffusion inference via API. This would be very convenient in the event that my RTX PCs become compromised.",{"type":28,"tag":29,"props":4148,"children":4149},{},[4150],{"type":33,"value":4151},"My RTX 4090 GPU with 24 GB of memory was able to run lots of different inference servers concurrently (LLM, Stable Diffusion WebUI, ComfyUI, InvokeAI, Stable Video Diffusion FastAPI service), but I generally stuck to doing one type of inference at a time, otherwise things would grind to a hault or crash. I also experimented with ChatTTS, a new text-to-speech model.",{"type":28,"tag":29,"props":4153,"children":4154},{},[4155],{"type":33,"value":4156},"I developed this project on a MacBook Pro, and I used my RTX PC as if it were a remote service providing inference for text, images and video. This is a helpful mindset when working with hybrid AI workflows that leverage inference services both on local machines and in the cloud.",{"type":28,"tag":55,"props":4158,"children":4159},{"id":1637},[4160],{"type":33,"value":4161},"How it works",{"type":28,"tag":29,"props":4163,"children":4164},{},[4165,4167,4173],{"type":33,"value":4166},"To run the program, you need to install python dependencies and then run an OpenAI compatible LLM and Stable Duffsion WebUI server with the ",{"type":28,"tag":390,"props":4168,"children":4170},{"className":4169},[],[4171],{"type":33,"value":4172},"--api",{"type":33,"value":4174}," flag. You also need to run the Stable Video Diffusion service. Apologies for any hardcoded local IP address in the source code. Deadlines, you know! With everything configured, you can run the following command:",{"type":28,"tag":399,"props":4176,"children":4178},{"code":4177},"~/git/github/agents-of-inference$ poetry run python agents_of_inference/main.py\n## 📀 Using local models 📀 ##\n## 🎭 Generating Cast 🎭 ##\n## 🗺️ Generating Locations 🗺️ ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to scene_agent ##\n## 📒 Generating Scenes 📒 ##\n## 🎬 Generating Shots 🎬 ##\n## Generated 5 shots for scene 1/5 ##\n## Generated 5 shots for scene 2/5 ##\n## Generated 5 shots for scene 3/5 ##\n## Generated 5 shots for scene 4/5 ##\n## Generated 5 shots for scene 5/5 ##\n\n000/0025\nA medium shot of a bustling Tokyo street, with neon lights reflecting off wet pavement. Jim Thompson, dressed in a black leather jacket and dark jeans, walks purposefully through the crowd, his piercing blue eyes scanning the area. The sound design features the hum of traffic and chatter of pedestrians.\nGenerated image output/1718426686/images/000.png\n\n001/0025\nA tight close-up shot of Emily Chen's face, her piercing brown eyes intense as she briefs Jim on the situation. Her short black hair is styled neatly, and she wears a crisp white blouse with a silver necklace. The camera lingers on her lips as she speaks, emphasizing the importance of the information.\nGenerated image output/1718426686/images/001.png\n\nGenerated video output/1718426686/videos/000.mp4\n== stable video diffusion generation complete ==\nGenerated video output/1718426686/videos/001.mp4\n== stable video diffusion generation complete ==\n",[4179],{"type":28,"tag":390,"props":4180,"children":4181},{"__ignoreMap":8},[4182],{"type":33,"value":4177},{"type":28,"tag":55,"props":4184,"children":4186},{"id":4185},"demo-video-for-contest-submission",[4187],{"type":33,"value":4188},"Demo Video for Contest Submission",{"type":28,"tag":4190,"props":4191,"children":4192},"agents-of-inference-video",{},[],{"type":28,"tag":29,"props":4194,"children":4195},{},[4196],{"type":33,"value":4197},"Making this video was a lot of fun. The \"Agents of Inference\" highlight reel includes some of the most interesting, exciting and fun clips that I found in the dozens of short films it created. It is important to note that a lot of the content is not very good. Misunderstood prompts, color confusion (prompt includes green eyes, but other things in the scene are also conspicuously green), unrealistic or noisy motion from Stable Video Diffusion--these are some of the issues you will find in the films. Generating AI images sometimes feels like panning for gold: you go through a lot of sediment to get a few good flakes.",{"type":28,"tag":29,"props":4199,"children":4200},{},[4201],{"type":33,"value":4202},"Also, I added a few short animations that I made with Blender. The final scene shows the NVIDIA Omniverse orange humanoid from the barrel of a pistol. I think we are rapidly approaching a future where agents can generate full-scale theatrical movies by generating OpenUSD code, directly or indirectly. Maybe for the next NVIDIA Developer contest!",{"type":28,"tag":55,"props":4204,"children":4206},{"id":4205},"shortcomings-of-my-project",[4207],{"type":33,"value":4208},"Shortcomings of my project",{"type":28,"tag":29,"props":4210,"children":4211},{},[4212],{"type":33,"value":4213},"My goodness, how embarrasing. There are quite a few shortcomings that can be easily identified looking over the output and the source code. Here are a few:",{"type":28,"tag":62,"props":4215,"children":4217},{"id":4216},"character-variety",[4218],{"type":33,"value":4219},"Character variety",{"type":28,"tag":29,"props":4221,"children":4222},{},[4223],{"type":33,"value":4224},"When generating characters I would frequently see one named Dr. Sophia Patel who is apparently a brilliant cryptologist. Other characters would often have different names or backgrounds, but a saw Dr. Sophia Patel more often than not.",{"type":28,"tag":62,"props":4226,"children":4228},{"id":4227},"character-consistency",[4229],{"type":33,"value":4230},"Character consistency",{"type":28,"tag":29,"props":4232,"children":4233},{},[4234],{"type":33,"value":4235},"The characters are not consistent. This is a notoriously difficult problem to solve, but I made a lot of progress on it during this contest. I experimented with calling the ComfyUI API to run a custom workflow built with the ComfyUI graph-based workflow tool for face transfer:",{"type":28,"tag":29,"props":4237,"children":4238},{},[4239],{"type":28,"tag":1215,"props":4240,"children":4243},{"alt":4241,"src":4242},"Dr. Sophia Patel","/static/aoi/sophia.png",[],{"type":28,"tag":29,"props":4245,"children":4246},{},[4247],{"type":33,"value":4248},"Using ComfyUI would be nice, but it wouldn't be as easy to tap into cloud APIs if my workflow heavily relied on ComfyUI server with custom models.",{"type":28,"tag":62,"props":4250,"children":4252},{"id":4251},"understanding-langchain",[4253],{"type":33,"value":4254},"Understanding LangChain",{"type":28,"tag":29,"props":4256,"children":4257},{},[4258,4260,4265],{"type":33,"value":4259},"I started out with the idea I would store all LLM calls to a local JSON to serve as a cache, allowing me to avoid regenerating responses from early in the workflow. This worked well, until I tried to serialize an Annotated list (required for cycles such as the one used with ",{"type":28,"tag":390,"props":4261,"children":4263},{"className":4262},[],[4264],{"type":33,"value":3753},{"type":33,"value":4266},"). I ended up wasting a lot of time trying to figure this out, and I came across some built-in LangChain features for storing state in memory and in Sqlite. I'm sure there are other areas where I used the wrong pattern, but I turned over a lot of stones and look forward to continuing development with LangChain.",{"type":28,"tag":55,"props":4268,"children":4270},{"id":4269},"whats-next",[4271],{"type":33,"value":4272},"What's next?",{"type":28,"tag":29,"props":4274,"children":4275},{},[4276],{"type":33,"value":4277},"Thank you to NVIDIA and LangChain for organizing this contest. It was a great way to explore a powerful toolset for automated content generation using AI agents.",{"type":28,"tag":29,"props":4279,"children":4280},{},[4281],{"type":33,"value":4282},"Video models like Dream Machine and Sora have made some big splashes on the internet and the results are remarkable. However, I'm still almost more interested in finding the limitations of quality content using open-source models on consumer hardware like RTX GPUs.",{"type":28,"tag":29,"props":4284,"children":4285},{},[4286],{"type":33,"value":4287},"I would also have loved to generate my own music for these films. I am a Suno poweruser and love the songs I have generated on that site. Will the gap between video and music generation on private, payed services and local machines? Or does it just need time to catch up? Hopefully a future installment of \"Agents of Inference\" will integrate music and voice, and can't wait to hear it!",{"type":28,"tag":2344,"props":4289,"children":4290},{},[4291],{"type":33,"value":2348},{"title":8,"searchDepth":439,"depth":439,"links":4293},[4294,4295,4296,4297,4298,4304,4305,4306,4307,4312],{"id":2839,"depth":439,"text":2842},{"id":2423,"depth":439,"text":2426},{"id":2883,"depth":439,"text":2886},{"id":2894,"depth":439,"text":2897},{"id":2905,"depth":439,"text":2908,"children":4299},[4300,4301,4302,4303],{"id":2972,"depth":448,"text":2975},{"id":3628,"depth":448,"text":3631},{"id":3785,"depth":448,"text":3788},{"id":3875,"depth":448,"text":3878},{"id":4077,"depth":439,"text":4080},{"id":1637,"depth":439,"text":4161},{"id":4185,"depth":439,"text":4188},{"id":4205,"depth":439,"text":4208,"children":4308},[4309,4310,4311],{"id":4216,"depth":448,"text":4219},{"id":4227,"depth":448,"text":4230},{"id":4251,"depth":448,"text":4254},{"id":4269,"depth":439,"text":4272},"content:2024:06:17:agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest.md","2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest.md","2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",{"_path":4317,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":4318,"description":4319,"date":4320,"image":4321,"tags":4322,"external":4326,"comments":23,"body":4335,"_type":2391,"_id":6559,"_source":2393,"_file":6560,"_stem":6561,"_extension":2396},"/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest","Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA's Generative AI on RTX PCs Developer Contest","This article discusses my entry for NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat","2024-02-17","/static/rlbc/cover.png",[14,16,2407,2409,15,2410,2411,4323,4324,4325],"rocket-league","gaming","windows",[4327,4329,4332],{"link":4328,"site":22},"https://twitter.com/briancaffey/status/1760529251072118901",{"link":4330,"site":4331},"https://www.reddit.com/r/RocketLeague/comments/1au0po3/rocket_league_botchat_an_llmpowered_bakkesmod/","reddit",{"link":4333,"site":4334},"https://dev.to/briancaffey/rocket-league-botchat-powered-by-tensorrt-llm-my-submission-for-nvidias-generative-ai-on-rtx-pcs-developer-contest-2oao","dev",{"type":25,"children":4336,"toc":6536},[4337,4341,4346,4350,4354,4366,4372,4377,4385,4390,4395,4400,4405,4413,4418,4423,4431,4439,4444,4452,4457,4462,4467,4472,4480,4484,4489,4598,4619,4632,4667,4672,4685,4714,4719,4731,4736,4858,4879,4887,4908,4955,4960,4993,4997,5002,5010,5079,5085,5097,5393,5399,5404,5536,5549,5554,5562,5574,5579,5584,5592,5596,5607,5613,5626,5782,5788,5801,5996,6002,6007,6072,6085,6118,6131,6137,6142,6150,6156,6161,6326,6347,6353,6358,6363,6368,6372,6376,6381,6389,6441,6449,6457,6465,6474,6478,6490,6495,6500,6513,6517,6522,6527,6532],{"type":28,"tag":55,"props":4338,"children":4339},{"id":2423},[4340],{"type":33,"value":2426},{"type":28,"tag":29,"props":4342,"children":4343},{},[4344],{"type":33,"value":4345},"This article is about my submission to NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA's TensorRT-LLM library.",{"type":28,"tag":29,"props":4347,"children":4348},{},[4349],{"type":33,"value":2867},{"type":28,"tag":4351,"props":4352,"children":4353},"rocket-league-bot-chat-tweet",{},[],{"type":28,"tag":29,"props":4355,"children":4356},{},[4357,4358,4365],{"type":33,"value":2452},{"type":28,"tag":764,"props":4359,"children":4362},{"href":4360,"rel":4361},"https://github.com/briancaffey/RocketLeagueBotChat",[768],[4363],{"type":33,"value":4364},"Rocket League BotChat GitHub repository",{"type":33,"value":520},{"type":28,"tag":55,"props":4367,"children":4369},{"id":4368},"nvidias-gen-ai-developer-contest",[4370],{"type":33,"value":4371},"NVIDIA's Gen AI Developer Contest",{"type":28,"tag":29,"props":4373,"children":4374},{},[4375],{"type":33,"value":4376},"The following email caught my attention last month:",{"type":28,"tag":2799,"props":4378,"children":4379},{},[4380],{"type":28,"tag":29,"props":4381,"children":4382},{},[4383],{"type":33,"value":4384},"Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems",{"type":28,"tag":29,"props":4386,"children":4387},{},[4388],{"type":33,"value":4389},"The part about “on Windows PC” made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had also never used any of the Windows-specific development frameworks like .NET or Visual Studio.",{"type":28,"tag":29,"props":4391,"children":4392},{},[4393],{"type":33,"value":4394},"My experience with Windows goes back to 2016 when I built my fist PC with an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives in order to dual boot into both Windows and Ubuntu operating systems. Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer.",{"type":28,"tag":29,"props":4396,"children":4397},{},[4398],{"type":33,"value":4399},"Last year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my PC's Ubuntu operating system, and frequently had to switch between using Ubuntu for working with stable diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems pushed me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy.",{"type":28,"tag":29,"props":4401,"children":4402},{},[4403],{"type":33,"value":4404},"I learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently compared ot Linux-based operating systems. And just like that, I became a Windows developer! This experience got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest.",{"type":28,"tag":29,"props":4406,"children":4407},{},[4408],{"type":28,"tag":1215,"props":4409,"children":4412},{"alt":4410,"src":4411},"Windows winfetch screenshot","/static/rlbc/winfetch.png",[],{"type":28,"tag":55,"props":4414,"children":4415},{"id":2894},[4416],{"type":33,"value":4417},"Coming up with an Idea",{"type":28,"tag":29,"props":4419,"children":4420},{},[4421],{"type":33,"value":4422},"The contest description and some related NVIDIA articles about the contest helped me with brainstorming:",{"type":28,"tag":2799,"props":4424,"children":4425},{},[4426],{"type":28,"tag":29,"props":4427,"children":4428},{},[4429],{"type":33,"value":4430},"Whether it’s a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless.",{"type":28,"tag":2799,"props":4432,"children":4433},{},[4434],{"type":28,"tag":29,"props":4435,"children":4436},{},[4437],{"type":33,"value":4438},"Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.",{"type":28,"tag":29,"props":4440,"children":4441},{},[4442],{"type":33,"value":4443},"This contest is focused on NVIDIA's consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software, creative tools and new AI techniques including image generation and LLM (Large Language Model) inference.",{"type":28,"tag":29,"props":4445,"children":4446},{},[4447],{"type":28,"tag":1215,"props":4448,"children":4451},{"alt":4449,"src":4450},"A stacked bar chart showing the composition of Nvidia's revenue each quarter going back to fiscal 2019.","https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&op=resize&w=700",[],{"type":28,"tag":29,"props":4453,"children":4454},{},[4455],{"type":33,"value":4456},"Gaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don't play these types of games and it didn't seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know.",{"type":28,"tag":29,"props":4458,"children":4459},{},[4460],{"type":33,"value":4461},"Rocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and all got hooked. We also came to learn that Rocket League's in-game is varies from entertaining, annoying, toxic and in some cases, sportsmanlike.",{"type":28,"tag":29,"props":4463,"children":4464},{},[4465],{"type":33,"value":4466},"One other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas \"bakkes\" Bakke, a Norwegian software engineer). Rocket League's in-game chat inspired the idea for my submission to NVIDIA's Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)",{"type":28,"tag":29,"props":4468,"children":4469},{},[4470],{"type":33,"value":4471},"Bots are built into the Rocket League game and you can play with or against them in offline matches. However, the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow and PyTorch. These bots are very good, but they are not infallible. My contest project idea was now clear: develop a plugin for Rocket League capable of sending messages from bot players. This idea seemed to check the boxes for the large language model category of NVIDIA's developer contest: develop a project in a Windows environment for a Windows-specific program, and use an LLM powered by TensorRT-LLM.",{"type":28,"tag":29,"props":4473,"children":4474},{},[4475],{"type":28,"tag":1215,"props":4476,"children":4479},{"alt":4477,"src":4478},"RLBot Ascii Art","/static/rlbc/bot.png",[],{"type":28,"tag":55,"props":4481,"children":4482},{"id":2905},[4483],{"type":33,"value":2908},{"type":28,"tag":29,"props":4485,"children":4486},{},[4487],{"type":33,"value":4488},"With this idea in mind, I looked into the project's feasibility. I really had no idea if this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept.",{"type":28,"tag":69,"props":4490,"children":4491},{},[4492,4538,4557,4568],{"type":28,"tag":73,"props":4493,"children":4494},{},[4495,4497,4503],{"type":33,"value":4496},"The Bakkesmod Plugin Wiki ",{"type":28,"tag":764,"props":4498,"children":4501},{"href":4499,"rel":4500},"https://wiki.bakkesplugins.com/",[768],[4502],{"type":33,"value":4499},{"type":28,"tag":69,"props":4504,"children":4505},{},[4506,4522],{"type":28,"tag":73,"props":4507,"children":4508},{},[4509,4520],{"type":28,"tag":764,"props":4510,"children":4513},{"href":4511,"rel":4512},"https://wiki.bakkesplugins.com/code_snippets/using_http_wrapper/",[768],[4514],{"type":28,"tag":390,"props":4515,"children":4517},{"className":4516},[],[4518],{"type":33,"value":4519},"HttpWrapper",{"type":33,"value":4521}," for sending HTTP requests from Bakkesmod",{"type":28,"tag":73,"props":4523,"children":4524},{},[4525,4536],{"type":28,"tag":764,"props":4526,"children":4529},{"href":4527,"rel":4528},"https://wiki.bakkesplugins.com/functions/stat_events/",[768],[4530],{"type":28,"tag":390,"props":4531,"children":4533},{"className":4532},[],[4534],{"type":33,"value":4535},"StatEvents",{"type":33,"value":4537}," that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save).",{"type":28,"tag":73,"props":4539,"children":4540},{},[4541,4543,4549],{"type":33,"value":4542},"The Bakkesmod plugin template: ",{"type":28,"tag":764,"props":4544,"children":4547},{"href":4545,"rel":4546},"https://github.com/Martinii89/BakkesmodPluginTemplate",[768],[4548],{"type":33,"value":4545},{"type":28,"tag":69,"props":4550,"children":4551},{},[4552],{"type":28,"tag":73,"props":4553,"children":4554},{},[4555],{"type":33,"value":4556},"This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly",{"type":28,"tag":73,"props":4558,"children":4559},{},[4560,4562],{"type":33,"value":4561},"Plugin Tutorial: ",{"type":28,"tag":764,"props":4563,"children":4566},{"href":4564,"rel":4565},"https://wiki.bakkesplugins.com/plugin_tutorial/getting_started/",[768],[4567],{"type":33,"value":4564},{"type":28,"tag":73,"props":4569,"children":4570},{},[4571,4573],{"type":33,"value":4572},"Open-source chat-related Bakkesmod plugins on GitHub\n",{"type":28,"tag":69,"props":4574,"children":4575},{},[4576,4587],{"type":28,"tag":73,"props":4577,"children":4578},{},[4579,4581],{"type":33,"value":4580},"BetterChat: ",{"type":28,"tag":764,"props":4582,"children":4585},{"href":4583,"rel":4584},"https://github.com/JulienML/BetterChat",[768],[4586],{"type":33,"value":4583},{"type":28,"tag":73,"props":4588,"children":4589},{},[4590,4592],{"type":33,"value":4591},"Translate: ",{"type":28,"tag":764,"props":4593,"children":4596},{"href":4594,"rel":4595},"https://github.com/0xleft/trnslt",[768],[4597],{"type":33,"value":4594},{"type":28,"tag":29,"props":4599,"children":4600},{},[4601,4603,4609,4611,4617],{"type":33,"value":4602},"Starting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to ",{"type":28,"tag":390,"props":4604,"children":4606},{"className":4605},[],[4607],{"type":33,"value":4608},"localhost:8000/hello",{"type":33,"value":4610},". I set up a Hello World Flask app running on ",{"type":28,"tag":390,"props":4612,"children":4614},{"className":4613},[],[4615],{"type":33,"value":4616},"localhost:8000",{"type":33,"value":4618}," and I was able to get a response from my Hello World server! There didn't seem to be any network or permission errors that would prevent my game code from communicating with other applications on my PC.",{"type":28,"tag":29,"props":4620,"children":4621},{},[4622,4624,4630],{"type":33,"value":4623},"Next I started looking into how to build and run optimized LLMs with NVIDIA's TensorRT-LLM library, the software that this contest is promoting. The contest announcement included an interesting building block that I thought could be very useful: an example repo showing how to run ",{"type":28,"tag":390,"props":4625,"children":4627},{"className":4626},[],[4628],{"type":33,"value":4629},"CodeLlama-13b-instruct-hf",{"type":33,"value":4631}," optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev).",{"type":28,"tag":69,"props":4633,"children":4634},{},[4635,4645,4657,4662],{"type":28,"tag":73,"props":4636,"children":4637},{},[4638,4643],{"type":28,"tag":390,"props":4639,"children":4641},{"className":4640},[],[4642],{"type":33,"value":4629},{"type":33,"value":4644}," is an open source model from Meta that is trained on code and can help with code generation tasks",{"type":28,"tag":73,"props":4646,"children":4647},{},[4648,4650,4655],{"type":33,"value":4649},"TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like ",{"type":28,"tag":390,"props":4651,"children":4653},{"className":4652},[],[4654],{"type":33,"value":4629},{"type":33,"value":4656}," and generates an engine that can be used for doing inference",{"type":28,"tag":73,"props":4658,"children":4659},{},[4660],{"type":33,"value":4661},"VSCode is an open source code editor developed by Microsoft with an large number of community plugins",{"type":28,"tag":73,"props":4663,"children":4664},{},[4665],{"type":33,"value":4666},"Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT",{"type":28,"tag":29,"props":4668,"children":4669},{},[4670],{"type":33,"value":4671},"To get the coding assistant project working I needed to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:",{"type":28,"tag":69,"props":4673,"children":4674},{},[4675,4680],{"type":28,"tag":73,"props":4676,"children":4677},{},[4678],{"type":33,"value":4679},"using a \"bare-metal\" virtual environment on Windows (with PowerShell)",{"type":28,"tag":73,"props":4681,"children":4682},{},[4683],{"type":33,"value":4684},"using WSL",{"type":28,"tag":29,"props":4686,"children":4687},{},[4688,4690,4696,4698,4704,4706,4712],{"type":33,"value":4689},"At the time of writing, building a TensorRT-LLM engine on Windows can only be done with version ",{"type":28,"tag":390,"props":4691,"children":4693},{"className":4692},[],[4694],{"type":33,"value":4695},"v0.6.1",{"type":33,"value":4697}," of the TensorRT-LLM repo and version ",{"type":28,"tag":390,"props":4699,"children":4701},{"className":4700},[],[4702],{"type":33,"value":4703},"v0.7.1",{"type":33,"value":4705}," of the ",{"type":28,"tag":390,"props":4707,"children":4709},{"className":4708},[],[4710],{"type":33,"value":4711},"tensorrt_llm",{"type":33,"value":4713}," Python package.",{"type":28,"tag":29,"props":4715,"children":4716},{},[4717],{"type":33,"value":4718},"With WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system.",{"type":28,"tag":29,"props":4720,"children":4721},{},[4722,4724,4729],{"type":33,"value":4723},"Once the engines are built you can use them to run the example from the ",{"type":28,"tag":390,"props":4725,"children":4727},{"className":4726},[],[4728],{"type":33,"value":2580},{"type":33,"value":4730}," repo.",{"type":28,"tag":29,"props":4732,"children":4733},{},[4734],{"type":33,"value":4735},"The example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:",{"type":28,"tag":399,"props":4737,"children":4739},{"code":4738,"language":558,"meta":8,"className":559,"style":8},"{\n  \"title\": \"CodeLlama-13b-instruct-hf\",\n  \"apiBase\": \"http://192.168.5.96:5000/\",\n  \"provider\": \"openai\",\n  \"apiKey\": \"None\",\n  \"model\": \"gpt-4\"\n}\n",[4740],{"type":28,"tag":390,"props":4741,"children":4742},{"__ignoreMap":8},[4743,4750,4771,4792,4813,4834,4851],{"type":28,"tag":409,"props":4744,"children":4745},{"class":411,"line":412},[4746],{"type":28,"tag":409,"props":4747,"children":4748},{"style":569},[4749],{"type":33,"value":572},{"type":28,"tag":409,"props":4751,"children":4752},{"class":411,"line":439},[4753,4758,4762,4767],{"type":28,"tag":409,"props":4754,"children":4755},{"style":578},[4756],{"type":33,"value":4757},"  \"title\"",{"type":28,"tag":409,"props":4759,"children":4760},{"style":569},[4761],{"type":33,"value":586},{"type":28,"tag":409,"props":4763,"children":4764},{"style":589},[4765],{"type":33,"value":4766},"\"CodeLlama-13b-instruct-hf\"",{"type":28,"tag":409,"props":4768,"children":4769},{"style":569},[4770],{"type":33,"value":607},{"type":28,"tag":409,"props":4772,"children":4773},{"class":411,"line":448},[4774,4779,4783,4788],{"type":28,"tag":409,"props":4775,"children":4776},{"style":578},[4777],{"type":33,"value":4778},"  \"apiBase\"",{"type":28,"tag":409,"props":4780,"children":4781},{"style":569},[4782],{"type":33,"value":586},{"type":28,"tag":409,"props":4784,"children":4785},{"style":589},[4786],{"type":33,"value":4787},"\"http://192.168.5.96:5000/\"",{"type":28,"tag":409,"props":4789,"children":4790},{"style":569},[4791],{"type":33,"value":607},{"type":28,"tag":409,"props":4793,"children":4794},{"class":411,"line":631},[4795,4800,4804,4809],{"type":28,"tag":409,"props":4796,"children":4797},{"style":578},[4798],{"type":33,"value":4799},"  \"provider\"",{"type":28,"tag":409,"props":4801,"children":4802},{"style":569},[4803],{"type":33,"value":586},{"type":28,"tag":409,"props":4805,"children":4806},{"style":589},[4807],{"type":33,"value":4808},"\"openai\"",{"type":28,"tag":409,"props":4810,"children":4811},{"style":569},[4812],{"type":33,"value":607},{"type":28,"tag":409,"props":4814,"children":4815},{"class":411,"line":653},[4816,4821,4825,4830],{"type":28,"tag":409,"props":4817,"children":4818},{"style":578},[4819],{"type":33,"value":4820},"  \"apiKey\"",{"type":28,"tag":409,"props":4822,"children":4823},{"style":569},[4824],{"type":33,"value":586},{"type":28,"tag":409,"props":4826,"children":4827},{"style":589},[4828],{"type":33,"value":4829},"\"None\"",{"type":28,"tag":409,"props":4831,"children":4832},{"style":569},[4833],{"type":33,"value":607},{"type":28,"tag":409,"props":4835,"children":4836},{"class":411,"line":675},[4837,4842,4846],{"type":28,"tag":409,"props":4838,"children":4839},{"style":578},[4840],{"type":33,"value":4841},"  \"model\"",{"type":28,"tag":409,"props":4843,"children":4844},{"style":569},[4845],{"type":33,"value":586},{"type":28,"tag":409,"props":4847,"children":4848},{"style":589},[4849],{"type":33,"value":4850},"\"gpt-4\"\n",{"type":28,"tag":409,"props":4852,"children":4853},{"class":411,"line":697},[4854],{"type":28,"tag":409,"props":4855,"children":4856},{"style":569},[4857],{"type":33,"value":751},{"type":28,"tag":29,"props":4859,"children":4860},{},[4861,4863,4868,4870,4877],{"type":33,"value":4862},"The Continue.dev extension using ",{"type":28,"tag":390,"props":4864,"children":4866},{"className":4865},[],[4867],{"type":33,"value":4629},{"type":33,"value":4869}," accelerated and optimized by TensorRT-LLM is very fast. According to ",{"type":28,"tag":764,"props":4871,"children":4874},{"href":4872,"rel":4873},"https://blog.continue.dev/programming-languages/",[768],[4875],{"type":33,"value":4876},"this post on Continue.dev's blog",{"type":33,"value":4878},", C++ is a \"first tier\" language:",{"type":28,"tag":2799,"props":4880,"children":4881},{},[4882],{"type":28,"tag":29,"props":4883,"children":4884},{},[4885],{"type":33,"value":4886},"C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode / TP3, MBXP / Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don’t believe that LLMs are as helpful for C++ as some of the other languages in this tier.",{"type":28,"tag":29,"props":4888,"children":4889},{},[4890,4892,4898,4900,4906],{"type":33,"value":4891},"Most of the time I'm working with either Python and TypeScript. I've read about C++ but haven't used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with the Continue.dev autopilot extension was helpful for tackling smaller problems in a REPL-like environment. For example, I used Continue.dev in VSCode to figure out how to handle JSON. Coming from Python and JavaScript languages, I found the ",{"type":28,"tag":390,"props":4893,"children":4895},{"className":4894},[],[4896],{"type":33,"value":4897},"nlohmann/json",{"type":33,"value":4899}," JSON library syntax to be somewhat different. For example, here is how to add a message to ",{"type":28,"tag":390,"props":4901,"children":4903},{"className":4902},[],[4904],{"type":33,"value":4905},"messages",{"type":33,"value":4907}," in the body of an OpenAI API request:",{"type":28,"tag":399,"props":4909,"children":4913},{"code":4910,"language":4911,"meta":8,"className":4912,"style":8},"messages.push_back({ {\"role\", role}, {\"content\", content } });\n","cpp","language-cpp shiki shiki-themes github-light github-dark monokai",[4914],{"type":28,"tag":390,"props":4915,"children":4916},{"__ignoreMap":8},[4917],{"type":28,"tag":409,"props":4918,"children":4919},{"class":411,"line":412},[4920,4925,4930,4935,4940,4945,4950],{"type":28,"tag":409,"props":4921,"children":4922},{"style":569},[4923],{"type":33,"value":4924},"messages.",{"type":28,"tag":409,"props":4926,"children":4927},{"style":416},[4928],{"type":33,"value":4929},"push_back",{"type":28,"tag":409,"props":4931,"children":4932},{"style":569},[4933],{"type":33,"value":4934},"({ {",{"type":28,"tag":409,"props":4936,"children":4937},{"style":428},[4938],{"type":33,"value":4939},"\"role\"",{"type":28,"tag":409,"props":4941,"children":4942},{"style":569},[4943],{"type":33,"value":4944},", role}, {",{"type":28,"tag":409,"props":4946,"children":4947},{"style":428},[4948],{"type":33,"value":4949},"\"content\"",{"type":28,"tag":409,"props":4951,"children":4952},{"style":569},[4953],{"type":33,"value":4954},", content } });\n",{"type":28,"tag":29,"props":4956,"children":4957},{},[4958],{"type":33,"value":4959},"In Python the code for appending a message to a list of messages would be written differently:",{"type":28,"tag":399,"props":4961,"children":4963},{"code":4962,"language":419,"meta":8,"className":876,"style":8},"messages.append({\"role\": role, \"content\": content})\n",[4964],{"type":28,"tag":390,"props":4965,"children":4966},{"__ignoreMap":8},[4967],{"type":28,"tag":409,"props":4968,"children":4969},{"class":411,"line":412},[4970,4975,4979,4984,4988],{"type":28,"tag":409,"props":4971,"children":4972},{"style":569},[4973],{"type":33,"value":4974},"messages.append({",{"type":28,"tag":409,"props":4976,"children":4977},{"style":428},[4978],{"type":33,"value":4939},{"type":28,"tag":409,"props":4980,"children":4981},{"style":569},[4982],{"type":33,"value":4983},": role, ",{"type":28,"tag":409,"props":4985,"children":4986},{"style":428},[4987],{"type":33,"value":4949},{"type":28,"tag":409,"props":4989,"children":4990},{"style":569},[4991],{"type":33,"value":4992},": content})\n",{"type":28,"tag":55,"props":4994,"children":4995},{"id":4077},[4996],{"type":33,"value":4080},{"type":28,"tag":29,"props":4998,"children":4999},{},[5000],{"type":33,"value":5001},"While working on different projects using web technologies and frameworks in the Python and JavaScript ecosystems, I developed an appreciation for well-structured development environments that are easy to use. Development environment refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running on a local environment. The local environment (the developer's computer) should be a close proxy for the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality, offering a tight feedback loop. I really like the development environment for this project. Here's a screenshot that shows the different parts of the development environment I used for working on Rocket League BotChat:",{"type":28,"tag":29,"props":5003,"children":5004},{},[5005],{"type":28,"tag":1215,"props":5006,"children":5009},{"alt":5007,"src":5008},"Screenshot of Rocket League BotChat development environment","/static/rlbc/devenv2.png",[],{"type":28,"tag":69,"props":5011,"children":5012},{},[5013,5026,5054,5067],{"type":28,"tag":73,"props":5014,"children":5015},{},[5016,5018,5024],{"type":33,"value":5017},"Rocket League (running with the ",{"type":28,"tag":390,"props":5019,"children":5021},{"className":5020},[],[5022],{"type":33,"value":5023},"-dev",{"type":33,"value":5025}," flag turned on). The console is helpful for viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine",{"type":28,"tag":73,"props":5027,"children":5028},{},[5029,5031,5037,5039,5045,5046,5052],{"type":33,"value":5030},"Visual Studio for working on the plugin code. ",{"type":28,"tag":390,"props":5032,"children":5034},{"className":5033},[],[5035],{"type":33,"value":5036},"Control",{"type":33,"value":5038},"+",{"type":28,"tag":390,"props":5040,"children":5042},{"className":5041},[],[5043],{"type":33,"value":5044},"Shift",{"type":33,"value":5038},{"type":28,"tag":390,"props":5047,"children":5049},{"className":5048},[],[5050],{"type":33,"value":5051},"B",{"type":33,"value":5053}," rebuilds the code and automatically reloads the plugin in the game",{"type":28,"tag":73,"props":5055,"children":5056},{},[5057,5059,5065],{"type":33,"value":5058},"OpenAI-compatible LLM server powered by TensorRT-LLM (using ",{"type":28,"tag":390,"props":5060,"children":5062},{"className":5061},[],[5063],{"type":33,"value":5064},"Llama-2-13b-chat-hf",{"type":33,"value":5066}," with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL",{"type":28,"tag":73,"props":5068,"children":5069},{},[5070,5072,5077],{"type":33,"value":5071},"VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using ",{"type":28,"tag":390,"props":5073,"children":5075},{"className":5074},[],[5076],{"type":33,"value":4629},{"type":33,"value":5078}," with AWQ INT4 quantization) running in a virtual environment on Windows",{"type":28,"tag":62,"props":5080,"children":5082},{"id":5081},"building-the-tensorrt-llm-engines",[5083],{"type":33,"value":5084},"Building the TensorRT-LLM engines",{"type":28,"tag":29,"props":5086,"children":5087},{},[5088,5090,5095],{"type":33,"value":5089},"I was able to build and run the TensorRT LLM engines for my game plugin's inference and the Continue.dev extension's inference both in Python virtual environments on Windows and on Ubuntu in WSL. For building the ",{"type":28,"tag":390,"props":5091,"children":5093},{"className":5092},[],[5094],{"type":33,"value":5064},{"type":33,"value":5096}," model with INT4 AWQ quantization on Windows 11 I used this command:",{"type":28,"tag":399,"props":5098,"children":5102},{"code":5099,"language":5100,"meta":8,"className":5101,"style":8},"(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT-LLM\\examples\\llama> python build.py --model_dir D:\\llama\\Llama-2-13b-chat-hf\\ --quant_ckpt_path D:\\llama\\Llama-2-13b-chat-hf\\llama_tp1_rank0.npz --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16 --use_weight_only --weight_only_precision int4_awq --per_group --enable_context_fmha --max_batch_size 1 --max_input_len 3500 --max_output_len 1024 --output_dir D:\\llama\\Llama-2-13b-chat-hf\\single-gpu\\ --vocab_size 32064\n","powershell","language-powershell shiki shiki-themes github-light github-dark monokai",[5103],{"type":28,"tag":390,"props":5104,"children":5105},{"__ignoreMap":8},[5106],{"type":28,"tag":409,"props":5107,"children":5108},{"class":411,"line":412},[5109,5114,5119,5124,5129,5134,5139,5144,5148,5153,5157,5162,5166,5171,5175,5180,5184,5189,5193,5197,5201,5205,5209,5213,5217,5222,5226,5231,5235,5240,5244,5249,5253,5258,5262,5267,5271,5276,5280,5285,5289,5294,5299,5304,5309,5314,5318,5323,5328,5332,5337,5341,5345,5349,5353,5357,5361,5365,5370,5374,5379,5383,5388],{"type":28,"tag":409,"props":5110,"children":5111},{"style":569},[5112],{"type":33,"value":5113},"(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT",{"type":28,"tag":409,"props":5115,"children":5116},{"style":891},[5117],{"type":33,"value":5118},"-",{"type":28,"tag":409,"props":5120,"children":5121},{"style":569},[5122],{"type":33,"value":5123},"LLM\\examples\\llama",{"type":28,"tag":409,"props":5125,"children":5126},{"style":891},[5127],{"type":33,"value":5128},">",{"type":28,"tag":409,"props":5130,"children":5131},{"style":569},[5132],{"type":33,"value":5133}," python build.py ",{"type":28,"tag":409,"props":5135,"children":5136},{"style":891},[5137],{"type":33,"value":5138},"--",{"type":28,"tag":409,"props":5140,"children":5141},{"style":569},[5142],{"type":33,"value":5143},"model_dir D:\\llama\\Llama",{"type":28,"tag":409,"props":5145,"children":5146},{"style":891},[5147],{"type":33,"value":5118},{"type":28,"tag":409,"props":5149,"children":5150},{"style":422},[5151],{"type":33,"value":5152},"2",{"type":28,"tag":409,"props":5154,"children":5155},{"style":891},[5156],{"type":33,"value":5118},{"type":28,"tag":409,"props":5158,"children":5159},{"style":569},[5160],{"type":33,"value":5161},"13b",{"type":28,"tag":409,"props":5163,"children":5164},{"style":891},[5165],{"type":33,"value":5118},{"type":28,"tag":409,"props":5167,"children":5168},{"style":569},[5169],{"type":33,"value":5170},"chat",{"type":28,"tag":409,"props":5172,"children":5173},{"style":891},[5174],{"type":33,"value":5118},{"type":28,"tag":409,"props":5176,"children":5177},{"style":569},[5178],{"type":33,"value":5179},"hf\\ ",{"type":28,"tag":409,"props":5181,"children":5182},{"style":891},[5183],{"type":33,"value":5138},{"type":28,"tag":409,"props":5185,"children":5186},{"style":569},[5187],{"type":33,"value":5188},"quant_ckpt_path D:\\llama\\Llama",{"type":28,"tag":409,"props":5190,"children":5191},{"style":891},[5192],{"type":33,"value":5118},{"type":28,"tag":409,"props":5194,"children":5195},{"style":422},[5196],{"type":33,"value":5152},{"type":28,"tag":409,"props":5198,"children":5199},{"style":891},[5200],{"type":33,"value":5118},{"type":28,"tag":409,"props":5202,"children":5203},{"style":569},[5204],{"type":33,"value":5161},{"type":28,"tag":409,"props":5206,"children":5207},{"style":891},[5208],{"type":33,"value":5118},{"type":28,"tag":409,"props":5210,"children":5211},{"style":569},[5212],{"type":33,"value":5170},{"type":28,"tag":409,"props":5214,"children":5215},{"style":891},[5216],{"type":33,"value":5118},{"type":28,"tag":409,"props":5218,"children":5219},{"style":569},[5220],{"type":33,"value":5221},"hf\\llama_tp1_rank0.npz ",{"type":28,"tag":409,"props":5223,"children":5224},{"style":891},[5225],{"type":33,"value":5138},{"type":28,"tag":409,"props":5227,"children":5228},{"style":569},[5229],{"type":33,"value":5230},"dtype float16 ",{"type":28,"tag":409,"props":5232,"children":5233},{"style":891},[5234],{"type":33,"value":5138},{"type":28,"tag":409,"props":5236,"children":5237},{"style":569},[5238],{"type":33,"value":5239},"use_gpt_attention_plugin float16 ",{"type":28,"tag":409,"props":5241,"children":5242},{"style":891},[5243],{"type":33,"value":5138},{"type":28,"tag":409,"props":5245,"children":5246},{"style":569},[5247],{"type":33,"value":5248},"use_gemm_plugin float16 ",{"type":28,"tag":409,"props":5250,"children":5251},{"style":891},[5252],{"type":33,"value":5138},{"type":28,"tag":409,"props":5254,"children":5255},{"style":569},[5256],{"type":33,"value":5257},"use_weight_only ",{"type":28,"tag":409,"props":5259,"children":5260},{"style":891},[5261],{"type":33,"value":5138},{"type":28,"tag":409,"props":5263,"children":5264},{"style":569},[5265],{"type":33,"value":5266},"weight_only_precision int4_awq ",{"type":28,"tag":409,"props":5268,"children":5269},{"style":891},[5270],{"type":33,"value":5138},{"type":28,"tag":409,"props":5272,"children":5273},{"style":569},[5274],{"type":33,"value":5275},"per_group ",{"type":28,"tag":409,"props":5277,"children":5278},{"style":891},[5279],{"type":33,"value":5138},{"type":28,"tag":409,"props":5281,"children":5282},{"style":569},[5283],{"type":33,"value":5284},"enable_context_fmha ",{"type":28,"tag":409,"props":5286,"children":5287},{"style":891},[5288],{"type":33,"value":5138},{"type":28,"tag":409,"props":5290,"children":5291},{"style":569},[5292],{"type":33,"value":5293},"max_batch_size ",{"type":28,"tag":409,"props":5295,"children":5296},{"style":422},[5297],{"type":33,"value":5298},"1",{"type":28,"tag":409,"props":5300,"children":5301},{"style":891},[5302],{"type":33,"value":5303}," --",{"type":28,"tag":409,"props":5305,"children":5306},{"style":569},[5307],{"type":33,"value":5308},"max_input_len ",{"type":28,"tag":409,"props":5310,"children":5311},{"style":422},[5312],{"type":33,"value":5313},"3500",{"type":28,"tag":409,"props":5315,"children":5316},{"style":891},[5317],{"type":33,"value":5303},{"type":28,"tag":409,"props":5319,"children":5320},{"style":569},[5321],{"type":33,"value":5322},"max_output_len ",{"type":28,"tag":409,"props":5324,"children":5325},{"style":422},[5326],{"type":33,"value":5327},"1024",{"type":28,"tag":409,"props":5329,"children":5330},{"style":891},[5331],{"type":33,"value":5303},{"type":28,"tag":409,"props":5333,"children":5334},{"style":569},[5335],{"type":33,"value":5336},"output_dir D:\\llama\\Llama",{"type":28,"tag":409,"props":5338,"children":5339},{"style":891},[5340],{"type":33,"value":5118},{"type":28,"tag":409,"props":5342,"children":5343},{"style":422},[5344],{"type":33,"value":5152},{"type":28,"tag":409,"props":5346,"children":5347},{"style":891},[5348],{"type":33,"value":5118},{"type":28,"tag":409,"props":5350,"children":5351},{"style":569},[5352],{"type":33,"value":5161},{"type":28,"tag":409,"props":5354,"children":5355},{"style":891},[5356],{"type":33,"value":5118},{"type":28,"tag":409,"props":5358,"children":5359},{"style":569},[5360],{"type":33,"value":5170},{"type":28,"tag":409,"props":5362,"children":5363},{"style":891},[5364],{"type":33,"value":5118},{"type":28,"tag":409,"props":5366,"children":5367},{"style":569},[5368],{"type":33,"value":5369},"hf\\single",{"type":28,"tag":409,"props":5371,"children":5372},{"style":891},[5373],{"type":33,"value":5118},{"type":28,"tag":409,"props":5375,"children":5376},{"style":569},[5377],{"type":33,"value":5378},"gpu\\ ",{"type":28,"tag":409,"props":5380,"children":5381},{"style":891},[5382],{"type":33,"value":5138},{"type":28,"tag":409,"props":5384,"children":5385},{"style":569},[5386],{"type":33,"value":5387},"vocab_size ",{"type":28,"tag":409,"props":5389,"children":5390},{"style":422},[5391],{"type":33,"value":5392},"32064\n",{"type":28,"tag":62,"props":5394,"children":5396},{"id":5395},"running-the-tensorrt-llm-engines",[5397],{"type":33,"value":5398},"Running the TensorRT-LLM engines",{"type":28,"tag":29,"props":5400,"children":5401},{},[5402],{"type":33,"value":5403},"Using Windows PowerShell to start the CodeLlama server for Continue.dev:",{"type":28,"tag":399,"props":5405,"children":5407},{"code":5406,"language":5100,"meta":8,"className":5101,"style":8},"(.venv) PS C:\\Users\\My PC\\GitHub\\trt-llm-as-openai-windows> python .\\app.py --trt_engine_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\" --trt_engine_name llama_float16_tp1_rank0.engine --tokenizer_dir_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\\" --port 5000 --host 0.0.0.0\n",[5408],{"type":28,"tag":390,"props":5409,"children":5410},{"__ignoreMap":8},[5411],{"type":28,"tag":409,"props":5412,"children":5413},{"class":411,"line":412},[5414,5419,5423,5427,5431,5436,5440,5445,5449,5453,5457,5462,5466,5471,5476,5480,5485,5489,5494,5499,5503,5508,5513,5517,5522,5527,5531],{"type":28,"tag":409,"props":5415,"children":5416},{"style":569},[5417],{"type":33,"value":5418},"(.venv) PS C:\\Users\\My PC\\GitHub\\trt",{"type":28,"tag":409,"props":5420,"children":5421},{"style":891},[5422],{"type":33,"value":5118},{"type":28,"tag":409,"props":5424,"children":5425},{"style":569},[5426],{"type":33,"value":2410},{"type":28,"tag":409,"props":5428,"children":5429},{"style":891},[5430],{"type":33,"value":5118},{"type":28,"tag":409,"props":5432,"children":5433},{"style":569},[5434],{"type":33,"value":5435},"as",{"type":28,"tag":409,"props":5437,"children":5438},{"style":891},[5439],{"type":33,"value":5118},{"type":28,"tag":409,"props":5441,"children":5442},{"style":569},[5443],{"type":33,"value":5444},"openai",{"type":28,"tag":409,"props":5446,"children":5447},{"style":891},[5448],{"type":33,"value":5118},{"type":28,"tag":409,"props":5450,"children":5451},{"style":569},[5452],{"type":33,"value":4325},{"type":28,"tag":409,"props":5454,"children":5455},{"style":891},[5456],{"type":33,"value":5128},{"type":28,"tag":409,"props":5458,"children":5459},{"style":569},[5460],{"type":33,"value":5461}," python .\\app.py ",{"type":28,"tag":409,"props":5463,"children":5464},{"style":891},[5465],{"type":33,"value":5138},{"type":28,"tag":409,"props":5467,"children":5468},{"style":569},[5469],{"type":33,"value":5470},"trt_engine_path ",{"type":28,"tag":409,"props":5472,"children":5473},{"style":428},[5474],{"type":33,"value":5475},"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\"",{"type":28,"tag":409,"props":5477,"children":5478},{"style":891},[5479],{"type":33,"value":5303},{"type":28,"tag":409,"props":5481,"children":5482},{"style":569},[5483],{"type":33,"value":5484},"trt_engine_name llama_float16_tp1_rank0.engine ",{"type":28,"tag":409,"props":5486,"children":5487},{"style":891},[5488],{"type":33,"value":5138},{"type":28,"tag":409,"props":5490,"children":5491},{"style":569},[5492],{"type":33,"value":5493},"tokenizer_dir_path ",{"type":28,"tag":409,"props":5495,"children":5496},{"style":428},[5497],{"type":33,"value":5498},"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\\"",{"type":28,"tag":409,"props":5500,"children":5501},{"style":891},[5502],{"type":33,"value":5303},{"type":28,"tag":409,"props":5504,"children":5505},{"style":569},[5506],{"type":33,"value":5507},"port ",{"type":28,"tag":409,"props":5509,"children":5510},{"style":422},[5511],{"type":33,"value":5512},"5000",{"type":28,"tag":409,"props":5514,"children":5515},{"style":891},[5516],{"type":33,"value":5303},{"type":28,"tag":409,"props":5518,"children":5519},{"style":569},[5520],{"type":33,"value":5521},"host ",{"type":28,"tag":409,"props":5523,"children":5524},{"style":422},[5525],{"type":33,"value":5526},"0.0",{"type":28,"tag":409,"props":5528,"children":5529},{"style":569},[5530],{"type":33,"value":520},{"type":28,"tag":409,"props":5532,"children":5533},{"style":422},[5534],{"type":33,"value":5535},"0.0\n",{"type":28,"tag":29,"props":5537,"children":5538},{},[5539,5541,5547],{"type":33,"value":5540},"Tip: Adding ",{"type":28,"tag":390,"props":5542,"children":5544},{"className":5543},[],[5545],{"type":33,"value":5546},"--host 0.0.0.0",{"type":33,"value":5548}," isn't required here, but it allows me to use the CodeLlama/TensorRT-LLM server with VSCode any computer on my local network using my PC's local IP address in the Continue.dev configuration.",{"type":28,"tag":29,"props":5550,"children":5551},{},[5552],{"type":33,"value":5553},"Using docker in WSL to start the Llama-2-13b-chat-hf LLM server:",{"type":28,"tag":399,"props":5555,"children":5557},{"code":5556},"root@0a5b5b75f079:/code/git/TensorRT-LLM/examples/server/flask# python3 app.py --trt_engine_path /llama/Llama-2-13b-chat-hf/trt_engines/1-gpu/ --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path /llama/Llama-2-13b-chat-hf/ --port 5001 --host 0.0.0.0\n",[5558],{"type":28,"tag":390,"props":5559,"children":5560},{"__ignoreMap":8},[5561],{"type":33,"value":5556},{"type":28,"tag":29,"props":5563,"children":5564},{},[5565,5567,5572],{"type":33,"value":5566},"Note: Here I also add ",{"type":28,"tag":390,"props":5568,"children":5570},{"className":5569},[],[5571],{"type":33,"value":5546},{"type":33,"value":5573},", but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows.",{"type":28,"tag":29,"props":5575,"children":5576},{},[5577],{"type":33,"value":5578},"BakkesMod includes a console window that came in handy for debugging errors during development.",{"type":28,"tag":29,"props":5580,"children":5581},{},[5582],{"type":33,"value":5583},"At the beginning of this developer contest on January 9, NVIDIA announced Chat with RTX. This is a demo program for Windows that automates a lots of the processes needed to set up a TensorRT-LLM-powered LLM running on your PC. Keep an eye on this project as it may become the best way to install and manage large language models on Windows PCs.",{"type":28,"tag":29,"props":5585,"children":5586},{},[5587],{"type":28,"tag":1215,"props":5588,"children":5591},{"alt":5589,"src":5590},"Chat with RTX image","/static/rlbc/chat_with_rtx.jpeg",[],{"type":28,"tag":55,"props":5593,"children":5594},{"id":1637},[5595],{"type":33,"value":4161},{"type":28,"tag":29,"props":5597,"children":5598},{},[5599,5601,5606],{"type":33,"value":5600},"Here's a quick look at key parts of the plugin source code (",{"type":28,"tag":764,"props":5602,"children":5604},{"href":4360,"rel":5603},[768],[5605],{"type":33,"value":4360},{"type":33,"value":1662},{"type":28,"tag":62,"props":5608,"children":5610},{"id":5609},"hooking-events",[5611],{"type":33,"value":5612},"Hooking events",{"type":28,"tag":29,"props":5614,"children":5615},{},[5616,5618,5624],{"type":33,"value":5617},"Hooking events is the core of how this plugin works. ",{"type":28,"tag":390,"props":5619,"children":5621},{"className":5620},[],[5622],{"type":33,"value":5623},"StatTickerMessage",{"type":33,"value":5625}," events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car.",{"type":28,"tag":399,"props":5627,"children":5629},{"code":5628,"language":4911,"meta":8,"className":4912,"style":8},"    // Hooks different types of events that are handled in onStatTickerMessage\n    // See https://wiki.bakkesplugins.com/functions/stat_events/\n    gameWrapper->HookEventWithCallerPost\u003CServerWrapper>(\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\",\n        [this](ServerWrapper caller, void* params, std::string eventname) {\n            onStatTickerMessage(params);\n        });\n",[5630],{"type":28,"tag":390,"props":5631,"children":5632},{"__ignoreMap":8},[5633,5642,5650,5685,5761,5774],{"type":28,"tag":409,"props":5634,"children":5635},{"class":411,"line":412},[5636],{"type":28,"tag":409,"props":5637,"children":5639},{"style":5638},"--shiki-default:#6A737D;--shiki-dark:#6A737D;--shiki-sepia:#88846F",[5640],{"type":33,"value":5641},"    // Hooks different types of events that are handled in onStatTickerMessage\n",{"type":28,"tag":409,"props":5643,"children":5644},{"class":411,"line":439},[5645],{"type":28,"tag":409,"props":5646,"children":5647},{"style":5638},[5648],{"type":33,"value":5649},"    // See https://wiki.bakkesplugins.com/functions/stat_events/\n",{"type":28,"tag":409,"props":5651,"children":5652},{"class":411,"line":448},[5653,5658,5663,5668,5672,5676,5681],{"type":28,"tag":409,"props":5654,"children":5655},{"style":569},[5656],{"type":33,"value":5657},"    gameWrapper->HookEventWithCallerPost",{"type":28,"tag":409,"props":5659,"children":5660},{"style":891},[5661],{"type":33,"value":5662},"\u003C",{"type":28,"tag":409,"props":5664,"children":5665},{"style":569},[5666],{"type":33,"value":5667},"ServerWrapper",{"type":28,"tag":409,"props":5669,"children":5670},{"style":891},[5671],{"type":33,"value":5128},{"type":28,"tag":409,"props":5673,"children":5674},{"style":569},[5675],{"type":33,"value":3044},{"type":28,"tag":409,"props":5677,"children":5678},{"style":428},[5679],{"type":33,"value":5680},"\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\"",{"type":28,"tag":409,"props":5682,"children":5683},{"style":569},[5684],{"type":33,"value":607},{"type":28,"tag":409,"props":5686,"children":5687},{"class":411,"line":631},[5688,5693,5699,5704,5708,5713,5717,5722,5727,5732,5736,5741,5746,5751,5756],{"type":28,"tag":409,"props":5689,"children":5690},{"style":569},[5691],{"type":33,"value":5692},"        [",{"type":28,"tag":409,"props":5694,"children":5696},{"style":5695},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#FD971F",[5697],{"type":33,"value":5698},"this",{"type":28,"tag":409,"props":5700,"children":5701},{"style":569},[5702],{"type":33,"value":5703},"](",{"type":28,"tag":409,"props":5705,"children":5706},{"style":3036},[5707],{"type":33,"value":5667},{"type":28,"tag":409,"props":5709,"children":5710},{"style":3104},[5711],{"type":33,"value":5712}," caller",{"type":28,"tag":409,"props":5714,"children":5715},{"style":569},[5716],{"type":33,"value":912},{"type":28,"tag":409,"props":5718,"children":5719},{"style":1013},[5720],{"type":33,"value":5721},"void",{"type":28,"tag":409,"props":5723,"children":5724},{"style":891},[5725],{"type":33,"value":5726},"*",{"type":28,"tag":409,"props":5728,"children":5729},{"style":3104},[5730],{"type":33,"value":5731}," params",{"type":28,"tag":409,"props":5733,"children":5734},{"style":569},[5735],{"type":33,"value":912},{"type":28,"tag":409,"props":5737,"children":5738},{"style":3036},[5739],{"type":33,"value":5740},"std",{"type":28,"tag":409,"props":5742,"children":5743},{"style":569},[5744],{"type":33,"value":5745},"::",{"type":28,"tag":409,"props":5747,"children":5748},{"style":3036},[5749],{"type":33,"value":5750},"string",{"type":28,"tag":409,"props":5752,"children":5753},{"style":3104},[5754],{"type":33,"value":5755}," eventname",{"type":28,"tag":409,"props":5757,"children":5758},{"style":569},[5759],{"type":33,"value":5760},") {\n",{"type":28,"tag":409,"props":5762,"children":5763},{"class":411,"line":653},[5764,5769],{"type":28,"tag":409,"props":5765,"children":5766},{"style":416},[5767],{"type":33,"value":5768},"            onStatTickerMessage",{"type":28,"tag":409,"props":5770,"children":5771},{"style":569},[5772],{"type":33,"value":5773},"(params);\n",{"type":28,"tag":409,"props":5775,"children":5776},{"class":411,"line":675},[5777],{"type":28,"tag":409,"props":5778,"children":5779},{"style":569},[5780],{"type":33,"value":5781},"        });\n",{"type":28,"tag":62,"props":5783,"children":5785},{"id":5784},"handling-events-and-building-the-prompt",[5786],{"type":33,"value":5787},"Handling events and building the prompt",{"type":28,"tag":29,"props":5789,"children":5790},{},[5791,5793,5799],{"type":33,"value":5792},"We can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the ",{"type":28,"tag":390,"props":5794,"children":5796},{"className":5795},[],[5797],{"type":33,"value":5798},"appendToPrompt",{"type":33,"value":5800}," method.",{"type":28,"tag":399,"props":5802,"children":5804},{"code":5803,"language":4911,"meta":8,"className":4912,"style":8},"    // handle different events like scoring a goal or making a save\n    if (statEvent.GetEventName() == \"Goal\") {\n\n        // was the goal scored by the human player or the bot?\n        if (playerPRI.memory_address == receiver.memory_address) {\n            appendToPrompt(\"Your human opponent just scored a goal against you! \" + score_sentence, \"user\");\n        }\n        else {\n            appendToPrompt(\"You just scored a goal against the human player! \" + score_sentence, \"user\");\n        }\n    }\n",[5805],{"type":28,"tag":390,"props":5806,"children":5807},{"__ignoreMap":8},[5808,5816,5853,5861,5869,5891,5928,5936,5949,5981,5988],{"type":28,"tag":409,"props":5809,"children":5810},{"class":411,"line":412},[5811],{"type":28,"tag":409,"props":5812,"children":5813},{"style":5638},[5814],{"type":33,"value":5815},"    // handle different events like scoring a goal or making a save\n",{"type":28,"tag":409,"props":5817,"children":5818},{"class":411,"line":439},[5819,5824,5829,5834,5839,5844,5849],{"type":28,"tag":409,"props":5820,"children":5821},{"style":891},[5822],{"type":33,"value":5823},"    if",{"type":28,"tag":409,"props":5825,"children":5826},{"style":569},[5827],{"type":33,"value":5828}," (statEvent.",{"type":28,"tag":409,"props":5830,"children":5831},{"style":416},[5832],{"type":33,"value":5833},"GetEventName",{"type":28,"tag":409,"props":5835,"children":5836},{"style":569},[5837],{"type":33,"value":5838},"() ",{"type":28,"tag":409,"props":5840,"children":5841},{"style":891},[5842],{"type":33,"value":5843},"==",{"type":28,"tag":409,"props":5845,"children":5846},{"style":428},[5847],{"type":33,"value":5848}," \"Goal\"",{"type":28,"tag":409,"props":5850,"children":5851},{"style":569},[5852],{"type":33,"value":5760},{"type":28,"tag":409,"props":5854,"children":5855},{"class":411,"line":448},[5856],{"type":28,"tag":409,"props":5857,"children":5858},{"emptyLinePlaceholder":23},[5859],{"type":33,"value":5860},"\n",{"type":28,"tag":409,"props":5862,"children":5863},{"class":411,"line":631},[5864],{"type":28,"tag":409,"props":5865,"children":5866},{"style":5638},[5867],{"type":33,"value":5868},"        // was the goal scored by the human player or the bot?\n",{"type":28,"tag":409,"props":5870,"children":5871},{"class":411,"line":653},[5872,5877,5882,5886],{"type":28,"tag":409,"props":5873,"children":5874},{"style":891},[5875],{"type":33,"value":5876},"        if",{"type":28,"tag":409,"props":5878,"children":5879},{"style":569},[5880],{"type":33,"value":5881}," (playerPRI.memory_address ",{"type":28,"tag":409,"props":5883,"children":5884},{"style":891},[5885],{"type":33,"value":5843},{"type":28,"tag":409,"props":5887,"children":5888},{"style":569},[5889],{"type":33,"value":5890}," receiver.memory_address) {\n",{"type":28,"tag":409,"props":5892,"children":5893},{"class":411,"line":675},[5894,5899,5903,5908,5913,5918,5923],{"type":28,"tag":409,"props":5895,"children":5896},{"style":416},[5897],{"type":33,"value":5898},"            appendToPrompt",{"type":28,"tag":409,"props":5900,"children":5901},{"style":569},[5902],{"type":33,"value":3044},{"type":28,"tag":409,"props":5904,"children":5905},{"style":428},[5906],{"type":33,"value":5907},"\"Your human opponent just scored a goal against you! \"",{"type":28,"tag":409,"props":5909,"children":5910},{"style":891},[5911],{"type":33,"value":5912}," +",{"type":28,"tag":409,"props":5914,"children":5915},{"style":569},[5916],{"type":33,"value":5917}," score_sentence, ",{"type":28,"tag":409,"props":5919,"children":5920},{"style":428},[5921],{"type":33,"value":5922},"\"user\"",{"type":28,"tag":409,"props":5924,"children":5925},{"style":569},[5926],{"type":33,"value":5927},");\n",{"type":28,"tag":409,"props":5929,"children":5930},{"class":411,"line":697},[5931],{"type":28,"tag":409,"props":5932,"children":5933},{"style":569},[5934],{"type":33,"value":5935},"        }\n",{"type":28,"tag":409,"props":5937,"children":5938},{"class":411,"line":719},[5939,5944],{"type":28,"tag":409,"props":5940,"children":5941},{"style":891},[5942],{"type":33,"value":5943},"        else",{"type":28,"tag":409,"props":5945,"children":5946},{"style":569},[5947],{"type":33,"value":5948}," {\n",{"type":28,"tag":409,"props":5950,"children":5951},{"class":411,"line":745},[5952,5956,5960,5965,5969,5973,5977],{"type":28,"tag":409,"props":5953,"children":5954},{"style":416},[5955],{"type":33,"value":5898},{"type":28,"tag":409,"props":5957,"children":5958},{"style":569},[5959],{"type":33,"value":3044},{"type":28,"tag":409,"props":5961,"children":5962},{"style":428},[5963],{"type":33,"value":5964},"\"You just scored a goal against the human player! \"",{"type":28,"tag":409,"props":5966,"children":5967},{"style":891},[5968],{"type":33,"value":5912},{"type":28,"tag":409,"props":5970,"children":5971},{"style":569},[5972],{"type":33,"value":5917},{"type":28,"tag":409,"props":5974,"children":5975},{"style":428},[5976],{"type":33,"value":5922},{"type":28,"tag":409,"props":5978,"children":5979},{"style":569},[5980],{"type":33,"value":5927},{"type":28,"tag":409,"props":5982,"children":5983},{"class":411,"line":1134},[5984],{"type":28,"tag":409,"props":5985,"children":5986},{"style":569},[5987],{"type":33,"value":5935},{"type":28,"tag":409,"props":5989,"children":5990},{"class":411,"line":3309},[5991],{"type":28,"tag":409,"props":5992,"children":5993},{"style":569},[5994],{"type":33,"value":5995},"    }\n",{"type":28,"tag":62,"props":5997,"children":5999},{"id":5998},"making-requests-and-handling-responses",[6000],{"type":33,"value":6001},"Making requests and handling responses",{"type":28,"tag":29,"props":6003,"children":6004},{},[6005],{"type":33,"value":6006},"The last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI's API.",{"type":28,"tag":399,"props":6008,"children":6010},{"code":6009,"language":4911,"meta":8,"className":4912,"style":8},"std::string message = response_json[\"choices\"][0][\"message\"][\"content\"];\n",[6011],{"type":28,"tag":390,"props":6012,"children":6013},{"__ignoreMap":8},[6014],{"type":28,"tag":409,"props":6015,"children":6016},{"class":411,"line":412},[6017,6021,6026,6030,6035,6040,6045,6050,6054,6059,6063,6067],{"type":28,"tag":409,"props":6018,"children":6019},{"style":3036},[6020],{"type":33,"value":5740},{"type":28,"tag":409,"props":6022,"children":6023},{"style":569},[6024],{"type":33,"value":6025},"::string message ",{"type":28,"tag":409,"props":6027,"children":6028},{"style":891},[6029],{"type":33,"value":894},{"type":28,"tag":409,"props":6031,"children":6032},{"style":569},[6033],{"type":33,"value":6034}," response_json[",{"type":28,"tag":409,"props":6036,"children":6037},{"style":428},[6038],{"type":33,"value":6039},"\"choices\"",{"type":28,"tag":409,"props":6041,"children":6042},{"style":569},[6043],{"type":33,"value":6044},"][",{"type":28,"tag":409,"props":6046,"children":6047},{"style":422},[6048],{"type":33,"value":6049},"0",{"type":28,"tag":409,"props":6051,"children":6052},{"style":569},[6053],{"type":33,"value":6044},{"type":28,"tag":409,"props":6055,"children":6056},{"style":428},[6057],{"type":33,"value":6058},"\"message\"",{"type":28,"tag":409,"props":6060,"children":6061},{"style":569},[6062],{"type":33,"value":6044},{"type":28,"tag":409,"props":6064,"children":6065},{"style":428},[6066],{"type":33,"value":4949},{"type":28,"tag":409,"props":6068,"children":6069},{"style":569},[6070],{"type":33,"value":6071},"];\n",{"type":28,"tag":29,"props":6073,"children":6074},{},[6075,6077,6083],{"type":33,"value":6076},"The ",{"type":28,"tag":390,"props":6078,"children":6080},{"className":6079},[],[6081],{"type":33,"value":6082},"LogToChatbox",{"type":33,"value":6084}," method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time.",{"type":28,"tag":399,"props":6086,"children":6088},{"code":6087,"language":4911,"meta":8,"className":4912,"style":8},"gameWrapper->LogToChatbox(messages[i], this->bot_name);\n",[6089],{"type":28,"tag":390,"props":6090,"children":6091},{"__ignoreMap":8},[6092],{"type":28,"tag":409,"props":6093,"children":6094},{"class":411,"line":412},[6095,6100,6104,6109,6113],{"type":28,"tag":409,"props":6096,"children":6097},{"style":569},[6098],{"type":33,"value":6099},"gameWrapper->",{"type":28,"tag":409,"props":6101,"children":6102},{"style":416},[6103],{"type":33,"value":6082},{"type":28,"tag":409,"props":6105,"children":6106},{"style":569},[6107],{"type":33,"value":6108},"(messages[i], ",{"type":28,"tag":409,"props":6110,"children":6111},{"style":5695},[6112],{"type":33,"value":5698},{"type":28,"tag":409,"props":6114,"children":6115},{"style":569},[6116],{"type":33,"value":6117},"->bot_name);\n",{"type":28,"tag":29,"props":6119,"children":6120},{},[6121,6123,6129],{"type":33,"value":6122},"That's it! The code isn't that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages (",{"type":28,"tag":390,"props":6124,"children":6126},{"className":6125},[],[6127],{"type":33,"value":6128},"\u003C/s>",{"type":33,"value":6130},"). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt.",{"type":28,"tag":55,"props":6132,"children":6134},{"id":6133},"rocket-league-botchat-ui",[6135],{"type":33,"value":6136},"Rocket League BotChat UI",{"type":28,"tag":29,"props":6138,"children":6139},{},[6140],{"type":33,"value":6141},"Most BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here's what the UI for Rocket League BotChat looks like:",{"type":28,"tag":29,"props":6143,"children":6144},{},[6145],{"type":28,"tag":1215,"props":6146,"children":6149},{"alt":6147,"src":6148},"Rocket League BotChat Plugin UI","/static/rlbc/rlbcui.png",[],{"type":28,"tag":62,"props":6151,"children":6153},{"id":6152},"system-prompt",[6154],{"type":33,"value":6155},"System prompt",{"type":28,"tag":29,"props":6157,"children":6158},{},[6159],{"type":33,"value":6160},"The system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here's the default prompt that I used:",{"type":28,"tag":399,"props":6162,"children":6164},{"code":6163,"language":4911,"meta":8,"className":4912,"style":8},"    std::string ai_player = \"You are an elite AI player in the car soccer game Rocket League. \";\n    std::string one_v_one = \"You are playing a 1v1 match against a human player. \";\n    std::string instructions = \"You will send short chat messages to your human opponent in response to what happens in the game. \";\n    std::string details = \"Respond to the human player with brief messages no more than 12 words long.\";\n    // initial system prompt\n    std::string initial_system_prompt = ai_player + one_v_one + instructions + details;\n",[6165],{"type":28,"tag":390,"props":6166,"children":6167},{"__ignoreMap":8},[6168,6195,6220,6245,6270,6278],{"type":28,"tag":409,"props":6169,"children":6170},{"class":411,"line":412},[6171,6176,6181,6185,6190],{"type":28,"tag":409,"props":6172,"children":6173},{"style":3036},[6174],{"type":33,"value":6175},"    std",{"type":28,"tag":409,"props":6177,"children":6178},{"style":569},[6179],{"type":33,"value":6180},"::string ai_player ",{"type":28,"tag":409,"props":6182,"children":6183},{"style":891},[6184],{"type":33,"value":894},{"type":28,"tag":409,"props":6186,"children":6187},{"style":428},[6188],{"type":33,"value":6189}," \"You are an elite AI player in the car soccer game Rocket League. \"",{"type":28,"tag":409,"props":6191,"children":6192},{"style":569},[6193],{"type":33,"value":6194},";\n",{"type":28,"tag":409,"props":6196,"children":6197},{"class":411,"line":439},[6198,6202,6207,6211,6216],{"type":28,"tag":409,"props":6199,"children":6200},{"style":3036},[6201],{"type":33,"value":6175},{"type":28,"tag":409,"props":6203,"children":6204},{"style":569},[6205],{"type":33,"value":6206},"::string one_v_one ",{"type":28,"tag":409,"props":6208,"children":6209},{"style":891},[6210],{"type":33,"value":894},{"type":28,"tag":409,"props":6212,"children":6213},{"style":428},[6214],{"type":33,"value":6215}," \"You are playing a 1v1 match against a human player. \"",{"type":28,"tag":409,"props":6217,"children":6218},{"style":569},[6219],{"type":33,"value":6194},{"type":28,"tag":409,"props":6221,"children":6222},{"class":411,"line":448},[6223,6227,6232,6236,6241],{"type":28,"tag":409,"props":6224,"children":6225},{"style":3036},[6226],{"type":33,"value":6175},{"type":28,"tag":409,"props":6228,"children":6229},{"style":569},[6230],{"type":33,"value":6231},"::string instructions ",{"type":28,"tag":409,"props":6233,"children":6234},{"style":891},[6235],{"type":33,"value":894},{"type":28,"tag":409,"props":6237,"children":6238},{"style":428},[6239],{"type":33,"value":6240}," \"You will send short chat messages to your human opponent in response to what happens in the game. \"",{"type":28,"tag":409,"props":6242,"children":6243},{"style":569},[6244],{"type":33,"value":6194},{"type":28,"tag":409,"props":6246,"children":6247},{"class":411,"line":631},[6248,6252,6257,6261,6266],{"type":28,"tag":409,"props":6249,"children":6250},{"style":3036},[6251],{"type":33,"value":6175},{"type":28,"tag":409,"props":6253,"children":6254},{"style":569},[6255],{"type":33,"value":6256},"::string details ",{"type":28,"tag":409,"props":6258,"children":6259},{"style":891},[6260],{"type":33,"value":894},{"type":28,"tag":409,"props":6262,"children":6263},{"style":428},[6264],{"type":33,"value":6265}," \"Respond to the human player with brief messages no more than 12 words long.\"",{"type":28,"tag":409,"props":6267,"children":6268},{"style":569},[6269],{"type":33,"value":6194},{"type":28,"tag":409,"props":6271,"children":6272},{"class":411,"line":653},[6273],{"type":28,"tag":409,"props":6274,"children":6275},{"style":5638},[6276],{"type":33,"value":6277},"    // initial system prompt\n",{"type":28,"tag":409,"props":6279,"children":6280},{"class":411,"line":675},[6281,6285,6290,6294,6299,6303,6308,6312,6317,6321],{"type":28,"tag":409,"props":6282,"children":6283},{"style":3036},[6284],{"type":33,"value":6175},{"type":28,"tag":409,"props":6286,"children":6287},{"style":569},[6288],{"type":33,"value":6289},"::string initial_system_prompt ",{"type":28,"tag":409,"props":6291,"children":6292},{"style":891},[6293],{"type":33,"value":894},{"type":28,"tag":409,"props":6295,"children":6296},{"style":569},[6297],{"type":33,"value":6298}," ai_player ",{"type":28,"tag":409,"props":6300,"children":6301},{"style":891},[6302],{"type":33,"value":5038},{"type":28,"tag":409,"props":6304,"children":6305},{"style":569},[6306],{"type":33,"value":6307}," one_v_one ",{"type":28,"tag":409,"props":6309,"children":6310},{"style":891},[6311],{"type":33,"value":5038},{"type":28,"tag":409,"props":6313,"children":6314},{"style":569},[6315],{"type":33,"value":6316}," instructions ",{"type":28,"tag":409,"props":6318,"children":6319},{"style":891},[6320],{"type":33,"value":5038},{"type":28,"tag":409,"props":6322,"children":6323},{"style":569},[6324],{"type":33,"value":6325}," details;\n",{"type":28,"tag":29,"props":6327,"children":6328},{},[6329,6331,6337,6339,6345],{"type":33,"value":6330},"The last part about ",{"type":28,"tag":390,"props":6332,"children":6334},{"className":6333},[],[6335],{"type":33,"value":6336},"no more than 12 words long",{"type":33,"value":6338}," was the most effective way of controlling the length responses from the LLM. I tried changing the ",{"type":28,"tag":390,"props":6340,"children":6342},{"className":6341},[],[6343],{"type":33,"value":6344},"max_output_len",{"type":33,"value":6346}," when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user. Changing the system prompt was a lot of fun to expirment with!",{"type":28,"tag":62,"props":6348,"children":6350},{"id":6349},"temperature-and-seed",[6351],{"type":33,"value":6352},"Temperature and Seed",{"type":28,"tag":29,"props":6354,"children":6355},{},[6356],{"type":33,"value":6357},"These values are included in the body of the request to the LLM, but I didn't have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn't really work.",{"type":28,"tag":62,"props":6359,"children":6360},{"id":4905},[6361],{"type":33,"value":6362},"Messages",{"type":28,"tag":29,"props":6364,"children":6365},{},[6366],{"type":33,"value":6367},"This section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the \"user\" (which are messages about game events) and the \"assistant\" (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt.",{"type":28,"tag":55,"props":6369,"children":6370},{"id":4185},[6371],{"type":33,"value":4188},{"type":28,"tag":6373,"props":6374,"children":6375},"rocket-league-bot-chat-video",{},[],{"type":28,"tag":29,"props":6377,"children":6378},{},[6379],{"type":33,"value":6380},"I used Blender's sequence editor to create a demo video for my contest submission. I don't edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here's how I approached creating the demo video for my project.",{"type":28,"tag":29,"props":6382,"children":6383},{},[6384],{"type":28,"tag":1215,"props":6385,"children":6388},{"alt":6386,"src":6387},"Blender video sequence editor UI used to create my project video","/static/rlbc/blender.png",[],{"type":28,"tag":69,"props":6390,"children":6391},{},[6392,6397,6409,6431,6436],{"type":28,"tag":73,"props":6393,"children":6394},{},[6395],{"type":33,"value":6396},"Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action",{"type":28,"tag":73,"props":6398,"children":6399},{},[6400,6402],{"type":33,"value":6401},"Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used ",{"type":28,"tag":764,"props":6403,"children":6406},{"href":6404,"rel":6405},"https://open.spotify.com/track/68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51",[768],[6407],{"type":33,"value":6408},"\"Dads in Space\" by Steven Walking",{"type":28,"tag":73,"props":6410,"children":6411},{},[6412,6414,6420,6422,6429],{"type":33,"value":6413},"Get stock Rocket League footage from YouTube with ",{"type":28,"tag":390,"props":6415,"children":6417},{"className":6416},[],[6418],{"type":33,"value":6419},"youtube-dl",{"type":33,"value":6421}," (this is an amazing tool!). I mostly used footage from the ",{"type":28,"tag":764,"props":6423,"children":6426},{"href":6424,"rel":6425},"https://www.youtube.com/watch?v=e1tqWldCYOI&pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D",[768],[6427],{"type":33,"value":6428},"RLCS 2023 Winter Major Trailer",{"type":33,"value":6430},". This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps.",{"type":28,"tag":73,"props":6432,"children":6433},{},[6434],{"type":33,"value":6435},"Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!",{"type":28,"tag":73,"props":6437,"children":6438},{},[6439],{"type":33,"value":6440},"Use ComfyUI to animate some of the images used in the contest description and use these in my video",{"type":28,"tag":29,"props":6442,"children":6443},{},[6444],{"type":28,"tag":1215,"props":6445,"children":6448},{"alt":6446,"src":6447},"ComfyUI workflow for animating images using img2vid model","/static/rlbc/comfyui.png",[],{"type":28,"tag":69,"props":6450,"children":6451},{},[6452],{"type":28,"tag":73,"props":6453,"children":6454},{},[6455],{"type":33,"value":6456},"Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice.",{"type":28,"tag":29,"props":6458,"children":6459},{},[6460],{"type":28,"tag":1215,"props":6461,"children":6464},{"alt":6462,"src":6463},"Eleven Labs Voice Generation Web UI","/static/rlbc/elevenlabs.png",[],{"type":28,"tag":29,"props":6466,"children":6467},{},[6468],{"type":28,"tag":764,"props":6469,"children":6471},{"href":6470},"#",[6472],{"type":33,"value":6473},"Embed twitter video here",{"type":28,"tag":55,"props":6475,"children":6476},{"id":4205},[6477],{"type":33,"value":4208},{"type":28,"tag":29,"props":6479,"children":6480},{},[6481,6483,6488],{"type":33,"value":6482},"This plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in \"user\" and \"assistant\" prompts getting out of order which breaks assertions on the ",{"type":28,"tag":390,"props":6484,"children":6486},{"className":6485},[],[6487],{"type":33,"value":2580},{"type":33,"value":6489}," repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM.",{"type":28,"tag":29,"props":6491,"children":6492},{},[6493],{"type":33,"value":6494},"There are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like \"shot on goal\" to avoid prompt messages getting out of order. This could also be addressed with more code logic.",{"type":28,"tag":29,"props":6496,"children":6497},{},[6498],{"type":33,"value":6499},"Prompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example.",{"type":28,"tag":29,"props":6501,"children":6502},{},[6503,6505,6511],{"type":33,"value":6504},"I faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like ",{"type":28,"tag":390,"props":6506,"children":6508},{"className":6507},[],[6509],{"type":33,"value":6510},"SendMessage Good shot, bot!",{"type":33,"value":6512},", for example.",{"type":28,"tag":55,"props":6514,"children":6515},{"id":4269},[6516],{"type":33,"value":4272},{"type":28,"tag":29,"props":6518,"children":6519},{},[6520],{"type":33,"value":6521},"Participating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a \"Rocket League Coach\" plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc.",{"type":28,"tag":29,"props":6523,"children":6524},{},[6525],{"type":33,"value":6526},"I think the gaming industry's adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I'm excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM.",{"type":28,"tag":29,"props":6528,"children":6529},{},[6530],{"type":33,"value":6531},"Thanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!",{"type":28,"tag":2344,"props":6533,"children":6534},{},[6535],{"type":33,"value":2348},{"title":8,"searchDepth":439,"depth":439,"links":6537},[6538,6539,6540,6541,6542,6546,6551,6556,6557,6558],{"id":2423,"depth":439,"text":2426},{"id":4368,"depth":439,"text":4371},{"id":2894,"depth":439,"text":4417},{"id":2905,"depth":439,"text":2908},{"id":4077,"depth":439,"text":4080,"children":6543},[6544,6545],{"id":5081,"depth":448,"text":5084},{"id":5395,"depth":448,"text":5398},{"id":1637,"depth":439,"text":4161,"children":6547},[6548,6549,6550],{"id":5609,"depth":448,"text":5612},{"id":5784,"depth":448,"text":5787},{"id":5998,"depth":448,"text":6001},{"id":6133,"depth":439,"text":6136,"children":6552},[6553,6554,6555],{"id":6152,"depth":448,"text":6155},{"id":6349,"depth":448,"text":6352},{"id":4905,"depth":448,"text":6362},{"id":4185,"depth":439,"text":4188},{"id":4205,"depth":439,"text":4208},{"id":4269,"depth":439,"text":4272},"content:2024:02:17:rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest.md","2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest.md","2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",1753036538998]