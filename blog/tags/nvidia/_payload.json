[{"data":1,"prerenderedAt":13702},["ShallowReactive",2],{"/blog/tags/nvidia/":3},[4,5567,5996,7476,9717,13078],{"_path":5,"_dir":6,"_draft":7,"_partial":7,"_locale":8,"title":9,"description":10,"date":11,"image":12,"tags":13,"draft":7,"comments":22,"body":23,"_type":5561,"_id":5562,"_source":5563,"_file":5564,"_stem":5565,"_extension":5566},"/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest","09",false,"","RedLM: My submission for the NVIDIA and LlamaIndex Developer Contest","RedLM is an AI-powered application for the study of China's greatest classical novel: Dream of the Red Chamber","2024-11-09","/static/redlm/title.png",[14,15,16,17,18,19,20,21],"nvidia","llama-index","ai","llm","rag","tensorrt-llm","chinese","redology",true,{"type":24,"children":25,"toc":5523},"root",[26,35,41,46,53,57,69,75,80,88,93,113,119,124,131,136,140,148,154,159,164,178,184,189,197,202,207,215,237,246,251,444,449,568,573,602,638,644,649,657,670,678,683,691,702,715,1324,1330,1335,2138,2151,2157,2162,2170,2188,2196,2210,2216,2221,2496,2516,2793,2801,2806,2819,2825,2830,2844,2868,2876,2881,2886,2891,2899,2904,2952,2971,3000,3005,3053,3058,3063,3071,3076,3084,3089,3095,3100,3105,3113,3118,3159,3167,3172,3177,3185,3190,3249,3254,3561,3566,3571,3576,3581,3970,3975,3981,3986,3991,4004,4012,4020,4026,4031,4163,4173,4187,4216,4662,4674,4680,4692,4722,4734,4742,4747,4752,4760,4765,4773,4778,4786,4792,4804,4812,4826,4834,4839,4845,4850,4858,4863,4870,4875,4881,4889,4917,4922,4936,4944,4957,4965,4978,4986,4991,4997,5002,5018,5054,5068,5078,5083,5093,5098,5107,5112,5126,5147,5156,5176,5191,5204,5212,5218,5223,5228,5236,5241,5247,5251,5256,5286,5291,5299,5304,5312,5317,5323,5328,5336,5341,5349,5384,5390,5395,5403,5408,5413,5427,5435,5440,5448,5462,5470,5475,5483,5488,5493,5501,5509,5517],{"type":27,"tag":28,"props":29,"children":31},"element","h2",{"id":30},"tddr",[32],{"type":33,"value":34},"text","td;dr",{"type":27,"tag":36,"props":37,"children":38},"p",{},[39],{"type":33,"value":40},"RedLM is a new way to study art and literature powered by artificial intelligence. It is an application that applies LLMs to the study of one of China’s most famous literary works: Dream of the Red Chamber. It uses leading language and vision models from Chinese AI groups including Alibaba’s Qwen, Baichuan Intelligence Technology and 01.AI. RedLM uses tools, techniques and services from NVIDIA and LlamaIndex including NVIDIA NIMs, Retrieval Augmented Generation and Multi-Modal RAG with vision language models. This project is my submission for the NVIDIA and LlamaIndex Developer Contest.",{"type":27,"tag":36,"props":42,"children":43},{},[44],{"type":33,"value":45},"This article will cover how I built the project, challenges I faced and some of the lessons I learned while working with NVIDIA and LlamaIndex technologies.",{"type":27,"tag":47,"props":48,"children":50},"h3",{"id":49},"links",[51],{"type":33,"value":52},"Links",{"type":27,"tag":54,"props":55,"children":56},"red-lm-tweet",{},[],{"type":27,"tag":36,"props":58,"children":59},{},[60],{"type":27,"tag":61,"props":62,"children":66},"a",{"href":63,"rel":64},"https://github.com/briancaffey/RedLM",[65],"nofollow",[67],{"type":33,"value":68},"RedLM GitHub repository",{"type":27,"tag":28,"props":70,"children":72},{"id":71},"what-is-redlm",[73],{"type":33,"value":74},"What is RedLM?",{"type":27,"tag":36,"props":76,"children":77},{},[78],{"type":33,"value":79},"RedLM is a combination of the word “Red” and LM, an abbreviation for “language model”. Dream of the Red Chamber is such an important book in Chinese literature that it has its own field of study called 红学 (literally “the study of red”), or Redology. So, RedLM is an application that uses language models for the study of Redology.",{"type":27,"tag":36,"props":81,"children":82},{},[83],{"type":27,"tag":84,"props":85,"children":87},"img",{"alt":86,"src":12},"RedLM",[],{"type":27,"tag":36,"props":89,"children":90},{},[91],{"type":33,"value":92},"In this project I focused on three applications of language models:",{"type":27,"tag":94,"props":95,"children":96},"ol",{},[97,103,108],{"type":27,"tag":98,"props":99,"children":100},"li",{},[101],{"type":33,"value":102},"Summary and translation of the source text",{"type":27,"tag":98,"props":104,"children":105},{},[106],{"type":33,"value":107},"A Q&A bot that can answer questions about the book providing references to the specific paragraphs used to give answers",{"type":27,"tag":98,"props":109,"children":110},{},[111],{"type":33,"value":112},"An image-based Q&A bot that can answer questions about sections of paintings that depict scenes from each of the book’s chapters.",{"type":27,"tag":28,"props":114,"children":116},{"id":115},"notebooklm",[117],{"type":33,"value":118},"NotebookLM",{"type":27,"tag":36,"props":120,"children":121},{},[122],{"type":33,"value":123},"I used this article to create a \"Deep Dive\" podcast episode for RedLM using Google's NotebookLM.",{"type":27,"tag":36,"props":125,"children":126},{},[127],{"type":27,"tag":84,"props":128,"children":130},{"alt":118,"src":129},"/static/redlm/notebooklm.png",[],{"type":27,"tag":36,"props":132,"children":133},{},[134],{"type":33,"value":135},"You can listen to this podcast episode here:",{"type":27,"tag":137,"props":138,"children":139},"red-lm-deep-dive-video",{},[],{"type":27,"tag":36,"props":141,"children":142},{},[143],{"type":27,"tag":84,"props":144,"children":147},{"alt":145,"src":146},"RedLM Deep Dive","/static/redlm/redlm_deep_dive.png",[],{"type":27,"tag":28,"props":149,"children":151},{"id":150},"how-i-built-redlm",[152],{"type":33,"value":153},"How I built RedLM",{"type":27,"tag":36,"props":155,"children":156},{},[157],{"type":33,"value":158},"RedLM consists of two parts: a web UI built with Vue 3 using the Nuxt Framework and a backend API built with Python, FastAPI and LlamaIndex. There are lots of great tools for building full-stack AI applications such as Gradio and Streamlit, but I wanted to build with the web tools that I’m most familiar with and that provide the most flexibility. These frameworks (Nuxt and FastAPI) are simple and effective and they allowed me to develop quickly.",{"type":27,"tag":36,"props":160,"children":161},{},[162],{"type":33,"value":163},"Most of the code for this project was written by AI. I used OpenAI’s ChatGPT 4o, Anthropic’s Claude 3.5 Sonnet and 01.AI’s Yi-1.5-9B-Chat model. In my development process with AI, I prompted for one logical piece of the application at a time, such as one API route, one Vue component, one pinia store or one utility function, for example. In this article I'll share some of the prompts I used in my development workflow.",{"type":27,"tag":36,"props":165,"children":166},{},[167,169,176],{"type":33,"value":168},"This project embraces a hybrid AI inference model, meaning that the AI inference can be done either on local RTX PCs or using NVIDIA’s Cloud APIs from ",{"type":27,"tag":170,"props":171,"children":173},"code",{"className":172},[],[174],{"type":33,"value":175},"build.nvidia.com",{"type":33,"value":177}," depending on configuration via environment variables. I used PCs with NVIDIA GeForce RTX 4090 GPUs to do inference with language and vision models, and with a change of configuration, I was able to do similar inference using NVIDIA’s API endpoints. This allowed me to develop the project both on powerful RTX desktop workstations and Mac laptops.",{"type":27,"tag":28,"props":179,"children":181},{"id":180},"translating-dream-of-the-red-chamber-with-tensorrt-llm",[182],{"type":33,"value":183},"Translating Dream of the Red Chamber with TensorRT-LLM",{"type":27,"tag":36,"props":185,"children":186},{},[187],{"type":33,"value":188},"Translation is often mentioned as one of the capabilities of bilingual LLMs from China. I wanted to try translating this book from Chinese to English, but I also wanted to better understand the meaning of the original text written in vernacular Chinese. Written vernacular Chinese is essentially a form of Chinese that closely resembles the way Chinese was spoken in imperial China by common people. The use of vernacular Chinese (Baihua) in literary works marked a significant cultural shift that started to make literature and education more accessible. Before the emergence of written vernacular Chinese, Chinese literature was dominated by Classical Chinese (Wenyanwen) which is a more concise, ambiguous and specialized for of languages that assumes an understanding of ancient texts and Confucian classics. The difference between vernacular Chinese and modern Mandarin Chinese is somewhat analogous to the different between Shakespearian English (Early Modern English) and Modern English.",{"type":27,"tag":36,"props":190,"children":191},{},[192],{"type":27,"tag":84,"props":193,"children":196},{"alt":194,"src":195},"Baihua, Mandarin and English","/static/redlm/translations.png",[],{"type":27,"tag":36,"props":198,"children":199},{},[200],{"type":33,"value":201},"Chinese large language models are well versed in Classical Chinese, written Chinese vernacular and modern Mandarin Chinese. I decided to rewrite the original vernacular text in simple, modern Mandarin Chinese and then using this new modern Mandarin version, translate the story into English.",{"type":27,"tag":36,"props":203,"children":204},{},[205],{"type":33,"value":206},"Dream of the Red Chamber is a large book. It is composed of over 800,000 Chinese characters, using 4303 unique Chinese characters. It has 120 chapters and a total of 3996 paragraphs. Here is a histogram showing the number of characters per paragraph.",{"type":27,"tag":36,"props":208,"children":209},{},[210],{"type":27,"tag":84,"props":211,"children":214},{"alt":212,"src":213},"Paragraph lengths","/static/redlm/paragraphs.png",[],{"type":27,"tag":36,"props":216,"children":217},{},[218,220,227,229,235],{"type":33,"value":219},"I rented a large multi-GPU instance from AWS using some of the credits I get as a member of the AWS Community Builders program. The g5.12xlarge instance I selected has 4 A10G Tensor Core GPUs. The TensorRT-LLM LLM API is a relatively new part of the TensorRT-LLM library. It provides a very simple, high-level interface for doing inference. Following the ",{"type":27,"tag":61,"props":221,"children":224},{"href":222,"rel":223},"https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_inference_distributed.html",[65],[225],{"type":33,"value":226},"LLM Generate Distributed example",{"type":33,"value":228}," from the TensorRT-LLM documentation, I was able to translate the entire book into simple Mandarin and then from Mandarin into English in about an hour and 15 minutes. The ",{"type":27,"tag":170,"props":230,"children":232},{"className":231},[],[233],{"type":33,"value":234},"tensor_parallel_size",{"type":33,"value":236}," option in the LLM API allows for distributed inference, this meant that up to 4 paragraphs could be translated at the same time on different GPUs on the same EC2 instance.",{"type":27,"tag":238,"props":239,"children":241},"pre",{"code":240},"Translating: data/book/22.json\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:15\u003C00:00,  2.41it/s]\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:24\u003C00:00,  1.54it/s]\nTranslated: data/book/22.json\nTranslating: data/book/114.json\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:11\u003C00:00,  1.81it/s]\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:12\u003C00:00,  1.58it/s]\nTranslated: data/book/114.json\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n\nreal    74m1.578s\nuser    0m45.936s\nsys 0m36.283s\n",[242],{"type":27,"tag":170,"props":243,"children":244},{"__ignoreMap":8},[245],{"type":33,"value":240},{"type":27,"tag":36,"props":247,"children":248},{},[249],{"type":33,"value":250},"Getting good results required a bit of experimentation with parameters. The LLM API makes this very easy. The following code configures settings and builds the inference engine that can be used for doing completions:",{"type":27,"tag":238,"props":252,"children":256},{"code":253,"language":254,"meta":8,"className":255,"style":8},"sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=256)\nbuild_config = BuildConfig(max_seq_len=2048)\nllm = LLM(model=MODEL, build_config=build_config, tensor_parallel_size=4)\n","python","language-python shiki shiki-themes github-light github-dark monokai",[257],{"type":27,"tag":170,"props":258,"children":259},{"__ignoreMap":8},[260,341,377],{"type":27,"tag":261,"props":262,"children":265},"span",{"class":263,"line":264},"line",1,[266,272,278,283,289,293,299,304,309,313,318,322,327,331,336],{"type":27,"tag":261,"props":267,"children":269},{"style":268},"--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#F8F8F2",[270],{"type":33,"value":271},"sampling_params ",{"type":27,"tag":261,"props":273,"children":275},{"style":274},"--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#F92672",[276],{"type":33,"value":277},"=",{"type":27,"tag":261,"props":279,"children":280},{"style":268},[281],{"type":33,"value":282}," SamplingParams(",{"type":27,"tag":261,"props":284,"children":286},{"style":285},"--shiki-default:#E36209;--shiki-dark:#FFAB70;--shiki-sepia:#FD971F;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[287],{"type":33,"value":288},"temperature",{"type":27,"tag":261,"props":290,"children":291},{"style":274},[292],{"type":33,"value":277},{"type":27,"tag":261,"props":294,"children":296},{"style":295},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#AE81FF",[297],{"type":33,"value":298},"0.7",{"type":27,"tag":261,"props":300,"children":301},{"style":268},[302],{"type":33,"value":303},", ",{"type":27,"tag":261,"props":305,"children":306},{"style":285},[307],{"type":33,"value":308},"top_p",{"type":27,"tag":261,"props":310,"children":311},{"style":274},[312],{"type":33,"value":277},{"type":27,"tag":261,"props":314,"children":315},{"style":295},[316],{"type":33,"value":317},"0.95",{"type":27,"tag":261,"props":319,"children":320},{"style":268},[321],{"type":33,"value":303},{"type":27,"tag":261,"props":323,"children":324},{"style":285},[325],{"type":33,"value":326},"max_tokens",{"type":27,"tag":261,"props":328,"children":329},{"style":274},[330],{"type":33,"value":277},{"type":27,"tag":261,"props":332,"children":333},{"style":295},[334],{"type":33,"value":335},"256",{"type":27,"tag":261,"props":337,"children":338},{"style":268},[339],{"type":33,"value":340},")\n",{"type":27,"tag":261,"props":342,"children":344},{"class":263,"line":343},2,[345,350,354,359,364,368,373],{"type":27,"tag":261,"props":346,"children":347},{"style":268},[348],{"type":33,"value":349},"build_config ",{"type":27,"tag":261,"props":351,"children":352},{"style":274},[353],{"type":33,"value":277},{"type":27,"tag":261,"props":355,"children":356},{"style":268},[357],{"type":33,"value":358}," BuildConfig(",{"type":27,"tag":261,"props":360,"children":361},{"style":285},[362],{"type":33,"value":363},"max_seq_len",{"type":27,"tag":261,"props":365,"children":366},{"style":274},[367],{"type":33,"value":277},{"type":27,"tag":261,"props":369,"children":370},{"style":295},[371],{"type":33,"value":372},"2048",{"type":27,"tag":261,"props":374,"children":375},{"style":268},[376],{"type":33,"value":340},{"type":27,"tag":261,"props":378,"children":380},{"class":263,"line":379},3,[381,386,390,395,400,404,409,413,418,422,427,431,435,440],{"type":27,"tag":261,"props":382,"children":383},{"style":268},[384],{"type":33,"value":385},"llm ",{"type":27,"tag":261,"props":387,"children":388},{"style":274},[389],{"type":33,"value":277},{"type":27,"tag":261,"props":391,"children":392},{"style":268},[393],{"type":33,"value":394}," LLM(",{"type":27,"tag":261,"props":396,"children":397},{"style":285},[398],{"type":33,"value":399},"model",{"type":27,"tag":261,"props":401,"children":402},{"style":274},[403],{"type":33,"value":277},{"type":27,"tag":261,"props":405,"children":406},{"style":295},[407],{"type":33,"value":408},"MODEL",{"type":27,"tag":261,"props":410,"children":411},{"style":268},[412],{"type":33,"value":303},{"type":27,"tag":261,"props":414,"children":415},{"style":285},[416],{"type":33,"value":417},"build_config",{"type":27,"tag":261,"props":419,"children":420},{"style":274},[421],{"type":33,"value":277},{"type":27,"tag":261,"props":423,"children":424},{"style":268},[425],{"type":33,"value":426},"build_config, ",{"type":27,"tag":261,"props":428,"children":429},{"style":285},[430],{"type":33,"value":234},{"type":27,"tag":261,"props":432,"children":433},{"style":274},[434],{"type":33,"value":277},{"type":27,"tag":261,"props":436,"children":437},{"style":295},[438],{"type":33,"value":439},"4",{"type":27,"tag":261,"props":441,"children":442},{"style":268},[443],{"type":33,"value":340},{"type":27,"tag":36,"props":445,"children":446},{},[447],{"type":33,"value":448},"I used the following prompts to rewrite each paragraph of the original text in simple, modern Mandarin Chinese:",{"type":27,"tag":238,"props":450,"children":452},{"code":451,"language":254,"meta":8,"className":255,"style":8},"bai_prompts = [\n    # Here are examples of how to rewrite Chinese vernacular into simple modern Mandarin.\\n\\nChinese vernacular:\\n\\n{p}\\n\\nSimple modern Mandarin\n    f\"以下是如何将中国白话改写为简单的现代普通话的示例。\\n\\n中文白话：\\n\\n{p}\\n\\n简单的现代普通话：\\n\\n\"\n    for p in flat_bai\n]\n",[453],{"type":27,"tag":170,"props":454,"children":455},{"__ignoreMap":8},[456,473,482,535,559],{"type":27,"tag":261,"props":457,"children":458},{"class":263,"line":264},[459,464,468],{"type":27,"tag":261,"props":460,"children":461},{"style":268},[462],{"type":33,"value":463},"bai_prompts ",{"type":27,"tag":261,"props":465,"children":466},{"style":274},[467],{"type":33,"value":277},{"type":27,"tag":261,"props":469,"children":470},{"style":268},[471],{"type":33,"value":472}," [\n",{"type":27,"tag":261,"props":474,"children":475},{"class":263,"line":343},[476],{"type":27,"tag":261,"props":477,"children":479},{"style":478},"--shiki-default:#6A737D;--shiki-dark:#6A737D;--shiki-sepia:#88846F",[480],{"type":33,"value":481},"    # Here are examples of how to rewrite Chinese vernacular into simple modern Mandarin.\\n\\nChinese vernacular:\\n\\n{p}\\n\\nSimple modern Mandarin\n",{"type":27,"tag":261,"props":483,"children":484},{"class":263,"line":379},[485,491,497,502,507,512,516,521,526,530],{"type":27,"tag":261,"props":486,"children":488},{"style":487},"--shiki-default:#D73A49;--shiki-dark:#F97583;--shiki-sepia:#66D9EF;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[489],{"type":33,"value":490},"    f",{"type":27,"tag":261,"props":492,"children":494},{"style":493},"--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#E6DB74",[495],{"type":33,"value":496},"\"以下是如何将中国白话改写为简单的现代普通话的示例。",{"type":27,"tag":261,"props":498,"children":499},{"style":295},[500],{"type":33,"value":501},"\\n\\n",{"type":27,"tag":261,"props":503,"children":504},{"style":493},[505],{"type":33,"value":506},"中文白话：",{"type":27,"tag":261,"props":508,"children":509},{"style":295},[510],{"type":33,"value":511},"\\n\\n{",{"type":27,"tag":261,"props":513,"children":514},{"style":268},[515],{"type":33,"value":36},{"type":27,"tag":261,"props":517,"children":518},{"style":295},[519],{"type":33,"value":520},"}\\n\\n",{"type":27,"tag":261,"props":522,"children":523},{"style":493},[524],{"type":33,"value":525},"简单的现代普通话：",{"type":27,"tag":261,"props":527,"children":528},{"style":295},[529],{"type":33,"value":501},{"type":27,"tag":261,"props":531,"children":532},{"style":493},[533],{"type":33,"value":534},"\"\n",{"type":27,"tag":261,"props":536,"children":538},{"class":263,"line":537},4,[539,544,549,554],{"type":27,"tag":261,"props":540,"children":541},{"style":274},[542],{"type":33,"value":543},"    for",{"type":27,"tag":261,"props":545,"children":546},{"style":268},[547],{"type":33,"value":548}," p ",{"type":27,"tag":261,"props":550,"children":551},{"style":274},[552],{"type":33,"value":553},"in",{"type":27,"tag":261,"props":555,"children":556},{"style":268},[557],{"type":33,"value":558}," flat_bai\n",{"type":27,"tag":261,"props":560,"children":562},{"class":263,"line":561},5,[563],{"type":27,"tag":261,"props":564,"children":565},{"style":268},[566],{"type":33,"value":567},"]\n",{"type":27,"tag":36,"props":569,"children":570},{},[571],{"type":33,"value":572},"It was difficult to get good results consistently. Here are some observations I had:",{"type":27,"tag":574,"props":575,"children":576},"ul",{},[577,582,587,592,597],{"type":27,"tag":98,"props":578,"children":579},{},[580],{"type":33,"value":581},"Some of the translated paragraphs were perfect",{"type":27,"tag":98,"props":583,"children":584},{},[585],{"type":33,"value":586},"some translated paragraphs would randomly hallucinate the same phrase over and over again",{"type":27,"tag":98,"props":588,"children":589},{},[590],{"type":33,"value":591},"Some requests to translate text to English would reply in Mandarin Chinese rather than in English",{"type":27,"tag":98,"props":593,"children":594},{},[595],{"type":33,"value":596},"Sometimes I would even see computer code generated when asking for a translation",{"type":27,"tag":98,"props":598,"children":599},{},[600],{"type":33,"value":601},"The names of characters were sometimes translated inconsistently, sometimes literally and sometimes using differing versions of pinyin, the Romanization system for transcribing the sounds of Mandarin Chinese",{"type":27,"tag":36,"props":603,"children":604},{},[605,607,613,615,621,623,629,631,636],{"type":33,"value":606},"I found that ChatGPT 4o could handle any Chinese translation task flawlessly, but the ",{"type":27,"tag":170,"props":608,"children":610},{"className":609},[],[611],{"type":33,"value":612},"Qwen2-7B",{"type":33,"value":614}," model I used had mixed results! The change that I made that seemed to have the biggest impact on translation quality was setting ",{"type":27,"tag":170,"props":616,"children":618},{"className":617},[],[619],{"type":33,"value":620},"*max_tokens*=256",{"type":33,"value":622}," in ",{"type":27,"tag":170,"props":624,"children":626},{"className":625},[],[627],{"type":33,"value":628},"SamplingParams",{"type":33,"value":630},". I probably could have used a dynamic value for ",{"type":27,"tag":170,"props":632,"children":634},{"className":633},[],[635],{"type":33,"value":326},{"type":33,"value":637}," based on the size of the current paragraph being translated. I also would have like to set up side-by-side comparisons of translations using different sized models, but rather than spend time and AWS credits on optimizing translation with TensorRT-LLM, I wanted to focus on the main part of this project: retrieval augmented generation (RAG) with LlamaIndex.",{"type":27,"tag":28,"props":639,"children":641},{"id":640},"building-qa-bots-with-rag-using-llamaindex",[642],{"type":33,"value":643},"Building Q&A bots with RAG using LlamaIndex",{"type":27,"tag":36,"props":645,"children":646},{},[647],{"type":33,"value":648},"My primary objective with this project was to implement a simple chat bot that responds to questions about the book with references to the specific paragraphs used in the response. The following shows images of the UI I built with one of the examples I included in the video I made for this project.",{"type":27,"tag":36,"props":650,"children":651},{},[652],{"type":27,"tag":84,"props":653,"children":656},{"alt":654,"src":655},"RAG Example","/static/redlm/rag_example.png",[],{"type":27,"tag":36,"props":658,"children":659},{},[660,662,668],{"type":33,"value":661},"I haven't read that much of the book before working on this project, but I have read a lot ",{"type":27,"tag":663,"props":664,"children":665},"em",{},[666],{"type":33,"value":667},"about",{"type":33,"value":669}," this book's characters, major themes and plot. This Q&A bot was a very interesting entrypoint to explore specific passages of the book starting with questions coming from my knowledge about the book. The question in the screenshots above is: “What does Jia Baoyu’s father think about him?” The response includes references to paragraphs where Jia Zheng (Baoyu’s father) is discussing his son. I was pretty amazed that the RAG query was able to pull out these two paragraphs.",{"type":27,"tag":36,"props":671,"children":672},{},[673],{"type":27,"tag":663,"props":674,"children":675},{},[676],{"type":33,"value":677},"In Dream of the Red Chamber, the relationship between protagonist Jia Baoyu and his father, Jia Zheng, is complex and fraught with tension. Jia Zheng, a strict, traditional Confucian patriarch, embodies values of discipline, scholarly rigor, and duty. He expects his son to excel in his studies and uphold the family’s honor by pursuing an official career in government. Baoyu, however, is sensitive, imaginative, and inclined toward poetry and the company of women, especially his cousins Lin Daiyu and Xue Baochai. This preference clashes with Jia Zheng’s expectations, leading to frequent misunderstandings and disappointment.",{"type":27,"tag":36,"props":679,"children":680},{},[681],{"type":33,"value":682},"By default, LlamaIndex uses cosine similarity as the distance metric for finding the vectors representing the documents (paragraphs) that are “closest” to the vector representing the user query. This is the central mechanism by which RAG works. LlamaIndex provides an abstraction of this process, hiding the implementation details and allowing rapid development of retrieval systems.",{"type":27,"tag":36,"props":684,"children":685},{},[686],{"type":27,"tag":84,"props":687,"children":690},{"alt":688,"src":689},"Cosine Similarity","/static/redlm/cosine_similarity.png",[],{"type":27,"tag":36,"props":692,"children":693},{},[694,696],{"type":33,"value":695},"Source: ",{"type":27,"tag":61,"props":697,"children":700},{"href":698,"rel":699},"https://medium.com/@kbdhunga/a-beginners-guide-to-similarity-search-vector-indexing-part-one-9cf5e9171976",[65],[701],{"type":33,"value":698},{"type":27,"tag":36,"props":703,"children":704},{},[705,707,713],{"type":33,"value":706},"Here is some of the code I wrote for the text-based Q&A bot using LlamaIndex’s ",{"type":27,"tag":170,"props":708,"children":710},{"className":709},[],[711],{"type":33,"value":712},"CustomQueryEngine",{"type":33,"value":714}," class to fetch the nodes from which I get the referenced paragraph text, chapter number and paragraph number.",{"type":27,"tag":238,"props":716,"children":718},{"code":717,"language":254,"meta":8,"className":255,"style":8},"class QAndAQueryEngine(CustomQueryEngine):\n    \"\"\"RAG Completion Query Engine optimized for Q&A\"\"\"\n\n    retriever: BaseRetriever\n    response_synthesizer: BaseSynthesizer\n    llm: OpenAILike\n    qa_prompt: PromptTemplate\n\n    def custom_query(self, query_str: str):\n        nodes = self.retriever.retrieve(query_str)\n        metadata = []\n        # Collect the metadata into a list of dicts so that it can be sent to UI for references\n        for node in nodes:\n            metadata_dict = {}\n            node_metadata = node.node.metadata\n            metadata_dict[\"content\"] = node.node.text\n            metadata_dict[\"chapter\"] = int(node_metadata.get(\"chapter\"))\n            metadata_dict[\"paragraph\"] = int(node_metadata.get(\"paragraph\"))\n\n            metadata.append(metadata_dict)\n\n        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n        response = self.llm.chat(\n            [\n                ChatMessage(\n                    role=\"user\",\n                    content=q_and_a_prompt.format( # the English and Chinese prompt templates are discussed below\n                        context_str=context_str, query_str=query_str\n                    ),\n                )\n            ]\n        )\n\n        return response, metadata\n",[719],{"type":27,"tag":170,"props":720,"children":721},{"__ignoreMap":8},[722,751,759,767,775,783,792,801,809,858,882,900,909,932,950,968,996,1036,1073,1081,1090,1098,1149,1171,1180,1189,1212,1235,1266,1275,1284,1293,1302,1310],{"type":27,"tag":261,"props":723,"children":724},{"class":263,"line":264},[725,730,736,741,746],{"type":27,"tag":261,"props":726,"children":727},{"style":487},[728],{"type":33,"value":729},"class",{"type":27,"tag":261,"props":731,"children":733},{"style":732},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E;--shiki-default-text-decoration:inherit;--shiki-dark-text-decoration:inherit;--shiki-sepia-text-decoration:underline",[734],{"type":33,"value":735}," QAndAQueryEngine",{"type":27,"tag":261,"props":737,"children":738},{"style":268},[739],{"type":33,"value":740},"(",{"type":27,"tag":261,"props":742,"children":744},{"style":743},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic;--shiki-default-text-decoration:inherit;--shiki-dark-text-decoration:inherit;--shiki-sepia-text-decoration:underline",[745],{"type":33,"value":712},{"type":27,"tag":261,"props":747,"children":748},{"style":268},[749],{"type":33,"value":750},"):\n",{"type":27,"tag":261,"props":752,"children":753},{"class":263,"line":343},[754],{"type":27,"tag":261,"props":755,"children":756},{"style":493},[757],{"type":33,"value":758},"    \"\"\"RAG Completion Query Engine optimized for Q&A\"\"\"\n",{"type":27,"tag":261,"props":760,"children":761},{"class":263,"line":379},[762],{"type":27,"tag":261,"props":763,"children":764},{"emptyLinePlaceholder":22},[765],{"type":33,"value":766},"\n",{"type":27,"tag":261,"props":768,"children":769},{"class":263,"line":537},[770],{"type":27,"tag":261,"props":771,"children":772},{"style":268},[773],{"type":33,"value":774},"    retriever: BaseRetriever\n",{"type":27,"tag":261,"props":776,"children":777},{"class":263,"line":561},[778],{"type":27,"tag":261,"props":779,"children":780},{"style":268},[781],{"type":33,"value":782},"    response_synthesizer: BaseSynthesizer\n",{"type":27,"tag":261,"props":784,"children":786},{"class":263,"line":785},6,[787],{"type":27,"tag":261,"props":788,"children":789},{"style":268},[790],{"type":33,"value":791},"    llm: OpenAILike\n",{"type":27,"tag":261,"props":793,"children":795},{"class":263,"line":794},7,[796],{"type":27,"tag":261,"props":797,"children":798},{"style":268},[799],{"type":33,"value":800},"    qa_prompt: PromptTemplate\n",{"type":27,"tag":261,"props":802,"children":804},{"class":263,"line":803},8,[805],{"type":27,"tag":261,"props":806,"children":807},{"emptyLinePlaceholder":22},[808],{"type":33,"value":766},{"type":27,"tag":261,"props":810,"children":812},{"class":263,"line":811},9,[813,818,824,828,834,838,843,848,854],{"type":27,"tag":261,"props":814,"children":815},{"style":487},[816],{"type":33,"value":817},"    def",{"type":27,"tag":261,"props":819,"children":821},{"style":820},"--shiki-default:#6F42C1;--shiki-dark:#B392F0;--shiki-sepia:#A6E22E",[822],{"type":33,"value":823}," custom_query",{"type":27,"tag":261,"props":825,"children":826},{"style":268},[827],{"type":33,"value":740},{"type":27,"tag":261,"props":829,"children":831},{"style":830},"--shiki-default:#24292E;--shiki-dark:#E1E4E8;--shiki-sepia:#FD971F;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[832],{"type":33,"value":833},"self",{"type":27,"tag":261,"props":835,"children":836},{"style":268},[837],{"type":33,"value":303},{"type":27,"tag":261,"props":839,"children":840},{"style":830},[841],{"type":33,"value":842},"query_str",{"type":27,"tag":261,"props":844,"children":845},{"style":268},[846],{"type":33,"value":847},": ",{"type":27,"tag":261,"props":849,"children":851},{"style":850},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#66D9EF;--shiki-default-font-style:inherit;--shiki-dark-font-style:inherit;--shiki-sepia-font-style:italic",[852],{"type":33,"value":853},"str",{"type":27,"tag":261,"props":855,"children":856},{"style":268},[857],{"type":33,"value":750},{"type":27,"tag":261,"props":859,"children":861},{"class":263,"line":860},10,[862,867,871,877],{"type":27,"tag":261,"props":863,"children":864},{"style":268},[865],{"type":33,"value":866},"        nodes ",{"type":27,"tag":261,"props":868,"children":869},{"style":274},[870],{"type":33,"value":277},{"type":27,"tag":261,"props":872,"children":874},{"style":873},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#FD971F",[875],{"type":33,"value":876}," self",{"type":27,"tag":261,"props":878,"children":879},{"style":268},[880],{"type":33,"value":881},".retriever.retrieve(query_str)\n",{"type":27,"tag":261,"props":883,"children":885},{"class":263,"line":884},11,[886,891,895],{"type":27,"tag":261,"props":887,"children":888},{"style":268},[889],{"type":33,"value":890},"        metadata ",{"type":27,"tag":261,"props":892,"children":893},{"style":274},[894],{"type":33,"value":277},{"type":27,"tag":261,"props":896,"children":897},{"style":268},[898],{"type":33,"value":899}," []\n",{"type":27,"tag":261,"props":901,"children":903},{"class":263,"line":902},12,[904],{"type":27,"tag":261,"props":905,"children":906},{"style":478},[907],{"type":33,"value":908},"        # Collect the metadata into a list of dicts so that it can be sent to UI for references\n",{"type":27,"tag":261,"props":910,"children":912},{"class":263,"line":911},13,[913,918,923,927],{"type":27,"tag":261,"props":914,"children":915},{"style":274},[916],{"type":33,"value":917},"        for",{"type":27,"tag":261,"props":919,"children":920},{"style":268},[921],{"type":33,"value":922}," node ",{"type":27,"tag":261,"props":924,"children":925},{"style":274},[926],{"type":33,"value":553},{"type":27,"tag":261,"props":928,"children":929},{"style":268},[930],{"type":33,"value":931}," nodes:\n",{"type":27,"tag":261,"props":933,"children":935},{"class":263,"line":934},14,[936,941,945],{"type":27,"tag":261,"props":937,"children":938},{"style":268},[939],{"type":33,"value":940},"            metadata_dict ",{"type":27,"tag":261,"props":942,"children":943},{"style":274},[944],{"type":33,"value":277},{"type":27,"tag":261,"props":946,"children":947},{"style":268},[948],{"type":33,"value":949}," {}\n",{"type":27,"tag":261,"props":951,"children":953},{"class":263,"line":952},15,[954,959,963],{"type":27,"tag":261,"props":955,"children":956},{"style":268},[957],{"type":33,"value":958},"            node_metadata ",{"type":27,"tag":261,"props":960,"children":961},{"style":274},[962],{"type":33,"value":277},{"type":27,"tag":261,"props":964,"children":965},{"style":268},[966],{"type":33,"value":967}," node.node.metadata\n",{"type":27,"tag":261,"props":969,"children":971},{"class":263,"line":970},16,[972,977,982,987,991],{"type":27,"tag":261,"props":973,"children":974},{"style":268},[975],{"type":33,"value":976},"            metadata_dict[",{"type":27,"tag":261,"props":978,"children":979},{"style":493},[980],{"type":33,"value":981},"\"content\"",{"type":27,"tag":261,"props":983,"children":984},{"style":268},[985],{"type":33,"value":986},"] ",{"type":27,"tag":261,"props":988,"children":989},{"style":274},[990],{"type":33,"value":277},{"type":27,"tag":261,"props":992,"children":993},{"style":268},[994],{"type":33,"value":995}," node.node.text\n",{"type":27,"tag":261,"props":997,"children":999},{"class":263,"line":998},17,[1000,1004,1009,1013,1017,1022,1027,1031],{"type":27,"tag":261,"props":1001,"children":1002},{"style":268},[1003],{"type":33,"value":976},{"type":27,"tag":261,"props":1005,"children":1006},{"style":493},[1007],{"type":33,"value":1008},"\"chapter\"",{"type":27,"tag":261,"props":1010,"children":1011},{"style":268},[1012],{"type":33,"value":986},{"type":27,"tag":261,"props":1014,"children":1015},{"style":274},[1016],{"type":33,"value":277},{"type":27,"tag":261,"props":1018,"children":1019},{"style":850},[1020],{"type":33,"value":1021}," int",{"type":27,"tag":261,"props":1023,"children":1024},{"style":268},[1025],{"type":33,"value":1026},"(node_metadata.get(",{"type":27,"tag":261,"props":1028,"children":1029},{"style":493},[1030],{"type":33,"value":1008},{"type":27,"tag":261,"props":1032,"children":1033},{"style":268},[1034],{"type":33,"value":1035},"))\n",{"type":27,"tag":261,"props":1037,"children":1039},{"class":263,"line":1038},18,[1040,1044,1049,1053,1057,1061,1065,1069],{"type":27,"tag":261,"props":1041,"children":1042},{"style":268},[1043],{"type":33,"value":976},{"type":27,"tag":261,"props":1045,"children":1046},{"style":493},[1047],{"type":33,"value":1048},"\"paragraph\"",{"type":27,"tag":261,"props":1050,"children":1051},{"style":268},[1052],{"type":33,"value":986},{"type":27,"tag":261,"props":1054,"children":1055},{"style":274},[1056],{"type":33,"value":277},{"type":27,"tag":261,"props":1058,"children":1059},{"style":850},[1060],{"type":33,"value":1021},{"type":27,"tag":261,"props":1062,"children":1063},{"style":268},[1064],{"type":33,"value":1026},{"type":27,"tag":261,"props":1066,"children":1067},{"style":493},[1068],{"type":33,"value":1048},{"type":27,"tag":261,"props":1070,"children":1071},{"style":268},[1072],{"type":33,"value":1035},{"type":27,"tag":261,"props":1074,"children":1076},{"class":263,"line":1075},19,[1077],{"type":27,"tag":261,"props":1078,"children":1079},{"emptyLinePlaceholder":22},[1080],{"type":33,"value":766},{"type":27,"tag":261,"props":1082,"children":1084},{"class":263,"line":1083},20,[1085],{"type":27,"tag":261,"props":1086,"children":1087},{"style":268},[1088],{"type":33,"value":1089},"            metadata.append(metadata_dict)\n",{"type":27,"tag":261,"props":1091,"children":1093},{"class":263,"line":1092},21,[1094],{"type":27,"tag":261,"props":1095,"children":1096},{"emptyLinePlaceholder":22},[1097],{"type":33,"value":766},{"type":27,"tag":261,"props":1099,"children":1101},{"class":263,"line":1100},22,[1102,1107,1111,1116,1120,1125,1130,1135,1140,1144],{"type":27,"tag":261,"props":1103,"children":1104},{"style":268},[1105],{"type":33,"value":1106},"        context_str ",{"type":27,"tag":261,"props":1108,"children":1109},{"style":274},[1110],{"type":33,"value":277},{"type":27,"tag":261,"props":1112,"children":1113},{"style":493},[1114],{"type":33,"value":1115}," \"",{"type":27,"tag":261,"props":1117,"children":1118},{"style":295},[1119],{"type":33,"value":501},{"type":27,"tag":261,"props":1121,"children":1122},{"style":493},[1123],{"type":33,"value":1124},"\"",{"type":27,"tag":261,"props":1126,"children":1127},{"style":268},[1128],{"type":33,"value":1129},".join([n.node.get_content() ",{"type":27,"tag":261,"props":1131,"children":1132},{"style":274},[1133],{"type":33,"value":1134},"for",{"type":27,"tag":261,"props":1136,"children":1137},{"style":268},[1138],{"type":33,"value":1139}," n ",{"type":27,"tag":261,"props":1141,"children":1142},{"style":274},[1143],{"type":33,"value":553},{"type":27,"tag":261,"props":1145,"children":1146},{"style":268},[1147],{"type":33,"value":1148}," nodes])\n",{"type":27,"tag":261,"props":1150,"children":1152},{"class":263,"line":1151},23,[1153,1158,1162,1166],{"type":27,"tag":261,"props":1154,"children":1155},{"style":268},[1156],{"type":33,"value":1157},"        response ",{"type":27,"tag":261,"props":1159,"children":1160},{"style":274},[1161],{"type":33,"value":277},{"type":27,"tag":261,"props":1163,"children":1164},{"style":873},[1165],{"type":33,"value":876},{"type":27,"tag":261,"props":1167,"children":1168},{"style":268},[1169],{"type":33,"value":1170},".llm.chat(\n",{"type":27,"tag":261,"props":1172,"children":1174},{"class":263,"line":1173},24,[1175],{"type":27,"tag":261,"props":1176,"children":1177},{"style":268},[1178],{"type":33,"value":1179},"            [\n",{"type":27,"tag":261,"props":1181,"children":1183},{"class":263,"line":1182},25,[1184],{"type":27,"tag":261,"props":1185,"children":1186},{"style":268},[1187],{"type":33,"value":1188},"                ChatMessage(\n",{"type":27,"tag":261,"props":1190,"children":1192},{"class":263,"line":1191},26,[1193,1198,1202,1207],{"type":27,"tag":261,"props":1194,"children":1195},{"style":285},[1196],{"type":33,"value":1197},"                    role",{"type":27,"tag":261,"props":1199,"children":1200},{"style":274},[1201],{"type":33,"value":277},{"type":27,"tag":261,"props":1203,"children":1204},{"style":493},[1205],{"type":33,"value":1206},"\"user\"",{"type":27,"tag":261,"props":1208,"children":1209},{"style":268},[1210],{"type":33,"value":1211},",\n",{"type":27,"tag":261,"props":1213,"children":1215},{"class":263,"line":1214},27,[1216,1221,1225,1230],{"type":27,"tag":261,"props":1217,"children":1218},{"style":285},[1219],{"type":33,"value":1220},"                    content",{"type":27,"tag":261,"props":1222,"children":1223},{"style":274},[1224],{"type":33,"value":277},{"type":27,"tag":261,"props":1226,"children":1227},{"style":268},[1228],{"type":33,"value":1229},"q_and_a_prompt.format( ",{"type":27,"tag":261,"props":1231,"children":1232},{"style":478},[1233],{"type":33,"value":1234},"# the English and Chinese prompt templates are discussed below\n",{"type":27,"tag":261,"props":1236,"children":1238},{"class":263,"line":1237},28,[1239,1244,1248,1253,1257,1261],{"type":27,"tag":261,"props":1240,"children":1241},{"style":285},[1242],{"type":33,"value":1243},"                        context_str",{"type":27,"tag":261,"props":1245,"children":1246},{"style":274},[1247],{"type":33,"value":277},{"type":27,"tag":261,"props":1249,"children":1250},{"style":268},[1251],{"type":33,"value":1252},"context_str, ",{"type":27,"tag":261,"props":1254,"children":1255},{"style":285},[1256],{"type":33,"value":842},{"type":27,"tag":261,"props":1258,"children":1259},{"style":274},[1260],{"type":33,"value":277},{"type":27,"tag":261,"props":1262,"children":1263},{"style":268},[1264],{"type":33,"value":1265},"query_str\n",{"type":27,"tag":261,"props":1267,"children":1269},{"class":263,"line":1268},29,[1270],{"type":27,"tag":261,"props":1271,"children":1272},{"style":268},[1273],{"type":33,"value":1274},"                    ),\n",{"type":27,"tag":261,"props":1276,"children":1278},{"class":263,"line":1277},30,[1279],{"type":27,"tag":261,"props":1280,"children":1281},{"style":268},[1282],{"type":33,"value":1283},"                )\n",{"type":27,"tag":261,"props":1285,"children":1287},{"class":263,"line":1286},31,[1288],{"type":27,"tag":261,"props":1289,"children":1290},{"style":268},[1291],{"type":33,"value":1292},"            ]\n",{"type":27,"tag":261,"props":1294,"children":1296},{"class":263,"line":1295},32,[1297],{"type":27,"tag":261,"props":1298,"children":1299},{"style":268},[1300],{"type":33,"value":1301},"        )\n",{"type":27,"tag":261,"props":1303,"children":1305},{"class":263,"line":1304},33,[1306],{"type":27,"tag":261,"props":1307,"children":1308},{"emptyLinePlaceholder":22},[1309],{"type":33,"value":766},{"type":27,"tag":261,"props":1311,"children":1313},{"class":263,"line":1312},34,[1314,1319],{"type":27,"tag":261,"props":1315,"children":1316},{"style":274},[1317],{"type":33,"value":1318},"        return",{"type":27,"tag":261,"props":1320,"children":1321},{"style":268},[1322],{"type":33,"value":1323}," response, metadata\n",{"type":27,"tag":47,"props":1325,"children":1327},{"id":1326},"indexing-the-book-data",[1328],{"type":33,"value":1329},"Indexing the book data",{"type":27,"tag":36,"props":1331,"children":1332},{},[1333],{"type":33,"value":1334},"In the indexing process, embedding models are used to translate chunks of text (paragraphs) into high-dimensional vectors that represent the relationships between the tokens in a chunk of text. These are the vectors stored in the \"Vector Database\" used by LlamaIndex. The chapter number, paragraph number and version (original, Mandarin Chinese and English) of each paragraph are added to the database entry as metadata during the indexing step which runs via a script before starting the FastAPI server. Here's how I indexed the original text and translations with LlamaIndex:",{"type":27,"tag":238,"props":1336,"children":1338},{"code":1337,"language":254,"meta":8,"className":255,"style":8},"from llama_index.core import Document, VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nen_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nzh_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-zh-v1.5\")\n\ndef persist_index():\n    documents = []\n    for chapter in range(1, 121):\n        with open(f\"data/book/{chapter}.json\", \"r\") as f:\n            data = json.load(f)\n            paragraphs = data[\"paragraphs\"]\n\n        for i, p in enumerate(paragraphs):\n            for lang in [\"original\", \"chinese\", \"english\"]:\n                document = Document(\n                    text=p[lang],\n                    metadata={\n                        \"chapter\": str(chapter),\n                        \"paragraph\": str(i),\n                        \"language\": lang,\n                    },\n                    metadata_seperator=\"::\",\n                    metadata_template=\"{key}=>{value}\",\n                    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n                    embed_model=(\n                        en_embedding_model if lang == \"english\" else zh_embedding_model\n                    ),\n                )\n                documents.append(document)\n\n    index = VectorStoreIndex.from_documents(documents)\n    index.storage_context.persist(persist_dir=\"storage\")\n\nif __name__ == \"__main__\":\n    persist_index()\n",[1339],{"type":27,"tag":170,"props":1340,"children":1341},{"__ignoreMap":8},[1342,1365,1386,1393,1428,1461,1468,1486,1502,1546,1617,1634,1660,1667,1693,1743,1760,1777,1794,1815,1836,1849,1857,1878,1917,1967,1984,2021,2028,2035,2043,2050,2067,2093,2100,2129],{"type":27,"tag":261,"props":1343,"children":1344},{"class":263,"line":264},[1345,1350,1355,1360],{"type":27,"tag":261,"props":1346,"children":1347},{"style":274},[1348],{"type":33,"value":1349},"from",{"type":27,"tag":261,"props":1351,"children":1352},{"style":268},[1353],{"type":33,"value":1354}," llama_index.core ",{"type":27,"tag":261,"props":1356,"children":1357},{"style":274},[1358],{"type":33,"value":1359},"import",{"type":27,"tag":261,"props":1361,"children":1362},{"style":268},[1363],{"type":33,"value":1364}," Document, VectorStoreIndex\n",{"type":27,"tag":261,"props":1366,"children":1367},{"class":263,"line":343},[1368,1372,1377,1381],{"type":27,"tag":261,"props":1369,"children":1370},{"style":274},[1371],{"type":33,"value":1349},{"type":27,"tag":261,"props":1373,"children":1374},{"style":268},[1375],{"type":33,"value":1376}," llama_index.embeddings.huggingface ",{"type":27,"tag":261,"props":1378,"children":1379},{"style":274},[1380],{"type":33,"value":1359},{"type":27,"tag":261,"props":1382,"children":1383},{"style":268},[1384],{"type":33,"value":1385}," HuggingFaceEmbedding\n",{"type":27,"tag":261,"props":1387,"children":1388},{"class":263,"line":379},[1389],{"type":27,"tag":261,"props":1390,"children":1391},{"emptyLinePlaceholder":22},[1392],{"type":33,"value":766},{"type":27,"tag":261,"props":1394,"children":1395},{"class":263,"line":537},[1396,1401,1405,1410,1415,1419,1424],{"type":27,"tag":261,"props":1397,"children":1398},{"style":268},[1399],{"type":33,"value":1400},"en_embedding_model ",{"type":27,"tag":261,"props":1402,"children":1403},{"style":274},[1404],{"type":33,"value":277},{"type":27,"tag":261,"props":1406,"children":1407},{"style":268},[1408],{"type":33,"value":1409}," HuggingFaceEmbedding(",{"type":27,"tag":261,"props":1411,"children":1412},{"style":285},[1413],{"type":33,"value":1414},"model_name",{"type":27,"tag":261,"props":1416,"children":1417},{"style":274},[1418],{"type":33,"value":277},{"type":27,"tag":261,"props":1420,"children":1421},{"style":493},[1422],{"type":33,"value":1423},"\"BAAI/bge-small-en-v1.5\"",{"type":27,"tag":261,"props":1425,"children":1426},{"style":268},[1427],{"type":33,"value":340},{"type":27,"tag":261,"props":1429,"children":1430},{"class":263,"line":561},[1431,1436,1440,1444,1448,1452,1457],{"type":27,"tag":261,"props":1432,"children":1433},{"style":268},[1434],{"type":33,"value":1435},"zh_embedding_model ",{"type":27,"tag":261,"props":1437,"children":1438},{"style":274},[1439],{"type":33,"value":277},{"type":27,"tag":261,"props":1441,"children":1442},{"style":268},[1443],{"type":33,"value":1409},{"type":27,"tag":261,"props":1445,"children":1446},{"style":285},[1447],{"type":33,"value":1414},{"type":27,"tag":261,"props":1449,"children":1450},{"style":274},[1451],{"type":33,"value":277},{"type":27,"tag":261,"props":1453,"children":1454},{"style":493},[1455],{"type":33,"value":1456},"\"BAAI/bge-small-zh-v1.5\"",{"type":27,"tag":261,"props":1458,"children":1459},{"style":268},[1460],{"type":33,"value":340},{"type":27,"tag":261,"props":1462,"children":1463},{"class":263,"line":785},[1464],{"type":27,"tag":261,"props":1465,"children":1466},{"emptyLinePlaceholder":22},[1467],{"type":33,"value":766},{"type":27,"tag":261,"props":1469,"children":1470},{"class":263,"line":794},[1471,1476,1481],{"type":27,"tag":261,"props":1472,"children":1473},{"style":487},[1474],{"type":33,"value":1475},"def",{"type":27,"tag":261,"props":1477,"children":1478},{"style":820},[1479],{"type":33,"value":1480}," persist_index",{"type":27,"tag":261,"props":1482,"children":1483},{"style":268},[1484],{"type":33,"value":1485},"():\n",{"type":27,"tag":261,"props":1487,"children":1488},{"class":263,"line":803},[1489,1494,1498],{"type":27,"tag":261,"props":1490,"children":1491},{"style":268},[1492],{"type":33,"value":1493},"    documents ",{"type":27,"tag":261,"props":1495,"children":1496},{"style":274},[1497],{"type":33,"value":277},{"type":27,"tag":261,"props":1499,"children":1500},{"style":268},[1501],{"type":33,"value":899},{"type":27,"tag":261,"props":1503,"children":1504},{"class":263,"line":811},[1505,1509,1514,1518,1524,1528,1533,1537,1542],{"type":27,"tag":261,"props":1506,"children":1507},{"style":274},[1508],{"type":33,"value":543},{"type":27,"tag":261,"props":1510,"children":1511},{"style":268},[1512],{"type":33,"value":1513}," chapter ",{"type":27,"tag":261,"props":1515,"children":1516},{"style":274},[1517],{"type":33,"value":553},{"type":27,"tag":261,"props":1519,"children":1521},{"style":1520},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#66D9EF",[1522],{"type":33,"value":1523}," range",{"type":27,"tag":261,"props":1525,"children":1526},{"style":268},[1527],{"type":33,"value":740},{"type":27,"tag":261,"props":1529,"children":1530},{"style":295},[1531],{"type":33,"value":1532},"1",{"type":27,"tag":261,"props":1534,"children":1535},{"style":268},[1536],{"type":33,"value":303},{"type":27,"tag":261,"props":1538,"children":1539},{"style":295},[1540],{"type":33,"value":1541},"121",{"type":27,"tag":261,"props":1543,"children":1544},{"style":268},[1545],{"type":33,"value":750},{"type":27,"tag":261,"props":1547,"children":1548},{"class":263,"line":860},[1549,1554,1559,1563,1568,1573,1578,1583,1588,1593,1597,1602,1607,1612],{"type":27,"tag":261,"props":1550,"children":1551},{"style":274},[1552],{"type":33,"value":1553},"        with",{"type":27,"tag":261,"props":1555,"children":1556},{"style":1520},[1557],{"type":33,"value":1558}," open",{"type":27,"tag":261,"props":1560,"children":1561},{"style":268},[1562],{"type":33,"value":740},{"type":27,"tag":261,"props":1564,"children":1565},{"style":487},[1566],{"type":33,"value":1567},"f",{"type":27,"tag":261,"props":1569,"children":1570},{"style":493},[1571],{"type":33,"value":1572},"\"data/book/",{"type":27,"tag":261,"props":1574,"children":1575},{"style":295},[1576],{"type":33,"value":1577},"{",{"type":27,"tag":261,"props":1579,"children":1580},{"style":268},[1581],{"type":33,"value":1582},"chapter",{"type":27,"tag":261,"props":1584,"children":1585},{"style":295},[1586],{"type":33,"value":1587},"}",{"type":27,"tag":261,"props":1589,"children":1590},{"style":493},[1591],{"type":33,"value":1592},".json\"",{"type":27,"tag":261,"props":1594,"children":1595},{"style":268},[1596],{"type":33,"value":303},{"type":27,"tag":261,"props":1598,"children":1599},{"style":493},[1600],{"type":33,"value":1601},"\"r\"",{"type":27,"tag":261,"props":1603,"children":1604},{"style":268},[1605],{"type":33,"value":1606},") ",{"type":27,"tag":261,"props":1608,"children":1609},{"style":274},[1610],{"type":33,"value":1611},"as",{"type":27,"tag":261,"props":1613,"children":1614},{"style":268},[1615],{"type":33,"value":1616}," f:\n",{"type":27,"tag":261,"props":1618,"children":1619},{"class":263,"line":884},[1620,1625,1629],{"type":27,"tag":261,"props":1621,"children":1622},{"style":268},[1623],{"type":33,"value":1624},"            data ",{"type":27,"tag":261,"props":1626,"children":1627},{"style":274},[1628],{"type":33,"value":277},{"type":27,"tag":261,"props":1630,"children":1631},{"style":268},[1632],{"type":33,"value":1633}," json.load(f)\n",{"type":27,"tag":261,"props":1635,"children":1636},{"class":263,"line":902},[1637,1642,1646,1651,1656],{"type":27,"tag":261,"props":1638,"children":1639},{"style":268},[1640],{"type":33,"value":1641},"            paragraphs ",{"type":27,"tag":261,"props":1643,"children":1644},{"style":274},[1645],{"type":33,"value":277},{"type":27,"tag":261,"props":1647,"children":1648},{"style":268},[1649],{"type":33,"value":1650}," data[",{"type":27,"tag":261,"props":1652,"children":1653},{"style":493},[1654],{"type":33,"value":1655},"\"paragraphs\"",{"type":27,"tag":261,"props":1657,"children":1658},{"style":268},[1659],{"type":33,"value":567},{"type":27,"tag":261,"props":1661,"children":1662},{"class":263,"line":911},[1663],{"type":27,"tag":261,"props":1664,"children":1665},{"emptyLinePlaceholder":22},[1666],{"type":33,"value":766},{"type":27,"tag":261,"props":1668,"children":1669},{"class":263,"line":934},[1670,1674,1679,1683,1688],{"type":27,"tag":261,"props":1671,"children":1672},{"style":274},[1673],{"type":33,"value":917},{"type":27,"tag":261,"props":1675,"children":1676},{"style":268},[1677],{"type":33,"value":1678}," i, p ",{"type":27,"tag":261,"props":1680,"children":1681},{"style":274},[1682],{"type":33,"value":553},{"type":27,"tag":261,"props":1684,"children":1685},{"style":1520},[1686],{"type":33,"value":1687}," enumerate",{"type":27,"tag":261,"props":1689,"children":1690},{"style":268},[1691],{"type":33,"value":1692},"(paragraphs):\n",{"type":27,"tag":261,"props":1694,"children":1695},{"class":263,"line":952},[1696,1701,1706,1710,1715,1720,1724,1729,1733,1738],{"type":27,"tag":261,"props":1697,"children":1698},{"style":274},[1699],{"type":33,"value":1700},"            for",{"type":27,"tag":261,"props":1702,"children":1703},{"style":268},[1704],{"type":33,"value":1705}," lang ",{"type":27,"tag":261,"props":1707,"children":1708},{"style":274},[1709],{"type":33,"value":553},{"type":27,"tag":261,"props":1711,"children":1712},{"style":268},[1713],{"type":33,"value":1714}," [",{"type":27,"tag":261,"props":1716,"children":1717},{"style":493},[1718],{"type":33,"value":1719},"\"original\"",{"type":27,"tag":261,"props":1721,"children":1722},{"style":268},[1723],{"type":33,"value":303},{"type":27,"tag":261,"props":1725,"children":1726},{"style":493},[1727],{"type":33,"value":1728},"\"chinese\"",{"type":27,"tag":261,"props":1730,"children":1731},{"style":268},[1732],{"type":33,"value":303},{"type":27,"tag":261,"props":1734,"children":1735},{"style":493},[1736],{"type":33,"value":1737},"\"english\"",{"type":27,"tag":261,"props":1739,"children":1740},{"style":268},[1741],{"type":33,"value":1742},"]:\n",{"type":27,"tag":261,"props":1744,"children":1745},{"class":263,"line":970},[1746,1751,1755],{"type":27,"tag":261,"props":1747,"children":1748},{"style":268},[1749],{"type":33,"value":1750},"                document ",{"type":27,"tag":261,"props":1752,"children":1753},{"style":274},[1754],{"type":33,"value":277},{"type":27,"tag":261,"props":1756,"children":1757},{"style":268},[1758],{"type":33,"value":1759}," Document(\n",{"type":27,"tag":261,"props":1761,"children":1762},{"class":263,"line":998},[1763,1768,1772],{"type":27,"tag":261,"props":1764,"children":1765},{"style":285},[1766],{"type":33,"value":1767},"                    text",{"type":27,"tag":261,"props":1769,"children":1770},{"style":274},[1771],{"type":33,"value":277},{"type":27,"tag":261,"props":1773,"children":1774},{"style":268},[1775],{"type":33,"value":1776},"p[lang],\n",{"type":27,"tag":261,"props":1778,"children":1779},{"class":263,"line":1038},[1780,1785,1789],{"type":27,"tag":261,"props":1781,"children":1782},{"style":285},[1783],{"type":33,"value":1784},"                    metadata",{"type":27,"tag":261,"props":1786,"children":1787},{"style":274},[1788],{"type":33,"value":277},{"type":27,"tag":261,"props":1790,"children":1791},{"style":268},[1792],{"type":33,"value":1793},"{\n",{"type":27,"tag":261,"props":1795,"children":1796},{"class":263,"line":1075},[1797,1802,1806,1810],{"type":27,"tag":261,"props":1798,"children":1799},{"style":493},[1800],{"type":33,"value":1801},"                        \"chapter\"",{"type":27,"tag":261,"props":1803,"children":1804},{"style":268},[1805],{"type":33,"value":847},{"type":27,"tag":261,"props":1807,"children":1808},{"style":850},[1809],{"type":33,"value":853},{"type":27,"tag":261,"props":1811,"children":1812},{"style":268},[1813],{"type":33,"value":1814},"(chapter),\n",{"type":27,"tag":261,"props":1816,"children":1817},{"class":263,"line":1083},[1818,1823,1827,1831],{"type":27,"tag":261,"props":1819,"children":1820},{"style":493},[1821],{"type":33,"value":1822},"                        \"paragraph\"",{"type":27,"tag":261,"props":1824,"children":1825},{"style":268},[1826],{"type":33,"value":847},{"type":27,"tag":261,"props":1828,"children":1829},{"style":850},[1830],{"type":33,"value":853},{"type":27,"tag":261,"props":1832,"children":1833},{"style":268},[1834],{"type":33,"value":1835},"(i),\n",{"type":27,"tag":261,"props":1837,"children":1838},{"class":263,"line":1092},[1839,1844],{"type":27,"tag":261,"props":1840,"children":1841},{"style":493},[1842],{"type":33,"value":1843},"                        \"language\"",{"type":27,"tag":261,"props":1845,"children":1846},{"style":268},[1847],{"type":33,"value":1848},": lang,\n",{"type":27,"tag":261,"props":1850,"children":1851},{"class":263,"line":1100},[1852],{"type":27,"tag":261,"props":1853,"children":1854},{"style":268},[1855],{"type":33,"value":1856},"                    },\n",{"type":27,"tag":261,"props":1858,"children":1859},{"class":263,"line":1151},[1860,1865,1869,1874],{"type":27,"tag":261,"props":1861,"children":1862},{"style":285},[1863],{"type":33,"value":1864},"                    metadata_seperator",{"type":27,"tag":261,"props":1866,"children":1867},{"style":274},[1868],{"type":33,"value":277},{"type":27,"tag":261,"props":1870,"children":1871},{"style":493},[1872],{"type":33,"value":1873},"\"::\"",{"type":27,"tag":261,"props":1875,"children":1876},{"style":268},[1877],{"type":33,"value":1211},{"type":27,"tag":261,"props":1879,"children":1880},{"class":263,"line":1173},[1881,1886,1890,1894,1899,1904,1909,1913],{"type":27,"tag":261,"props":1882,"children":1883},{"style":285},[1884],{"type":33,"value":1885},"                    metadata_template",{"type":27,"tag":261,"props":1887,"children":1888},{"style":274},[1889],{"type":33,"value":277},{"type":27,"tag":261,"props":1891,"children":1892},{"style":493},[1893],{"type":33,"value":1124},{"type":27,"tag":261,"props":1895,"children":1896},{"style":295},[1897],{"type":33,"value":1898},"{key}",{"type":27,"tag":261,"props":1900,"children":1901},{"style":493},[1902],{"type":33,"value":1903},"=>",{"type":27,"tag":261,"props":1905,"children":1906},{"style":295},[1907],{"type":33,"value":1908},"{value}",{"type":27,"tag":261,"props":1910,"children":1911},{"style":493},[1912],{"type":33,"value":1124},{"type":27,"tag":261,"props":1914,"children":1915},{"style":268},[1916],{"type":33,"value":1211},{"type":27,"tag":261,"props":1918,"children":1919},{"class":263,"line":1182},[1920,1925,1929,1934,1939,1944,1949,1954,1959,1963],{"type":27,"tag":261,"props":1921,"children":1922},{"style":285},[1923],{"type":33,"value":1924},"                    text_template",{"type":27,"tag":261,"props":1926,"children":1927},{"style":274},[1928],{"type":33,"value":277},{"type":27,"tag":261,"props":1930,"children":1931},{"style":493},[1932],{"type":33,"value":1933},"\"Metadata: ",{"type":27,"tag":261,"props":1935,"children":1936},{"style":295},[1937],{"type":33,"value":1938},"{metadata_str}\\n",{"type":27,"tag":261,"props":1940,"children":1941},{"style":493},[1942],{"type":33,"value":1943},"-----",{"type":27,"tag":261,"props":1945,"children":1946},{"style":295},[1947],{"type":33,"value":1948},"\\n",{"type":27,"tag":261,"props":1950,"children":1951},{"style":493},[1952],{"type":33,"value":1953},"Content: ",{"type":27,"tag":261,"props":1955,"children":1956},{"style":295},[1957],{"type":33,"value":1958},"{content}",{"type":27,"tag":261,"props":1960,"children":1961},{"style":493},[1962],{"type":33,"value":1124},{"type":27,"tag":261,"props":1964,"children":1965},{"style":268},[1966],{"type":33,"value":1211},{"type":27,"tag":261,"props":1968,"children":1969},{"class":263,"line":1191},[1970,1975,1979],{"type":27,"tag":261,"props":1971,"children":1972},{"style":285},[1973],{"type":33,"value":1974},"                    embed_model",{"type":27,"tag":261,"props":1976,"children":1977},{"style":274},[1978],{"type":33,"value":277},{"type":27,"tag":261,"props":1980,"children":1981},{"style":268},[1982],{"type":33,"value":1983},"(\n",{"type":27,"tag":261,"props":1985,"children":1986},{"class":263,"line":1214},[1987,1992,1997,2001,2006,2011,2016],{"type":27,"tag":261,"props":1988,"children":1989},{"style":268},[1990],{"type":33,"value":1991},"                        en_embedding_model ",{"type":27,"tag":261,"props":1993,"children":1994},{"style":274},[1995],{"type":33,"value":1996},"if",{"type":27,"tag":261,"props":1998,"children":1999},{"style":268},[2000],{"type":33,"value":1705},{"type":27,"tag":261,"props":2002,"children":2003},{"style":274},[2004],{"type":33,"value":2005},"==",{"type":27,"tag":261,"props":2007,"children":2008},{"style":493},[2009],{"type":33,"value":2010}," \"english\"",{"type":27,"tag":261,"props":2012,"children":2013},{"style":274},[2014],{"type":33,"value":2015}," else",{"type":27,"tag":261,"props":2017,"children":2018},{"style":268},[2019],{"type":33,"value":2020}," zh_embedding_model\n",{"type":27,"tag":261,"props":2022,"children":2023},{"class":263,"line":1237},[2024],{"type":27,"tag":261,"props":2025,"children":2026},{"style":268},[2027],{"type":33,"value":1274},{"type":27,"tag":261,"props":2029,"children":2030},{"class":263,"line":1268},[2031],{"type":27,"tag":261,"props":2032,"children":2033},{"style":268},[2034],{"type":33,"value":1283},{"type":27,"tag":261,"props":2036,"children":2037},{"class":263,"line":1277},[2038],{"type":27,"tag":261,"props":2039,"children":2040},{"style":268},[2041],{"type":33,"value":2042},"                documents.append(document)\n",{"type":27,"tag":261,"props":2044,"children":2045},{"class":263,"line":1286},[2046],{"type":27,"tag":261,"props":2047,"children":2048},{"emptyLinePlaceholder":22},[2049],{"type":33,"value":766},{"type":27,"tag":261,"props":2051,"children":2052},{"class":263,"line":1295},[2053,2058,2062],{"type":27,"tag":261,"props":2054,"children":2055},{"style":268},[2056],{"type":33,"value":2057},"    index ",{"type":27,"tag":261,"props":2059,"children":2060},{"style":274},[2061],{"type":33,"value":277},{"type":27,"tag":261,"props":2063,"children":2064},{"style":268},[2065],{"type":33,"value":2066}," VectorStoreIndex.from_documents(documents)\n",{"type":27,"tag":261,"props":2068,"children":2069},{"class":263,"line":1304},[2070,2075,2080,2084,2089],{"type":27,"tag":261,"props":2071,"children":2072},{"style":268},[2073],{"type":33,"value":2074},"    index.storage_context.persist(",{"type":27,"tag":261,"props":2076,"children":2077},{"style":285},[2078],{"type":33,"value":2079},"persist_dir",{"type":27,"tag":261,"props":2081,"children":2082},{"style":274},[2083],{"type":33,"value":277},{"type":27,"tag":261,"props":2085,"children":2086},{"style":493},[2087],{"type":33,"value":2088},"\"storage\"",{"type":27,"tag":261,"props":2090,"children":2091},{"style":268},[2092],{"type":33,"value":340},{"type":27,"tag":261,"props":2094,"children":2095},{"class":263,"line":1312},[2096],{"type":27,"tag":261,"props":2097,"children":2098},{"emptyLinePlaceholder":22},[2099],{"type":33,"value":766},{"type":27,"tag":261,"props":2101,"children":2103},{"class":263,"line":2102},35,[2104,2108,2114,2119,2124],{"type":27,"tag":261,"props":2105,"children":2106},{"style":274},[2107],{"type":33,"value":1996},{"type":27,"tag":261,"props":2109,"children":2111},{"style":2110},"--shiki-default:#005CC5;--shiki-dark:#79B8FF;--shiki-sepia:#F8F8F2",[2112],{"type":33,"value":2113}," __name__",{"type":27,"tag":261,"props":2115,"children":2116},{"style":274},[2117],{"type":33,"value":2118}," ==",{"type":27,"tag":261,"props":2120,"children":2121},{"style":493},[2122],{"type":33,"value":2123}," \"__main__\"",{"type":27,"tag":261,"props":2125,"children":2126},{"style":268},[2127],{"type":33,"value":2128},":\n",{"type":27,"tag":261,"props":2130,"children":2132},{"class":263,"line":2131},36,[2133],{"type":27,"tag":261,"props":2134,"children":2135},{"style":268},[2136],{"type":33,"value":2137},"    persist_index()\n",{"type":27,"tag":36,"props":2139,"children":2140},{},[2141,2143,2149],{"type":33,"value":2142},"For the embedding models, I used the small BAAI General Embedding models (BGE) for English and Chinese. BAAI is the Beijing Academy of Artificial Intelligence, and I learned about this organization through some of the examples on the LlamaIndex site that use BAAI embeddings. There are multi-lingual embedding models (e.g. ",{"type":27,"tag":170,"props":2144,"children":2146},{"className":2145},[],[2147],{"type":33,"value":2148},"BAAI/bge-m3",{"type":33,"value":2150},"), but setting the embedding model on a per-document basis is possible and in some cases it might be preferable to using a single embedding model for all documents.",{"type":27,"tag":47,"props":2152,"children":2154},{"id":2153},"milvus-vector-database",[2155],{"type":33,"value":2156},"Milvus Vector Database",{"type":27,"tag":36,"props":2158,"children":2159},{},[2160],{"type":33,"value":2161},"I did most of the development for this project using the in-memory VectorIndexStore provided by LlamaIndex. This worked well, but making any changes to the FastAPI server required the data to be reloaded into memory which took several seconds each time. This can really hinder a good development flow, so I looked into using an external service for the vector database instead of running it in memory.",{"type":27,"tag":36,"props":2163,"children":2164},{},[2165],{"type":27,"tag":84,"props":2166,"children":2169},{"alt":2167,"src":2168},"Vector Database Options","/static/redlm/vectordbs.png",[],{"type":27,"tag":36,"props":2171,"children":2172},{},[2173,2175,2186],{"type":33,"value":2174},"There are a LOT of options to consider when picking a vector database for a RAG application. Milvus has a highly decoupled architecture, it is fully open source and I had seen it in some examples in the ",{"type":27,"tag":61,"props":2176,"children":2179},{"href":2177,"rel":2178},"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/examples/advanced_rag/multimodal_rag",[65],[2180],{"type":27,"tag":170,"props":2181,"children":2183},{"className":2182},[],[2184],{"type":33,"value":2185},"NVIDIA/GenerativeAIExamples",{"type":33,"value":2187}," repo, so I decided to give it a try.",{"type":27,"tag":36,"props":2189,"children":2190},{},[2191],{"type":27,"tag":84,"props":2192,"children":2195},{"alt":2193,"src":2194},"Milvus Vector Database Architecture","/static/redlm/milvus.png",[],{"type":27,"tag":36,"props":2197,"children":2198},{},[2199,2201,2208],{"type":33,"value":2200},"Using the ",{"type":27,"tag":61,"props":2202,"children":2205},{"href":2203,"rel":2204},"https://milvus.io/docs/v2.0.x/install_standalone-docker.md",[65],[2206],{"type":33,"value":2207},"Milvus docker compose example",{"type":33,"value":2209}," I was able to set up an external vector database based on etcd and minio. Milvus also provides a Helm chart for running their vector database, this would be helpful if I was going to be running everything in Kubernetes (inference, vector database and application containers).",{"type":27,"tag":47,"props":2211,"children":2213},{"id":2212},"other-examples-of-rag-with-english-questions",[2214],{"type":33,"value":2215},"Other examples of RAG with English questions",{"type":27,"tag":36,"props":2217,"children":2218},{},[2219],{"type":33,"value":2220},"One interesting design question I faced was how to support answering questions in both English and Chinese. I initially built the Q&A bot with only Chinese language support. Later, I added a simple helper function to determine if the input text is Chinese:",{"type":27,"tag":238,"props":2222,"children":2224},{"code":2223,"language":254,"meta":8,"className":255,"style":8},"def is_chinese_text(text: str) -> bool:\n    \"\"\"\n    This is a simple helper function that is used to determine which prompt to use\n    depending on the language of the original user query\n    \"\"\"\n    chinese_count = sum(1 for char in text if '\\u4e00' \u003C= char \u003C= '\\u9fff')\n    english_count = sum(1 for char in text if 'a' \u003C= char.lower() \u003C= 'z')\n\n    return chinese_count > english_count\n",[2225],{"type":27,"tag":170,"props":2226,"children":2227},{"__ignoreMap":8},[2228,2270,2278,2286,2294,2301,2395,2466,2473],{"type":27,"tag":261,"props":2229,"children":2230},{"class":263,"line":264},[2231,2235,2240,2244,2248,2252,2256,2261,2266],{"type":27,"tag":261,"props":2232,"children":2233},{"style":487},[2234],{"type":33,"value":1475},{"type":27,"tag":261,"props":2236,"children":2237},{"style":820},[2238],{"type":33,"value":2239}," is_chinese_text",{"type":27,"tag":261,"props":2241,"children":2242},{"style":268},[2243],{"type":33,"value":740},{"type":27,"tag":261,"props":2245,"children":2246},{"style":830},[2247],{"type":33,"value":33},{"type":27,"tag":261,"props":2249,"children":2250},{"style":268},[2251],{"type":33,"value":847},{"type":27,"tag":261,"props":2253,"children":2254},{"style":850},[2255],{"type":33,"value":853},{"type":27,"tag":261,"props":2257,"children":2258},{"style":268},[2259],{"type":33,"value":2260},") -> ",{"type":27,"tag":261,"props":2262,"children":2263},{"style":850},[2264],{"type":33,"value":2265},"bool",{"type":27,"tag":261,"props":2267,"children":2268},{"style":268},[2269],{"type":33,"value":2128},{"type":27,"tag":261,"props":2271,"children":2272},{"class":263,"line":343},[2273],{"type":27,"tag":261,"props":2274,"children":2275},{"style":493},[2276],{"type":33,"value":2277},"    \"\"\"\n",{"type":27,"tag":261,"props":2279,"children":2280},{"class":263,"line":379},[2281],{"type":27,"tag":261,"props":2282,"children":2283},{"style":493},[2284],{"type":33,"value":2285},"    This is a simple helper function that is used to determine which prompt to use\n",{"type":27,"tag":261,"props":2287,"children":2288},{"class":263,"line":537},[2289],{"type":27,"tag":261,"props":2290,"children":2291},{"style":493},[2292],{"type":33,"value":2293},"    depending on the language of the original user query\n",{"type":27,"tag":261,"props":2295,"children":2296},{"class":263,"line":561},[2297],{"type":27,"tag":261,"props":2298,"children":2299},{"style":493},[2300],{"type":33,"value":2277},{"type":27,"tag":261,"props":2302,"children":2303},{"class":263,"line":785},[2304,2309,2313,2318,2322,2326,2331,2336,2340,2345,2349,2354,2359,2364,2369,2373,2378,2382,2387,2391],{"type":27,"tag":261,"props":2305,"children":2306},{"style":268},[2307],{"type":33,"value":2308},"    chinese_count ",{"type":27,"tag":261,"props":2310,"children":2311},{"style":274},[2312],{"type":33,"value":277},{"type":27,"tag":261,"props":2314,"children":2315},{"style":1520},[2316],{"type":33,"value":2317}," sum",{"type":27,"tag":261,"props":2319,"children":2320},{"style":268},[2321],{"type":33,"value":740},{"type":27,"tag":261,"props":2323,"children":2324},{"style":295},[2325],{"type":33,"value":1532},{"type":27,"tag":261,"props":2327,"children":2328},{"style":274},[2329],{"type":33,"value":2330}," for",{"type":27,"tag":261,"props":2332,"children":2333},{"style":268},[2334],{"type":33,"value":2335}," char ",{"type":27,"tag":261,"props":2337,"children":2338},{"style":274},[2339],{"type":33,"value":553},{"type":27,"tag":261,"props":2341,"children":2342},{"style":268},[2343],{"type":33,"value":2344}," text ",{"type":27,"tag":261,"props":2346,"children":2347},{"style":274},[2348],{"type":33,"value":1996},{"type":27,"tag":261,"props":2350,"children":2351},{"style":493},[2352],{"type":33,"value":2353}," '",{"type":27,"tag":261,"props":2355,"children":2356},{"style":295},[2357],{"type":33,"value":2358},"\\u4e00",{"type":27,"tag":261,"props":2360,"children":2361},{"style":493},[2362],{"type":33,"value":2363},"'",{"type":27,"tag":261,"props":2365,"children":2366},{"style":274},[2367],{"type":33,"value":2368}," \u003C=",{"type":27,"tag":261,"props":2370,"children":2371},{"style":268},[2372],{"type":33,"value":2335},{"type":27,"tag":261,"props":2374,"children":2375},{"style":274},[2376],{"type":33,"value":2377},"\u003C=",{"type":27,"tag":261,"props":2379,"children":2380},{"style":493},[2381],{"type":33,"value":2353},{"type":27,"tag":261,"props":2383,"children":2384},{"style":295},[2385],{"type":33,"value":2386},"\\u9fff",{"type":27,"tag":261,"props":2388,"children":2389},{"style":493},[2390],{"type":33,"value":2363},{"type":27,"tag":261,"props":2392,"children":2393},{"style":268},[2394],{"type":33,"value":340},{"type":27,"tag":261,"props":2396,"children":2397},{"class":263,"line":794},[2398,2403,2407,2411,2415,2419,2423,2427,2431,2435,2439,2444,2448,2453,2457,2462],{"type":27,"tag":261,"props":2399,"children":2400},{"style":268},[2401],{"type":33,"value":2402},"    english_count ",{"type":27,"tag":261,"props":2404,"children":2405},{"style":274},[2406],{"type":33,"value":277},{"type":27,"tag":261,"props":2408,"children":2409},{"style":1520},[2410],{"type":33,"value":2317},{"type":27,"tag":261,"props":2412,"children":2413},{"style":268},[2414],{"type":33,"value":740},{"type":27,"tag":261,"props":2416,"children":2417},{"style":295},[2418],{"type":33,"value":1532},{"type":27,"tag":261,"props":2420,"children":2421},{"style":274},[2422],{"type":33,"value":2330},{"type":27,"tag":261,"props":2424,"children":2425},{"style":268},[2426],{"type":33,"value":2335},{"type":27,"tag":261,"props":2428,"children":2429},{"style":274},[2430],{"type":33,"value":553},{"type":27,"tag":261,"props":2432,"children":2433},{"style":268},[2434],{"type":33,"value":2344},{"type":27,"tag":261,"props":2436,"children":2437},{"style":274},[2438],{"type":33,"value":1996},{"type":27,"tag":261,"props":2440,"children":2441},{"style":493},[2442],{"type":33,"value":2443}," 'a'",{"type":27,"tag":261,"props":2445,"children":2446},{"style":274},[2447],{"type":33,"value":2368},{"type":27,"tag":261,"props":2449,"children":2450},{"style":268},[2451],{"type":33,"value":2452}," char.lower() ",{"type":27,"tag":261,"props":2454,"children":2455},{"style":274},[2456],{"type":33,"value":2377},{"type":27,"tag":261,"props":2458,"children":2459},{"style":493},[2460],{"type":33,"value":2461}," 'z'",{"type":27,"tag":261,"props":2463,"children":2464},{"style":268},[2465],{"type":33,"value":340},{"type":27,"tag":261,"props":2467,"children":2468},{"class":263,"line":803},[2469],{"type":27,"tag":261,"props":2470,"children":2471},{"emptyLinePlaceholder":22},[2472],{"type":33,"value":766},{"type":27,"tag":261,"props":2474,"children":2475},{"class":263,"line":811},[2476,2481,2486,2491],{"type":27,"tag":261,"props":2477,"children":2478},{"style":274},[2479],{"type":33,"value":2480},"    return",{"type":27,"tag":261,"props":2482,"children":2483},{"style":268},[2484],{"type":33,"value":2485}," chinese_count ",{"type":27,"tag":261,"props":2487,"children":2488},{"style":274},[2489],{"type":33,"value":2490},">",{"type":27,"tag":261,"props":2492,"children":2493},{"style":268},[2494],{"type":33,"value":2495}," english_count\n",{"type":27,"tag":36,"props":2497,"children":2498},{},[2499,2501,2506,2508,2514],{"type":33,"value":2500},"This boolean value would then be used in the ",{"type":27,"tag":170,"props":2502,"children":2504},{"className":2503},[],[2505],{"type":33,"value":712},{"type":33,"value":2507}," to use either the Chinese or English ",{"type":27,"tag":170,"props":2509,"children":2511},{"className":2510},[],[2512],{"type":33,"value":2513},"PromptTemplate",{"type":33,"value":2515},". This allowed the Q&A bot to answer questions in either Chinese or English, and it does not require translating back and forth between Chinese and English. However, this method relies on high-quality translations, so I don't expect English language questions to be answered as accurately as Chinese language questions. Here are the Chinese and English prompts that I used for the text-based Q&A bot, as well as some examples of the Q&A bot answering questions in English. The referenced materials include paragraphs from the English translation.",{"type":27,"tag":238,"props":2517,"children":2519},{"code":2518,"language":254,"meta":8,"className":255,"style":8},"# Chinese prompt for text-based Q&A bot\nq_and_a_prompt = PromptTemplate(\n    \"这是相关的参考资料：\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\" # context_str contains Chinese paragraphs retrieved via RAG query\n    \"---------------------\\n\"\n    \"根据上述的参考资料，回答下面的问题\\n\"\n    \"问题：{user_question}\\n\"\n)\n\n# English prompt for text-based Q&A bot\nq_and_a_prompt_english = PromptTemplate(\n    \"This is some related reference material:\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\" # context_str contains English paragraphs retrieved via RAG query\n    \"---------------------\\n\"\n    \"Based on the above material, answer the following question:\\n\"\n    \"Question: {user_question}\\n\"\n)\n",[2520],{"type":27,"tag":170,"props":2521,"children":2522},{"__ignoreMap":8},[2523,2531,2548,2564,2580,2602,2617,2633,2650,2657,2664,2672,2688,2704,2719,2739,2754,2770,2786],{"type":27,"tag":261,"props":2524,"children":2525},{"class":263,"line":264},[2526],{"type":27,"tag":261,"props":2527,"children":2528},{"style":478},[2529],{"type":33,"value":2530},"# Chinese prompt for text-based Q&A bot\n",{"type":27,"tag":261,"props":2532,"children":2533},{"class":263,"line":343},[2534,2539,2543],{"type":27,"tag":261,"props":2535,"children":2536},{"style":268},[2537],{"type":33,"value":2538},"q_and_a_prompt ",{"type":27,"tag":261,"props":2540,"children":2541},{"style":274},[2542],{"type":33,"value":277},{"type":27,"tag":261,"props":2544,"children":2545},{"style":268},[2546],{"type":33,"value":2547}," PromptTemplate(\n",{"type":27,"tag":261,"props":2549,"children":2550},{"class":263,"line":379},[2551,2556,2560],{"type":27,"tag":261,"props":2552,"children":2553},{"style":493},[2554],{"type":33,"value":2555},"    \"这是相关的参考资料：",{"type":27,"tag":261,"props":2557,"children":2558},{"style":295},[2559],{"type":33,"value":1948},{"type":27,"tag":261,"props":2561,"children":2562},{"style":493},[2563],{"type":33,"value":534},{"type":27,"tag":261,"props":2565,"children":2566},{"class":263,"line":537},[2567,2572,2576],{"type":27,"tag":261,"props":2568,"children":2569},{"style":493},[2570],{"type":33,"value":2571},"    \"---------------------",{"type":27,"tag":261,"props":2573,"children":2574},{"style":295},[2575],{"type":33,"value":1948},{"type":27,"tag":261,"props":2577,"children":2578},{"style":493},[2579],{"type":33,"value":534},{"type":27,"tag":261,"props":2581,"children":2582},{"class":263,"line":561},[2583,2588,2593,2597],{"type":27,"tag":261,"props":2584,"children":2585},{"style":493},[2586],{"type":33,"value":2587},"    \"",{"type":27,"tag":261,"props":2589,"children":2590},{"style":295},[2591],{"type":33,"value":2592},"{context_str}\\n",{"type":27,"tag":261,"props":2594,"children":2595},{"style":493},[2596],{"type":33,"value":1124},{"type":27,"tag":261,"props":2598,"children":2599},{"style":478},[2600],{"type":33,"value":2601}," # context_str contains Chinese paragraphs retrieved via RAG query\n",{"type":27,"tag":261,"props":2603,"children":2604},{"class":263,"line":785},[2605,2609,2613],{"type":27,"tag":261,"props":2606,"children":2607},{"style":493},[2608],{"type":33,"value":2571},{"type":27,"tag":261,"props":2610,"children":2611},{"style":295},[2612],{"type":33,"value":1948},{"type":27,"tag":261,"props":2614,"children":2615},{"style":493},[2616],{"type":33,"value":534},{"type":27,"tag":261,"props":2618,"children":2619},{"class":263,"line":794},[2620,2625,2629],{"type":27,"tag":261,"props":2621,"children":2622},{"style":493},[2623],{"type":33,"value":2624},"    \"根据上述的参考资料，回答下面的问题",{"type":27,"tag":261,"props":2626,"children":2627},{"style":295},[2628],{"type":33,"value":1948},{"type":27,"tag":261,"props":2630,"children":2631},{"style":493},[2632],{"type":33,"value":534},{"type":27,"tag":261,"props":2634,"children":2635},{"class":263,"line":803},[2636,2641,2646],{"type":27,"tag":261,"props":2637,"children":2638},{"style":493},[2639],{"type":33,"value":2640},"    \"问题：",{"type":27,"tag":261,"props":2642,"children":2643},{"style":295},[2644],{"type":33,"value":2645},"{user_question}\\n",{"type":27,"tag":261,"props":2647,"children":2648},{"style":493},[2649],{"type":33,"value":534},{"type":27,"tag":261,"props":2651,"children":2652},{"class":263,"line":811},[2653],{"type":27,"tag":261,"props":2654,"children":2655},{"style":268},[2656],{"type":33,"value":340},{"type":27,"tag":261,"props":2658,"children":2659},{"class":263,"line":860},[2660],{"type":27,"tag":261,"props":2661,"children":2662},{"emptyLinePlaceholder":22},[2663],{"type":33,"value":766},{"type":27,"tag":261,"props":2665,"children":2666},{"class":263,"line":884},[2667],{"type":27,"tag":261,"props":2668,"children":2669},{"style":478},[2670],{"type":33,"value":2671},"# English prompt for text-based Q&A bot\n",{"type":27,"tag":261,"props":2673,"children":2674},{"class":263,"line":902},[2675,2680,2684],{"type":27,"tag":261,"props":2676,"children":2677},{"style":268},[2678],{"type":33,"value":2679},"q_and_a_prompt_english ",{"type":27,"tag":261,"props":2681,"children":2682},{"style":274},[2683],{"type":33,"value":277},{"type":27,"tag":261,"props":2685,"children":2686},{"style":268},[2687],{"type":33,"value":2547},{"type":27,"tag":261,"props":2689,"children":2690},{"class":263,"line":911},[2691,2696,2700],{"type":27,"tag":261,"props":2692,"children":2693},{"style":493},[2694],{"type":33,"value":2695},"    \"This is some related reference material:",{"type":27,"tag":261,"props":2697,"children":2698},{"style":295},[2699],{"type":33,"value":1948},{"type":27,"tag":261,"props":2701,"children":2702},{"style":493},[2703],{"type":33,"value":534},{"type":27,"tag":261,"props":2705,"children":2706},{"class":263,"line":934},[2707,2711,2715],{"type":27,"tag":261,"props":2708,"children":2709},{"style":493},[2710],{"type":33,"value":2571},{"type":27,"tag":261,"props":2712,"children":2713},{"style":295},[2714],{"type":33,"value":1948},{"type":27,"tag":261,"props":2716,"children":2717},{"style":493},[2718],{"type":33,"value":534},{"type":27,"tag":261,"props":2720,"children":2721},{"class":263,"line":952},[2722,2726,2730,2734],{"type":27,"tag":261,"props":2723,"children":2724},{"style":493},[2725],{"type":33,"value":2587},{"type":27,"tag":261,"props":2727,"children":2728},{"style":295},[2729],{"type":33,"value":2592},{"type":27,"tag":261,"props":2731,"children":2732},{"style":493},[2733],{"type":33,"value":1124},{"type":27,"tag":261,"props":2735,"children":2736},{"style":478},[2737],{"type":33,"value":2738}," # context_str contains English paragraphs retrieved via RAG query\n",{"type":27,"tag":261,"props":2740,"children":2741},{"class":263,"line":970},[2742,2746,2750],{"type":27,"tag":261,"props":2743,"children":2744},{"style":493},[2745],{"type":33,"value":2571},{"type":27,"tag":261,"props":2747,"children":2748},{"style":295},[2749],{"type":33,"value":1948},{"type":27,"tag":261,"props":2751,"children":2752},{"style":493},[2753],{"type":33,"value":534},{"type":27,"tag":261,"props":2755,"children":2756},{"class":263,"line":998},[2757,2762,2766],{"type":27,"tag":261,"props":2758,"children":2759},{"style":493},[2760],{"type":33,"value":2761},"    \"Based on the above material, answer the following question:",{"type":27,"tag":261,"props":2763,"children":2764},{"style":295},[2765],{"type":33,"value":1948},{"type":27,"tag":261,"props":2767,"children":2768},{"style":493},[2769],{"type":33,"value":534},{"type":27,"tag":261,"props":2771,"children":2772},{"class":263,"line":1038},[2773,2778,2782],{"type":27,"tag":261,"props":2774,"children":2775},{"style":493},[2776],{"type":33,"value":2777},"    \"Question: ",{"type":27,"tag":261,"props":2779,"children":2780},{"style":295},[2781],{"type":33,"value":2645},{"type":27,"tag":261,"props":2783,"children":2784},{"style":493},[2785],{"type":33,"value":534},{"type":27,"tag":261,"props":2787,"children":2788},{"class":263,"line":1075},[2789],{"type":27,"tag":261,"props":2790,"children":2791},{"style":268},[2792],{"type":33,"value":340},{"type":27,"tag":36,"props":2794,"children":2795},{},[2796],{"type":27,"tag":84,"props":2797,"children":2800},{"alt":2798,"src":2799},"Multi-modal Q&A example 1","/static/redlm/qa_example_01.png",[],{"type":27,"tag":36,"props":2802,"children":2803},{},[2804],{"type":33,"value":2805},"Asking random questions like this one is a fun way to explore the many scenes of Dream of the Red Chamber.",{"type":27,"tag":36,"props":2807,"children":2808},{},[2809,2814],{"type":27,"tag":84,"props":2810,"children":2813},{"alt":2811,"src":2812},"RAG Flower Pedal Example","/static/redlm/qa_example_flower_pedals.png",[],{"type":27,"tag":84,"props":2815,"children":2818},{"alt":2816,"src":2817},"RAG Flower Pedal Example with Reference","/static/redlm/qa_example_flower_pedals_a.png",[],{"type":27,"tag":28,"props":2820,"children":2822},{"id":2821},"redlm-rag-evaluation",[2823],{"type":33,"value":2824},"RedLM RAG Evaluation",{"type":27,"tag":36,"props":2826,"children":2827},{},[2828],{"type":33,"value":2829},"Examinations have long been a cornerstone of Chinese society, shaping individual aspirations, cultural values, and even government structures. This legacy began with the imperial civil service exams, kējǔ (科举), established during the Sui and Tang dynasties, and carries through in Modern China with the gaokao (高考) college entrance examination, both of which have allowed for unprecedented meritocratic routes to power and prestige. Given how widely this novel is studied in China, I was not surprised to find a wealth of examination questions written for students studying Dream of the Red Chamber.",{"type":27,"tag":36,"props":2831,"children":2832},{},[2833,2835,2842],{"type":33,"value":2834},"I used ",{"type":27,"tag":61,"props":2836,"children":2839},{"href":2837,"rel":2838},"https://www.examcoo.com/editor/do/view/id/246401",[65],[2840],{"type":33,"value":2841},"a set of 1000 multiple choice questions about Dream of the Red Chamber on examcoo.com",{"type":33,"value":2843}," to evaluate the effectiveness of the RAG system I built with LlamaIndex. I wrote a script to parse the questions from the website HTML using ChatGPT (parsing HTML is one of my favorite use cases of LLMs!) I filtered the list of 1000 questions down to 877 questions based on the following criteria:",{"type":27,"tag":574,"props":2845,"children":2846},{},[2847,2858],{"type":27,"tag":98,"props":2848,"children":2849},{},[2850,2856],{"type":27,"tag":2851,"props":2852,"children":2853},"strong",{},[2854],{"type":33,"value":2855},"Four answer choices",{"type":33,"value":2857},": some of the questions had more than four answer choices. I filtered questions with more than four answer choices to keep the evaluation simple. This would allow me to assume that random answer choices would have a 25% chance of being correct.",{"type":27,"tag":98,"props":2859,"children":2860},{},[2861,2866],{"type":27,"tag":2851,"props":2862,"children":2863},{},[2864],{"type":33,"value":2865},"Only one answer",{"type":33,"value":2867},": For some questions the correct answer required selecting multiple answer choices. This would also help keep the evaluation logic simple.",{"type":27,"tag":36,"props":2869,"children":2870},{},[2871],{"type":27,"tag":84,"props":2872,"children":2875},{"alt":2873,"src":2874},"Multiple Choice Questions from Dream of the Red Chamber Test","/static/redlm/hlm_mcq.png",[],{"type":27,"tag":36,"props":2877,"children":2878},{},[2879],{"type":33,"value":2880},"Multiple choice questions from a Dream of the Red Chamber test (examcoo.com)",{"type":27,"tag":36,"props":2882,"children":2883},{},[2884],{"type":33,"value":2885},"To run the evaluation I set up two scripts. The first script would prompt the LLM to answer the question without any additional information from the RAG system. This served as a baseline to see how well the LLM could do at answering multiple choice questions about the book. The script simply checks to see if the LLM response contains the letter (A, B, C or D) of the correct answer and keeps track of the number of questions answered correctly.",{"type":27,"tag":36,"props":2887,"children":2888},{},[2889],{"type":33,"value":2890},"Another script was used to take the test using large language models with RAG. In this script, the prompt sent to the LLM included relevant paragraphs from the book based on how similar the query is to each paragraph in the book based on the cosine similarity metric mentioned earlier.",{"type":27,"tag":36,"props":2892,"children":2893},{},[2894],{"type":27,"tag":84,"props":2895,"children":2898},{"alt":2896,"src":2897},"RAG evaluation","/static/redlm/rag_eval.png",[],{"type":27,"tag":36,"props":2900,"children":2901},{},[2902],{"type":33,"value":2903},"Here are some results and other observations from this experiment:",{"type":27,"tag":574,"props":2905,"children":2906},{},[2907,2912,2917,2929,2934,2947],{"type":27,"tag":98,"props":2908,"children":2909},{},[2910],{"type":33,"value":2911},"LLMs alone scored in the mid 30% range (36%)",{"type":27,"tag":98,"props":2913,"children":2914},{},[2915],{"type":33,"value":2916},"LLMs using retrieval augmented generation with the set of questions score in the mid 40% range (44%)",{"type":27,"tag":98,"props":2918,"children":2919},{},[2920,2922,2927],{"type":33,"value":2921},"I used the completion API rather than the chat API and set the ",{"type":27,"tag":170,"props":2923,"children":2925},{"className":2924},[],[2926],{"type":33,"value":326},{"type":33,"value":2928}," to 16. This was done to ensure that the LLM only gave a short response with a valid answer choice rather than giving a long response with an explanation.",{"type":27,"tag":98,"props":2930,"children":2931},{},[2932],{"type":33,"value":2933},"The evaluation took longer for LLM + RAG test because of the time required for making the RAG query and the longer prompt (including both the original multiple-choice question and the referenced paragraphs).",{"type":27,"tag":98,"props":2935,"children":2936},{},[2937,2939,2945],{"type":33,"value":2938},"I used the ",{"type":27,"tag":170,"props":2940,"children":2942},{"className":2941},[],[2943],{"type":33,"value":2944},"01-ai/Yi-1.5-9B-Chat",{"type":33,"value":2946}," model for this test, but I probably should have used the base model rather than the chat model.",{"type":27,"tag":98,"props":2948,"children":2949},{},[2950],{"type":33,"value":2951},"Some questions would not be capable of being answered by RAG. For example, some of the questions are about film renditions of the novel. Most of the questions seemed relevant to the content of the book, so I didn’t bother to filter out the questions that were not directly related to the book’s content.",{"type":27,"tag":36,"props":2953,"children":2954},{},[2955,2957,2962,2964,2969],{"type":33,"value":2956},"Here is an example of a question that the LLM test script answered ",{"type":27,"tag":663,"props":2958,"children":2959},{},[2960],{"type":33,"value":2961},"incorrectly",{"type":33,"value":2963}," and the LLM + RAG test script answered ",{"type":27,"tag":2851,"props":2965,"children":2966},{},[2967],{"type":33,"value":2968},"correctly",{"type":33,"value":2970},".",{"type":27,"tag":2972,"props":2973,"children":2974},"blockquote",{},[2975,2980,2985,2990,2995],{"type":27,"tag":36,"props":2976,"children":2977},{},[2978],{"type":33,"value":2979},"秦钟的父亲是如何死的？",{"type":27,"tag":36,"props":2981,"children":2982},{},[2983],{"type":33,"value":2984},"A、外感风寒、风毒之症",{"type":27,"tag":36,"props":2986,"children":2987},{},[2988],{"type":33,"value":2989},"B、被智能儿气死的",{"type":27,"tag":36,"props":2991,"children":2992},{},[2993],{"type":33,"value":2994},"C、生气引发旧病加重",{"type":27,"tag":36,"props":2996,"children":2997},{},[2998],{"type":33,"value":2999},"D、生气而诱发中风而死",{"type":27,"tag":36,"props":3001,"children":3002},{},[3003],{"type":33,"value":3004},"Translation:",{"type":27,"tag":2972,"props":3006,"children":3007},{},[3008,3013,3023,3033,3043],{"type":27,"tag":36,"props":3009,"children":3010},{},[3011],{"type":33,"value":3012},"How did Qin Zhong's father die?",{"type":27,"tag":36,"props":3014,"children":3015},{},[3016,3021],{"type":27,"tag":2851,"props":3017,"children":3018},{},[3019],{"type":33,"value":3020},"A.",{"type":33,"value":3022}," He caught a cold and developed wind-related illnesses.",{"type":27,"tag":36,"props":3024,"children":3025},{},[3026,3031],{"type":27,"tag":2851,"props":3027,"children":3028},{},[3029],{"type":33,"value":3030},"B.",{"type":33,"value":3032}," He was angered to death by Zhineng'er (a character).",{"type":27,"tag":36,"props":3034,"children":3035},{},[3036,3041],{"type":27,"tag":2851,"props":3037,"children":3038},{},[3039],{"type":33,"value":3040},"C.",{"type":33,"value":3042}," His old illness worsened due to anger.",{"type":27,"tag":36,"props":3044,"children":3045},{},[3046,3051],{"type":27,"tag":2851,"props":3047,"children":3048},{},[3049],{"type":33,"value":3050},"D.",{"type":33,"value":3052}," He had a stroke induced by anger and died.",{"type":27,"tag":36,"props":3054,"children":3055},{},[3056],{"type":33,"value":3057},"Here is the paragraphs that the RAG query returned along with the English translation:",{"type":27,"tag":36,"props":3059,"children":3060},{},[3061],{"type":33,"value":3062},"Original",{"type":27,"tag":2972,"props":3064,"children":3065},{},[3066],{"type":27,"tag":36,"props":3067,"children":3068},{},[3069],{"type":33,"value":3070},"荣两处上下内外人等莫不欢天喜地，独有宝玉置若罔闻。你道什么缘故？原来近日水月庵的智能私逃入城，来找秦钟，不意被秦邦业知觉，将智能逐出，将秦钟打了一顿，自己气的老病发了，三五日便呜呼哀哉了。秦钟本自怯弱，又带病未痊，受了笞杖，今见老父气死，悔痛无及，又添了许多病症。因此，宝玉心中怅怅不乐。虽有元春晋封之事，那解得他的愁闷？贾母等如何谢恩，如何回家，亲友如何来庆贺，宁荣两府近日如何热闹，众人如何得意，独他一个皆视有如无，毫不介意。因此，众人嘲他越发呆了。",{"type":27,"tag":36,"props":3072,"children":3073},{},[3074],{"type":33,"value":3075},"English",{"type":27,"tag":2972,"props":3077,"children":3078},{},[3079],{"type":27,"tag":36,"props":3080,"children":3081},{},[3082],{"type":33,"value":3083},"Everyone in the Rong and Ning households, both inside and outside, were extremely happy, except for Baoyu, who seemed indifferent. Do you want to know why? It turns out that recently, the nun Zhineng from Shuiyue Temple secretly ran into the city to find Qin Zhong. Unexpectedly, she was discovered by Qin Zhong's father, Qin Banger. Qin Banger not only drove Zhineng away but also gave Qin Zhong a beating. This made Qin Banger so angry that his old illness relapsed, and within three to five days, he passed away. Qin Zhong had always been weak and hadn't fully recovered from a previous illness. After being beaten and seeing his father die in anger, he was overwhelmed with regret and sorrow, which worsened his condition. As a result, Baoyu felt very melancholic. Although the promotion of Yuan Chun to imperial concubine was a joyful event, it couldn't alleviate the gloom in his heart. While Grandmother Jia and others were busy expressing their gratitude and returning home, and relatives and friends came to celebrate, and the Rong and Ning households were bustling with excitement, Baoyu alone remained completely indifferent to it all. Consequently, everyone started to mock him for becoming more and more absent-minded.",{"type":27,"tag":36,"props":3085,"children":3086},{},[3087],{"type":33,"value":3088},"The correct answer for this question is C.",{"type":27,"tag":28,"props":3090,"children":3092},{"id":3091},"multi-modal-rag-for-visual-reasoning",[3093],{"type":33,"value":3094},"Multi-modal RAG for visual reasoning",{"type":27,"tag":36,"props":3096,"children":3097},{},[3098],{"type":33,"value":3099},"Qwen2-VL is a new AI model that was released in late August 2024. Qwen is the name of Alibaba’s AI Lab, and it is an abbreviation of the Chinese characters: 千问 (\"qian wen\", meaning 1000 questions). VL stands for vision-language, meaning that the model is capable of understanding both text and images. I had tested out the previous version of Qwen’s vision-language model and was very impressed by how it could accurately describe the contents of images and also answer general questions about images.",{"type":27,"tag":36,"props":3101,"children":3102},{},[3103],{"type":33,"value":3104},"Sun Wen was a Qing-era painter who spent 36 years of his life creating a series of 230 paintings capturing scenes from Dream of the Red Chamber. The paintings are incredibly detailed and often contain repeated figures in a temporal sequence. If you asked a Qwen-VL model to describe one of the images, it might return lengthy description that doesn't fully capture the full detail of the scene. It might also be difficult for a language model to \"focus\" on a portion of the whole image.",{"type":27,"tag":36,"props":3106,"children":3107},{},[3108],{"type":27,"tag":84,"props":3109,"children":3112},{"alt":3110,"src":3111},"Dream of the Red Chamber Painting 131","/static/redlm/painting_131.png",[],{"type":27,"tag":36,"props":3114,"children":3115},{},[3116],{"type":33,"value":3117},"This sparked the idea to create a feature where users can click and drag over an image to select part of a painting, then ask questions specifically about the selected portion. I knew that while this could be achieved with tools like HTML canvas, writing it on my own would be quite time-consuming. It took me just a few minutes to write out the prompt, and Claude 3.5 Sonnet generated a perfect prototype of this feature in under a minute. Here’s the prompt I used:",{"type":27,"tag":2972,"props":3119,"children":3120},{},[3121,3126,3131,3149,3154],{"type":27,"tag":36,"props":3122,"children":3123},{},[3124],{"type":33,"value":3125},"I'm going to describe a Vue component and I want you to write it using Vue 3 to the best of your ability.",{"type":27,"tag":36,"props":3127,"children":3128},{},[3129],{"type":33,"value":3130},"write a simple single-file vue component using Vue 3 that does the following:",{"type":27,"tag":574,"props":3132,"children":3133},{},[3134,3139,3144],{"type":27,"tag":98,"props":3135,"children":3136},{},[3137],{"type":33,"value":3138},"displays an image",{"type":27,"tag":98,"props":3140,"children":3141},{},[3142],{"type":33,"value":3143},"allows the users to click and drag to select a subsection of the image",{"type":27,"tag":98,"props":3145,"children":3146},{},[3147],{"type":33,"value":3148},"the subsection of the image is saved as a base64-encoded data url to a variable that is displayed below the image",{"type":27,"tag":36,"props":3150,"children":3151},{},[3152],{"type":33,"value":3153},"The solution should make use of HTML canvas. When you click down on the image you begin selecting the subsection. You then move the mouse to make your subsection on the image, and when you mouse-up the subsection is selected and the data url is updated. Then the subsection is displayed at the very bottom of the page as a \"preview\" image using the base 64 image string as the image source.",{"type":27,"tag":36,"props":3155,"children":3156},{},[3157],{"type":33,"value":3158},"The selection box should be a dashed red line",{"type":27,"tag":36,"props":3160,"children":3161},{},[3162],{"type":27,"tag":84,"props":3163,"children":3166},{"alt":3164,"src":3165},"RedLM Image Q&A","/static/redlm/image-qa.png",[],{"type":27,"tag":36,"props":3168,"children":3169},{},[3170],{"type":33,"value":3171},"This shows the final result of the UI I built for the image Q&A feature in RedLM. It uses a similar chat layout that the text-based Q&A feature uses, with the addition of the image preview included in the chat log. The user query in this example just says “Please describe the contents of the image”. This was the first image that I tested when building the image Q&A feature to see if the correct passage can be referenced based on the description of an image. This pulled the exact passage and the answer provides details about what happened (a fire broke out) where it happened (at the Gourd Temple) and why it happened (a Monk accidentally set an oil pot on fire).",{"type":27,"tag":36,"props":3173,"children":3174},{},[3175],{"type":33,"value":3176},"Here is a diagram showing the overall flow of data in the image Q&A feature:",{"type":27,"tag":36,"props":3178,"children":3179},{},[3180],{"type":27,"tag":84,"props":3181,"children":3184},{"alt":3182,"src":3183},"Diagram of RedLM Image Q&A with RAG and Vision Language Models","/static/redlm/redlm.drawio.png",[],{"type":27,"tag":36,"props":3186,"children":3187},{},[3188],{"type":33,"value":3189},"This flow chart shows how the image Q&A feature works.",{"type":27,"tag":94,"props":3191,"children":3192},{},[3193,3206,3227,3232,3244],{"type":27,"tag":98,"props":3194,"children":3195},{},[3196,3198,3204],{"type":33,"value":3197},"The user selects part of an image and writes a question. This data is then sent to the RedLM API as a post request to the ",{"type":27,"tag":170,"props":3199,"children":3201},{"className":3200},[],[3202],{"type":33,"value":3203},"/mm-q-and-a",{"type":33,"value":3205}," endpoint (multi-modal Q&A).",{"type":27,"tag":98,"props":3207,"children":3208},{},[3209,3211,3217,3219,3225],{"type":33,"value":3210},"Vision language models are used to get a description of the image. Depending on the application configuration, this query can use models such as ",{"type":27,"tag":170,"props":3212,"children":3214},{"className":3213},[],[3215],{"type":33,"value":3216},"Qwen/Qwen2-VL-2B-Instruct",{"type":33,"value":3218}," on RTX PCs or using the NVIDIA API Catalog using larger models such as ",{"type":27,"tag":170,"props":3220,"children":3222},{"className":3221},[],[3223],{"type":33,"value":3224},"meta/llama-3.2-90b-vision-instruct",{"type":33,"value":3226},". Not all vision language models have the same interface, so I added some logic to handle different model formats.",{"type":27,"tag":98,"props":3228,"children":3229},{},[3230],{"type":33,"value":3231},"The image description is used to fetch relevant documents from the Vector Database",{"type":27,"tag":98,"props":3233,"children":3234},{},[3235,3237,3242],{"type":33,"value":3236},"The full prompt with the image description and relevant documents is sent to the LLM. Again, inference for this step is done either with RTX PCs or using models from the ",{"type":27,"tag":170,"props":3238,"children":3240},{"className":3239},[],[3241],{"type":33,"value":175},{"type":33,"value":3243}," API catalog.",{"type":27,"tag":98,"props":3245,"children":3246},{},[3247],{"type":33,"value":3248},"The response from the LLM is sent back to the browser and is displayed to the user as a chat message.",{"type":27,"tag":36,"props":3250,"children":3251},{},[3252],{"type":33,"value":3253},"Here is the prompt I used for the image Q&A feature:",{"type":27,"tag":238,"props":3255,"children":3257},{"code":3256,"language":254,"meta":8,"className":255,"style":8},"# Chinese prompt for image-based Q&A bot\nmm_q_and_a_prompt = PromptTemplate(\n    \"这是书中相关的内容：\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"下面是场景的描述：\\n\"\n    \"---------------------\\n\"\n    \"{image_description}\\n\"\n    \"---------------------\\n\"\n    \"根据上述的信息，尽量解释上说的场景和书的关系。\"\n)\n\n# English prompt for image-based Q&A bot\nmm_q_and_a_prompt_english = PromptTemplate(\n    \"Here is relevant content from the book:\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Below is the description of a scene:\\n\"\n    \"---------------------\\n\"\n    \"{image_description}\\n\"\n    \"---------------------\\n\"\n    \"Based on the information provided above, try to explain the relationship between the described scene and the book content.\"\n)\n",[3258],{"type":27,"tag":170,"props":3259,"children":3260},{"__ignoreMap":8},[3261,3269,3285,3301,3316,3331,3347,3362,3378,3393,3401,3408,3415,3423,3439,3455,3470,3485,3501,3516,3531,3546,3554],{"type":27,"tag":261,"props":3262,"children":3263},{"class":263,"line":264},[3264],{"type":27,"tag":261,"props":3265,"children":3266},{"style":478},[3267],{"type":33,"value":3268},"# Chinese prompt for image-based Q&A bot\n",{"type":27,"tag":261,"props":3270,"children":3271},{"class":263,"line":343},[3272,3277,3281],{"type":27,"tag":261,"props":3273,"children":3274},{"style":268},[3275],{"type":33,"value":3276},"mm_q_and_a_prompt ",{"type":27,"tag":261,"props":3278,"children":3279},{"style":274},[3280],{"type":33,"value":277},{"type":27,"tag":261,"props":3282,"children":3283},{"style":268},[3284],{"type":33,"value":2547},{"type":27,"tag":261,"props":3286,"children":3287},{"class":263,"line":379},[3288,3293,3297],{"type":27,"tag":261,"props":3289,"children":3290},{"style":493},[3291],{"type":33,"value":3292},"    \"这是书中相关的内容：",{"type":27,"tag":261,"props":3294,"children":3295},{"style":295},[3296],{"type":33,"value":1948},{"type":27,"tag":261,"props":3298,"children":3299},{"style":493},[3300],{"type":33,"value":534},{"type":27,"tag":261,"props":3302,"children":3303},{"class":263,"line":537},[3304,3308,3312],{"type":27,"tag":261,"props":3305,"children":3306},{"style":493},[3307],{"type":33,"value":2587},{"type":27,"tag":261,"props":3309,"children":3310},{"style":295},[3311],{"type":33,"value":2592},{"type":27,"tag":261,"props":3313,"children":3314},{"style":493},[3315],{"type":33,"value":534},{"type":27,"tag":261,"props":3317,"children":3318},{"class":263,"line":561},[3319,3323,3327],{"type":27,"tag":261,"props":3320,"children":3321},{"style":493},[3322],{"type":33,"value":2571},{"type":27,"tag":261,"props":3324,"children":3325},{"style":295},[3326],{"type":33,"value":1948},{"type":27,"tag":261,"props":3328,"children":3329},{"style":493},[3330],{"type":33,"value":534},{"type":27,"tag":261,"props":3332,"children":3333},{"class":263,"line":785},[3334,3339,3343],{"type":27,"tag":261,"props":3335,"children":3336},{"style":493},[3337],{"type":33,"value":3338},"    \"下面是场景的描述：",{"type":27,"tag":261,"props":3340,"children":3341},{"style":295},[3342],{"type":33,"value":1948},{"type":27,"tag":261,"props":3344,"children":3345},{"style":493},[3346],{"type":33,"value":534},{"type":27,"tag":261,"props":3348,"children":3349},{"class":263,"line":794},[3350,3354,3358],{"type":27,"tag":261,"props":3351,"children":3352},{"style":493},[3353],{"type":33,"value":2571},{"type":27,"tag":261,"props":3355,"children":3356},{"style":295},[3357],{"type":33,"value":1948},{"type":27,"tag":261,"props":3359,"children":3360},{"style":493},[3361],{"type":33,"value":534},{"type":27,"tag":261,"props":3363,"children":3364},{"class":263,"line":803},[3365,3369,3374],{"type":27,"tag":261,"props":3366,"children":3367},{"style":493},[3368],{"type":33,"value":2587},{"type":27,"tag":261,"props":3370,"children":3371},{"style":295},[3372],{"type":33,"value":3373},"{image_description}\\n",{"type":27,"tag":261,"props":3375,"children":3376},{"style":493},[3377],{"type":33,"value":534},{"type":27,"tag":261,"props":3379,"children":3380},{"class":263,"line":811},[3381,3385,3389],{"type":27,"tag":261,"props":3382,"children":3383},{"style":493},[3384],{"type":33,"value":2571},{"type":27,"tag":261,"props":3386,"children":3387},{"style":295},[3388],{"type":33,"value":1948},{"type":27,"tag":261,"props":3390,"children":3391},{"style":493},[3392],{"type":33,"value":534},{"type":27,"tag":261,"props":3394,"children":3395},{"class":263,"line":860},[3396],{"type":27,"tag":261,"props":3397,"children":3398},{"style":493},[3399],{"type":33,"value":3400},"    \"根据上述的信息，尽量解释上说的场景和书的关系。\"\n",{"type":27,"tag":261,"props":3402,"children":3403},{"class":263,"line":884},[3404],{"type":27,"tag":261,"props":3405,"children":3406},{"style":268},[3407],{"type":33,"value":340},{"type":27,"tag":261,"props":3409,"children":3410},{"class":263,"line":902},[3411],{"type":27,"tag":261,"props":3412,"children":3413},{"emptyLinePlaceholder":22},[3414],{"type":33,"value":766},{"type":27,"tag":261,"props":3416,"children":3417},{"class":263,"line":911},[3418],{"type":27,"tag":261,"props":3419,"children":3420},{"style":478},[3421],{"type":33,"value":3422},"# English prompt for image-based Q&A bot\n",{"type":27,"tag":261,"props":3424,"children":3425},{"class":263,"line":934},[3426,3431,3435],{"type":27,"tag":261,"props":3427,"children":3428},{"style":268},[3429],{"type":33,"value":3430},"mm_q_and_a_prompt_english ",{"type":27,"tag":261,"props":3432,"children":3433},{"style":274},[3434],{"type":33,"value":277},{"type":27,"tag":261,"props":3436,"children":3437},{"style":268},[3438],{"type":33,"value":2547},{"type":27,"tag":261,"props":3440,"children":3441},{"class":263,"line":952},[3442,3447,3451],{"type":27,"tag":261,"props":3443,"children":3444},{"style":493},[3445],{"type":33,"value":3446},"    \"Here is relevant content from the book:",{"type":27,"tag":261,"props":3448,"children":3449},{"style":295},[3450],{"type":33,"value":1948},{"type":27,"tag":261,"props":3452,"children":3453},{"style":493},[3454],{"type":33,"value":534},{"type":27,"tag":261,"props":3456,"children":3457},{"class":263,"line":970},[3458,3462,3466],{"type":27,"tag":261,"props":3459,"children":3460},{"style":493},[3461],{"type":33,"value":2587},{"type":27,"tag":261,"props":3463,"children":3464},{"style":295},[3465],{"type":33,"value":2592},{"type":27,"tag":261,"props":3467,"children":3468},{"style":493},[3469],{"type":33,"value":534},{"type":27,"tag":261,"props":3471,"children":3472},{"class":263,"line":998},[3473,3477,3481],{"type":27,"tag":261,"props":3474,"children":3475},{"style":493},[3476],{"type":33,"value":2571},{"type":27,"tag":261,"props":3478,"children":3479},{"style":295},[3480],{"type":33,"value":1948},{"type":27,"tag":261,"props":3482,"children":3483},{"style":493},[3484],{"type":33,"value":534},{"type":27,"tag":261,"props":3486,"children":3487},{"class":263,"line":1038},[3488,3493,3497],{"type":27,"tag":261,"props":3489,"children":3490},{"style":493},[3491],{"type":33,"value":3492},"    \"Below is the description of a scene:",{"type":27,"tag":261,"props":3494,"children":3495},{"style":295},[3496],{"type":33,"value":1948},{"type":27,"tag":261,"props":3498,"children":3499},{"style":493},[3500],{"type":33,"value":534},{"type":27,"tag":261,"props":3502,"children":3503},{"class":263,"line":1075},[3504,3508,3512],{"type":27,"tag":261,"props":3505,"children":3506},{"style":493},[3507],{"type":33,"value":2571},{"type":27,"tag":261,"props":3509,"children":3510},{"style":295},[3511],{"type":33,"value":1948},{"type":27,"tag":261,"props":3513,"children":3514},{"style":493},[3515],{"type":33,"value":534},{"type":27,"tag":261,"props":3517,"children":3518},{"class":263,"line":1083},[3519,3523,3527],{"type":27,"tag":261,"props":3520,"children":3521},{"style":493},[3522],{"type":33,"value":2587},{"type":27,"tag":261,"props":3524,"children":3525},{"style":295},[3526],{"type":33,"value":3373},{"type":27,"tag":261,"props":3528,"children":3529},{"style":493},[3530],{"type":33,"value":534},{"type":27,"tag":261,"props":3532,"children":3533},{"class":263,"line":1092},[3534,3538,3542],{"type":27,"tag":261,"props":3535,"children":3536},{"style":493},[3537],{"type":33,"value":2571},{"type":27,"tag":261,"props":3539,"children":3540},{"style":295},[3541],{"type":33,"value":1948},{"type":27,"tag":261,"props":3543,"children":3544},{"style":493},[3545],{"type":33,"value":534},{"type":27,"tag":261,"props":3547,"children":3548},{"class":263,"line":1100},[3549],{"type":27,"tag":261,"props":3550,"children":3551},{"style":493},[3552],{"type":33,"value":3553},"    \"Based on the information provided above, try to explain the relationship between the described scene and the book content.\"\n",{"type":27,"tag":261,"props":3555,"children":3556},{"class":263,"line":1151},[3557],{"type":27,"tag":261,"props":3558,"children":3559},{"style":268},[3560],{"type":33,"value":340},{"type":27,"tag":36,"props":3562,"children":3563},{},[3564],{"type":33,"value":3565},"The prompt engineering for this feature was tricky. I was able to get some awesome results that would give me detailed and accurate responses, and then sometimes the LLM would seem confused about my query and tell me that there was no relationship between the scene description and the book content. Sometimes it would give me an accurate description of the scene, but then proceed to tell me that the book content is not related to the scene at all.",{"type":27,"tag":36,"props":3567,"children":3568},{},[3569],{"type":33,"value":3570},"There is another important concept from LlamaIndex that I used to build the image Q&A feature: metadata filtering. Metadata filtering is an important concept in RAG systems  because it helps you focus your query on relevant documents in a precise way. A very simple example might be a RAG system that indexes news articles and stores the associated date as metadata. You could allow a user to set a date range for their query and only include articles that match the given date range.",{"type":27,"tag":36,"props":3572,"children":3573},{},[3574],{"type":33,"value":3575},"For my image Q&A system, I have a mapping between the paintings and their associated chapters. When I ask a question about a painting, I want to use the description of the image to find similar paragraphs, but only the paragraphs that occur in the painting’s associated chapter. What I ended up doing was filtering the entire index before making the query. The alternative would be filtering the returned nodes after making the query, but this would have the possibility of not returning any nodes.",{"type":27,"tag":36,"props":3577,"children":3578},{},[3579],{"type":33,"value":3580},"Here’s what some of the metadata filtering code looks like:",{"type":27,"tag":238,"props":3582,"children":3584},{"code":3583,"language":254,"meta":8,"className":255,"style":8},"# main.py\n# filter by chapters associated with the queried image\nfilters = MetadataFilters(\n    filters=[ExactMatchFilter(key=\"chapter\", value=str(req_data.chapter))]\n)\nquery_engine = get_query_engine_for_multi_modal(filters)\n\n# rag.py\n# utility function that returns the query engine use for image Q&A\n# the index is filtered to only include nodes associated with the image being queried\ndef get_query_engine_for_multi_modal(filters):\n    retriever = index.as_retriever(filters=filters)\n    synthesizer = get_response_synthesizer(response_mode=\"compact\")\n    try:\n        query_engine = QAndAQueryEngine(\n            retriever=retriever,\n            response_synthesizer=synthesizer,\n            llm=model,\n            qa_prompt=mm_q_and_a_prompt,\n        )\n    except Exception as e:\n        print(e)\n    return query_engine\n",[3585],{"type":27,"tag":170,"props":3586,"children":3587},{"__ignoreMap":8},[3588,3596,3604,3621,3673,3680,3697,3704,3712,3720,3728,3753,3783,3818,3830,3847,3864,3881,3898,3915,3922,3945,3958],{"type":27,"tag":261,"props":3589,"children":3590},{"class":263,"line":264},[3591],{"type":27,"tag":261,"props":3592,"children":3593},{"style":478},[3594],{"type":33,"value":3595},"# main.py\n",{"type":27,"tag":261,"props":3597,"children":3598},{"class":263,"line":343},[3599],{"type":27,"tag":261,"props":3600,"children":3601},{"style":478},[3602],{"type":33,"value":3603},"# filter by chapters associated with the queried image\n",{"type":27,"tag":261,"props":3605,"children":3606},{"class":263,"line":379},[3607,3612,3616],{"type":27,"tag":261,"props":3608,"children":3609},{"style":268},[3610],{"type":33,"value":3611},"filters ",{"type":27,"tag":261,"props":3613,"children":3614},{"style":274},[3615],{"type":33,"value":277},{"type":27,"tag":261,"props":3617,"children":3618},{"style":268},[3619],{"type":33,"value":3620}," MetadataFilters(\n",{"type":27,"tag":261,"props":3622,"children":3623},{"class":263,"line":537},[3624,3629,3633,3638,3643,3647,3651,3655,3660,3664,3668],{"type":27,"tag":261,"props":3625,"children":3626},{"style":285},[3627],{"type":33,"value":3628},"    filters",{"type":27,"tag":261,"props":3630,"children":3631},{"style":274},[3632],{"type":33,"value":277},{"type":27,"tag":261,"props":3634,"children":3635},{"style":268},[3636],{"type":33,"value":3637},"[ExactMatchFilter(",{"type":27,"tag":261,"props":3639,"children":3640},{"style":285},[3641],{"type":33,"value":3642},"key",{"type":27,"tag":261,"props":3644,"children":3645},{"style":274},[3646],{"type":33,"value":277},{"type":27,"tag":261,"props":3648,"children":3649},{"style":493},[3650],{"type":33,"value":1008},{"type":27,"tag":261,"props":3652,"children":3653},{"style":268},[3654],{"type":33,"value":303},{"type":27,"tag":261,"props":3656,"children":3657},{"style":285},[3658],{"type":33,"value":3659},"value",{"type":27,"tag":261,"props":3661,"children":3662},{"style":274},[3663],{"type":33,"value":277},{"type":27,"tag":261,"props":3665,"children":3666},{"style":850},[3667],{"type":33,"value":853},{"type":27,"tag":261,"props":3669,"children":3670},{"style":268},[3671],{"type":33,"value":3672},"(req_data.chapter))]\n",{"type":27,"tag":261,"props":3674,"children":3675},{"class":263,"line":561},[3676],{"type":27,"tag":261,"props":3677,"children":3678},{"style":268},[3679],{"type":33,"value":340},{"type":27,"tag":261,"props":3681,"children":3682},{"class":263,"line":785},[3683,3688,3692],{"type":27,"tag":261,"props":3684,"children":3685},{"style":268},[3686],{"type":33,"value":3687},"query_engine ",{"type":27,"tag":261,"props":3689,"children":3690},{"style":274},[3691],{"type":33,"value":277},{"type":27,"tag":261,"props":3693,"children":3694},{"style":268},[3695],{"type":33,"value":3696}," get_query_engine_for_multi_modal(filters)\n",{"type":27,"tag":261,"props":3698,"children":3699},{"class":263,"line":794},[3700],{"type":27,"tag":261,"props":3701,"children":3702},{"emptyLinePlaceholder":22},[3703],{"type":33,"value":766},{"type":27,"tag":261,"props":3705,"children":3706},{"class":263,"line":803},[3707],{"type":27,"tag":261,"props":3708,"children":3709},{"style":478},[3710],{"type":33,"value":3711},"# rag.py\n",{"type":27,"tag":261,"props":3713,"children":3714},{"class":263,"line":811},[3715],{"type":27,"tag":261,"props":3716,"children":3717},{"style":478},[3718],{"type":33,"value":3719},"# utility function that returns the query engine use for image Q&A\n",{"type":27,"tag":261,"props":3721,"children":3722},{"class":263,"line":860},[3723],{"type":27,"tag":261,"props":3724,"children":3725},{"style":478},[3726],{"type":33,"value":3727},"# the index is filtered to only include nodes associated with the image being queried\n",{"type":27,"tag":261,"props":3729,"children":3730},{"class":263,"line":884},[3731,3735,3740,3744,3749],{"type":27,"tag":261,"props":3732,"children":3733},{"style":487},[3734],{"type":33,"value":1475},{"type":27,"tag":261,"props":3736,"children":3737},{"style":820},[3738],{"type":33,"value":3739}," get_query_engine_for_multi_modal",{"type":27,"tag":261,"props":3741,"children":3742},{"style":268},[3743],{"type":33,"value":740},{"type":27,"tag":261,"props":3745,"children":3746},{"style":830},[3747],{"type":33,"value":3748},"filters",{"type":27,"tag":261,"props":3750,"children":3751},{"style":268},[3752],{"type":33,"value":750},{"type":27,"tag":261,"props":3754,"children":3755},{"class":263,"line":902},[3756,3761,3765,3770,3774,3778],{"type":27,"tag":261,"props":3757,"children":3758},{"style":268},[3759],{"type":33,"value":3760},"    retriever ",{"type":27,"tag":261,"props":3762,"children":3763},{"style":274},[3764],{"type":33,"value":277},{"type":27,"tag":261,"props":3766,"children":3767},{"style":268},[3768],{"type":33,"value":3769}," index.as_retriever(",{"type":27,"tag":261,"props":3771,"children":3772},{"style":285},[3773],{"type":33,"value":3748},{"type":27,"tag":261,"props":3775,"children":3776},{"style":274},[3777],{"type":33,"value":277},{"type":27,"tag":261,"props":3779,"children":3780},{"style":268},[3781],{"type":33,"value":3782},"filters)\n",{"type":27,"tag":261,"props":3784,"children":3785},{"class":263,"line":911},[3786,3791,3795,3800,3805,3809,3814],{"type":27,"tag":261,"props":3787,"children":3788},{"style":268},[3789],{"type":33,"value":3790},"    synthesizer ",{"type":27,"tag":261,"props":3792,"children":3793},{"style":274},[3794],{"type":33,"value":277},{"type":27,"tag":261,"props":3796,"children":3797},{"style":268},[3798],{"type":33,"value":3799}," get_response_synthesizer(",{"type":27,"tag":261,"props":3801,"children":3802},{"style":285},[3803],{"type":33,"value":3804},"response_mode",{"type":27,"tag":261,"props":3806,"children":3807},{"style":274},[3808],{"type":33,"value":277},{"type":27,"tag":261,"props":3810,"children":3811},{"style":493},[3812],{"type":33,"value":3813},"\"compact\"",{"type":27,"tag":261,"props":3815,"children":3816},{"style":268},[3817],{"type":33,"value":340},{"type":27,"tag":261,"props":3819,"children":3820},{"class":263,"line":934},[3821,3826],{"type":27,"tag":261,"props":3822,"children":3823},{"style":274},[3824],{"type":33,"value":3825},"    try",{"type":27,"tag":261,"props":3827,"children":3828},{"style":268},[3829],{"type":33,"value":2128},{"type":27,"tag":261,"props":3831,"children":3832},{"class":263,"line":952},[3833,3838,3842],{"type":27,"tag":261,"props":3834,"children":3835},{"style":268},[3836],{"type":33,"value":3837},"        query_engine ",{"type":27,"tag":261,"props":3839,"children":3840},{"style":274},[3841],{"type":33,"value":277},{"type":27,"tag":261,"props":3843,"children":3844},{"style":268},[3845],{"type":33,"value":3846}," QAndAQueryEngine(\n",{"type":27,"tag":261,"props":3848,"children":3849},{"class":263,"line":970},[3850,3855,3859],{"type":27,"tag":261,"props":3851,"children":3852},{"style":285},[3853],{"type":33,"value":3854},"            retriever",{"type":27,"tag":261,"props":3856,"children":3857},{"style":274},[3858],{"type":33,"value":277},{"type":27,"tag":261,"props":3860,"children":3861},{"style":268},[3862],{"type":33,"value":3863},"retriever,\n",{"type":27,"tag":261,"props":3865,"children":3866},{"class":263,"line":998},[3867,3872,3876],{"type":27,"tag":261,"props":3868,"children":3869},{"style":285},[3870],{"type":33,"value":3871},"            response_synthesizer",{"type":27,"tag":261,"props":3873,"children":3874},{"style":274},[3875],{"type":33,"value":277},{"type":27,"tag":261,"props":3877,"children":3878},{"style":268},[3879],{"type":33,"value":3880},"synthesizer,\n",{"type":27,"tag":261,"props":3882,"children":3883},{"class":263,"line":1038},[3884,3889,3893],{"type":27,"tag":261,"props":3885,"children":3886},{"style":285},[3887],{"type":33,"value":3888},"            llm",{"type":27,"tag":261,"props":3890,"children":3891},{"style":274},[3892],{"type":33,"value":277},{"type":27,"tag":261,"props":3894,"children":3895},{"style":268},[3896],{"type":33,"value":3897},"model,\n",{"type":27,"tag":261,"props":3899,"children":3900},{"class":263,"line":1075},[3901,3906,3910],{"type":27,"tag":261,"props":3902,"children":3903},{"style":285},[3904],{"type":33,"value":3905},"            qa_prompt",{"type":27,"tag":261,"props":3907,"children":3908},{"style":274},[3909],{"type":33,"value":277},{"type":27,"tag":261,"props":3911,"children":3912},{"style":268},[3913],{"type":33,"value":3914},"mm_q_and_a_prompt,\n",{"type":27,"tag":261,"props":3916,"children":3917},{"class":263,"line":1083},[3918],{"type":27,"tag":261,"props":3919,"children":3920},{"style":268},[3921],{"type":33,"value":1301},{"type":27,"tag":261,"props":3923,"children":3924},{"class":263,"line":1092},[3925,3930,3935,3940],{"type":27,"tag":261,"props":3926,"children":3927},{"style":274},[3928],{"type":33,"value":3929},"    except",{"type":27,"tag":261,"props":3931,"children":3932},{"style":850},[3933],{"type":33,"value":3934}," Exception",{"type":27,"tag":261,"props":3936,"children":3937},{"style":274},[3938],{"type":33,"value":3939}," as",{"type":27,"tag":261,"props":3941,"children":3942},{"style":268},[3943],{"type":33,"value":3944}," e:\n",{"type":27,"tag":261,"props":3946,"children":3947},{"class":263,"line":1100},[3948,3953],{"type":27,"tag":261,"props":3949,"children":3950},{"style":1520},[3951],{"type":33,"value":3952},"        print",{"type":27,"tag":261,"props":3954,"children":3955},{"style":268},[3956],{"type":33,"value":3957},"(e)\n",{"type":27,"tag":261,"props":3959,"children":3960},{"class":263,"line":1151},[3961,3965],{"type":27,"tag":261,"props":3962,"children":3963},{"style":274},[3964],{"type":33,"value":2480},{"type":27,"tag":261,"props":3966,"children":3967},{"style":268},[3968],{"type":33,"value":3969}," query_engine\n",{"type":27,"tag":36,"props":3971,"children":3972},{},[3973],{"type":33,"value":3974},"This seemed to work well for my use case, but it might not be a best practice, and it might not be efficient at a bigger scale.",{"type":27,"tag":47,"props":3976,"children":3978},{"id":3977},"multi-modal-qa-examples",[3979],{"type":33,"value":3980},"Multi-modal Q&A examples",{"type":27,"tag":36,"props":3982,"children":3983},{},[3984],{"type":33,"value":3985},"Here are some more examples of results from different types of questions from the multi-modal Q&A bot.",{"type":27,"tag":36,"props":3987,"children":3988},{},[3989],{"type":33,"value":3990},"The response to the following query did a good job of combining information gathered from the image description and image from related passages.",{"type":27,"tag":36,"props":3992,"children":3993},{},[3994,3999],{"type":27,"tag":84,"props":3995,"children":3998},{"alt":3996,"src":3997},"Multi-modal Q&A example 2","/static/redlm/qa_example_02.png",[],{"type":27,"tag":84,"props":4000,"children":4003},{"alt":4001,"src":4002},"Multi-modal Q&A example 3","/static/redlm/qa_example_03.png",[],{"type":27,"tag":36,"props":4005,"children":4006},{},[4007],{"type":27,"tag":84,"props":4008,"children":4011},{"alt":4009,"src":4010},"Q&A Example with Carriage","/static/redlm/qa_example_carriage.png",[],{"type":27,"tag":36,"props":4013,"children":4014},{},[4015],{"type":27,"tag":84,"props":4016,"children":4019},{"alt":4017,"src":4018},"Ou Guan Example","/static/redlm/qa_example_ou_guan.png",[],{"type":27,"tag":28,"props":4021,"children":4023},{"id":4022},"llamaindex-developer-experience",[4024],{"type":33,"value":4025},"LlamaIndex Developer Experience",{"type":27,"tag":36,"props":4027,"children":4028},{},[4029],{"type":33,"value":4030},"Overall, I found the LlamaIndex documentation to be very helpful. Before using LlamaIndex for this project I had used LangChain to build a RAG POC, but I didn’t get very good results. I love how the LlamaIndex documentation has a 5-line starter example for building a RAG system:",{"type":27,"tag":238,"props":4032,"children":4034},{"code":4033,"language":254,"meta":8,"className":255,"style":8},"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n",[4035],{"type":27,"tag":170,"props":4036,"children":4037},{"__ignoreMap":8},[4038,4058,4065,4092,4108,4124,4150],{"type":27,"tag":261,"props":4039,"children":4040},{"class":263,"line":264},[4041,4045,4049,4053],{"type":27,"tag":261,"props":4042,"children":4043},{"style":274},[4044],{"type":33,"value":1349},{"type":27,"tag":261,"props":4046,"children":4047},{"style":268},[4048],{"type":33,"value":1354},{"type":27,"tag":261,"props":4050,"children":4051},{"style":274},[4052],{"type":33,"value":1359},{"type":27,"tag":261,"props":4054,"children":4055},{"style":268},[4056],{"type":33,"value":4057}," VectorStoreIndex, SimpleDirectoryReader\n",{"type":27,"tag":261,"props":4059,"children":4060},{"class":263,"line":343},[4061],{"type":27,"tag":261,"props":4062,"children":4063},{"emptyLinePlaceholder":22},[4064],{"type":33,"value":766},{"type":27,"tag":261,"props":4066,"children":4067},{"class":263,"line":379},[4068,4073,4077,4082,4087],{"type":27,"tag":261,"props":4069,"children":4070},{"style":268},[4071],{"type":33,"value":4072},"documents ",{"type":27,"tag":261,"props":4074,"children":4075},{"style":274},[4076],{"type":33,"value":277},{"type":27,"tag":261,"props":4078,"children":4079},{"style":268},[4080],{"type":33,"value":4081}," SimpleDirectoryReader(",{"type":27,"tag":261,"props":4083,"children":4084},{"style":493},[4085],{"type":33,"value":4086},"\"data\"",{"type":27,"tag":261,"props":4088,"children":4089},{"style":268},[4090],{"type":33,"value":4091},").load_data()\n",{"type":27,"tag":261,"props":4093,"children":4094},{"class":263,"line":537},[4095,4100,4104],{"type":27,"tag":261,"props":4096,"children":4097},{"style":268},[4098],{"type":33,"value":4099},"index ",{"type":27,"tag":261,"props":4101,"children":4102},{"style":274},[4103],{"type":33,"value":277},{"type":27,"tag":261,"props":4105,"children":4106},{"style":268},[4107],{"type":33,"value":2066},{"type":27,"tag":261,"props":4109,"children":4110},{"class":263,"line":561},[4111,4115,4119],{"type":27,"tag":261,"props":4112,"children":4113},{"style":268},[4114],{"type":33,"value":3687},{"type":27,"tag":261,"props":4116,"children":4117},{"style":274},[4118],{"type":33,"value":277},{"type":27,"tag":261,"props":4120,"children":4121},{"style":268},[4122],{"type":33,"value":4123}," index.as_query_engine()\n",{"type":27,"tag":261,"props":4125,"children":4126},{"class":263,"line":785},[4127,4132,4136,4141,4146],{"type":27,"tag":261,"props":4128,"children":4129},{"style":268},[4130],{"type":33,"value":4131},"response ",{"type":27,"tag":261,"props":4133,"children":4134},{"style":274},[4135],{"type":33,"value":277},{"type":27,"tag":261,"props":4137,"children":4138},{"style":268},[4139],{"type":33,"value":4140}," query_engine.query(",{"type":27,"tag":261,"props":4142,"children":4143},{"style":493},[4144],{"type":33,"value":4145},"\"Some question about the data should go here\"",{"type":27,"tag":261,"props":4147,"children":4148},{"style":268},[4149],{"type":33,"value":340},{"type":27,"tag":261,"props":4151,"children":4152},{"class":263,"line":794},[4153,4158],{"type":27,"tag":261,"props":4154,"children":4155},{"style":1520},[4156],{"type":33,"value":4157},"print",{"type":27,"tag":261,"props":4159,"children":4160},{"style":268},[4161],{"type":33,"value":4162},"(response)\n",{"type":27,"tag":36,"props":4164,"children":4165},{},[4166,4167],{"type":33,"value":695},{"type":27,"tag":61,"props":4168,"children":4171},{"href":4169,"rel":4170},"https://docs.llamaindex.ai/en/stable/#getting-started",[65],[4172],{"type":33,"value":4169},{"type":27,"tag":36,"props":4174,"children":4175},{},[4176,4178,4185],{"type":33,"value":4177},"I was able to expand this simple example to implement the text and image Q&A bots for RedLM fairly easily. The application I built is somewhat similar to the ",{"type":27,"tag":61,"props":4179,"children":4182},{"href":4180,"rel":4181},"https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_app_guide/",[65],[4183],{"type":33,"value":4184},"Full-Stack Web App with LLamaIndex",{"type":33,"value":4186}," included in their documentation.",{"type":27,"tag":36,"props":4188,"children":4189},{},[4190,4192,4197,4199,4206,4208,4214],{"type":33,"value":4191},"Most of the early development I did on this project used the ",{"type":27,"tag":170,"props":4193,"children":4195},{"className":4194},[],[4196],{"type":33,"value":712},{"type":33,"value":4198},". Later I tried using ",{"type":27,"tag":61,"props":4200,"children":4203},{"href":4201,"rel":4202},"https://docs.llamaindex.ai/en/stable/module_guides/workflow/",[65],[4204],{"type":33,"value":4205},"LlamaIndex Workflows",{"type":33,"value":4207}," to better organize the logic in the text and image-based Q&A bots. The same workflow ",{"type":27,"tag":170,"props":4209,"children":4211},{"className":4210},[],[4212],{"type":33,"value":4213},"RAGWorkflow",{"type":33,"value":4215}," is used to handle requests for both the text and image Q&A bot queries. Workflows also work seamlessly with asynchronous Python frameworks like FastAPI. Here's the API endpoint for the multimodal image-Q&A bot using a LlamaIndex Workflow:",{"type":27,"tag":238,"props":4217,"children":4219},{"code":4218,"language":254,"meta":8,"className":255,"style":8},"@app.post(\"/mm-q-and-a\")\nasync def mm_q_and_a_workflow(req_data: MultiModalRequest):\n    \"\"\"\n    This function handles Multimodal Q&A bot requests using a LlamaIndex workflow\n    \"\"\"\n    try:\n        # parse data from request object\n        image_b64 = req_data.image\n        prompt = req_data.prompt\n        chapter = req_data.chapter\n\n        # setup LlamaIndex Workflow and run it with data from request\n        w = RAGWorkflow(timeout=None)\n        result = await w.run(query=prompt, image_data=image_b64, chapter_number=chapter)\n\n        # return response\n        return QAQueryResponse(\n            response=result[\"response\"].message.content,\n            metadata=result[\"metadata\"],\n            image_desc=result[\"image_description\"],\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n",[4220],{"type":27,"tag":170,"props":4221,"children":4222},{"__ignoreMap":8},[4223,4244,4276,4283,4291,4298,4309,4317,4334,4351,4368,4375,4383,4418,4482,4489,4497,4509,4536,4562,4587,4594,4613],{"type":27,"tag":261,"props":4224,"children":4225},{"class":263,"line":264},[4226,4231,4235,4240],{"type":27,"tag":261,"props":4227,"children":4228},{"style":820},[4229],{"type":33,"value":4230},"@app.post",{"type":27,"tag":261,"props":4232,"children":4233},{"style":268},[4234],{"type":33,"value":740},{"type":27,"tag":261,"props":4236,"children":4237},{"style":493},[4238],{"type":33,"value":4239},"\"/mm-q-and-a\"",{"type":27,"tag":261,"props":4241,"children":4242},{"style":268},[4243],{"type":33,"value":340},{"type":27,"tag":261,"props":4245,"children":4246},{"class":263,"line":343},[4247,4252,4257,4262,4266,4271],{"type":27,"tag":261,"props":4248,"children":4249},{"style":487},[4250],{"type":33,"value":4251},"async",{"type":27,"tag":261,"props":4253,"children":4254},{"style":487},[4255],{"type":33,"value":4256}," def",{"type":27,"tag":261,"props":4258,"children":4259},{"style":820},[4260],{"type":33,"value":4261}," mm_q_and_a_workflow",{"type":27,"tag":261,"props":4263,"children":4264},{"style":268},[4265],{"type":33,"value":740},{"type":27,"tag":261,"props":4267,"children":4268},{"style":830},[4269],{"type":33,"value":4270},"req_data",{"type":27,"tag":261,"props":4272,"children":4273},{"style":268},[4274],{"type":33,"value":4275},": MultiModalRequest):\n",{"type":27,"tag":261,"props":4277,"children":4278},{"class":263,"line":379},[4279],{"type":27,"tag":261,"props":4280,"children":4281},{"style":493},[4282],{"type":33,"value":2277},{"type":27,"tag":261,"props":4284,"children":4285},{"class":263,"line":537},[4286],{"type":27,"tag":261,"props":4287,"children":4288},{"style":493},[4289],{"type":33,"value":4290},"    This function handles Multimodal Q&A bot requests using a LlamaIndex workflow\n",{"type":27,"tag":261,"props":4292,"children":4293},{"class":263,"line":561},[4294],{"type":27,"tag":261,"props":4295,"children":4296},{"style":493},[4297],{"type":33,"value":2277},{"type":27,"tag":261,"props":4299,"children":4300},{"class":263,"line":785},[4301,4305],{"type":27,"tag":261,"props":4302,"children":4303},{"style":274},[4304],{"type":33,"value":3825},{"type":27,"tag":261,"props":4306,"children":4307},{"style":268},[4308],{"type":33,"value":2128},{"type":27,"tag":261,"props":4310,"children":4311},{"class":263,"line":794},[4312],{"type":27,"tag":261,"props":4313,"children":4314},{"style":478},[4315],{"type":33,"value":4316},"        # parse data from request object\n",{"type":27,"tag":261,"props":4318,"children":4319},{"class":263,"line":803},[4320,4325,4329],{"type":27,"tag":261,"props":4321,"children":4322},{"style":268},[4323],{"type":33,"value":4324},"        image_b64 ",{"type":27,"tag":261,"props":4326,"children":4327},{"style":274},[4328],{"type":33,"value":277},{"type":27,"tag":261,"props":4330,"children":4331},{"style":268},[4332],{"type":33,"value":4333}," req_data.image\n",{"type":27,"tag":261,"props":4335,"children":4336},{"class":263,"line":811},[4337,4342,4346],{"type":27,"tag":261,"props":4338,"children":4339},{"style":268},[4340],{"type":33,"value":4341},"        prompt ",{"type":27,"tag":261,"props":4343,"children":4344},{"style":274},[4345],{"type":33,"value":277},{"type":27,"tag":261,"props":4347,"children":4348},{"style":268},[4349],{"type":33,"value":4350}," req_data.prompt\n",{"type":27,"tag":261,"props":4352,"children":4353},{"class":263,"line":860},[4354,4359,4363],{"type":27,"tag":261,"props":4355,"children":4356},{"style":268},[4357],{"type":33,"value":4358},"        chapter ",{"type":27,"tag":261,"props":4360,"children":4361},{"style":274},[4362],{"type":33,"value":277},{"type":27,"tag":261,"props":4364,"children":4365},{"style":268},[4366],{"type":33,"value":4367}," req_data.chapter\n",{"type":27,"tag":261,"props":4369,"children":4370},{"class":263,"line":884},[4371],{"type":27,"tag":261,"props":4372,"children":4373},{"emptyLinePlaceholder":22},[4374],{"type":33,"value":766},{"type":27,"tag":261,"props":4376,"children":4377},{"class":263,"line":902},[4378],{"type":27,"tag":261,"props":4379,"children":4380},{"style":478},[4381],{"type":33,"value":4382},"        # setup LlamaIndex Workflow and run it with data from request\n",{"type":27,"tag":261,"props":4384,"children":4385},{"class":263,"line":911},[4386,4391,4395,4400,4405,4409,4414],{"type":27,"tag":261,"props":4387,"children":4388},{"style":268},[4389],{"type":33,"value":4390},"        w ",{"type":27,"tag":261,"props":4392,"children":4393},{"style":274},[4394],{"type":33,"value":277},{"type":27,"tag":261,"props":4396,"children":4397},{"style":268},[4398],{"type":33,"value":4399}," RAGWorkflow(",{"type":27,"tag":261,"props":4401,"children":4402},{"style":285},[4403],{"type":33,"value":4404},"timeout",{"type":27,"tag":261,"props":4406,"children":4407},{"style":274},[4408],{"type":33,"value":277},{"type":27,"tag":261,"props":4410,"children":4411},{"style":295},[4412],{"type":33,"value":4413},"None",{"type":27,"tag":261,"props":4415,"children":4416},{"style":268},[4417],{"type":33,"value":340},{"type":27,"tag":261,"props":4419,"children":4420},{"class":263,"line":934},[4421,4426,4430,4435,4440,4445,4449,4454,4459,4463,4468,4473,4477],{"type":27,"tag":261,"props":4422,"children":4423},{"style":268},[4424],{"type":33,"value":4425},"        result ",{"type":27,"tag":261,"props":4427,"children":4428},{"style":274},[4429],{"type":33,"value":277},{"type":27,"tag":261,"props":4431,"children":4432},{"style":274},[4433],{"type":33,"value":4434}," await",{"type":27,"tag":261,"props":4436,"children":4437},{"style":268},[4438],{"type":33,"value":4439}," w.run(",{"type":27,"tag":261,"props":4441,"children":4442},{"style":285},[4443],{"type":33,"value":4444},"query",{"type":27,"tag":261,"props":4446,"children":4447},{"style":274},[4448],{"type":33,"value":277},{"type":27,"tag":261,"props":4450,"children":4451},{"style":268},[4452],{"type":33,"value":4453},"prompt, ",{"type":27,"tag":261,"props":4455,"children":4456},{"style":285},[4457],{"type":33,"value":4458},"image_data",{"type":27,"tag":261,"props":4460,"children":4461},{"style":274},[4462],{"type":33,"value":277},{"type":27,"tag":261,"props":4464,"children":4465},{"style":268},[4466],{"type":33,"value":4467},"image_b64, ",{"type":27,"tag":261,"props":4469,"children":4470},{"style":285},[4471],{"type":33,"value":4472},"chapter_number",{"type":27,"tag":261,"props":4474,"children":4475},{"style":274},[4476],{"type":33,"value":277},{"type":27,"tag":261,"props":4478,"children":4479},{"style":268},[4480],{"type":33,"value":4481},"chapter)\n",{"type":27,"tag":261,"props":4483,"children":4484},{"class":263,"line":952},[4485],{"type":27,"tag":261,"props":4486,"children":4487},{"emptyLinePlaceholder":22},[4488],{"type":33,"value":766},{"type":27,"tag":261,"props":4490,"children":4491},{"class":263,"line":970},[4492],{"type":27,"tag":261,"props":4493,"children":4494},{"style":478},[4495],{"type":33,"value":4496},"        # return response\n",{"type":27,"tag":261,"props":4498,"children":4499},{"class":263,"line":998},[4500,4504],{"type":27,"tag":261,"props":4501,"children":4502},{"style":274},[4503],{"type":33,"value":1318},{"type":27,"tag":261,"props":4505,"children":4506},{"style":268},[4507],{"type":33,"value":4508}," QAQueryResponse(\n",{"type":27,"tag":261,"props":4510,"children":4511},{"class":263,"line":1038},[4512,4517,4521,4526,4531],{"type":27,"tag":261,"props":4513,"children":4514},{"style":285},[4515],{"type":33,"value":4516},"            response",{"type":27,"tag":261,"props":4518,"children":4519},{"style":274},[4520],{"type":33,"value":277},{"type":27,"tag":261,"props":4522,"children":4523},{"style":268},[4524],{"type":33,"value":4525},"result[",{"type":27,"tag":261,"props":4527,"children":4528},{"style":493},[4529],{"type":33,"value":4530},"\"response\"",{"type":27,"tag":261,"props":4532,"children":4533},{"style":268},[4534],{"type":33,"value":4535},"].message.content,\n",{"type":27,"tag":261,"props":4537,"children":4538},{"class":263,"line":1075},[4539,4544,4548,4552,4557],{"type":27,"tag":261,"props":4540,"children":4541},{"style":285},[4542],{"type":33,"value":4543},"            metadata",{"type":27,"tag":261,"props":4545,"children":4546},{"style":274},[4547],{"type":33,"value":277},{"type":27,"tag":261,"props":4549,"children":4550},{"style":268},[4551],{"type":33,"value":4525},{"type":27,"tag":261,"props":4553,"children":4554},{"style":493},[4555],{"type":33,"value":4556},"\"metadata\"",{"type":27,"tag":261,"props":4558,"children":4559},{"style":268},[4560],{"type":33,"value":4561},"],\n",{"type":27,"tag":261,"props":4563,"children":4564},{"class":263,"line":1083},[4565,4570,4574,4578,4583],{"type":27,"tag":261,"props":4566,"children":4567},{"style":285},[4568],{"type":33,"value":4569},"            image_desc",{"type":27,"tag":261,"props":4571,"children":4572},{"style":274},[4573],{"type":33,"value":277},{"type":27,"tag":261,"props":4575,"children":4576},{"style":268},[4577],{"type":33,"value":4525},{"type":27,"tag":261,"props":4579,"children":4580},{"style":493},[4581],{"type":33,"value":4582},"\"image_description\"",{"type":27,"tag":261,"props":4584,"children":4585},{"style":268},[4586],{"type":33,"value":4561},{"type":27,"tag":261,"props":4588,"children":4589},{"class":263,"line":1092},[4590],{"type":27,"tag":261,"props":4591,"children":4592},{"style":268},[4593],{"type":33,"value":1301},{"type":27,"tag":261,"props":4595,"children":4596},{"class":263,"line":1100},[4597,4601,4605,4609],{"type":27,"tag":261,"props":4598,"children":4599},{"style":274},[4600],{"type":33,"value":3929},{"type":27,"tag":261,"props":4602,"children":4603},{"style":850},[4604],{"type":33,"value":3934},{"type":27,"tag":261,"props":4606,"children":4607},{"style":274},[4608],{"type":33,"value":3939},{"type":27,"tag":261,"props":4610,"children":4611},{"style":268},[4612],{"type":33,"value":3944},{"type":27,"tag":261,"props":4614,"children":4615},{"class":263,"line":1151},[4616,4621,4626,4631,4635,4640,4644,4649,4653,4657],{"type":27,"tag":261,"props":4617,"children":4618},{"style":274},[4619],{"type":33,"value":4620},"        raise",{"type":27,"tag":261,"props":4622,"children":4623},{"style":268},[4624],{"type":33,"value":4625}," HTTPException(",{"type":27,"tag":261,"props":4627,"children":4628},{"style":285},[4629],{"type":33,"value":4630},"status_code",{"type":27,"tag":261,"props":4632,"children":4633},{"style":274},[4634],{"type":33,"value":277},{"type":27,"tag":261,"props":4636,"children":4637},{"style":295},[4638],{"type":33,"value":4639},"500",{"type":27,"tag":261,"props":4641,"children":4642},{"style":268},[4643],{"type":33,"value":303},{"type":27,"tag":261,"props":4645,"children":4646},{"style":285},[4647],{"type":33,"value":4648},"detail",{"type":27,"tag":261,"props":4650,"children":4651},{"style":274},[4652],{"type":33,"value":277},{"type":27,"tag":261,"props":4654,"children":4655},{"style":850},[4656],{"type":33,"value":853},{"type":27,"tag":261,"props":4658,"children":4659},{"style":268},[4660],{"type":33,"value":4661},"(e))\n",{"type":27,"tag":36,"props":4663,"children":4664},{},[4665,4667,4672],{"type":33,"value":4666},"Using LlamaIndex Workflows also helped me add additional logic in a maintainable and standardized way. For example, I expanded the ",{"type":27,"tag":170,"props":4668,"children":4670},{"className":4669},[],[4671],{"type":33,"value":4213},{"type":33,"value":4673}," logic to include LLM-based re-ranking in order to ensure retrieval of the most relevant documents for my chatbot queries. This technique increases request latency, but this is an acceptable tradeoff for an application like RedLM.",{"type":27,"tag":47,"props":4675,"children":4677},{"id":4676},"llmrerank",[4678],{"type":33,"value":4679},"LLMRerank",{"type":27,"tag":36,"props":4681,"children":4682},{},[4683,4685,4690],{"type":33,"value":4684},"LLM Rerank was an interesting technique to try out, and LlamaIndex provides ",{"type":27,"tag":170,"props":4686,"children":4688},{"className":4687},[],[4689],{"type":33,"value":4679},{"type":33,"value":4691}," to make the implementation as simple as possible. Here's my understanding of how it works:",{"type":27,"tag":574,"props":4693,"children":4694},{},[4695,4700,4705,4717],{"type":27,"tag":98,"props":4696,"children":4697},{},[4698],{"type":33,"value":4699},"LLMRerank searches in the vector database for a high number of documents that are relevant to your query. This is done using cosine similarity, which essentially compares the vectors that represent the query and the documents.",{"type":27,"tag":98,"props":4701,"children":4702},{},[4703],{"type":33,"value":4704},"Next, LLMRerank goes through a process of assigning a numerical to each document to score relevancy. It does this via a special prompt that requests relevancy score for each document in batches.",{"type":27,"tag":98,"props":4706,"children":4707},{},[4708,4710,4715],{"type":33,"value":4709},"For example, I configured ",{"type":27,"tag":170,"props":4711,"children":4713},{"className":4712},[],[4714],{"type":33,"value":4679},{"type":33,"value":4716}," to initially fetch 4 documents from the vector database based on cosine similarity. Then in batches of 2, relevancy scores are assigned. Finally, the top 2 most relevant documents based on the LLM-give scores are used to make the RAG query.",{"type":27,"tag":98,"props":4718,"children":4719},{},[4720],{"type":33,"value":4721},"Adding LLMRerank can require a number of additional queries based on how you configure the batch size and the number of documents you would like to compare. This will increase latency for your application and use more resources to make the extra calls.",{"type":27,"tag":36,"props":4723,"children":4724},{},[4725,4727,4732],{"type":33,"value":4726},"Here's an example LLM query that ",{"type":27,"tag":170,"props":4728,"children":4730},{"className":4729},[],[4731],{"type":33,"value":4679},{"type":33,"value":4733}," uses to do assign scores:",{"type":27,"tag":36,"props":4735,"children":4736},{},[4737],{"type":27,"tag":84,"props":4738,"children":4741},{"alt":4739,"src":4740},"LLMRerank Prompt","/static/redlm/llmrerank_prompt.png",[],{"type":27,"tag":36,"props":4743,"children":4744},{},[4745],{"type":33,"value":4746},"Here are logs from my application showing what happens inside the workflow.",{"type":27,"tag":36,"props":4748,"children":4749},{},[4750],{"type":33,"value":4751},"Application for text-base Q&A query:",{"type":27,"tag":238,"props":4753,"children":4755},{"code":4754},"INFO:     💬Request for Q&A chatbot: query='宝玉和谁打架？'\nINFO:     🔀Routing Workflow to next step\nINFO:     💬Routing to QueryEvent\nINFO:     🧮Query the vector database with: 宝玉和谁打架？\nINFO:     🖥️Using in-memory embedding database\nINFO:     ⏳Loading index from storage directory...\nINFO:     ✅Finished loading index.\nINFO:     📐Retrieved 4 nodes.\nINFO:     🔀Doing LLMRerank\nINFO:     ℹ️ Chat Model Info:\nINFO:     🟩Using NVIDIA Cloud API for inference\nINFO:     🔘Chat Model: baichuan-inc/baichuan2-13b-chat\nINFO:     🔢Reranked nodes to 2\nINFO:     🤖Doing inference step\nINFO:     ⚙️ Getting query engine..\nINFO:     🔎Getting response from custom query engine\nINFO:     💬Text-based Q&A query\nINFO:     🀄Text is Chinese\nINFO:     Using nodes from workflow...\nINFO:     🔏Formatting prompt\nINFO:     Prompt is\n\n这是相关的参考资料：\n---------------------\n宝玉从来没有经历过这样的痛苦。起初，他觉得被打得很痛，乱喊乱叫。后来，他的气变得虚弱，声音变得嘶哑，无法说话。众门客见他被打得很惨，赶上来恳求他停下来。贾政不肯听，说：“你们知道他干了什么坏事，还能饶他吗？平时都是你们这些人把他带坏了，现在到了这步田地，你们还来劝他。明天，如果他杀父弑君，你们才不劝吗？”\n\n宝玉从来没有经历过这样的痛苦。起初，他觉得打得很痛，乱喊乱叫。后来，他的气变得虚弱，声音变得嘶哑，无法说话。众门客见他被打得很惨，赶上来恳求他停下来。贾政不肯听，说：“你们知道他干了什么坏事，还能饶他吗？平时都是你们这些人把他带坏了，现在到了这步田地，你们还来劝他。明天，如果他杀父弑君，你们才不劝吗？”\n---------------------\n根据上述的参考资料，回答下面的问题\n问题：宝玉和谁打架？\n\nResponse...\n宝玉和贾政打架。\n",[4756],{"type":27,"tag":170,"props":4757,"children":4758},{"__ignoreMap":8},[4759],{"type":33,"value":4754},{"type":27,"tag":36,"props":4761,"children":4762},{},[4763],{"type":33,"value":4764},"My question here was basically asking \"Who gets in a fight with Baoyu?\" The reply says that his father, Jiazheng, gets in a fight with Baoyu, and the documents that are used here very similar, differing by only one character. One of the documents is supposed to be and English translation, but in fact there was a failure in the translation for this paragraph and it \"translated\" the Chinese by simply repeating it. A translation of this paragraph using GPT 4o describes a tense scene between protagonist Jia Baoyu and his father Jia Zheng:",{"type":27,"tag":2972,"props":4766,"children":4767},{},[4768],{"type":27,"tag":36,"props":4769,"children":4770},{},[4771],{"type":33,"value":4772},"Baoyu had never endured such agony before. At first, he felt the pain intensely and cried out loudly. Later, his breath grew weak, his voice turned hoarse, and he couldn’t speak. The attendants, seeing how severely he was being beaten, rushed forward to plead for him to stop. Jia Zheng refused to listen, saying, “Do you know the misdeeds he’s committed, and still you want to spare him? Normally, it’s you people who lead him astray, and now that it’s come to this, you still try to persuade him? Tomorrow, if he were to commit patricide or treason, would you still not advise him?”",{"type":27,"tag":36,"props":4774,"children":4775},{},[4776],{"type":33,"value":4777},"Another benefit of LlamaIndex workflows is the ability to create visualizations of each step, the branches between them and the overall flow of events and the functions that accept/emit them as arguments/return values. It took a little bit of getting used to the patterns used to create workflows, but the documentation for Workflows provided a good starting point that I could adapt for my application. Here's a visualization of the LlamaIndex Workflow that is used by the image and text-based Q&A bots:",{"type":27,"tag":36,"props":4779,"children":4780},{},[4781],{"type":27,"tag":84,"props":4782,"children":4785},{"alt":4783,"src":4784},"RedLM RAG Workflow","/static/redlm/rag_workflow.png",[],{"type":27,"tag":47,"props":4787,"children":4789},{"id":4788},"observability-and-tracing-with-langfuse",[4790],{"type":33,"value":4791},"Observability and Tracing with Langfuse",{"type":27,"tag":36,"props":4793,"children":4794},{},[4795,4797,4802],{"type":33,"value":4796},"It is never too soon to add observability and tracing to a RAG application! I learned this the hard way after doing some refactoring of prompts and ",{"type":27,"tag":170,"props":4798,"children":4800},{"className":4799},[],[4801],{"type":33,"value":712},{"type":33,"value":4803}," logic.",{"type":27,"tag":2972,"props":4805,"children":4806},{},[4807],{"type":27,"tag":36,"props":4808,"children":4809},{},[4810],{"type":33,"value":4811},"Langfuse is an open source LLM engineering platform to help teams collaboratively debug, analyze and iterate on their LLM Applications. With the Langfuse integration, you can seamlessly track and monitor performance, traces, and metrics of your LlamaIndex application. Detailed traces of the LlamaIndex context augmentation and the LLM querying processes are captured and can be inspected directly in the Langfuse UI.",{"type":27,"tag":36,"props":4813,"children":4814},{},[4815,4817,4824],{"type":33,"value":4816},"LlamaIndex supports lots of different observability and tracing solutions. I tried using ",{"type":27,"tag":61,"props":4818,"children":4821},{"href":4819,"rel":4820},"https://langfuse.com/",[65],[4822],{"type":33,"value":4823},"Langfuse",{"type":33,"value":4825}," (YC W23) which is an open-source option that has a self hosted option.",{"type":27,"tag":36,"props":4827,"children":4828},{},[4829],{"type":27,"tag":84,"props":4830,"children":4833},{"alt":4831,"src":4832},"Langfuse tracing for RedLM","/static/redlm/langfuse.png",[],{"type":27,"tag":36,"props":4835,"children":4836},{},[4837],{"type":33,"value":4838},"Langfuse came in handy when debugging the prompts for the image-based Q&A bot. This screenshot shows a trace of a multi-modal Q&A bot query about the fire at the Gourd Temple that occurs in Chapter 1 of the book.",{"type":27,"tag":28,"props":4840,"children":4842},{"id":4841},"nvidia-inference-stack-tensorrt-llm-and-buildnvidiacom",[4843],{"type":33,"value":4844},"NVIDIA inference stack (TensorRT-LLM and build.nvidia.com)",{"type":27,"tag":36,"props":4846,"children":4847},{},[4848],{"type":33,"value":4849},"The LLM API for TensorRT-LLM is a very nice developer experience compared with my earlier attempts with manually building inference engines. The roadmap for TensorRT-LLM looks promising, I’m looking forward to support for an OpenAI Compatible API and more models. NVIDIA NIMs using TensorRT-LLM are an easy way to run models as OpenAI compatible API servers, but the selection of models is still pretty limited. vLLM provides a strong alternative with a wide range of support models. NVIDIA NIMs for LLMs build on vLLM libraries and the TensorRT-LLM library, so it is helpful to have an understanding of both of these libraries to stay on the bleeding edge of performant inference engines.",{"type":27,"tag":36,"props":4851,"children":4852},{},[4853],{"type":27,"tag":84,"props":4854,"children":4857},{"alt":4855,"src":4856},"trt-llm-roadmap","/static/redlm/trt-llm-roadmap.png",[],{"type":27,"tag":36,"props":4859,"children":4860},{},[4861],{"type":33,"value":4862},"The NVIDIA API catalog is a great way to test a variety of different models, especially large models that cannot fit into consumer hardware like RTX PCs or high-end MacBooks. I got to try out the new meta/llama-3.2-90b-vision-instruct model in my project by simply changing a value in my .env file, this is a great developer experience!",{"type":27,"tag":36,"props":4864,"children":4865},{},[4866],{"type":27,"tag":84,"props":4867,"children":4869},{"alt":175,"src":4868},"/static/redlm/build.nvidia.com.png",[],{"type":27,"tag":36,"props":4871,"children":4872},{},[4873],{"type":33,"value":4874},"The NVIDIA API catalog doesn’t have every model in every size, however. For example, it has the qwen/qwen2-7b-instruct model, but doesn’t have the qwen/qwen2-7b-instruct model. Also, only some of the models are labeled as “Run Anywhere”; a lot of the models say “Self-Hosted API Coming Soon” meaning that they can’t be downloaded an run locally as a container. To get around this, I ran inferences services locally using both vLLM’s vllm/vllm-openai container and my own container running Qwen and other services.",{"type":27,"tag":28,"props":4876,"children":4878},{"id":4877},"my-local-inference-stack-rtx",[4879],{"type":33,"value":4880},"My local inference stack (RTX)",{"type":27,"tag":36,"props":4882,"children":4883},{},[4884],{"type":27,"tag":84,"props":4885,"children":4888},{"alt":4886,"src":4887},"RTX PCs","/static/redlm/rtxpcs.png",[],{"type":27,"tag":36,"props":4890,"children":4891},{},[4892,4894,4900,4902,4908,4910,4915],{"type":33,"value":4893},"Two of the RTX PCs in my home network: ",{"type":27,"tag":170,"props":4895,"children":4897},{"className":4896},[],[4898],{"type":33,"value":4899},"a1",{"type":33,"value":4901}," and ",{"type":27,"tag":170,"props":4903,"children":4905},{"className":4904},[],[4906],{"type":33,"value":4907},"a3",{"type":33,"value":4909},". ",{"type":27,"tag":170,"props":4911,"children":4913},{"className":4912},[],[4914],{"type":33,"value":4899},{"type":33,"value":4916}," was the first PC I built by myself and was the beginning of my GeForce journey. Luckily I built it with an over-provisioned PSU, so it can use a 4090 FE card! The front panel doesn't fit, however.",{"type":27,"tag":36,"props":4918,"children":4919},{},[4920],{"type":33,"value":4921},"One limitation of the NVIDIA API catalog is the number of free credits given for a trial account. Using 1 credit per API call, I would use up the 1000 credits very quickly when running scripts like translation or the RAG evaluation. The same would be true with rate limits of the OpenAI API. That’s why running LLMs locally is still an important part of the development cycle for this type of project.",{"type":27,"tag":36,"props":4923,"children":4924},{},[4925,4927,4934],{"type":33,"value":4926},"This project primarily uses two models: a large language model and a vision language models. Running the Yi-1.5-9B-Chat model from ",{"type":27,"tag":61,"props":4928,"children":4931},{"href":4929,"rel":4930},"http://01.AI",[65],[4932],{"type":33,"value":4933},"01.AI",{"type":33,"value":4935}," takes up just about all of the GPU memory on one of my RTX 4090 PCs, so I had to run the vision model on another PC. In a previous project, I used Kubernetes to manage lots of different inference services: LLMs, ComfyUI, ChatTTS and MusicGen for making AI videos and I found it to a nice way to manage different containerized inference services.",{"type":27,"tag":238,"props":4937,"children":4939},{"code":4938},"brian@a3:~$ microk8s kubectl get no -o wide\nNAME   STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\na1     Ready    \u003Cnone>   4d4h   v1.30.5   192.168.5.182   \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\na2     Ready    \u003Cnone>   11d    v1.30.5   192.168.5.96    \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\na3     Ready    \u003Cnone>   11d    v1.30.5   192.168.5.173   \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\n",[4940],{"type":27,"tag":170,"props":4941,"children":4942},{"__ignoreMap":8},[4943],{"type":33,"value":4938},{"type":27,"tag":36,"props":4945,"children":4946},{},[4947,4949,4955],{"type":33,"value":4948},"In the RedLM GitHub repo I included kubernetes manifests that show how to run the LLM and VLM across two different computers. I used Kustomize as a way to replace dynamic values in the YAML files for different resources. The kubernetes set up is experimental; the LLM and VLM can more reliably be run with ",{"type":27,"tag":170,"props":4950,"children":4952},{"className":4951},[],[4953],{"type":33,"value":4954},"docker run",{"type":33,"value":4956}," commands.",{"type":27,"tag":36,"props":4958,"children":4959},{},[4960],{"type":27,"tag":84,"props":4961,"children":4964},{"alt":4962,"src":4963},"k8s dashboard for local inference services","/static/redlm/k8s-dashboard.png",[],{"type":27,"tag":36,"props":4966,"children":4967},{},[4968,4970,4976],{"type":33,"value":4969},"I had a lot of driver issues when trying to get kubernetes to run the vLLM container for the Yi LLM. I struggled with the following error message when trying to run the ",{"type":27,"tag":170,"props":4971,"children":4973},{"className":4972},[],[4974],{"type":33,"value":4975},"vllm",{"type":33,"value":4977}," LLM service:",{"type":27,"tag":2972,"props":4979,"children":4980},{},[4981],{"type":27,"tag":36,"props":4982,"children":4983},{},[4984],{"type":33,"value":4985},"RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",{"type":27,"tag":36,"props":4987,"children":4988},{},[4989],{"type":33,"value":4990},"I tried uninstalling and reinstalling different versions of the NVIDIA drivers and CUDA but kept seeing the same message once the server would try to start up in the vLLM container logs. Rebooting my PC didn't work either. I saw a recommendation to turn off secure boot in BIOS. I didn't turn it on, but having nothing else to try I went into the BIOS settings and found that there were some keys configured in the secure boot section. After I deleted these keys and reboot, everything seemed to work normally. I'm not sure why my PC was in secure boot mode, though!",{"type":27,"tag":28,"props":4992,"children":4994},{"id":4993},"ai-models-used-in-this-project",[4995],{"type":33,"value":4996},"AI Models used in this project",{"type":27,"tag":36,"props":4998,"children":4999},{},[5000],{"type":33,"value":5001},"I selected LLMs that run efficiently on RTX PCs, are available in the NVIDIA API catalog, and offer strong bilingual support in Chinese and English, ensuring compatibility, performance, and linguistic flexibility. Here are the models that I ended up using with RedLM:",{"type":27,"tag":47,"props":5003,"children":5005},{"id":5004},"_01-aiyi-15-9b-chat-and-nvidiayi-large",[5006,5011,5012],{"type":27,"tag":170,"props":5007,"children":5009},{"className":5008},[],[5010],{"type":33,"value":2944},{"type":33,"value":4901},{"type":27,"tag":170,"props":5013,"children":5015},{"className":5014},[],[5016],{"type":33,"value":5017},"nvidia/yi-large",{"type":27,"tag":36,"props":5019,"children":5020},{},[5021,5022,5027,5029,5036,5038,5044,5046,5052],{"type":33,"value":2834},{"type":27,"tag":170,"props":5023,"children":5025},{"className":5024},[],[5026],{"type":33,"value":2944},{"type":33,"value":5028}," for most of the LLM inference while developing RedLM on my RTX PCs. ",{"type":27,"tag":61,"props":5030,"children":5033},{"href":5031,"rel":5032},"https://github.com/01-ai/Yi",[65],[5034],{"type":33,"value":5035},"This model family",{"type":33,"value":5037}," performs well on both Chinese and English benchmarks, and has a variety of model sizes. I was able to try using the ",{"type":27,"tag":170,"props":5039,"children":5041},{"className":5040},[],[5042],{"type":33,"value":5043},"01-ai/yi-large",{"type":33,"value":5045}," model from the NVIDIA API catalog when using remote cloud inference. I used the ",{"type":27,"tag":170,"props":5047,"children":5049},{"className":5048},[],[5050],{"type":33,"value":5051},"vllm/vllm-openai:latest",{"type":33,"value":5053}," container to run this locally.",{"type":27,"tag":36,"props":5055,"children":5056},{},[5057,5059,5066],{"type":33,"value":5058},"There are also vision models in the Yi series, such as ",{"type":27,"tag":61,"props":5060,"children":5063},{"href":5061,"rel":5062},"https://huggingface.co/01-ai/Yi-VL-34B",[65],[5064],{"type":33,"value":5065},"01-ai/Yi-VL-34B",{"type":33,"value":5067},", but I didn't use these models in my project.",{"type":27,"tag":47,"props":5069,"children":5071},{"id":5070},"baichuan-incbaichuan2-13b-chat",[5072],{"type":27,"tag":170,"props":5073,"children":5075},{"className":5074},[],[5076],{"type":33,"value":5077},"baichuan-inc/baichuan2-13b-chat",{"type":27,"tag":36,"props":5079,"children":5080},{},[5081],{"type":33,"value":5082},"This model is available in the NVIDIA API catalog, and it was the main model I used when testing remote inference. It performs well in a variety of tasks and scores highly on the the Chinese Massive Multitask Language Understanding (CMMLU) benchmark.",{"type":27,"tag":47,"props":5084,"children":5086},{"id":5085},"qwenqwen2-7b",[5087],{"type":27,"tag":170,"props":5088,"children":5090},{"className":5089},[],[5091],{"type":33,"value":5092},"Qwen/Qwen2-7B",{"type":27,"tag":36,"props":5094,"children":5095},{},[5096],{"type":33,"value":5097},"This model was used for summary and translation of the source text. It was supported by the TensorRT-LLM LLM API and I didn't have any issues building the TensorRT-LLM model with it on the EC2 instance used to do the completion inference for translations.",{"type":27,"tag":47,"props":5099,"children":5101},{"id":5100},"qwenqwen2-vl-2b-instruct",[5102],{"type":27,"tag":170,"props":5103,"children":5105},{"className":5104},[],[5106],{"type":33,"value":3216},{"type":27,"tag":36,"props":5108,"children":5109},{},[5110],{"type":33,"value":5111},"This was the vision language model (VLM) that I used locally when developing on RTX. I was impressed at how well it could describe images given the small parameter count of the model (2 billion parameters). The small size of this model made it easy to run in my RTX PC cluster.",{"type":27,"tag":36,"props":5113,"children":5114},{},[5115,5117,5124],{"type":33,"value":5116},"There is ",{"type":27,"tag":61,"props":5118,"children":5121},{"href":5119,"rel":5120},"https://github.com/NVIDIA/TensorRT-LLM/issues/2183",[65],[5122],{"type":33,"value":5123},"an open GitHub issue for TensorRT-LLM support for Qwen2-VL",{"type":33,"value":5125}," at the time of writing.",{"type":27,"tag":36,"props":5127,"children":5128},{},[5129,5131,5137,5139,5145],{"type":33,"value":5130},"I wrote a simple FastAPI server using the Hugging Face ",{"type":27,"tag":170,"props":5132,"children":5134},{"className":5133},[],[5135],{"type":33,"value":5136},"transformers",{"type":33,"value":5138}," library based on example code from this model's documentation (see ",{"type":27,"tag":170,"props":5140,"children":5142},{"className":5141},[],[5143],{"type":33,"value":5144},"services/qwen2-vl",{"type":33,"value":5146}," in the RedLM GitHub repo for more details). I packaged this service into a container in order to run it in my local kubernetes cluster along with other inference services.",{"type":27,"tag":47,"props":5148,"children":5150},{"id":5149},"metallama-32-90b-vision-instruct",[5151],{"type":27,"tag":170,"props":5152,"children":5154},{"className":5153},[],[5155],{"type":33,"value":3224},{"type":27,"tag":36,"props":5157,"children":5158},{},[5159,5161,5167,5169,5174],{"type":33,"value":5160},"This model came out while I was working on the project, and I decided to use it instead of the ",{"type":27,"tag":170,"props":5162,"children":5164},{"className":5163},[],[5165],{"type":33,"value":5166},"adept/fuyu-8b",{"type":33,"value":5168}," model that was previously one of the only vision language models in the NVIDIA API catalog. The ",{"type":27,"tag":170,"props":5170,"children":5172},{"className":5171},[],[5173],{"type":33,"value":3224},{"type":33,"value":5175}," model has strong Chinese language skills, so it was a good model to use when doing remote inference for the image Q&A bot.",{"type":27,"tag":47,"props":5177,"children":5179},{"id":5178},"nvidianvlm-d-72b",[5180],{"type":27,"tag":61,"props":5181,"children":5184},{"href":5182,"rel":5183},"https://huggingface.co/nvidia/NVLM-D-72B",[65],[5185],{"type":27,"tag":170,"props":5186,"children":5188},{"className":5187},[],[5189],{"type":33,"value":5190},"nvidia/NVLM-D-72B",{"type":27,"tag":36,"props":5192,"children":5193},{},[5194,5196,5202],{"type":33,"value":5195},"I didn't use this model in my project, but it came out recently and looks awesome! Hopefully this model will be available on the NVIDIA API catalog soon. It is trained on the ",{"type":27,"tag":170,"props":5197,"children":5199},{"className":5198},[],[5200],{"type":33,"value":5201},"Qwen2-72B-Instruct",{"type":33,"value":5203}," text-only model, so it likely also has very strong support for Chinese language.",{"type":27,"tag":2972,"props":5205,"children":5206},{},[5207],{"type":27,"tag":36,"props":5208,"children":5209},{},[5210],{"type":33,"value":5211},"Today (September 17th, 2024), we introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.",{"type":27,"tag":28,"props":5213,"children":5215},{"id":5214},"the-success-of-black-myth-wukong",[5216],{"type":33,"value":5217},"The success of Black Myth: Wukong",{"type":27,"tag":36,"props":5219,"children":5220},{},[5221],{"type":33,"value":5222},"I originally got the idea to build this project after seeing the release of Black Myth: Wukong. This game is a blockbuster success from a Chinese developer that tells the story of the Monkey King’s adventure in the Journey West universe. Journey West (西游记) is another one of the “Four Great Works” of Chinese literature. It tells the story of the legendary pilgrimage of the monk Xuanzang (also known as Tang Sanzang) to India, accompanied by his three disciples—Sun Wukong (the Monkey King), Zhu Bajie (Pigsy), and Sha Wujing (Sandy). The group travels from China to India to retrieve sacred Buddhist scriptures, facing numerous challenges, demons, and supernatural beings along the way.",{"type":27,"tag":36,"props":5224,"children":5225},{},[5226],{"type":33,"value":5227},"The novel blends elements of adventure, mythology, and spiritual allegory, with Sun Wukong's mischievous nature and extraordinary powers adding humor and excitement. Through their journey, the characters grow and overcome personal flaws, ultimately achieving enlightenment and spiritual success. The video game adaptation has set world records for numbers of concurrent players, and it has rewritten the narrative around what is possible with single-player, offline games in the gaming industry.",{"type":27,"tag":36,"props":5229,"children":5230},{},[5231],{"type":27,"tag":84,"props":5232,"children":5235},{"alt":5233,"src":5234},"Black Myth: Wukong","/static/redlm/wukong.png",[],{"type":27,"tag":36,"props":5237,"children":5238},{},[5239],{"type":33,"value":5240},"Three renditions of Journey West: Songokū (The Monkey King) polychrome woodblock (surimono) (1824) by Yashima Gakutei (1786–1868), Black Myth: Wukong video game by Game Science (2024), Journey to the West TV series by CCTV (1982-2000)",{"type":27,"tag":28,"props":5242,"children":5244},{"id":5243},"redlm-video",[5245],{"type":33,"value":5246},"RedLM video",{"type":27,"tag":5248,"props":5249,"children":5250},"red-lm-video",{},[],{"type":27,"tag":36,"props":5252,"children":5253},{},[5254],{"type":33,"value":5255},"I created the video for this project using Blender.The Blender sequencer editor is a great non-linear video editing tool for simple video projects like this one. I used the following formula to create the project video for RedLM:",{"type":27,"tag":94,"props":5257,"children":5258},{},[5259,5271,5276,5281],{"type":27,"tag":98,"props":5260,"children":5261},{},[5262,5264],{"type":33,"value":5263},"Background music: I used the AI music generation service called Suno with the prompt “mystical strange traditional Chinese music from the Qing Dynasty”. Here’s the link to my Suno playlist called “Qing Dynasty Music” where you can find the original song and some other good songs that I generated using this prompt. My ",{"type":27,"tag":61,"props":5265,"children":5268},{"href":5266,"rel":5267},"https://suno.com/playlist/863ea0dd-1921-467c-8b69-16dbd126d966",[65],[5269],{"type":33,"value":5270},"Qing Dynasty Music Playlist on Suno",{"type":27,"tag":98,"props":5272,"children":5273},{},[5274],{"type":33,"value":5275},"Outline: For this project, the main sections are the introduction, then explaining each part with a short demo: translation, text-based Q&A, evaluation for text-based Q&A, image-based Q&A, and finally a short outro. I wrote an outline and then ChatGPT helped with filling out the content.",{"type":27,"tag":98,"props":5277,"children":5278},{},[5279],{"type":33,"value":5280},"Narration: I used ElevenLabs to narrate the main part of the video using a clone of my voice using the ElevenLabs Voice Lab. The Chinese voices were generated on my computer with an open-source text-to-speech model called ChatTTS.",{"type":27,"tag":98,"props":5282,"children":5283},{},[5284],{"type":33,"value":5285},"Images and videos: I gathered images and screen captures of different parts of the project including code snippets, paintings of the book, flow diagrams and screen recordings of the application.",{"type":27,"tag":36,"props":5287,"children":5288},{},[5289],{"type":33,"value":5290},"The video is composed of different “strips”. The green strips represent the music and voice clips. Red strips are images and yellow strips are videos. Here is what the final cut of the video looks like in Blender’s Sequencer view:",{"type":27,"tag":36,"props":5292,"children":5293},{},[5294],{"type":27,"tag":84,"props":5295,"children":5298},{"alt":5296,"src":5297},"Blender Sequence Editor","/static/redlm/blender_sequence_editor.png",[],{"type":27,"tag":36,"props":5300,"children":5301},{},[5302],{"type":33,"value":5303},"ChatTTS is one of the most impressive open-source models I have seen for generating conversational speech with prosodic elements (pausing, laughter, etc.) It is developed by a Chinese company called 2noise. Earlier this year I made a small contribution to this project with an API example using FastAPI to show how to run a standalone API using the model. Another example in this project provides a comprehensive example application built with gradio:",{"type":27,"tag":36,"props":5305,"children":5306},{},[5307],{"type":27,"tag":84,"props":5308,"children":5311},{"alt":5309,"src":5310},"ChatTTS UI","/static/redlm/chattts_ui.png",[],{"type":27,"tag":36,"props":5313,"children":5314},{},[5315],{"type":33,"value":5316},"I was planning on streaming the narration audio for Q&A answers using my ChatTTS API service, but I didn’t get around to doing this. Instead, I just used the Gradio application to generate the Chinese narration for Q&A and image Q&A examples included in the video.",{"type":27,"tag":47,"props":5318,"children":5320},{"id":5319},"redlm-deep-dive-video-with-notebooklm",[5321],{"type":33,"value":5322},"RedLM Deep Dive video with NotebookLM",{"type":27,"tag":36,"props":5324,"children":5325},{},[5326],{"type":33,"value":5327},"NotebookLM is a new application from Google that is a truly magical application of retrieval augmented generation.",{"type":27,"tag":2972,"props":5329,"children":5330},{},[5331],{"type":27,"tag":36,"props":5332,"children":5333},{},[5334],{"type":33,"value":5335},"NotebookLM is a research and note-taking online tool developed by Google Labs that uses artificial intelligence, specifically Google Gemini, to assist users in interacting with their documents. It can generate summaries, explanations, and answers based on content uploaded by users.",{"type":27,"tag":36,"props":5337,"children":5338},{},[5339],{"type":33,"value":5340},"I used NotebookLM to generate a \"Deep Dive\" podcast episode using only this article. I was pretty impressed with what it was able to produce, and I wanted to share it as part of this project, so I used Blender and some Python scripts to put together a simple and engaging visualization.",{"type":27,"tag":36,"props":5342,"children":5343},{},[5344],{"type":27,"tag":84,"props":5345,"children":5348},{"alt":5346,"src":5347},"Deep Dive video in Blender","/static/redlm/deep_dive_blender.png",[],{"type":27,"tag":36,"props":5350,"children":5351},{},[5352,5354,5360,5362,5373,5375,5382],{"type":33,"value":5353},"The ",{"type":27,"tag":170,"props":5355,"children":5357},{"className":5356},[],[5358],{"type":33,"value":5359},"openai/whisper-base",{"type":33,"value":5361}," model was used to get time stamps for the start and end of each spoken word using Automated Speech Recognition (ASR). A speaker segmentation library called ",{"type":27,"tag":61,"props":5363,"children":5366},{"href":5364,"rel":5365},"https://github.com/pyannote/pyannote-audio",[65],[5367],{"type":27,"tag":170,"props":5368,"children":5370},{"className":5369},[],[5371],{"type":33,"value":5372},"pyannote/audio",{"type":33,"value":5374}," was used to perform speaker diarization. This is an interesting algorithm that can segment any number of distinct speakers in an audio recording using a series of models and a discrete-time stochastic process known as the ",{"type":27,"tag":61,"props":5376,"children":5379},{"href":5377,"rel":5378},"https://en.wikipedia.org/wiki/Chinese_restaurant_process",[65],[5380],{"type":33,"value":5381},"Chinese restaurant process",{"type":33,"value":5383},". This gave a list of time intervals with a speaker ID, and I used the intervals to attribute a speaker ID to each word. Then I segmented the audio into two files using this data and used the files to generate audio waveforms using Blender's geometry nodes. Another script was used to animate each word of as it is spoken in one of two positions for each speaker.",{"type":27,"tag":28,"props":5385,"children":5387},{"id":5386},"final-thoughts",[5388],{"type":33,"value":5389},"Final thoughts",{"type":27,"tag":36,"props":5391,"children":5392},{},[5393],{"type":33,"value":5394},"I’m glad to have had the opportunity to join three NVIDIA developer contests this year. I like the idea of a “developer contest” that takes place over several weeks compared to hackathons that take place over just a few days. Having more time allows you to learn about a new tool or framework at a deeper level and think about how to apply it in a creative project.",{"type":27,"tag":36,"props":5396,"children":5397},{},[5398],{"type":27,"tag":84,"props":5399,"children":5402},{"alt":5400,"src":5401},"NVIDIA and LlamaIndex Contest","/static/redlm/llama-contest-og.jpg",[],{"type":27,"tag":36,"props":5404,"children":5405},{},[5406],{"type":33,"value":5407},"I also like how this contest is not team based. Working on this project I was able to do a lot of high-level thinking, write out features as detailed prompts, and then delegate the code writing to LLMs as if I was giving tasks to teammates.",{"type":27,"tag":36,"props":5409,"children":5410},{},[5411],{"type":33,"value":5412},"NVIDIA’s contests are “global developer contests”, but the contests so far are not open to developers in India and China. This is probably due to local rules and regulations governing how contests, prizes and taxes work. It is too bad; I would love to see what types of applications would come from participants in these countries. Also, there are also a lot of really interesting developments happening in the LLM space in both China and India!",{"type":27,"tag":36,"props":5414,"children":5415},{},[5416,5418,5425],{"type":33,"value":5417},"The LLMs I used in this project were developed by some of the Chinese AI companies, and they are competitive with LLMs from Western countries on LLM benchmarks despite having access to fewer GPU resources. ",{"type":27,"tag":61,"props":5419,"children":5422},{"href":5420,"rel":5421},"https://www.youtube.com/watch?v=UitJxc9LE60",[65],[5423],{"type":33,"value":5424},"Kaifu Lee mentioned in a Bloomberg interview",{"type":33,"value":5426}," that the scarcity of GPU resources in China will force Chinese engineers to innovate in new ways to gain an advantage. One example of this we saw recently was when Chinese hardware hackers doubled the usable memory of the RTX 4090D (a variant of the RTX 4090 card with lower processing power to comply with US export regulations for China - the D stands for Dragon, apparently!).",{"type":27,"tag":36,"props":5428,"children":5429},{},[5430],{"type":27,"tag":84,"props":5431,"children":5434},{"alt":5432,"src":5433},"RTX 4090D 48GB","/static/redlm/RTX4090D.jpg",[],{"type":27,"tag":36,"props":5436,"children":5437},{},[5438],{"type":33,"value":5439},"NVIDIA recently concluded it's AI Summit in Mumbai. I was intrigued by the fact that Hindi has unique challenges that have have limited the development of Hindi LLMs compared to the development of English and Chinese LLMs. In a conversation with Jensen Huang, Indian industrial titan and CEO of Reliance Industries Mukesh Ambani spoke about his aspirations and ambition for India to overcome these challenges and develop a Hindi LLM. In a viral moment Mukesh Ambani shared that through devotion to attaining knowledge through the Hindu Goddess of knowledge Sarawati, India will be met by the Goddess of prosperity, Lakshmi.",{"type":27,"tag":36,"props":5441,"children":5442},{},[5443],{"type":27,"tag":84,"props":5444,"children":5447},{"alt":5445,"src":5446},"Mukesh Ambani","/static/redlm/mukesh_ambani.png",[],{"type":27,"tag":36,"props":5449,"children":5450},{},[5451,5453,5460],{"type":33,"value":5452},"NVIDIA recently released a small language model for Hindi at the AI Summit in Mumbai called  ",{"type":27,"tag":61,"props":5454,"children":5457},{"href":5455,"rel":5456},"https://indiaai.gov.in/article/nvidia-unveils-nemotron-4-mini-hindi-4b-ai-for-india-s-500-million-hindi-speakers",[65],[5458],{"type":33,"value":5459},"Nemotron-4-Mini-Hindi-4B",{"type":33,"value":5461},". Hindi LLMs could enable applications to explore important works of literature from India. I don't know that much about India literature, but a comparable work of literature in size and cultural significance might be the Ramayana.",{"type":27,"tag":36,"props":5463,"children":5464},{},[5465],{"type":27,"tag":663,"props":5466,"children":5467},{},[5468],{"type":33,"value":5469},"The Ramayana is an ancient Indian epic that tells the story of Prince Rama's heroic quest to rescue his wife, Sita, who has been kidnapped by the demon king Ravana. Set in a world of gods, demons, and celestial beings, the story explores themes of duty, loyalty, and the triumph of good over evil. Guided by wisdom, strength, and the support of devoted allies like Hanuman, the monkey god, and his brother Lakshmana, Rama's journey is a deeply spiritual tale, celebrated for its poetic beauty and moral depth. The Ramayana continues to inspire and captivate audiences across cultures.",{"type":27,"tag":36,"props":5471,"children":5472},{},[5473],{"type":33,"value":5474},"The Ramayana story journeyed to Thailand centuries ago, transforming into the Ramakien, a Thai adaptation that retains the essence of the original Indian epic while adding distinctive Thai cultural elements. Introduced through trade, diplomacy, and cultural exchange between India and Southeast Asia, the story became deeply woven into Thailand’s art, literature, and performance traditions. Thai kings, particularly King Rama I, adapted and documented the Ramakien, giving it a prominent place in Thai history. Lavishly detailed murals surrounding the Temple of the Emerald Buddha in Bangkok’s Grand Palace depict the Ramakien in over 178 panels that totaling over 2 kilometers in length. On a recent visit to the Grand Palace, I imagined having an application that could link the detailed murals to elements of the story in Hindi, Thai, English, Chinese or any language.",{"type":27,"tag":36,"props":5476,"children":5477},{},[5478],{"type":27,"tag":84,"props":5479,"children":5482},{"alt":5480,"src":5481},"Ramakien murals surrounding Temple of the Emerald Buddha","/static/redlm/ramakien.png",[],{"type":27,"tag":36,"props":5484,"children":5485},{},[5486],{"type":33,"value":5487},"The Dream of the Red Chamber, originally titled The Story of the Stone, is one of China’s greatest literary works and a masterpiece of world literature. The novel begins with a frame story centered on a magical stone, left over from the Chinese creation myth where the goddess Nuwa mends the heavens. Longing to experience the human world, the sentient stone persuades a Buddhist monk and a Taoist priest to reincarnate it as a boy. This boy, Baoyu, is born into a wealthy and influential family—a character partly based on the author, Cao Xueqin, and his own aristocratic upbringing. Through Baoyu's life, friendships, and romantic relationships, the novel delves into his family’s gradual decline, mirroring the instability of China’s own noble families in the late Qing dynasty. The story also portrays the era's customs, social structures, and beliefs, offering readers a richly detailed exploration of life in Qing China.",{"type":27,"tag":36,"props":5489,"children":5490},{},[5491],{"type":33,"value":5492},"It was a lot of fun to work on this project with tools from LlamaIndex and NVIDIA. With AI technology, GPUs are now essentially sentient stones, and I was able to share this important touchstone of the human experience with my computers using LlamaIndex and open source language models. In turn, RedLM shared with me delightful insights into world of Dream of the Red Chamber.",{"type":27,"tag":36,"props":5494,"children":5495},{},[5496],{"type":27,"tag":84,"props":5497,"children":5500},{"alt":5498,"src":5499},"Story of a Stone","/static/redlm/stone_story.png",[],{"type":27,"tag":36,"props":5502,"children":5503},{},[5504],{"type":27,"tag":84,"props":5505,"children":5508},{"alt":5506,"src":5507},"Story of a Stone Analysis","/static/redlm/stone_story_analysis.png",[],{"type":27,"tag":2972,"props":5510,"children":5511},{},[5512],{"type":27,"tag":36,"props":5513,"children":5514},{},[5515],{"type":33,"value":5516},"This scene describes a piece of traditional Chinese painting, depicting two elderly figures conversing amidst mountains and rivers. The painting likely visually represents the scene from the book where a monk and a Taoist are chatting at the foot of Qinggeng Peak. The two elderly figures in the painting may represent the monk and Taoist from the book, discussing their discovery of a bright and pristine stone, and planning to take it to a bustling, splendid place for a happy life. The painting’s elements—mountains, peaks, flowing water, trees, and rocks—might echo the book's descriptions, illustrating the natural environment at the base of Qinggeng Peak where the monk and Taoist reside. The painting’s tranquil and harmonious atmosphere may also align with the storyline, expressing the monk and Taoist's care for the stone and their wish for it to live a happy life. In summary, this painted scene might be an artistic portrayal of the story between the monk, the Taoist, and the stone from the book, using visual elements and ambiance to convey the narrative and themes within the story.",{"type":27,"tag":5518,"props":5519,"children":5520},"style",{},[5521],{"type":33,"value":5522},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}html.sepia .shiki span {color: var(--shiki-sepia);background: var(--shiki-sepia-bg);font-style: var(--shiki-sepia-font-style);font-weight: var(--shiki-sepia-font-weight);text-decoration: var(--shiki-sepia-text-decoration);}",{"title":8,"searchDepth":343,"depth":343,"links":5524},[5525,5528,5529,5530,5531,5532,5537,5538,5541,5545,5546,5547,5556,5557,5560],{"id":30,"depth":343,"text":34,"children":5526},[5527],{"id":49,"depth":379,"text":52},{"id":71,"depth":343,"text":74},{"id":115,"depth":343,"text":118},{"id":150,"depth":343,"text":153},{"id":180,"depth":343,"text":183},{"id":640,"depth":343,"text":643,"children":5533},[5534,5535,5536],{"id":1326,"depth":379,"text":1329},{"id":2153,"depth":379,"text":2156},{"id":2212,"depth":379,"text":2215},{"id":2821,"depth":343,"text":2824},{"id":3091,"depth":343,"text":3094,"children":5539},[5540],{"id":3977,"depth":379,"text":3980},{"id":4022,"depth":343,"text":4025,"children":5542},[5543,5544],{"id":4676,"depth":379,"text":4679},{"id":4788,"depth":379,"text":4791},{"id":4841,"depth":343,"text":4844},{"id":4877,"depth":343,"text":4880},{"id":4993,"depth":343,"text":4996,"children":5548},[5549,5551,5552,5553,5554,5555],{"id":5004,"depth":379,"text":5550},"01-ai/Yi-1.5-9B-Chat and nvidia/yi-large",{"id":5070,"depth":379,"text":5077},{"id":5085,"depth":379,"text":5092},{"id":5100,"depth":379,"text":3216},{"id":5149,"depth":379,"text":3224},{"id":5178,"depth":379,"text":5190},{"id":5214,"depth":343,"text":5217},{"id":5243,"depth":343,"text":5246,"children":5558},[5559],{"id":5319,"depth":379,"text":5322},{"id":5386,"depth":343,"text":5389},"markdown","content:2024:10:09:redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest.md","content","2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest.md","2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest","md",{"_path":5568,"_dir":5569,"_draft":7,"_partial":7,"_locale":8,"title":5570,"description":5571,"date":5572,"image":5573,"tags":5574,"draft":7,"external":5585,"comments":22,"body":5589,"_type":5561,"_id":5993,"_source":5563,"_file":5994,"_stem":5995,"_extension":5566},"/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update","24","Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM","This article is a brief discusion on recent updates to my project for the Generative AI Agents Developer Contest by NVIDIA and LangChain","2024-06-24","/static/aoi/aoi_title.png",[14,5575,5576,5577,5578,5579,19,16,17,5580,5581,5582,5583,5584],"langchain","agents","rtx","gpu","tensorrt","llama","007","stable-diffusion","stable-video-diffusion","comfyui",[5586],{"link":5587,"site":5588},"https://x.com/briancaffey/status/1802754703207583886","x",{"type":24,"children":5590,"toc":5986},[5591,5597,5610,5615,5619,5632,5638,5643,5656,5664,5677,5685,5693,5698,5706,5711,5719,5724,5732,5790,5795,5803,5808,5816,5830,5836,5841,5849,5862,5870,5875,5883,5888,5894,5908,5913,5921,5926,5934,5939,5947,5952,5957,5963,5968,5976,5981],{"type":27,"tag":28,"props":5592,"children":5594},{"id":5593},"tldr",[5595],{"type":33,"value":5596},"tl;dr",{"type":27,"tag":36,"props":5598,"children":5599},{},[5600,5602,5609],{"type":33,"value":5601},"\"Agents of Inference: Speed of Light\" is an update to my original entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This update focuses on how I accelerated local text, image and video generation using TensorRT, TensorRT-LLM and NVIDIA NIMs. You can read the original article about \"Agents of Inference\" ",{"type":27,"tag":61,"props":5603,"children":5606},{"href":5604,"rel":5605},"https://briancaffey.github.io/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",[65],[5607],{"type":33,"value":5608},"here",{"type":33,"value":2970},{"type":27,"tag":36,"props":5611,"children":5612},{},[5613],{"type":33,"value":5614},"Here's my original project submission post on 𝕏 that introduces the idea of generating short 007-style films using agents, LLMs and stable diffusion:",{"type":27,"tag":5616,"props":5617,"children":5618},"agents-of-inference-tweet",{},[],{"type":27,"tag":36,"props":5620,"children":5621},{},[5622,5624,5631],{"type":33,"value":5623},"Here's a link to the ",{"type":27,"tag":61,"props":5625,"children":5628},{"href":5626,"rel":5627},"https://github.com/briancaffey/agents-of-inference",[65],[5629],{"type":33,"value":5630},"Agents of Inference code repository on GitHub",{"type":33,"value":2970},{"type":27,"tag":28,"props":5633,"children":5635},{"id":5634},"nvidia-nim-inference-microservices",[5636],{"type":33,"value":5637},"NVIDIA NIM inference microservices",{"type":27,"tag":36,"props":5639,"children":5640},{},[5641],{"type":33,"value":5642},"I thought NVIDIA NIMs was one of the most exciting announcements from GTC 2024. I'm a big fan of using docker containers everywhere, and the idea of standardizing NVIDIA tools and dependencies seemed to make a lot of sense. I had previously struggled to get TensorRT-LLM installed on Windows using example repos provided by NVIDIA.",{"type":27,"tag":36,"props":5644,"children":5645},{},[5646,5648,5654],{"type":33,"value":5647},"A few weeks ago NVIDIA announced that NVIDIA NIMs can be downloaded and run anywhere. I was able to download this NIM for the ",{"type":27,"tag":170,"props":5649,"children":5651},{"className":5650},[],[5652],{"type":33,"value":5653},"meta/llama3-8b-instruct",{"type":33,"value":5655}," model:",{"type":27,"tag":36,"props":5657,"children":5658},{},[5659],{"type":27,"tag":84,"props":5660,"children":5663},{"alt":5661,"src":5662},"llama3 nim","/static/aoi/meta-llama3-nim.png",[],{"type":27,"tag":36,"props":5665,"children":5666},{},[5667,5669,5675],{"type":33,"value":5668},"Here are the logs for my NVIDIA NIM ",{"type":27,"tag":170,"props":5670,"children":5672},{"className":5671},[],[5673],{"type":33,"value":5674},"Meta/Llama-3-8B-Instruct",{"type":33,"value":5676}," running in docker container on Windows Subsystem for Linux on my NVIDIA GeForce RTX 4090 GPU-powered PC. Notice that it generates over 50 tokens per second!",{"type":27,"tag":36,"props":5678,"children":5679},{},[5680],{"type":27,"tag":84,"props":5681,"children":5684},{"alt":5682,"src":5683},"trt llama3 local","/static/aoi/trt-llama3.png",[],{"type":27,"tag":36,"props":5686,"children":5687},{},[5688],{"type":27,"tag":84,"props":5689,"children":5692},{"alt":5690,"src":5691},"token factory","/static/aoi/token-factory.png",[],{"type":27,"tag":36,"props":5694,"children":5695},{},[5696],{"type":33,"value":5697},"The one main hurdle I faced when running the NIM local was an error about no runnable profiles being available:",{"type":27,"tag":238,"props":5699,"children":5701},{"code":5700},"ERROR 06-23 15:41:21.19 utils.py:21] Could not find a profile that is currently runnable with the detected hardware. Please check the system information below and make sure you have enough free GPUs.\nSYSTEM INFO\n- Free GPUs: \u003CNone>\n- Non-free GPUs:\n  -  [2684:10de] (0) NVIDIA GeForce RTX 4090 [current utilization: 7%]\n",[5702],{"type":27,"tag":170,"props":5703,"children":5704},{"__ignoreMap":8},[5705],{"type":33,"value":5700},{"type":27,"tag":36,"props":5707,"children":5708},{},[5709],{"type":33,"value":5710},"This seemed odd, and I found another user with the same issue on the NVIDIA Developer Forum. I was able to get around this by going into the EUFI/BIOS of my PC and switch to integrated graphics:",{"type":27,"tag":36,"props":5712,"children":5713},{},[5714],{"type":27,"tag":84,"props":5715,"children":5718},{"alt":5716,"src":5717},"bios","/static/aoi/bios.jpg",[],{"type":27,"tag":36,"props":5720,"children":5721},{},[5722],{"type":33,"value":5723},"It was great to be able to run \"Agents of Inference\" using NVIDIA NIM because it is just as simple as running a docker container:",{"type":27,"tag":238,"props":5725,"children":5727},{"code":5726},"export CONTAINER_NAME=llama3-8b-instruct\nexport IMG_NAME=\"nvcr.io/nim/meta/${CONTAINER_NAME}:1.0.0\"\nexport LOCAL_NIM_CACHE=~/.cache/nim\nmkdir -p \"$LOCAL_NIM_CACHE\"\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus all \\\n  --shm-size=16GB \\\n  -e NGC_API_KEY \\\n  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n  -u $(id -u) \\\n  -p 8000:8000 \\\n  $IMG_NAME\n",[5728],{"type":27,"tag":170,"props":5729,"children":5730},{"__ignoreMap":8},[5731],{"type":33,"value":5726},{"type":27,"tag":36,"props":5733,"children":5734},{},[5735,5737,5743,5745,5752,5754,5760,5762,5773,5775,5780,5782,5788],{"type":33,"value":5736},"Before getting this to work, I was able to get a ",{"type":27,"tag":170,"props":5738,"children":5740},{"className":5739},[],[5741],{"type":33,"value":5742},"/chat/completions",{"type":33,"value":5744}," endpoint working with the Llama3 model on my fork of the ",{"type":27,"tag":61,"props":5746,"children":5749},{"href":5747,"rel":5748},"https://github.com/briancaffey/trt-llm-as-openai-windows/commit/edaa15fd026fe95e645e3d4ae9718dc3ecc3bb65",[65],[5750],{"type":33,"value":5751},"trt-llm-as-openai-windows",{"type":33,"value":5753},". I borrowed code for the ",{"type":27,"tag":170,"props":5755,"children":5757},{"className":5756},[],[5758],{"type":33,"value":5759},"TrtLlmAPI",{"type":33,"value":5761}," from the ",{"type":27,"tag":61,"props":5763,"children":5766},{"href":5764,"rel":5765},"https://github.com/NVIDIA/ChatRTX",[65],[5767],{"type":27,"tag":170,"props":5768,"children":5770},{"className":5769},[],[5771],{"type":33,"value":5772},"NVIDIA/ChatRTX",{"type":33,"value":5774}," repo and a function from ",{"type":27,"tag":170,"props":5776,"children":5778},{"className":5777},[],[5779],{"type":33,"value":15},{"type":33,"value":5781}," called ",{"type":27,"tag":170,"props":5783,"children":5785},{"className":5784},[],[5786],{"type":33,"value":5787},"messages_to_prompt_v3_instruct",{"type":33,"value":5789}," which encodes messages with special tokens for chat. This was an interesting exercise and it taught me a lot about how LLMs do chat. I would like to continue working on this fork and see how to implement streaming endpoints for the Llama 3 model.",{"type":27,"tag":36,"props":5791,"children":5792},{},[5793],{"type":33,"value":5794},"Here is how Llama 3 does the instruct prompting:",{"type":27,"tag":238,"props":5796,"children":5798},{"code":5797},"\u003C|begin_of_text|>\u003C|start_header_id|>system\u003C|end_header_id|>\n\nYou are a helpful AI assistant for travel tips and recommendations\u003C|eot_id|>\u003C|start_header_id|>user\u003C|end_header_id|>\n\nWhat can you help me with?\u003C|eot_id|>\u003C|start_header_id|>assistant\u003C|end_header_id|>\n",[5799],{"type":27,"tag":170,"props":5800,"children":5801},{"__ignoreMap":8},[5802],{"type":33,"value":5797},{"type":27,"tag":36,"props":5804,"children":5805},{},[5806],{"type":33,"value":5807},"Compare this with how it was done with Llama2 chat:",{"type":27,"tag":238,"props":5809,"children":5811},{"code":5810},"\u003Cs>[INST] \u003C\u003CSYS>>\n{{ system_prompt }}\n\u003C\u003C/SYS>>\n\n{{ user_message_1 }} [/INST] {{ model_answer_1 }} \u003C/s>\n\u003Cs>[INST] {{ user_message_2 }} [/INST]\n",[5812],{"type":27,"tag":170,"props":5813,"children":5814},{"__ignoreMap":8},[5815],{"type":33,"value":5810},{"type":27,"tag":36,"props":5817,"children":5818},{},[5819,5821,5828],{"type":33,"value":5820},"You can read more about the difference between Llama 2 and 3 on the ",{"type":27,"tag":61,"props":5822,"children":5825},{"href":5823,"rel":5824},"https://llama.meta.com/docs/model-cards-and-prompt-formats",[65],[5826],{"type":33,"value":5827},"Model Card & Prompt formats",{"type":33,"value":5829}," page on Meta's Llama website.",{"type":27,"tag":28,"props":5831,"children":5833},{"id":5832},"langsmith",[5834],{"type":33,"value":5835},"LangSmith",{"type":27,"tag":36,"props":5837,"children":5838},{},[5839],{"type":33,"value":5840},"I recently started using LangSmith. It is an awesome product and it ties in really well to doing prototype work like in my project \"Agents of Inference\". I wish I had started using it earlier in my development cycle! All you need to do is add an API key to your environment and your application automatically starts tracing LLM calls. It also works well with LangGraph and allows you to trace the execution path of your graph. Also it is good to be aware that there are other products similar to LangSmith like LangFuse. I also saw a really neat demo from Datadog at GTC showing an alpha version of their LLM tracing and observability product.",{"type":27,"tag":36,"props":5842,"children":5843},{},[5844],{"type":27,"tag":84,"props":5845,"children":5848},{"alt":5846,"src":5847},"langsmith screenshot","/static/aoi/langsmith.png",[],{"type":27,"tag":36,"props":5850,"children":5851},{},[5852,5854,5860],{"type":33,"value":5853},"LangSmith can also be helpful when the wrong JSON shape is parsed. I had a lot of difficulty with this in my project. When I used the Q4_K_M gguf quantized ",{"type":27,"tag":170,"props":5855,"children":5857},{"className":5856},[],[5858],{"type":33,"value":5859},"Meta-Llama-3 8B-Instruct",{"type":33,"value":5861}," model I had no issues with output parsing. Switching to the TensorRT-LLM model provided by the NIM resulted in some parsing errors. The application would report that JSON could not be parsed because the result contained text like: \"Here is the JSON that you requested\". I was able to get around this by changing the prompt template from:",{"type":27,"tag":238,"props":5863,"children":5865},{"code":5864},"Answer the user query.\n",[5866],{"type":27,"tag":170,"props":5867,"children":5868},{"__ignoreMap":8},[5869],{"type":33,"value":5864},{"type":27,"tag":36,"props":5871,"children":5872},{},[5873],{"type":33,"value":5874},"to",{"type":27,"tag":238,"props":5876,"children":5878},{"code":5877},"Don't include ANYTHING except for valid JSON in your response. Answer the user query.\n",[5879],{"type":27,"tag":170,"props":5880,"children":5881},{"__ignoreMap":8},[5882],{"type":33,"value":5877},{"type":27,"tag":36,"props":5884,"children":5885},{},[5886],{"type":33,"value":5887},"This was the most frustrating part of development, and I'm still getting occasional errors that I just skip over. I'm also probably have not exhausted all of the tools that LangChain provides to avoid these types of errors. Don't assume that output parsing that works with one model will work with another! This is another good reason to use something like LangSmith when developing LLM-based applications.",{"type":27,"tag":28,"props":5889,"children":5891},{"id":5890},"comfyui-tensorrt",[5892],{"type":33,"value":5893},"ComfyUI TensorRT",{"type":27,"tag":36,"props":5895,"children":5896},{},[5897,5899,5906],{"type":33,"value":5898},"My goal with \"Agents of Inference\" was to be able to test out how small upstream prompt changes can impact the quality and consistency of a series of generated images and videos. Iteration speed is very important! I was able to significantly speed up image and video generation by using the ",{"type":27,"tag":61,"props":5900,"children":5903},{"href":5901,"rel":5902},"https://github.com/comfyanonymous/ComfyUI_TensorRT",[65],[5904],{"type":33,"value":5905},"ComfyUI TensorRT custom nodes",{"type":33,"value":5907},". These nodes allow you to build engines with specifications for parameters that can be either static or dynamic. I had better luck with building dynamic engines. I was able to build and use engines for Stable Diffusion SDXL and Stable Video Diffusion XT.",{"type":27,"tag":36,"props":5909,"children":5910},{},[5911],{"type":33,"value":5912},"Building a TensorRT engine for ComfyUI can be done using the following workflow:",{"type":27,"tag":36,"props":5914,"children":5915},{},[5916],{"type":27,"tag":84,"props":5917,"children":5920},{"alt":5918,"src":5919},"trt comfyUI build process","/static/aoi/comfyui-trt-svd-xt.png",[],{"type":27,"tag":36,"props":5922,"children":5923},{},[5924],{"type":33,"value":5925},"The engines can then be used in custom workflows like the following:",{"type":27,"tag":36,"props":5927,"children":5928},{},[5929],{"type":27,"tag":84,"props":5930,"children":5933},{"alt":5931,"src":5932},"trt comfyui workflow","/static/aoi/svd-workflow-trt.png",[],{"type":27,"tag":36,"props":5935,"children":5936},{},[5937],{"type":33,"value":5938},"Once these workflows are configured and are working as expected, you can export them in API format (JSON) and use them to make API calls to the ComfyUI backend. The agents for stable diffusion and stable video diffusion made API calls in this way and it worked pretty well.",{"type":27,"tag":36,"props":5940,"children":5941},{},[5942],{"type":27,"tag":84,"props":5943,"children":5946},{"alt":5944,"src":5945},"comfy its","/static/aoi/comfy-its.png",[],{"type":27,"tag":36,"props":5948,"children":5949},{},[5950],{"type":33,"value":5951},"Using 50 iterations, I was able to generate 1024x576 images in 3 seconds or about 19 iterations per second (it/s). Videos",{"type":27,"tag":36,"props":5953,"children":5954},{},[5955],{"type":33,"value":5956},"ComfyUI is still early in development and it refers to itself as \"alpha software\" even though it has a large adoption by a very active community already. I'm excited to see what is next from the developers of ComfyUI.",{"type":27,"tag":28,"props":5958,"children":5960},{"id":5959},"speed-of-light",[5961],{"type":33,"value":5962},"Speed of Light",{"type":27,"tag":36,"props":5964,"children":5965},{},[5966],{"type":33,"value":5967},"\"Speed of Light\" is a term that I learned from a stable diffusion talk at GTC.",{"type":27,"tag":2972,"props":5969,"children":5970},{},[5971],{"type":27,"tag":36,"props":5972,"children":5973},{},[5974],{"type":33,"value":5975},"SOL analysis reveals how your code performs, and device utilization compared to relevant maximums.",{"type":27,"tag":36,"props":5977,"children":5978},{},[5979],{"type":33,"value":5980},"Adding TensorRT and TensorRT-LLM to inference services on my RTX PC helped increase the throughput of text, image and video generation for my \"Agents of Inference\" project. I'm looking forward to learning more about profiling and optimization techniques for both LLMs and Stable Diffusion workloads.",{"type":27,"tag":36,"props":5982,"children":5983},{},[5984],{"type":33,"value":5985},"Thanks again to NVIDIA and LangChain for organizing this contest! It was a lot of fun to learn about builing agents with LangChain and LangGraph and the latest developments from NVIDIA in Generative AI.",{"title":8,"searchDepth":343,"depth":343,"links":5987},[5988,5989,5990,5991,5992],{"id":5593,"depth":343,"text":5596},{"id":5634,"depth":343,"text":5637},{"id":5832,"depth":343,"text":5835},{"id":5890,"depth":343,"text":5893},{"id":5959,"depth":343,"text":5962},"content:2024:06:24:agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update.md","2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update.md","2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",{"_path":5997,"_dir":5998,"_draft":7,"_partial":7,"_locale":8,"title":5999,"description":6000,"date":6001,"image":5573,"tags":6002,"draft":7,"external":6003,"comments":22,"body":6005,"_type":5561,"_id":7473,"_source":5563,"_file":7474,"_stem":7475,"_extension":5566},"/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest","17","Agents of Inference: My submission for NVIDIA's Generative AI Agents Developer Contest by NVIDIA and LangChain","This article discusses my entry for NVIDIA's Generative AI Agents Developer Contest entry: Agents of Inference","2024-06-17",[14,5575,5576,5577,5578,5579,19,16,17,5580,5581,5582,5583,5584],[6004],{"link":5587,"site":5588},{"type":24,"children":6006,"toc":7452},[6007,6013,6024,6028,6033,6038,6041,6051,6057,6062,6068,6073,6079,6084,6127,6135,6140,6146,6151,6164,6177,6189,6542,6547,6590,6595,6757,6783,6789,6794,6900,6928,6935,6940,6946,6951,7030,7036,7041,7143,7151,7156,7161,7213,7234,7240,7261,7306,7311,7316,7322,7335,7343,7349,7353,7358,7363,7369,7374,7380,7385,7391,7396,7404,7409,7415,7427,7433,7438,7443,7448],{"type":27,"tag":28,"props":6008,"children":6010},{"id":6009},"update",[6011],{"type":33,"value":6012},"Update",{"type":27,"tag":36,"props":6014,"children":6015},{},[6016,6018],{"type":33,"value":6017},"I recently posted another article about optimizing this project with TensorRT and TensorRT-LLM running on local NVIDIA NIM inference microservices, please have a look here: ",{"type":27,"tag":61,"props":6019,"children":6022},{"href":6020,"rel":6021},"https://briancaffey.github.io/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",[65],[6023],{"type":33,"value":6020},{"type":27,"tag":28,"props":6025,"children":6026},{"id":5593},[6027],{"type":33,"value":5596},{"type":27,"tag":36,"props":6029,"children":6030},{},[6031],{"type":33,"value":6032},"“Agents of Inference” is my entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This project aims to integrate techniques for generating text, images and video to create an application capable of producing short thematic films. In this article, I will detail how I developed the project leveraging LangGraph—a library for building stateful, multi-actor applications with LLMs--and hybrid AI workflows using NVIDIA AI-powered tools and technologies running on RTX PCs and in the cloud.",{"type":27,"tag":36,"props":6034,"children":6035},{},[6036],{"type":33,"value":6037},"Here's my project submission post on 𝕏:",{"type":27,"tag":5616,"props":6039,"children":6040},{},[],{"type":27,"tag":36,"props":6042,"children":6043},{},[6044,6045,6050],{"type":33,"value":5623},{"type":27,"tag":61,"props":6046,"children":6048},{"href":5626,"rel":6047},[65],[6049],{"type":33,"value":5630},{"type":33,"value":2970},{"type":27,"tag":28,"props":6052,"children":6054},{"id":6053},"nvidias-generative-ai-agents-developer-contest",[6055],{"type":33,"value":6056},"NVIDIA's Generative AI Agents Developer Contest",{"type":27,"tag":36,"props":6058,"children":6059},{},[6060],{"type":33,"value":6061},"AI agents are having a moment. They are the building blocks for building \"applications that reason\", and LangChain is a company that provides a comprehensive set of tools for developing, deploying and monitoring AI agents. I have struggled to understand how I can build or use agents in my own projects, and with the contest I have been able to just scratch the surface of what is possible with AI agents--but I think it is a promising paradigm for developing AI-driven applications.",{"type":27,"tag":28,"props":6063,"children":6065},{"id":6064},"coming-up-with-an-idea",[6066],{"type":33,"value":6067},"Coming up with an idea",{"type":27,"tag":36,"props":6069,"children":6070},{},[6071],{"type":33,"value":6072},"I love stable diffusion. I closely follow the development of the three leading applications for generating images with stable dissuion models: Stable Diffusion WebUI, InvokeAI and ComfyUI. Write a prompt, instantly see the result, tweak the prompt and generate again. This is the basic process by which I have previously used stable diffusion. It is a satisfying mental exercise that feeds the creative and imaginative part of my brain. My idea for this project came from wanting to automate this process: use large language models to build cohesive scenes and detailed prompts and then feed them into my stable diffusion programs via API. Using LangChain and LangGraph allowed me to rapidly prototype the idea and start generating short feature films in the style of my favorite British Secret Agent: 007.",{"type":27,"tag":28,"props":6074,"children":6076},{"id":6075},"putting-together-the-puzzle-pieces",[6077],{"type":33,"value":6078},"Putting together the puzzle pieces",{"type":27,"tag":36,"props":6080,"children":6081},{},[6082],{"type":33,"value":6083},"Here's how I set up an MVP for my project project to get started. I set up a simple graph (a linked list, really) that included the following nodes. *Important: in this context, a node is an agent, and that agent is a simple Python function. It takes one parameter which is the state, a Python dictionary, that holds the output of LLM calls that the agents make. Not all nodes make LLM calls, some just run basic functions like initializing directories or calling external stable diffusion APIs.",{"type":27,"tag":574,"props":6085,"children":6086},{},[6087,6092,6097,6102,6107,6112,6117,6122],{"type":27,"tag":98,"props":6088,"children":6089},{},[6090],{"type":33,"value":6091},"Casting Agent → come up with some characters",{"type":27,"tag":98,"props":6093,"children":6094},{},[6095],{"type":33,"value":6096},"Location Agent → come up with some locations",{"type":27,"tag":98,"props":6098,"children":6099},{},[6100],{"type":33,"value":6101},"Synopsis Agent → write a synopsis based on the characters and locations",{"type":27,"tag":98,"props":6103,"children":6104},{},[6105],{"type":33,"value":6106},"Scene Agent → write some number of scenes based on the synopsis based on the synopsis",{"type":27,"tag":98,"props":6108,"children":6109},{},[6110],{"type":33,"value":6111},"Shot agent → describe some number of camera shots for each scene based on the scene",{"type":27,"tag":98,"props":6113,"children":6114},{},[6115],{"type":33,"value":6116},"Photography agent → take each shot description and generate and image",{"type":27,"tag":98,"props":6118,"children":6119},{},[6120],{"type":33,"value":6121},"Videography agent → take each image generated by the photography agent and convert it to a 4 second clip using stable video diffusion",{"type":27,"tag":98,"props":6123,"children":6124},{},[6125],{"type":33,"value":6126},"Editor agent → compile the movie clips together",{"type":27,"tag":36,"props":6128,"children":6129},{},[6130],{"type":27,"tag":84,"props":6131,"children":6134},{"alt":6132,"src":6133},"simple graph of agents of inference","/static/aoi/graph.png",[],{"type":27,"tag":36,"props":6136,"children":6137},{},[6138],{"type":33,"value":6139},"It may look simple, but there is a lot going on in this graph.",{"type":27,"tag":47,"props":6141,"children":6143},{"id":6142},"casting-and-location",[6144],{"type":33,"value":6145},"Casting and Location",{"type":27,"tag":36,"props":6147,"children":6148},{},[6149],{"type":33,"value":6150},"The first two agents in my graph are tasked with generating characters and locations that would appear in a British secret agent film. The prompts used for these agents are as follows:",{"type":27,"tag":2972,"props":6152,"children":6153},{},[6154],{"type":27,"tag":36,"props":6155,"children":6156},{},[6157,6162],{"type":27,"tag":2851,"props":6158,"children":6159},{},[6160],{"type":33,"value":6161},"casting",{"type":33,"value":6163},": \"Come up with four to five characters who will appear in an upcoming British spy movie. The list should include the main character who is male, the villain, an attractive female actress who eventually falls in love with the main character, and some other characters as well.\"",{"type":27,"tag":2972,"props":6165,"children":6166},{},[6167],{"type":27,"tag":36,"props":6168,"children":6169},{},[6170,6175],{"type":27,"tag":2851,"props":6171,"children":6172},{},[6173],{"type":33,"value":6174},"locations",{"type":33,"value":6176},": \"Provide three main locations that can be used in an international British Spy movie. The locations should include a variety of cities, remote environments, iconic landmarks, etc. The locations should make for good background scenes for an action movie with lots of stunts, chases, explosions, fights, etc. and other things you would find in an action movie. Be sure to include the country and a description of the environment where these places are.\"",{"type":27,"tag":36,"props":6178,"children":6179},{},[6180,6182,6187],{"type":33,"value":6181},"These agents leverage the LangChain Expression Language (LCEL) to generate ",{"type":27,"tag":2851,"props":6183,"children":6184},{},[6185],{"type":33,"value":6186},"structured output",{"type":33,"value":6188}," based on Pydantic models. For",{"type":27,"tag":238,"props":6190,"children":6192},{"code":6191,"language":254,"meta":8,"className":255,"style":8},"class Character(BaseModel):\n    \"\"\"\n    The type for character that the casting agent casts for a role in the movie\n    \"\"\"\n    full_name: str = Field(description=\"The character's name\")\n    short_name: str = Field(description=\"The character's short name\")\n    background: str = Field(description=\"The character's background\")\n    physical_traits: str = Field(description=\"The physical traits of the character\")\n    ethnicity: str = Field(description=\"The character's ethnicity\")\n    gender: str = Field(description=\"The character's gender, either male of female\")\n    nationality: str = Field(description=\"The character's nationality\")\n    main_character: bool = Field(description=\"If the character is or is not the main character\")\n\n",[6193],{"type":27,"tag":170,"props":6194,"children":6195},{"__ignoreMap":8},[6196,6221,6228,6236,6243,6283,6320,6357,6394,6431,6468,6505],{"type":27,"tag":261,"props":6197,"children":6198},{"class":263,"line":264},[6199,6203,6208,6212,6217],{"type":27,"tag":261,"props":6200,"children":6201},{"style":487},[6202],{"type":33,"value":729},{"type":27,"tag":261,"props":6204,"children":6205},{"style":732},[6206],{"type":33,"value":6207}," Character",{"type":27,"tag":261,"props":6209,"children":6210},{"style":268},[6211],{"type":33,"value":740},{"type":27,"tag":261,"props":6213,"children":6214},{"style":743},[6215],{"type":33,"value":6216},"BaseModel",{"type":27,"tag":261,"props":6218,"children":6219},{"style":268},[6220],{"type":33,"value":750},{"type":27,"tag":261,"props":6222,"children":6223},{"class":263,"line":343},[6224],{"type":27,"tag":261,"props":6225,"children":6226},{"style":493},[6227],{"type":33,"value":2277},{"type":27,"tag":261,"props":6229,"children":6230},{"class":263,"line":379},[6231],{"type":27,"tag":261,"props":6232,"children":6233},{"style":493},[6234],{"type":33,"value":6235},"    The type for character that the casting agent casts for a role in the movie\n",{"type":27,"tag":261,"props":6237,"children":6238},{"class":263,"line":537},[6239],{"type":27,"tag":261,"props":6240,"children":6241},{"style":493},[6242],{"type":33,"value":2277},{"type":27,"tag":261,"props":6244,"children":6245},{"class":263,"line":561},[6246,6251,6255,6260,6265,6270,6274,6279],{"type":27,"tag":261,"props":6247,"children":6248},{"style":268},[6249],{"type":33,"value":6250},"    full_name: ",{"type":27,"tag":261,"props":6252,"children":6253},{"style":850},[6254],{"type":33,"value":853},{"type":27,"tag":261,"props":6256,"children":6257},{"style":274},[6258],{"type":33,"value":6259}," =",{"type":27,"tag":261,"props":6261,"children":6262},{"style":268},[6263],{"type":33,"value":6264}," Field(",{"type":27,"tag":261,"props":6266,"children":6267},{"style":285},[6268],{"type":33,"value":6269},"description",{"type":27,"tag":261,"props":6271,"children":6272},{"style":274},[6273],{"type":33,"value":277},{"type":27,"tag":261,"props":6275,"children":6276},{"style":493},[6277],{"type":33,"value":6278},"\"The character's name\"",{"type":27,"tag":261,"props":6280,"children":6281},{"style":268},[6282],{"type":33,"value":340},{"type":27,"tag":261,"props":6284,"children":6285},{"class":263,"line":785},[6286,6291,6295,6299,6303,6307,6311,6316],{"type":27,"tag":261,"props":6287,"children":6288},{"style":268},[6289],{"type":33,"value":6290},"    short_name: ",{"type":27,"tag":261,"props":6292,"children":6293},{"style":850},[6294],{"type":33,"value":853},{"type":27,"tag":261,"props":6296,"children":6297},{"style":274},[6298],{"type":33,"value":6259},{"type":27,"tag":261,"props":6300,"children":6301},{"style":268},[6302],{"type":33,"value":6264},{"type":27,"tag":261,"props":6304,"children":6305},{"style":285},[6306],{"type":33,"value":6269},{"type":27,"tag":261,"props":6308,"children":6309},{"style":274},[6310],{"type":33,"value":277},{"type":27,"tag":261,"props":6312,"children":6313},{"style":493},[6314],{"type":33,"value":6315},"\"The character's short name\"",{"type":27,"tag":261,"props":6317,"children":6318},{"style":268},[6319],{"type":33,"value":340},{"type":27,"tag":261,"props":6321,"children":6322},{"class":263,"line":794},[6323,6328,6332,6336,6340,6344,6348,6353],{"type":27,"tag":261,"props":6324,"children":6325},{"style":268},[6326],{"type":33,"value":6327},"    background: ",{"type":27,"tag":261,"props":6329,"children":6330},{"style":850},[6331],{"type":33,"value":853},{"type":27,"tag":261,"props":6333,"children":6334},{"style":274},[6335],{"type":33,"value":6259},{"type":27,"tag":261,"props":6337,"children":6338},{"style":268},[6339],{"type":33,"value":6264},{"type":27,"tag":261,"props":6341,"children":6342},{"style":285},[6343],{"type":33,"value":6269},{"type":27,"tag":261,"props":6345,"children":6346},{"style":274},[6347],{"type":33,"value":277},{"type":27,"tag":261,"props":6349,"children":6350},{"style":493},[6351],{"type":33,"value":6352},"\"The character's background\"",{"type":27,"tag":261,"props":6354,"children":6355},{"style":268},[6356],{"type":33,"value":340},{"type":27,"tag":261,"props":6358,"children":6359},{"class":263,"line":803},[6360,6365,6369,6373,6377,6381,6385,6390],{"type":27,"tag":261,"props":6361,"children":6362},{"style":268},[6363],{"type":33,"value":6364},"    physical_traits: ",{"type":27,"tag":261,"props":6366,"children":6367},{"style":850},[6368],{"type":33,"value":853},{"type":27,"tag":261,"props":6370,"children":6371},{"style":274},[6372],{"type":33,"value":6259},{"type":27,"tag":261,"props":6374,"children":6375},{"style":268},[6376],{"type":33,"value":6264},{"type":27,"tag":261,"props":6378,"children":6379},{"style":285},[6380],{"type":33,"value":6269},{"type":27,"tag":261,"props":6382,"children":6383},{"style":274},[6384],{"type":33,"value":277},{"type":27,"tag":261,"props":6386,"children":6387},{"style":493},[6388],{"type":33,"value":6389},"\"The physical traits of the character\"",{"type":27,"tag":261,"props":6391,"children":6392},{"style":268},[6393],{"type":33,"value":340},{"type":27,"tag":261,"props":6395,"children":6396},{"class":263,"line":811},[6397,6402,6406,6410,6414,6418,6422,6427],{"type":27,"tag":261,"props":6398,"children":6399},{"style":268},[6400],{"type":33,"value":6401},"    ethnicity: ",{"type":27,"tag":261,"props":6403,"children":6404},{"style":850},[6405],{"type":33,"value":853},{"type":27,"tag":261,"props":6407,"children":6408},{"style":274},[6409],{"type":33,"value":6259},{"type":27,"tag":261,"props":6411,"children":6412},{"style":268},[6413],{"type":33,"value":6264},{"type":27,"tag":261,"props":6415,"children":6416},{"style":285},[6417],{"type":33,"value":6269},{"type":27,"tag":261,"props":6419,"children":6420},{"style":274},[6421],{"type":33,"value":277},{"type":27,"tag":261,"props":6423,"children":6424},{"style":493},[6425],{"type":33,"value":6426},"\"The character's ethnicity\"",{"type":27,"tag":261,"props":6428,"children":6429},{"style":268},[6430],{"type":33,"value":340},{"type":27,"tag":261,"props":6432,"children":6433},{"class":263,"line":860},[6434,6439,6443,6447,6451,6455,6459,6464],{"type":27,"tag":261,"props":6435,"children":6436},{"style":268},[6437],{"type":33,"value":6438},"    gender: ",{"type":27,"tag":261,"props":6440,"children":6441},{"style":850},[6442],{"type":33,"value":853},{"type":27,"tag":261,"props":6444,"children":6445},{"style":274},[6446],{"type":33,"value":6259},{"type":27,"tag":261,"props":6448,"children":6449},{"style":268},[6450],{"type":33,"value":6264},{"type":27,"tag":261,"props":6452,"children":6453},{"style":285},[6454],{"type":33,"value":6269},{"type":27,"tag":261,"props":6456,"children":6457},{"style":274},[6458],{"type":33,"value":277},{"type":27,"tag":261,"props":6460,"children":6461},{"style":493},[6462],{"type":33,"value":6463},"\"The character's gender, either male of female\"",{"type":27,"tag":261,"props":6465,"children":6466},{"style":268},[6467],{"type":33,"value":340},{"type":27,"tag":261,"props":6469,"children":6470},{"class":263,"line":884},[6471,6476,6480,6484,6488,6492,6496,6501],{"type":27,"tag":261,"props":6472,"children":6473},{"style":268},[6474],{"type":33,"value":6475},"    nationality: ",{"type":27,"tag":261,"props":6477,"children":6478},{"style":850},[6479],{"type":33,"value":853},{"type":27,"tag":261,"props":6481,"children":6482},{"style":274},[6483],{"type":33,"value":6259},{"type":27,"tag":261,"props":6485,"children":6486},{"style":268},[6487],{"type":33,"value":6264},{"type":27,"tag":261,"props":6489,"children":6490},{"style":285},[6491],{"type":33,"value":6269},{"type":27,"tag":261,"props":6493,"children":6494},{"style":274},[6495],{"type":33,"value":277},{"type":27,"tag":261,"props":6497,"children":6498},{"style":493},[6499],{"type":33,"value":6500},"\"The character's nationality\"",{"type":27,"tag":261,"props":6502,"children":6503},{"style":268},[6504],{"type":33,"value":340},{"type":27,"tag":261,"props":6506,"children":6507},{"class":263,"line":902},[6508,6513,6517,6521,6525,6529,6533,6538],{"type":27,"tag":261,"props":6509,"children":6510},{"style":268},[6511],{"type":33,"value":6512},"    main_character: ",{"type":27,"tag":261,"props":6514,"children":6515},{"style":850},[6516],{"type":33,"value":2265},{"type":27,"tag":261,"props":6518,"children":6519},{"style":274},[6520],{"type":33,"value":6259},{"type":27,"tag":261,"props":6522,"children":6523},{"style":268},[6524],{"type":33,"value":6264},{"type":27,"tag":261,"props":6526,"children":6527},{"style":285},[6528],{"type":33,"value":6269},{"type":27,"tag":261,"props":6530,"children":6531},{"style":274},[6532],{"type":33,"value":277},{"type":27,"tag":261,"props":6534,"children":6535},{"style":493},[6536],{"type":33,"value":6537},"\"If the character is or is not the main character\"",{"type":27,"tag":261,"props":6539,"children":6540},{"style":268},[6541],{"type":33,"value":340},{"type":27,"tag":36,"props":6543,"children":6544},{},[6545],{"type":33,"value":6546},"LCEL offers wonderful syntactic sugar, I can use this model in a parse and pip that into the output from the mode:",{"type":27,"tag":238,"props":6548,"children":6550},{"code":6549,"language":254,"meta":8,"className":255,"style":8},"chain = prompt | model | parser\n",[6551],{"type":27,"tag":170,"props":6552,"children":6553},{"__ignoreMap":8},[6554],{"type":27,"tag":261,"props":6555,"children":6556},{"class":263,"line":264},[6557,6562,6566,6571,6576,6581,6585],{"type":27,"tag":261,"props":6558,"children":6559},{"style":268},[6560],{"type":33,"value":6561},"chain ",{"type":27,"tag":261,"props":6563,"children":6564},{"style":274},[6565],{"type":33,"value":277},{"type":27,"tag":261,"props":6567,"children":6568},{"style":268},[6569],{"type":33,"value":6570}," prompt ",{"type":27,"tag":261,"props":6572,"children":6573},{"style":274},[6574],{"type":33,"value":6575},"|",{"type":27,"tag":261,"props":6577,"children":6578},{"style":268},[6579],{"type":33,"value":6580}," model ",{"type":27,"tag":261,"props":6582,"children":6583},{"style":274},[6584],{"type":33,"value":6575},{"type":27,"tag":261,"props":6586,"children":6587},{"style":268},[6588],{"type":33,"value":6589}," parser\n",{"type":27,"tag":36,"props":6591,"children":6592},{},[6593],{"type":33,"value":6594},"This results in our structured data:",{"type":27,"tag":238,"props":6596,"children":6600},{"code":6597,"language":6598,"meta":8,"className":6599,"style":8},"cast:\n- background: Former MI6 agent\n  ethnicity: British\n  full_name: James Alexander\n  gender: Male\n  main_character: true\n  nationality: British\n  physical_traits: Tall, dark hair, blue eyes\n  short_name: Jamie\n","yml","language-yml shiki shiki-themes github-light github-dark monokai",[6601],{"type":27,"tag":170,"props":6602,"children":6603},{"__ignoreMap":8},[6604,6617,6639,6656,6673,6690,6707,6723,6740],{"type":27,"tag":261,"props":6605,"children":6606},{"class":263,"line":264},[6607,6613],{"type":27,"tag":261,"props":6608,"children":6610},{"style":6609},"--shiki-default:#22863A;--shiki-dark:#85E89D;--shiki-sepia:#F92672",[6611],{"type":33,"value":6612},"cast",{"type":27,"tag":261,"props":6614,"children":6615},{"style":268},[6616],{"type":33,"value":2128},{"type":27,"tag":261,"props":6618,"children":6619},{"class":263,"line":343},[6620,6625,6630,6634],{"type":27,"tag":261,"props":6621,"children":6622},{"style":268},[6623],{"type":33,"value":6624},"- ",{"type":27,"tag":261,"props":6626,"children":6627},{"style":6609},[6628],{"type":33,"value":6629},"background",{"type":27,"tag":261,"props":6631,"children":6632},{"style":268},[6633],{"type":33,"value":847},{"type":27,"tag":261,"props":6635,"children":6636},{"style":493},[6637],{"type":33,"value":6638},"Former MI6 agent\n",{"type":27,"tag":261,"props":6640,"children":6641},{"class":263,"line":379},[6642,6647,6651],{"type":27,"tag":261,"props":6643,"children":6644},{"style":6609},[6645],{"type":33,"value":6646},"  ethnicity",{"type":27,"tag":261,"props":6648,"children":6649},{"style":268},[6650],{"type":33,"value":847},{"type":27,"tag":261,"props":6652,"children":6653},{"style":493},[6654],{"type":33,"value":6655},"British\n",{"type":27,"tag":261,"props":6657,"children":6658},{"class":263,"line":537},[6659,6664,6668],{"type":27,"tag":261,"props":6660,"children":6661},{"style":6609},[6662],{"type":33,"value":6663},"  full_name",{"type":27,"tag":261,"props":6665,"children":6666},{"style":268},[6667],{"type":33,"value":847},{"type":27,"tag":261,"props":6669,"children":6670},{"style":493},[6671],{"type":33,"value":6672},"James Alexander\n",{"type":27,"tag":261,"props":6674,"children":6675},{"class":263,"line":561},[6676,6681,6685],{"type":27,"tag":261,"props":6677,"children":6678},{"style":6609},[6679],{"type":33,"value":6680},"  gender",{"type":27,"tag":261,"props":6682,"children":6683},{"style":268},[6684],{"type":33,"value":847},{"type":27,"tag":261,"props":6686,"children":6687},{"style":493},[6688],{"type":33,"value":6689},"Male\n",{"type":27,"tag":261,"props":6691,"children":6692},{"class":263,"line":785},[6693,6698,6702],{"type":27,"tag":261,"props":6694,"children":6695},{"style":6609},[6696],{"type":33,"value":6697},"  main_character",{"type":27,"tag":261,"props":6699,"children":6700},{"style":268},[6701],{"type":33,"value":847},{"type":27,"tag":261,"props":6703,"children":6704},{"style":295},[6705],{"type":33,"value":6706},"true\n",{"type":27,"tag":261,"props":6708,"children":6709},{"class":263,"line":794},[6710,6715,6719],{"type":27,"tag":261,"props":6711,"children":6712},{"style":6609},[6713],{"type":33,"value":6714},"  nationality",{"type":27,"tag":261,"props":6716,"children":6717},{"style":268},[6718],{"type":33,"value":847},{"type":27,"tag":261,"props":6720,"children":6721},{"style":493},[6722],{"type":33,"value":6655},{"type":27,"tag":261,"props":6724,"children":6725},{"class":263,"line":803},[6726,6731,6735],{"type":27,"tag":261,"props":6727,"children":6728},{"style":6609},[6729],{"type":33,"value":6730},"  physical_traits",{"type":27,"tag":261,"props":6732,"children":6733},{"style":268},[6734],{"type":33,"value":847},{"type":27,"tag":261,"props":6736,"children":6737},{"style":493},[6738],{"type":33,"value":6739},"Tall, dark hair, blue eyes\n",{"type":27,"tag":261,"props":6741,"children":6742},{"class":263,"line":811},[6743,6748,6752],{"type":27,"tag":261,"props":6744,"children":6745},{"style":6609},[6746],{"type":33,"value":6747},"  short_name",{"type":27,"tag":261,"props":6749,"children":6750},{"style":268},[6751],{"type":33,"value":847},{"type":27,"tag":261,"props":6753,"children":6754},{"style":493},[6755],{"type":33,"value":6756},"Jamie\n",{"type":27,"tag":36,"props":6758,"children":6759},{},[6760,6762,6768,6770,6781],{"type":33,"value":6761},"I saved the state for all \"Agents of Inference\" invocations in the ",{"type":27,"tag":170,"props":6763,"children":6765},{"className":6764},[],[6766],{"type":33,"value":6767},"output",{"type":33,"value":6769}," directory of my ",{"type":27,"tag":61,"props":6771,"children":6774},{"href":6772,"rel":6773},"https://github.com/briancaffey/agents-of-inference/tree/main/output",[65],[6775],{"type":27,"tag":170,"props":6776,"children":6778},{"className":6777},[],[6779],{"type":33,"value":6780},"agents-of-inference",{"type":33,"value":6782}," GitHub repo. I didn't commit the images and videos, but you can follow @AgentInference on X to see more of the results from my development process and future improvements, as well!",{"type":27,"tag":47,"props":6784,"children":6786},{"id":6785},"synopsis-agent",[6787],{"type":33,"value":6788},"Synopsis Agent",{"type":27,"tag":36,"props":6790,"children":6791},{},[6792],{"type":33,"value":6793},"With a cast of characters and locations selected, we need a synopsis to determine what happens. Here's the prompt:",{"type":27,"tag":238,"props":6795,"children":6799},{"code":6796,"language":6797,"meta":8,"className":6798,"style":8},"synopsis: |\n  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n","yaml","language-yaml shiki shiki-themes github-light github-dark monokai",[6800],{"type":27,"tag":170,"props":6801,"children":6802},{"__ignoreMap":8},[6803,6820,6828,6836,6844,6852,6860,6868,6876,6884,6892],{"type":27,"tag":261,"props":6804,"children":6805},{"class":263,"line":264},[6806,6811,6815],{"type":27,"tag":261,"props":6807,"children":6808},{"style":6609},[6809],{"type":33,"value":6810},"synopsis",{"type":27,"tag":261,"props":6812,"children":6813},{"style":268},[6814],{"type":33,"value":847},{"type":27,"tag":261,"props":6816,"children":6817},{"style":274},[6818],{"type":33,"value":6819},"|\n",{"type":27,"tag":261,"props":6821,"children":6822},{"class":263,"line":343},[6823],{"type":27,"tag":261,"props":6824,"children":6825},{"style":493},[6826],{"type":33,"value":6827},"  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n",{"type":27,"tag":261,"props":6829,"children":6830},{"class":263,"line":379},[6831],{"type":27,"tag":261,"props":6832,"children":6833},{"style":493},[6834],{"type":33,"value":6835},"  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n",{"type":27,"tag":261,"props":6837,"children":6838},{"class":263,"line":537},[6839],{"type":27,"tag":261,"props":6840,"children":6841},{"style":493},[6842],{"type":33,"value":6843},"  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n",{"type":27,"tag":261,"props":6845,"children":6846},{"class":263,"line":561},[6847],{"type":27,"tag":261,"props":6848,"children":6849},{"style":493},[6850],{"type":33,"value":6851},"  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n",{"type":27,"tag":261,"props":6853,"children":6854},{"class":263,"line":785},[6855],{"type":27,"tag":261,"props":6856,"children":6857},{"style":493},[6858],{"type":33,"value":6859},"  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n",{"type":27,"tag":261,"props":6861,"children":6862},{"class":263,"line":794},[6863],{"type":27,"tag":261,"props":6864,"children":6865},{"style":493},[6866],{"type":33,"value":6867},"  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n",{"type":27,"tag":261,"props":6869,"children":6870},{"class":263,"line":803},[6871],{"type":27,"tag":261,"props":6872,"children":6873},{"style":493},[6874],{"type":33,"value":6875},"  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n",{"type":27,"tag":261,"props":6877,"children":6878},{"class":263,"line":811},[6879],{"type":27,"tag":261,"props":6880,"children":6881},{"style":493},[6882],{"type":33,"value":6883},"  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n",{"type":27,"tag":261,"props":6885,"children":6886},{"class":263,"line":860},[6887],{"type":27,"tag":261,"props":6888,"children":6889},{"style":493},[6890],{"type":33,"value":6891},"  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n",{"type":27,"tag":261,"props":6893,"children":6894},{"class":263,"line":884},[6895],{"type":27,"tag":261,"props":6896,"children":6897},{"style":493},[6898],{"type":33,"value":6899},"  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n",{"type":27,"tag":36,"props":6901,"children":6902},{},[6903,6905,6911,6913,6919,6921,6926],{"type":33,"value":6904},"The synopsis to any good film is key, so I decided to use a feature of LangGraph that would allow a ",{"type":27,"tag":170,"props":6906,"children":6908},{"className":6907},[],[6909],{"type":33,"value":6910},"synopsis_review_agent",{"type":33,"value":6912}," to provide multiple rounds of feedback to the ",{"type":27,"tag":170,"props":6914,"children":6916},{"className":6915},[],[6917],{"type":33,"value":6918},"synopsis_agent",{"type":33,"value":6920}," to make it even better. Here's what the new graph look like after implementing the ",{"type":27,"tag":170,"props":6922,"children":6924},{"className":6923},[],[6925],{"type":33,"value":6910},{"type":33,"value":6927}," using conditional graph edges:",{"type":27,"tag":36,"props":6929,"children":6930},{},[6931],{"type":27,"tag":84,"props":6932,"children":6934},{"alt":6910,"src":6933},"/static/aoi/graph_with_cycle.png",[],{"type":27,"tag":36,"props":6936,"children":6937},{},[6938],{"type":33,"value":6939},"Conditional edges are a very powerful feature and I just used it in one part of my graph. Other parts of the graph could benefit from this as well, and they can allow for \"human-in-the-loop\" interactions which are becoming very popular in AI-powered applications.",{"type":27,"tag":47,"props":6941,"children":6943},{"id":6942},"scene-and-shot-agents",[6944],{"type":33,"value":6945},"Scene and shot agents",{"type":27,"tag":36,"props":6947,"children":6948},{},[6949],{"type":33,"value":6950},"With our perfected synopsis, we are ready to put more agents to work. The scene agent builds out the basic structure of the storyline. It provides a structured list of the main sections of the movie. The shot agent then loops over the scenes and creates a number of different shots for the given scene. This was an effective way to have consistent thematic content for shots within a scene. Here are the prompts I used for the scene and shot agents:",{"type":27,"tag":238,"props":6952,"children":6954},{"code":6953,"language":6797,"meta":8,"className":6798,"style":8},"scenes: |\n  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\nshot: |\n  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",[6955],{"type":27,"tag":170,"props":6956,"children":6957},{"__ignoreMap":8},[6958,6974,6982,6990,6998,7006,7022],{"type":27,"tag":261,"props":6959,"children":6960},{"class":263,"line":264},[6961,6966,6970],{"type":27,"tag":261,"props":6962,"children":6963},{"style":6609},[6964],{"type":33,"value":6965},"scenes",{"type":27,"tag":261,"props":6967,"children":6968},{"style":268},[6969],{"type":33,"value":847},{"type":27,"tag":261,"props":6971,"children":6972},{"style":274},[6973],{"type":33,"value":6819},{"type":27,"tag":261,"props":6975,"children":6976},{"class":263,"line":343},[6977],{"type":27,"tag":261,"props":6978,"children":6979},{"style":493},[6980],{"type":33,"value":6981},"  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n",{"type":27,"tag":261,"props":6983,"children":6984},{"class":263,"line":379},[6985],{"type":27,"tag":261,"props":6986,"children":6987},{"style":493},[6988],{"type":33,"value":6989},"  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n",{"type":27,"tag":261,"props":6991,"children":6992},{"class":263,"line":537},[6993],{"type":27,"tag":261,"props":6994,"children":6995},{"style":493},[6996],{"type":33,"value":6997},"  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n",{"type":27,"tag":261,"props":6999,"children":7000},{"class":263,"line":561},[7001],{"type":27,"tag":261,"props":7002,"children":7003},{"style":493},[7004],{"type":33,"value":7005},"  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\n",{"type":27,"tag":261,"props":7007,"children":7008},{"class":263,"line":785},[7009,7014,7018],{"type":27,"tag":261,"props":7010,"children":7011},{"style":6609},[7012],{"type":33,"value":7013},"shot",{"type":27,"tag":261,"props":7015,"children":7016},{"style":268},[7017],{"type":33,"value":847},{"type":27,"tag":261,"props":7019,"children":7020},{"style":274},[7021],{"type":33,"value":6819},{"type":27,"tag":261,"props":7023,"children":7024},{"class":263,"line":794},[7025],{"type":27,"tag":261,"props":7026,"children":7027},{"style":493},[7028],{"type":33,"value":7029},"  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",{"type":27,"tag":47,"props":7031,"children":7033},{"id":7032},"stable-diffusion-and-stable-video-diffusion-agents",[7034],{"type":33,"value":7035},"Stable Diffusion and Stable Video Diffusion agents",{"type":27,"tag":36,"props":7037,"children":7038},{},[7039],{"type":33,"value":7040},"The stable diffusion agent makes an API call to a local instance of the Stable Diffusion WebUI API, saves the generated image and saves a reference to that image in the state:",{"type":27,"tag":238,"props":7042,"children":7044},{"code":7043,"language":6797,"meta":8,"className":6798,"style":8},"- description: A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n  image: 000.png\n  title: Ethan's Concerned Expression\n  video: 000.mp4\n",[7045],{"type":27,"tag":170,"props":7046,"children":7047},{"__ignoreMap":8},[7048,7068,7076,7084,7092,7109,7126],{"type":27,"tag":261,"props":7049,"children":7050},{"class":263,"line":264},[7051,7055,7059,7063],{"type":27,"tag":261,"props":7052,"children":7053},{"style":268},[7054],{"type":33,"value":6624},{"type":27,"tag":261,"props":7056,"children":7057},{"style":6609},[7058],{"type":33,"value":6269},{"type":27,"tag":261,"props":7060,"children":7061},{"style":268},[7062],{"type":33,"value":847},{"type":27,"tag":261,"props":7064,"children":7065},{"style":493},[7066],{"type":33,"value":7067},"A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n",{"type":27,"tag":261,"props":7069,"children":7070},{"class":263,"line":343},[7071],{"type":27,"tag":261,"props":7072,"children":7073},{"style":493},[7074],{"type":33,"value":7075},"    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n",{"type":27,"tag":261,"props":7077,"children":7078},{"class":263,"line":379},[7079],{"type":27,"tag":261,"props":7080,"children":7081},{"style":493},[7082],{"type":33,"value":7083},"    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n",{"type":27,"tag":261,"props":7085,"children":7086},{"class":263,"line":537},[7087],{"type":27,"tag":261,"props":7088,"children":7089},{"style":493},[7090],{"type":33,"value":7091},"    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n",{"type":27,"tag":261,"props":7093,"children":7094},{"class":263,"line":561},[7095,7100,7104],{"type":27,"tag":261,"props":7096,"children":7097},{"style":6609},[7098],{"type":33,"value":7099},"  image",{"type":27,"tag":261,"props":7101,"children":7102},{"style":268},[7103],{"type":33,"value":847},{"type":27,"tag":261,"props":7105,"children":7106},{"style":493},[7107],{"type":33,"value":7108},"000.png\n",{"type":27,"tag":261,"props":7110,"children":7111},{"class":263,"line":785},[7112,7117,7121],{"type":27,"tag":261,"props":7113,"children":7114},{"style":6609},[7115],{"type":33,"value":7116},"  title",{"type":27,"tag":261,"props":7118,"children":7119},{"style":268},[7120],{"type":33,"value":847},{"type":27,"tag":261,"props":7122,"children":7123},{"style":493},[7124],{"type":33,"value":7125},"Ethan's Concerned Expression\n",{"type":27,"tag":261,"props":7127,"children":7128},{"class":263,"line":794},[7129,7134,7138],{"type":27,"tag":261,"props":7130,"children":7131},{"style":6609},[7132],{"type":33,"value":7133},"  video",{"type":27,"tag":261,"props":7135,"children":7136},{"style":268},[7137],{"type":33,"value":847},{"type":27,"tag":261,"props":7139,"children":7140},{"style":493},[7141],{"type":33,"value":7142},"000.mp4\n",{"type":27,"tag":36,"props":7144,"children":7145},{},[7146],{"type":27,"tag":84,"props":7147,"children":7150},{"alt":7148,"src":7149},"A medium close-up shot of Ethan Jameson's face","/static/aoi/ethan.png",[],{"type":27,"tag":36,"props":7152,"children":7153},{},[7154],{"type":33,"value":7155},"With the perfectly prompted image in hand, we can use Stable Video Diffusion to bring it to life. I prompted phind to come up with a FastAPI service that would accept an image in a post request and return a short video created with stable video diffusion using the diffusers library.",{"type":27,"tag":36,"props":7157,"children":7158},{},[7159],{"type":33,"value":7160},"Stable video diffusion can generate about 4 seconds of text at 7 frames per second. This isn't great, but I was able to use ffmpeg to do frame interpolation bringing the frame rate to a much smoother 14 fps using motion compensated interpolation (MCI):",{"type":27,"tag":238,"props":7162,"children":7166},{"code":7163,"language":7164,"meta":8,"className":7165,"style":8},"ffmpeg -i output/1718453390/final.mp4 -crf 10 -vf \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\" output/1718453390/final.14fps.mp4\n","bash","language-bash shiki shiki-themes github-light github-dark monokai",[7167],{"type":27,"tag":170,"props":7168,"children":7169},{"__ignoreMap":8},[7170],{"type":27,"tag":261,"props":7171,"children":7172},{"class":263,"line":264},[7173,7178,7183,7188,7193,7198,7203,7208],{"type":27,"tag":261,"props":7174,"children":7175},{"style":820},[7176],{"type":33,"value":7177},"ffmpeg",{"type":27,"tag":261,"props":7179,"children":7180},{"style":295},[7181],{"type":33,"value":7182}," -i",{"type":27,"tag":261,"props":7184,"children":7185},{"style":493},[7186],{"type":33,"value":7187}," output/1718453390/final.mp4",{"type":27,"tag":261,"props":7189,"children":7190},{"style":295},[7191],{"type":33,"value":7192}," -crf",{"type":27,"tag":261,"props":7194,"children":7195},{"style":295},[7196],{"type":33,"value":7197}," 10",{"type":27,"tag":261,"props":7199,"children":7200},{"style":295},[7201],{"type":33,"value":7202}," -vf",{"type":27,"tag":261,"props":7204,"children":7205},{"style":493},[7206],{"type":33,"value":7207}," \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\"",{"type":27,"tag":261,"props":7209,"children":7210},{"style":493},[7211],{"type":33,"value":7212}," output/1718453390/final.14fps.mp4\n",{"type":27,"tag":36,"props":7214,"children":7215},{},[7216,7218,7224,7226,7232],{"type":33,"value":7217},"Finally, the ",{"type":27,"tag":170,"props":7219,"children":7221},{"className":7220},[],[7222],{"type":33,"value":7223},"editor_agent",{"type":33,"value":7225}," uses ",{"type":27,"tag":170,"props":7227,"children":7229},{"className":7228},[],[7230],{"type":33,"value":7231},"moviepy",{"type":33,"value":7233}," to join the clips together into a single video.",{"type":27,"tag":28,"props":7235,"children":7237},{"id":7236},"development-environment",[7238],{"type":33,"value":7239},"Development environment",{"type":27,"tag":36,"props":7241,"children":7242},{},[7243,7245,7251,7253,7259],{"type":33,"value":7244},"I struggled to optimize the ",{"type":27,"tag":170,"props":7246,"children":7248},{"className":7247},[],[7249],{"type":33,"value":7250},"meta-llama/Meta-Llama-3-8B-Instruct",{"type":33,"value":7252}," with TensorRT-LLM, so I ran LLM inference on a combination of older Llama2 TensorRT-LLM models, and ",{"type":27,"tag":170,"props":7254,"children":7256},{"className":7255},[],[7257],{"type":33,"value":7258},"Meta-Llama-3-8B-Instruct",{"type":33,"value":7260}," on LM Studio (which I found to be painfully slow compared to TensorRT-LLM).",{"type":27,"tag":36,"props":7262,"children":7263},{},[7264,7266,7272,7274,7280,7282,7288,7290,7297,7299,7304],{"type":33,"value":7265},"If you provide an ",{"type":27,"tag":170,"props":7267,"children":7269},{"className":7268},[],[7270],{"type":33,"value":7271},"NVIDIA_API_KEY",{"type":33,"value":7273}," in the ",{"type":27,"tag":170,"props":7275,"children":7277},{"className":7276},[],[7278],{"type":33,"value":7279},".env",{"type":33,"value":7281}," file, LLM calls will be made using the ",{"type":27,"tag":170,"props":7283,"children":7285},{"className":7284},[],[7286],{"type":33,"value":7287},"meta/llam3-70b-instruct",{"type":33,"value":7289}," model on ",{"type":27,"tag":61,"props":7291,"children":7294},{"href":7292,"rel":7293},"https://build.nvidia.com/meta/llama3-70b",[65],[7295],{"type":33,"value":7296},"build.nvidia.com/meta/llama3-70b",{"type":33,"value":7298},". In fact, ",{"type":27,"tag":170,"props":7300,"children":7302},{"className":7301},[],[7303],{"type":33,"value":175},{"type":33,"value":7305}," also provides stable diffusion and stable video diffusion inference via API. This would be very convenient in the event that my RTX PCs become compromised.",{"type":27,"tag":36,"props":7307,"children":7308},{},[7309],{"type":33,"value":7310},"My RTX 4090 GPU with 24 GB of memory was able to run lots of different inference servers concurrently (LLM, Stable Diffusion WebUI, ComfyUI, InvokeAI, Stable Video Diffusion FastAPI service), but I generally stuck to doing one type of inference at a time, otherwise things would grind to a hault or crash. I also experimented with ChatTTS, a new text-to-speech model.",{"type":27,"tag":36,"props":7312,"children":7313},{},[7314],{"type":33,"value":7315},"I developed this project on a MacBook Pro, and I used my RTX PC as if it were a remote service providing inference for text, images and video. This is a helpful mindset when working with hybrid AI workflows that leverage inference services both on local machines and in the cloud.",{"type":27,"tag":28,"props":7317,"children":7319},{"id":7318},"how-it-works",[7320],{"type":33,"value":7321},"How it works",{"type":27,"tag":36,"props":7323,"children":7324},{},[7325,7327,7333],{"type":33,"value":7326},"To run the program, you need to install python dependencies and then run an OpenAI compatible LLM and Stable Duffsion WebUI server with the ",{"type":27,"tag":170,"props":7328,"children":7330},{"className":7329},[],[7331],{"type":33,"value":7332},"--api",{"type":33,"value":7334}," flag. You also need to run the Stable Video Diffusion service. Apologies for any hardcoded local IP address in the source code. Deadlines, you know! With everything configured, you can run the following command:",{"type":27,"tag":238,"props":7336,"children":7338},{"code":7337},"~/git/github/agents-of-inference$ poetry run python agents_of_inference/main.py\n## 📀 Using local models 📀 ##\n## 🎭 Generating Cast 🎭 ##\n## 🗺️ Generating Locations 🗺️ ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to scene_agent ##\n## 📒 Generating Scenes 📒 ##\n## 🎬 Generating Shots 🎬 ##\n## Generated 5 shots for scene 1/5 ##\n## Generated 5 shots for scene 2/5 ##\n## Generated 5 shots for scene 3/5 ##\n## Generated 5 shots for scene 4/5 ##\n## Generated 5 shots for scene 5/5 ##\n\n000/0025\nA medium shot of a bustling Tokyo street, with neon lights reflecting off wet pavement. Jim Thompson, dressed in a black leather jacket and dark jeans, walks purposefully through the crowd, his piercing blue eyes scanning the area. The sound design features the hum of traffic and chatter of pedestrians.\nGenerated image output/1718426686/images/000.png\n\n001/0025\nA tight close-up shot of Emily Chen's face, her piercing brown eyes intense as she briefs Jim on the situation. Her short black hair is styled neatly, and she wears a crisp white blouse with a silver necklace. The camera lingers on her lips as she speaks, emphasizing the importance of the information.\nGenerated image output/1718426686/images/001.png\n\nGenerated video output/1718426686/videos/000.mp4\n== stable video diffusion generation complete ==\nGenerated video output/1718426686/videos/001.mp4\n== stable video diffusion generation complete ==\n",[7339],{"type":27,"tag":170,"props":7340,"children":7341},{"__ignoreMap":8},[7342],{"type":33,"value":7337},{"type":27,"tag":28,"props":7344,"children":7346},{"id":7345},"demo-video-for-contest-submission",[7347],{"type":33,"value":7348},"Demo Video for Contest Submission",{"type":27,"tag":7350,"props":7351,"children":7352},"agents-of-inference-video",{},[],{"type":27,"tag":36,"props":7354,"children":7355},{},[7356],{"type":33,"value":7357},"Making this video was a lot of fun. The \"Agents of Inference\" highlight reel includes some of the most interesting, exciting and fun clips that I found in the dozens of short films it created. It is important to note that a lot of the content is not very good. Misunderstood prompts, color confusion (prompt includes green eyes, but other things in the scene are also conspicuously green), unrealistic or noisy motion from Stable Video Diffusion--these are some of the issues you will find in the films. Generating AI images sometimes feels like panning for gold: you go through a lot of sediment to get a few good flakes.",{"type":27,"tag":36,"props":7359,"children":7360},{},[7361],{"type":33,"value":7362},"Also, I added a few short animations that I made with Blender. The final scene shows the NVIDIA Omniverse orange humanoid from the barrel of a pistol. I think we are rapidly approaching a future where agents can generate full-scale theatrical movies by generating OpenUSD code, directly or indirectly. Maybe for the next NVIDIA Developer contest!",{"type":27,"tag":28,"props":7364,"children":7366},{"id":7365},"shortcomings-of-my-project",[7367],{"type":33,"value":7368},"Shortcomings of my project",{"type":27,"tag":36,"props":7370,"children":7371},{},[7372],{"type":33,"value":7373},"My goodness, how embarrasing. There are quite a few shortcomings that can be easily identified looking over the output and the source code. Here are a few:",{"type":27,"tag":47,"props":7375,"children":7377},{"id":7376},"character-variety",[7378],{"type":33,"value":7379},"Character variety",{"type":27,"tag":36,"props":7381,"children":7382},{},[7383],{"type":33,"value":7384},"When generating characters I would frequently see one named Dr. Sophia Patel who is apparently a brilliant cryptologist. Other characters would often have different names or backgrounds, but a saw Dr. Sophia Patel more often than not.",{"type":27,"tag":47,"props":7386,"children":7388},{"id":7387},"character-consistency",[7389],{"type":33,"value":7390},"Character consistency",{"type":27,"tag":36,"props":7392,"children":7393},{},[7394],{"type":33,"value":7395},"The characters are not consistent. This is a notoriously difficult problem to solve, but I made a lot of progress on it during this contest. I experimented with calling the ComfyUI API to run a custom workflow built with the ComfyUI graph-based workflow tool for face transfer:",{"type":27,"tag":36,"props":7397,"children":7398},{},[7399],{"type":27,"tag":84,"props":7400,"children":7403},{"alt":7401,"src":7402},"Dr. Sophia Patel","/static/aoi/sophia.png",[],{"type":27,"tag":36,"props":7405,"children":7406},{},[7407],{"type":33,"value":7408},"Using ComfyUI would be nice, but it wouldn't be as easy to tap into cloud APIs if my workflow heavily relied on ComfyUI server with custom models.",{"type":27,"tag":47,"props":7410,"children":7412},{"id":7411},"understanding-langchain",[7413],{"type":33,"value":7414},"Understanding LangChain",{"type":27,"tag":36,"props":7416,"children":7417},{},[7418,7420,7425],{"type":33,"value":7419},"I started out with the idea I would store all LLM calls to a local JSON to serve as a cache, allowing me to avoid regenerating responses from early in the workflow. This worked well, until I tried to serialize an Annotated list (required for cycles such as the one used with ",{"type":27,"tag":170,"props":7421,"children":7423},{"className":7422},[],[7424],{"type":33,"value":6910},{"type":33,"value":7426},"). I ended up wasting a lot of time trying to figure this out, and I came across some built-in LangChain features for storing state in memory and in Sqlite. I'm sure there are other areas where I used the wrong pattern, but I turned over a lot of stones and look forward to continuing development with LangChain.",{"type":27,"tag":28,"props":7428,"children":7430},{"id":7429},"whats-next",[7431],{"type":33,"value":7432},"What's next?",{"type":27,"tag":36,"props":7434,"children":7435},{},[7436],{"type":33,"value":7437},"Thank you to NVIDIA and LangChain for organizing this contest. It was a great way to explore a powerful toolset for automated content generation using AI agents.",{"type":27,"tag":36,"props":7439,"children":7440},{},[7441],{"type":33,"value":7442},"Video models like Dream Machine and Sora have made some big splashes on the internet and the results are remarkable. However, I'm still almost more interested in finding the limitations of quality content using open-source models on consumer hardware like RTX GPUs.",{"type":27,"tag":36,"props":7444,"children":7445},{},[7446],{"type":33,"value":7447},"I would also have loved to generate my own music for these films. I am a Suno poweruser and love the songs I have generated on that site. Will the gap between video and music generation on private, payed services and local machines? Or does it just need time to catch up? Hopefully a future installment of \"Agents of Inference\" will integrate music and voice, and can't wait to hear it!",{"type":27,"tag":5518,"props":7449,"children":7450},{},[7451],{"type":33,"value":5522},{"title":8,"searchDepth":343,"depth":343,"links":7453},[7454,7455,7456,7457,7458,7464,7465,7466,7467,7472],{"id":6009,"depth":343,"text":6012},{"id":5593,"depth":343,"text":5596},{"id":6053,"depth":343,"text":6056},{"id":6064,"depth":343,"text":6067},{"id":6075,"depth":343,"text":6078,"children":7459},[7460,7461,7462,7463],{"id":6142,"depth":379,"text":6145},{"id":6785,"depth":379,"text":6788},{"id":6942,"depth":379,"text":6945},{"id":7032,"depth":379,"text":7035},{"id":7236,"depth":343,"text":7239},{"id":7318,"depth":343,"text":7321},{"id":7345,"depth":343,"text":7348},{"id":7365,"depth":343,"text":7368,"children":7468},[7469,7470,7471],{"id":7376,"depth":379,"text":7379},{"id":7387,"depth":379,"text":7390},{"id":7411,"depth":379,"text":7414},{"id":7429,"depth":343,"text":7432},"content:2024:06:17:agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest.md","2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest.md","2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",{"_path":7477,"_dir":5998,"_draft":7,"_partial":7,"_locale":8,"title":7478,"description":7479,"date":7480,"image":7481,"tags":7482,"external":7486,"comments":22,"body":7495,"_type":5561,"_id":9714,"_source":5563,"_file":9715,"_stem":9716,"_extension":5566},"/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest","Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA's Generative AI on RTX PCs Developer Contest","This article discusses my entry for NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat","2024-02-17","/static/rlbc/cover.png",[14,5577,5578,19,16,17,5580,7483,7484,7485],"rocket-league","gaming","windows",[7487,7489,7492],{"link":7488,"site":5588},"https://twitter.com/briancaffey/status/1760529251072118901",{"link":7490,"site":7491},"https://www.reddit.com/r/RocketLeague/comments/1au0po3/rocket_league_botchat_an_llmpowered_bakkesmod/","reddit",{"link":7493,"site":7494},"https://dev.to/briancaffey/rocket-league-botchat-powered-by-tensorrt-llm-my-submission-for-nvidias-generative-ai-on-rtx-pcs-developer-contest-2oao","dev",{"type":24,"children":7496,"toc":9691},[7497,7501,7506,7510,7514,7526,7532,7537,7545,7550,7555,7560,7565,7573,7578,7583,7591,7599,7604,7612,7617,7622,7627,7632,7640,7644,7649,7758,7779,7792,7827,7832,7845,7874,7879,7891,7896,8022,8043,8051,8072,8118,8123,8156,8160,8165,8173,8242,8248,8260,8554,8560,8565,8696,8709,8714,8722,8734,8739,8744,8752,8756,8768,8774,8787,8941,8947,8960,9152,9158,9163,9228,9240,9273,9286,9292,9297,9305,9311,9316,9481,9502,9508,9513,9518,9523,9527,9531,9536,9544,9596,9604,9612,9620,9629,9633,9645,9650,9655,9668,9672,9677,9682,9687],{"type":27,"tag":28,"props":7498,"children":7499},{"id":5593},[7500],{"type":33,"value":5596},{"type":27,"tag":36,"props":7502,"children":7503},{},[7504],{"type":33,"value":7505},"This article is about my submission to NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA's TensorRT-LLM library.",{"type":27,"tag":36,"props":7507,"children":7508},{},[7509],{"type":33,"value":6037},{"type":27,"tag":7511,"props":7512,"children":7513},"rocket-league-bot-chat-tweet",{},[],{"type":27,"tag":36,"props":7515,"children":7516},{},[7517,7518,7525],{"type":33,"value":5623},{"type":27,"tag":61,"props":7519,"children":7522},{"href":7520,"rel":7521},"https://github.com/briancaffey/RocketLeagueBotChat",[65],[7523],{"type":33,"value":7524},"Rocket League BotChat GitHub repository",{"type":33,"value":2970},{"type":27,"tag":28,"props":7527,"children":7529},{"id":7528},"nvidias-gen-ai-developer-contest",[7530],{"type":33,"value":7531},"NVIDIA's Gen AI Developer Contest",{"type":27,"tag":36,"props":7533,"children":7534},{},[7535],{"type":33,"value":7536},"The following email caught my attention last month:",{"type":27,"tag":2972,"props":7538,"children":7539},{},[7540],{"type":27,"tag":36,"props":7541,"children":7542},{},[7543],{"type":33,"value":7544},"Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems",{"type":27,"tag":36,"props":7546,"children":7547},{},[7548],{"type":33,"value":7549},"The part about “on Windows PC” made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had also never used any of the Windows-specific development frameworks like .NET or Visual Studio.",{"type":27,"tag":36,"props":7551,"children":7552},{},[7553],{"type":33,"value":7554},"My experience with Windows goes back to 2016 when I built my fist PC with an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives in order to dual boot into both Windows and Ubuntu operating systems. Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer.",{"type":27,"tag":36,"props":7556,"children":7557},{},[7558],{"type":33,"value":7559},"Last year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my PC's Ubuntu operating system, and frequently had to switch between using Ubuntu for working with stable diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems pushed me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy.",{"type":27,"tag":36,"props":7561,"children":7562},{},[7563],{"type":33,"value":7564},"I learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently compared ot Linux-based operating systems. And just like that, I became a Windows developer! This experience got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest.",{"type":27,"tag":36,"props":7566,"children":7567},{},[7568],{"type":27,"tag":84,"props":7569,"children":7572},{"alt":7570,"src":7571},"Windows winfetch screenshot","/static/rlbc/winfetch.png",[],{"type":27,"tag":28,"props":7574,"children":7575},{"id":6064},[7576],{"type":33,"value":7577},"Coming up with an Idea",{"type":27,"tag":36,"props":7579,"children":7580},{},[7581],{"type":33,"value":7582},"The contest description and some related NVIDIA articles about the contest helped me with brainstorming:",{"type":27,"tag":2972,"props":7584,"children":7585},{},[7586],{"type":27,"tag":36,"props":7587,"children":7588},{},[7589],{"type":33,"value":7590},"Whether it’s a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless.",{"type":27,"tag":2972,"props":7592,"children":7593},{},[7594],{"type":27,"tag":36,"props":7595,"children":7596},{},[7597],{"type":33,"value":7598},"Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.",{"type":27,"tag":36,"props":7600,"children":7601},{},[7602],{"type":33,"value":7603},"This contest is focused on NVIDIA's consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software, creative tools and new AI techniques including image generation and LLM (Large Language Model) inference.",{"type":27,"tag":36,"props":7605,"children":7606},{},[7607],{"type":27,"tag":84,"props":7608,"children":7611},{"alt":7609,"src":7610},"A stacked bar chart showing the composition of Nvidia's revenue each quarter going back to fiscal 2019.","https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&op=resize&w=700",[],{"type":27,"tag":36,"props":7613,"children":7614},{},[7615],{"type":33,"value":7616},"Gaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don't play these types of games and it didn't seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know.",{"type":27,"tag":36,"props":7618,"children":7619},{},[7620],{"type":33,"value":7621},"Rocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and all got hooked. We also came to learn that Rocket League's in-game is varies from entertaining, annoying, toxic and in some cases, sportsmanlike.",{"type":27,"tag":36,"props":7623,"children":7624},{},[7625],{"type":33,"value":7626},"One other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas \"bakkes\" Bakke, a Norwegian software engineer). Rocket League's in-game chat inspired the idea for my submission to NVIDIA's Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)",{"type":27,"tag":36,"props":7628,"children":7629},{},[7630],{"type":33,"value":7631},"Bots are built into the Rocket League game and you can play with or against them in offline matches. However, the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow and PyTorch. These bots are very good, but they are not infallible. My contest project idea was now clear: develop a plugin for Rocket League capable of sending messages from bot players. This idea seemed to check the boxes for the large language model category of NVIDIA's developer contest: develop a project in a Windows environment for a Windows-specific program, and use an LLM powered by TensorRT-LLM.",{"type":27,"tag":36,"props":7633,"children":7634},{},[7635],{"type":27,"tag":84,"props":7636,"children":7639},{"alt":7637,"src":7638},"RLBot Ascii Art","/static/rlbc/bot.png",[],{"type":27,"tag":28,"props":7641,"children":7642},{"id":6075},[7643],{"type":33,"value":6078},{"type":27,"tag":36,"props":7645,"children":7646},{},[7647],{"type":33,"value":7648},"With this idea in mind, I looked into the project's feasibility. I really had no idea if this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept.",{"type":27,"tag":574,"props":7650,"children":7651},{},[7652,7698,7717,7728],{"type":27,"tag":98,"props":7653,"children":7654},{},[7655,7657,7663],{"type":33,"value":7656},"The Bakkesmod Plugin Wiki ",{"type":27,"tag":61,"props":7658,"children":7661},{"href":7659,"rel":7660},"https://wiki.bakkesplugins.com/",[65],[7662],{"type":33,"value":7659},{"type":27,"tag":574,"props":7664,"children":7665},{},[7666,7682],{"type":27,"tag":98,"props":7667,"children":7668},{},[7669,7680],{"type":27,"tag":61,"props":7670,"children":7673},{"href":7671,"rel":7672},"https://wiki.bakkesplugins.com/code_snippets/using_http_wrapper/",[65],[7674],{"type":27,"tag":170,"props":7675,"children":7677},{"className":7676},[],[7678],{"type":33,"value":7679},"HttpWrapper",{"type":33,"value":7681}," for sending HTTP requests from Bakkesmod",{"type":27,"tag":98,"props":7683,"children":7684},{},[7685,7696],{"type":27,"tag":61,"props":7686,"children":7689},{"href":7687,"rel":7688},"https://wiki.bakkesplugins.com/functions/stat_events/",[65],[7690],{"type":27,"tag":170,"props":7691,"children":7693},{"className":7692},[],[7694],{"type":33,"value":7695},"StatEvents",{"type":33,"value":7697}," that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save).",{"type":27,"tag":98,"props":7699,"children":7700},{},[7701,7703,7709],{"type":33,"value":7702},"The Bakkesmod plugin template: ",{"type":27,"tag":61,"props":7704,"children":7707},{"href":7705,"rel":7706},"https://github.com/Martinii89/BakkesmodPluginTemplate",[65],[7708],{"type":33,"value":7705},{"type":27,"tag":574,"props":7710,"children":7711},{},[7712],{"type":27,"tag":98,"props":7713,"children":7714},{},[7715],{"type":33,"value":7716},"This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly",{"type":27,"tag":98,"props":7718,"children":7719},{},[7720,7722],{"type":33,"value":7721},"Plugin Tutorial: ",{"type":27,"tag":61,"props":7723,"children":7726},{"href":7724,"rel":7725},"https://wiki.bakkesplugins.com/plugin_tutorial/getting_started/",[65],[7727],{"type":33,"value":7724},{"type":27,"tag":98,"props":7729,"children":7730},{},[7731,7733],{"type":33,"value":7732},"Open-source chat-related Bakkesmod plugins on GitHub\n",{"type":27,"tag":574,"props":7734,"children":7735},{},[7736,7747],{"type":27,"tag":98,"props":7737,"children":7738},{},[7739,7741],{"type":33,"value":7740},"BetterChat: ",{"type":27,"tag":61,"props":7742,"children":7745},{"href":7743,"rel":7744},"https://github.com/JulienML/BetterChat",[65],[7746],{"type":33,"value":7743},{"type":27,"tag":98,"props":7748,"children":7749},{},[7750,7752],{"type":33,"value":7751},"Translate: ",{"type":27,"tag":61,"props":7753,"children":7756},{"href":7754,"rel":7755},"https://github.com/0xleft/trnslt",[65],[7757],{"type":33,"value":7754},{"type":27,"tag":36,"props":7759,"children":7760},{},[7761,7763,7769,7771,7777],{"type":33,"value":7762},"Starting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to ",{"type":27,"tag":170,"props":7764,"children":7766},{"className":7765},[],[7767],{"type":33,"value":7768},"localhost:8000/hello",{"type":33,"value":7770},". I set up a Hello World Flask app running on ",{"type":27,"tag":170,"props":7772,"children":7774},{"className":7773},[],[7775],{"type":33,"value":7776},"localhost:8000",{"type":33,"value":7778}," and I was able to get a response from my Hello World server! There didn't seem to be any network or permission errors that would prevent my game code from communicating with other applications on my PC.",{"type":27,"tag":36,"props":7780,"children":7781},{},[7782,7784,7790],{"type":33,"value":7783},"Next I started looking into how to build and run optimized LLMs with NVIDIA's TensorRT-LLM library, the software that this contest is promoting. The contest announcement included an interesting building block that I thought could be very useful: an example repo showing how to run ",{"type":27,"tag":170,"props":7785,"children":7787},{"className":7786},[],[7788],{"type":33,"value":7789},"CodeLlama-13b-instruct-hf",{"type":33,"value":7791}," optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev).",{"type":27,"tag":574,"props":7793,"children":7794},{},[7795,7805,7817,7822],{"type":27,"tag":98,"props":7796,"children":7797},{},[7798,7803],{"type":27,"tag":170,"props":7799,"children":7801},{"className":7800},[],[7802],{"type":33,"value":7789},{"type":33,"value":7804}," is an open source model from Meta that is trained on code and can help with code generation tasks",{"type":27,"tag":98,"props":7806,"children":7807},{},[7808,7810,7815],{"type":33,"value":7809},"TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like ",{"type":27,"tag":170,"props":7811,"children":7813},{"className":7812},[],[7814],{"type":33,"value":7789},{"type":33,"value":7816}," and generates an engine that can be used for doing inference",{"type":27,"tag":98,"props":7818,"children":7819},{},[7820],{"type":33,"value":7821},"VSCode is an open source code editor developed by Microsoft with an large number of community plugins",{"type":27,"tag":98,"props":7823,"children":7824},{},[7825],{"type":33,"value":7826},"Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT",{"type":27,"tag":36,"props":7828,"children":7829},{},[7830],{"type":33,"value":7831},"To get the coding assistant project working I needed to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:",{"type":27,"tag":574,"props":7833,"children":7834},{},[7835,7840],{"type":27,"tag":98,"props":7836,"children":7837},{},[7838],{"type":33,"value":7839},"using a \"bare-metal\" virtual environment on Windows (with PowerShell)",{"type":27,"tag":98,"props":7841,"children":7842},{},[7843],{"type":33,"value":7844},"using WSL",{"type":27,"tag":36,"props":7846,"children":7847},{},[7848,7850,7856,7858,7864,7866,7872],{"type":33,"value":7849},"At the time of writing, building a TensorRT-LLM engine on Windows can only be done with version ",{"type":27,"tag":170,"props":7851,"children":7853},{"className":7852},[],[7854],{"type":33,"value":7855},"v0.6.1",{"type":33,"value":7857}," of the TensorRT-LLM repo and version ",{"type":27,"tag":170,"props":7859,"children":7861},{"className":7860},[],[7862],{"type":33,"value":7863},"v0.7.1",{"type":33,"value":7865}," of the ",{"type":27,"tag":170,"props":7867,"children":7869},{"className":7868},[],[7870],{"type":33,"value":7871},"tensorrt_llm",{"type":33,"value":7873}," Python package.",{"type":27,"tag":36,"props":7875,"children":7876},{},[7877],{"type":33,"value":7878},"With WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system.",{"type":27,"tag":36,"props":7880,"children":7881},{},[7882,7884,7889],{"type":33,"value":7883},"Once the engines are built you can use them to run the example from the ",{"type":27,"tag":170,"props":7885,"children":7887},{"className":7886},[],[7888],{"type":33,"value":5751},{"type":33,"value":7890}," repo.",{"type":27,"tag":36,"props":7892,"children":7893},{},[7894],{"type":33,"value":7895},"The example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:",{"type":27,"tag":238,"props":7897,"children":7901},{"code":7898,"language":7899,"meta":8,"className":7900,"style":8},"{\n  \"title\": \"CodeLlama-13b-instruct-hf\",\n  \"apiBase\": \"http://192.168.5.96:5000/\",\n  \"provider\": \"openai\",\n  \"apiKey\": \"None\",\n  \"model\": \"gpt-4\"\n}\n","json","language-json shiki shiki-themes github-light github-dark monokai",[7902],{"type":27,"tag":170,"props":7903,"children":7904},{"__ignoreMap":8},[7905,7912,7934,7955,7976,7997,8014],{"type":27,"tag":261,"props":7906,"children":7907},{"class":263,"line":264},[7908],{"type":27,"tag":261,"props":7909,"children":7910},{"style":268},[7911],{"type":33,"value":1793},{"type":27,"tag":261,"props":7913,"children":7914},{"class":263,"line":343},[7915,7920,7924,7930],{"type":27,"tag":261,"props":7916,"children":7917},{"style":850},[7918],{"type":33,"value":7919},"  \"title\"",{"type":27,"tag":261,"props":7921,"children":7922},{"style":268},[7923],{"type":33,"value":847},{"type":27,"tag":261,"props":7925,"children":7927},{"style":7926},"--shiki-default:#032F62;--shiki-dark:#9ECBFF;--shiki-sepia:#CFCFC2",[7928],{"type":33,"value":7929},"\"CodeLlama-13b-instruct-hf\"",{"type":27,"tag":261,"props":7931,"children":7932},{"style":268},[7933],{"type":33,"value":1211},{"type":27,"tag":261,"props":7935,"children":7936},{"class":263,"line":379},[7937,7942,7946,7951],{"type":27,"tag":261,"props":7938,"children":7939},{"style":850},[7940],{"type":33,"value":7941},"  \"apiBase\"",{"type":27,"tag":261,"props":7943,"children":7944},{"style":268},[7945],{"type":33,"value":847},{"type":27,"tag":261,"props":7947,"children":7948},{"style":7926},[7949],{"type":33,"value":7950},"\"http://192.168.5.96:5000/\"",{"type":27,"tag":261,"props":7952,"children":7953},{"style":268},[7954],{"type":33,"value":1211},{"type":27,"tag":261,"props":7956,"children":7957},{"class":263,"line":537},[7958,7963,7967,7972],{"type":27,"tag":261,"props":7959,"children":7960},{"style":850},[7961],{"type":33,"value":7962},"  \"provider\"",{"type":27,"tag":261,"props":7964,"children":7965},{"style":268},[7966],{"type":33,"value":847},{"type":27,"tag":261,"props":7968,"children":7969},{"style":7926},[7970],{"type":33,"value":7971},"\"openai\"",{"type":27,"tag":261,"props":7973,"children":7974},{"style":268},[7975],{"type":33,"value":1211},{"type":27,"tag":261,"props":7977,"children":7978},{"class":263,"line":561},[7979,7984,7988,7993],{"type":27,"tag":261,"props":7980,"children":7981},{"style":850},[7982],{"type":33,"value":7983},"  \"apiKey\"",{"type":27,"tag":261,"props":7985,"children":7986},{"style":268},[7987],{"type":33,"value":847},{"type":27,"tag":261,"props":7989,"children":7990},{"style":7926},[7991],{"type":33,"value":7992},"\"None\"",{"type":27,"tag":261,"props":7994,"children":7995},{"style":268},[7996],{"type":33,"value":1211},{"type":27,"tag":261,"props":7998,"children":7999},{"class":263,"line":785},[8000,8005,8009],{"type":27,"tag":261,"props":8001,"children":8002},{"style":850},[8003],{"type":33,"value":8004},"  \"model\"",{"type":27,"tag":261,"props":8006,"children":8007},{"style":268},[8008],{"type":33,"value":847},{"type":27,"tag":261,"props":8010,"children":8011},{"style":7926},[8012],{"type":33,"value":8013},"\"gpt-4\"\n",{"type":27,"tag":261,"props":8015,"children":8016},{"class":263,"line":794},[8017],{"type":27,"tag":261,"props":8018,"children":8019},{"style":268},[8020],{"type":33,"value":8021},"}\n",{"type":27,"tag":36,"props":8023,"children":8024},{},[8025,8027,8032,8034,8041],{"type":33,"value":8026},"The Continue.dev extension using ",{"type":27,"tag":170,"props":8028,"children":8030},{"className":8029},[],[8031],{"type":33,"value":7789},{"type":33,"value":8033}," accelerated and optimized by TensorRT-LLM is very fast. According to ",{"type":27,"tag":61,"props":8035,"children":8038},{"href":8036,"rel":8037},"https://blog.continue.dev/programming-languages/",[65],[8039],{"type":33,"value":8040},"this post on Continue.dev's blog",{"type":33,"value":8042},", C++ is a \"first tier\" language:",{"type":27,"tag":2972,"props":8044,"children":8045},{},[8046],{"type":27,"tag":36,"props":8047,"children":8048},{},[8049],{"type":33,"value":8050},"C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode / TP3, MBXP / Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don’t believe that LLMs are as helpful for C++ as some of the other languages in this tier.",{"type":27,"tag":36,"props":8052,"children":8053},{},[8054,8056,8062,8064,8070],{"type":33,"value":8055},"Most of the time I'm working with either Python and TypeScript. I've read about C++ but haven't used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with the Continue.dev autopilot extension was helpful for tackling smaller problems in a REPL-like environment. For example, I used Continue.dev in VSCode to figure out how to handle JSON. Coming from Python and JavaScript languages, I found the ",{"type":27,"tag":170,"props":8057,"children":8059},{"className":8058},[],[8060],{"type":33,"value":8061},"nlohmann/json",{"type":33,"value":8063}," JSON library syntax to be somewhat different. For example, here is how to add a message to ",{"type":27,"tag":170,"props":8065,"children":8067},{"className":8066},[],[8068],{"type":33,"value":8069},"messages",{"type":33,"value":8071}," in the body of an OpenAI API request:",{"type":27,"tag":238,"props":8073,"children":8077},{"code":8074,"language":8075,"meta":8,"className":8076,"style":8},"messages.push_back({ {\"role\", role}, {\"content\", content } });\n","cpp","language-cpp shiki shiki-themes github-light github-dark monokai",[8078],{"type":27,"tag":170,"props":8079,"children":8080},{"__ignoreMap":8},[8081],{"type":27,"tag":261,"props":8082,"children":8083},{"class":263,"line":264},[8084,8089,8094,8099,8104,8109,8113],{"type":27,"tag":261,"props":8085,"children":8086},{"style":268},[8087],{"type":33,"value":8088},"messages.",{"type":27,"tag":261,"props":8090,"children":8091},{"style":820},[8092],{"type":33,"value":8093},"push_back",{"type":27,"tag":261,"props":8095,"children":8096},{"style":268},[8097],{"type":33,"value":8098},"({ {",{"type":27,"tag":261,"props":8100,"children":8101},{"style":493},[8102],{"type":33,"value":8103},"\"role\"",{"type":27,"tag":261,"props":8105,"children":8106},{"style":268},[8107],{"type":33,"value":8108},", role}, {",{"type":27,"tag":261,"props":8110,"children":8111},{"style":493},[8112],{"type":33,"value":981},{"type":27,"tag":261,"props":8114,"children":8115},{"style":268},[8116],{"type":33,"value":8117},", content } });\n",{"type":27,"tag":36,"props":8119,"children":8120},{},[8121],{"type":33,"value":8122},"In Python the code for appending a message to a list of messages would be written differently:",{"type":27,"tag":238,"props":8124,"children":8126},{"code":8125,"language":254,"meta":8,"className":255,"style":8},"messages.append({\"role\": role, \"content\": content})\n",[8127],{"type":27,"tag":170,"props":8128,"children":8129},{"__ignoreMap":8},[8130],{"type":27,"tag":261,"props":8131,"children":8132},{"class":263,"line":264},[8133,8138,8142,8147,8151],{"type":27,"tag":261,"props":8134,"children":8135},{"style":268},[8136],{"type":33,"value":8137},"messages.append({",{"type":27,"tag":261,"props":8139,"children":8140},{"style":493},[8141],{"type":33,"value":8103},{"type":27,"tag":261,"props":8143,"children":8144},{"style":268},[8145],{"type":33,"value":8146},": role, ",{"type":27,"tag":261,"props":8148,"children":8149},{"style":493},[8150],{"type":33,"value":981},{"type":27,"tag":261,"props":8152,"children":8153},{"style":268},[8154],{"type":33,"value":8155},": content})\n",{"type":27,"tag":28,"props":8157,"children":8158},{"id":7236},[8159],{"type":33,"value":7239},{"type":27,"tag":36,"props":8161,"children":8162},{},[8163],{"type":33,"value":8164},"While working on different projects using web technologies and frameworks in the Python and JavaScript ecosystems, I developed an appreciation for well-structured development environments that are easy to use. Development environment refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running on a local environment. The local environment (the developer's computer) should be a close proxy for the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality, offering a tight feedback loop. I really like the development environment for this project. Here's a screenshot that shows the different parts of the development environment I used for working on Rocket League BotChat:",{"type":27,"tag":36,"props":8166,"children":8167},{},[8168],{"type":27,"tag":84,"props":8169,"children":8172},{"alt":8170,"src":8171},"Screenshot of Rocket League BotChat development environment","/static/rlbc/devenv2.png",[],{"type":27,"tag":574,"props":8174,"children":8175},{},[8176,8189,8217,8230],{"type":27,"tag":98,"props":8177,"children":8178},{},[8179,8181,8187],{"type":33,"value":8180},"Rocket League (running with the ",{"type":27,"tag":170,"props":8182,"children":8184},{"className":8183},[],[8185],{"type":33,"value":8186},"-dev",{"type":33,"value":8188}," flag turned on). The console is helpful for viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine",{"type":27,"tag":98,"props":8190,"children":8191},{},[8192,8194,8200,8202,8208,8209,8215],{"type":33,"value":8193},"Visual Studio for working on the plugin code. ",{"type":27,"tag":170,"props":8195,"children":8197},{"className":8196},[],[8198],{"type":33,"value":8199},"Control",{"type":33,"value":8201},"+",{"type":27,"tag":170,"props":8203,"children":8205},{"className":8204},[],[8206],{"type":33,"value":8207},"Shift",{"type":33,"value":8201},{"type":27,"tag":170,"props":8210,"children":8212},{"className":8211},[],[8213],{"type":33,"value":8214},"B",{"type":33,"value":8216}," rebuilds the code and automatically reloads the plugin in the game",{"type":27,"tag":98,"props":8218,"children":8219},{},[8220,8222,8228],{"type":33,"value":8221},"OpenAI-compatible LLM server powered by TensorRT-LLM (using ",{"type":27,"tag":170,"props":8223,"children":8225},{"className":8224},[],[8226],{"type":33,"value":8227},"Llama-2-13b-chat-hf",{"type":33,"value":8229}," with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL",{"type":27,"tag":98,"props":8231,"children":8232},{},[8233,8235,8240],{"type":33,"value":8234},"VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using ",{"type":27,"tag":170,"props":8236,"children":8238},{"className":8237},[],[8239],{"type":33,"value":7789},{"type":33,"value":8241}," with AWQ INT4 quantization) running in a virtual environment on Windows",{"type":27,"tag":47,"props":8243,"children":8245},{"id":8244},"building-the-tensorrt-llm-engines",[8246],{"type":33,"value":8247},"Building the TensorRT-LLM engines",{"type":27,"tag":36,"props":8249,"children":8250},{},[8251,8253,8258],{"type":33,"value":8252},"I was able to build and run the TensorRT LLM engines for my game plugin's inference and the Continue.dev extension's inference both in Python virtual environments on Windows and on Ubuntu in WSL. For building the ",{"type":27,"tag":170,"props":8254,"children":8256},{"className":8255},[],[8257],{"type":33,"value":8227},{"type":33,"value":8259}," model with INT4 AWQ quantization on Windows 11 I used this command:",{"type":27,"tag":238,"props":8261,"children":8265},{"code":8262,"language":8263,"meta":8,"className":8264,"style":8},"(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT-LLM\\examples\\llama> python build.py --model_dir D:\\llama\\Llama-2-13b-chat-hf\\ --quant_ckpt_path D:\\llama\\Llama-2-13b-chat-hf\\llama_tp1_rank0.npz --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16 --use_weight_only --weight_only_precision int4_awq --per_group --enable_context_fmha --max_batch_size 1 --max_input_len 3500 --max_output_len 1024 --output_dir D:\\llama\\Llama-2-13b-chat-hf\\single-gpu\\ --vocab_size 32064\n","powershell","language-powershell shiki shiki-themes github-light github-dark monokai",[8266],{"type":27,"tag":170,"props":8267,"children":8268},{"__ignoreMap":8},[8269],{"type":27,"tag":261,"props":8270,"children":8271},{"class":263,"line":264},[8272,8277,8282,8287,8291,8296,8301,8306,8310,8315,8319,8324,8328,8333,8337,8342,8346,8351,8355,8359,8363,8367,8371,8375,8379,8384,8388,8393,8397,8402,8406,8411,8415,8420,8424,8429,8433,8438,8442,8447,8451,8456,8460,8465,8470,8475,8479,8484,8489,8493,8498,8502,8506,8510,8514,8518,8522,8526,8531,8535,8540,8544,8549],{"type":27,"tag":261,"props":8273,"children":8274},{"style":268},[8275],{"type":33,"value":8276},"(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT",{"type":27,"tag":261,"props":8278,"children":8279},{"style":274},[8280],{"type":33,"value":8281},"-",{"type":27,"tag":261,"props":8283,"children":8284},{"style":268},[8285],{"type":33,"value":8286},"LLM\\examples\\llama",{"type":27,"tag":261,"props":8288,"children":8289},{"style":274},[8290],{"type":33,"value":2490},{"type":27,"tag":261,"props":8292,"children":8293},{"style":268},[8294],{"type":33,"value":8295}," python build.py ",{"type":27,"tag":261,"props":8297,"children":8298},{"style":274},[8299],{"type":33,"value":8300},"--",{"type":27,"tag":261,"props":8302,"children":8303},{"style":268},[8304],{"type":33,"value":8305},"model_dir D:\\llama\\Llama",{"type":27,"tag":261,"props":8307,"children":8308},{"style":274},[8309],{"type":33,"value":8281},{"type":27,"tag":261,"props":8311,"children":8312},{"style":295},[8313],{"type":33,"value":8314},"2",{"type":27,"tag":261,"props":8316,"children":8317},{"style":274},[8318],{"type":33,"value":8281},{"type":27,"tag":261,"props":8320,"children":8321},{"style":268},[8322],{"type":33,"value":8323},"13b",{"type":27,"tag":261,"props":8325,"children":8326},{"style":274},[8327],{"type":33,"value":8281},{"type":27,"tag":261,"props":8329,"children":8330},{"style":268},[8331],{"type":33,"value":8332},"chat",{"type":27,"tag":261,"props":8334,"children":8335},{"style":274},[8336],{"type":33,"value":8281},{"type":27,"tag":261,"props":8338,"children":8339},{"style":268},[8340],{"type":33,"value":8341},"hf\\ ",{"type":27,"tag":261,"props":8343,"children":8344},{"style":274},[8345],{"type":33,"value":8300},{"type":27,"tag":261,"props":8347,"children":8348},{"style":268},[8349],{"type":33,"value":8350},"quant_ckpt_path D:\\llama\\Llama",{"type":27,"tag":261,"props":8352,"children":8353},{"style":274},[8354],{"type":33,"value":8281},{"type":27,"tag":261,"props":8356,"children":8357},{"style":295},[8358],{"type":33,"value":8314},{"type":27,"tag":261,"props":8360,"children":8361},{"style":274},[8362],{"type":33,"value":8281},{"type":27,"tag":261,"props":8364,"children":8365},{"style":268},[8366],{"type":33,"value":8323},{"type":27,"tag":261,"props":8368,"children":8369},{"style":274},[8370],{"type":33,"value":8281},{"type":27,"tag":261,"props":8372,"children":8373},{"style":268},[8374],{"type":33,"value":8332},{"type":27,"tag":261,"props":8376,"children":8377},{"style":274},[8378],{"type":33,"value":8281},{"type":27,"tag":261,"props":8380,"children":8381},{"style":268},[8382],{"type":33,"value":8383},"hf\\llama_tp1_rank0.npz ",{"type":27,"tag":261,"props":8385,"children":8386},{"style":274},[8387],{"type":33,"value":8300},{"type":27,"tag":261,"props":8389,"children":8390},{"style":268},[8391],{"type":33,"value":8392},"dtype float16 ",{"type":27,"tag":261,"props":8394,"children":8395},{"style":274},[8396],{"type":33,"value":8300},{"type":27,"tag":261,"props":8398,"children":8399},{"style":268},[8400],{"type":33,"value":8401},"use_gpt_attention_plugin float16 ",{"type":27,"tag":261,"props":8403,"children":8404},{"style":274},[8405],{"type":33,"value":8300},{"type":27,"tag":261,"props":8407,"children":8408},{"style":268},[8409],{"type":33,"value":8410},"use_gemm_plugin float16 ",{"type":27,"tag":261,"props":8412,"children":8413},{"style":274},[8414],{"type":33,"value":8300},{"type":27,"tag":261,"props":8416,"children":8417},{"style":268},[8418],{"type":33,"value":8419},"use_weight_only ",{"type":27,"tag":261,"props":8421,"children":8422},{"style":274},[8423],{"type":33,"value":8300},{"type":27,"tag":261,"props":8425,"children":8426},{"style":268},[8427],{"type":33,"value":8428},"weight_only_precision int4_awq ",{"type":27,"tag":261,"props":8430,"children":8431},{"style":274},[8432],{"type":33,"value":8300},{"type":27,"tag":261,"props":8434,"children":8435},{"style":268},[8436],{"type":33,"value":8437},"per_group ",{"type":27,"tag":261,"props":8439,"children":8440},{"style":274},[8441],{"type":33,"value":8300},{"type":27,"tag":261,"props":8443,"children":8444},{"style":268},[8445],{"type":33,"value":8446},"enable_context_fmha ",{"type":27,"tag":261,"props":8448,"children":8449},{"style":274},[8450],{"type":33,"value":8300},{"type":27,"tag":261,"props":8452,"children":8453},{"style":268},[8454],{"type":33,"value":8455},"max_batch_size ",{"type":27,"tag":261,"props":8457,"children":8458},{"style":295},[8459],{"type":33,"value":1532},{"type":27,"tag":261,"props":8461,"children":8462},{"style":274},[8463],{"type":33,"value":8464}," --",{"type":27,"tag":261,"props":8466,"children":8467},{"style":268},[8468],{"type":33,"value":8469},"max_input_len ",{"type":27,"tag":261,"props":8471,"children":8472},{"style":295},[8473],{"type":33,"value":8474},"3500",{"type":27,"tag":261,"props":8476,"children":8477},{"style":274},[8478],{"type":33,"value":8464},{"type":27,"tag":261,"props":8480,"children":8481},{"style":268},[8482],{"type":33,"value":8483},"max_output_len ",{"type":27,"tag":261,"props":8485,"children":8486},{"style":295},[8487],{"type":33,"value":8488},"1024",{"type":27,"tag":261,"props":8490,"children":8491},{"style":274},[8492],{"type":33,"value":8464},{"type":27,"tag":261,"props":8494,"children":8495},{"style":268},[8496],{"type":33,"value":8497},"output_dir D:\\llama\\Llama",{"type":27,"tag":261,"props":8499,"children":8500},{"style":274},[8501],{"type":33,"value":8281},{"type":27,"tag":261,"props":8503,"children":8504},{"style":295},[8505],{"type":33,"value":8314},{"type":27,"tag":261,"props":8507,"children":8508},{"style":274},[8509],{"type":33,"value":8281},{"type":27,"tag":261,"props":8511,"children":8512},{"style":268},[8513],{"type":33,"value":8323},{"type":27,"tag":261,"props":8515,"children":8516},{"style":274},[8517],{"type":33,"value":8281},{"type":27,"tag":261,"props":8519,"children":8520},{"style":268},[8521],{"type":33,"value":8332},{"type":27,"tag":261,"props":8523,"children":8524},{"style":274},[8525],{"type":33,"value":8281},{"type":27,"tag":261,"props":8527,"children":8528},{"style":268},[8529],{"type":33,"value":8530},"hf\\single",{"type":27,"tag":261,"props":8532,"children":8533},{"style":274},[8534],{"type":33,"value":8281},{"type":27,"tag":261,"props":8536,"children":8537},{"style":268},[8538],{"type":33,"value":8539},"gpu\\ ",{"type":27,"tag":261,"props":8541,"children":8542},{"style":274},[8543],{"type":33,"value":8300},{"type":27,"tag":261,"props":8545,"children":8546},{"style":268},[8547],{"type":33,"value":8548},"vocab_size ",{"type":27,"tag":261,"props":8550,"children":8551},{"style":295},[8552],{"type":33,"value":8553},"32064\n",{"type":27,"tag":47,"props":8555,"children":8557},{"id":8556},"running-the-tensorrt-llm-engines",[8558],{"type":33,"value":8559},"Running the TensorRT-LLM engines",{"type":27,"tag":36,"props":8561,"children":8562},{},[8563],{"type":33,"value":8564},"Using Windows PowerShell to start the CodeLlama server for Continue.dev:",{"type":27,"tag":238,"props":8566,"children":8568},{"code":8567,"language":8263,"meta":8,"className":8264,"style":8},"(.venv) PS C:\\Users\\My PC\\GitHub\\trt-llm-as-openai-windows> python .\\app.py --trt_engine_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\" --trt_engine_name llama_float16_tp1_rank0.engine --tokenizer_dir_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\\" --port 5000 --host 0.0.0.0\n",[8569],{"type":27,"tag":170,"props":8570,"children":8571},{"__ignoreMap":8},[8572],{"type":27,"tag":261,"props":8573,"children":8574},{"class":263,"line":264},[8575,8580,8584,8588,8592,8596,8600,8605,8609,8613,8617,8622,8626,8631,8636,8640,8645,8649,8654,8659,8663,8668,8673,8677,8682,8687,8691],{"type":27,"tag":261,"props":8576,"children":8577},{"style":268},[8578],{"type":33,"value":8579},"(.venv) PS C:\\Users\\My PC\\GitHub\\trt",{"type":27,"tag":261,"props":8581,"children":8582},{"style":274},[8583],{"type":33,"value":8281},{"type":27,"tag":261,"props":8585,"children":8586},{"style":268},[8587],{"type":33,"value":17},{"type":27,"tag":261,"props":8589,"children":8590},{"style":274},[8591],{"type":33,"value":8281},{"type":27,"tag":261,"props":8593,"children":8594},{"style":268},[8595],{"type":33,"value":1611},{"type":27,"tag":261,"props":8597,"children":8598},{"style":274},[8599],{"type":33,"value":8281},{"type":27,"tag":261,"props":8601,"children":8602},{"style":268},[8603],{"type":33,"value":8604},"openai",{"type":27,"tag":261,"props":8606,"children":8607},{"style":274},[8608],{"type":33,"value":8281},{"type":27,"tag":261,"props":8610,"children":8611},{"style":268},[8612],{"type":33,"value":7485},{"type":27,"tag":261,"props":8614,"children":8615},{"style":274},[8616],{"type":33,"value":2490},{"type":27,"tag":261,"props":8618,"children":8619},{"style":268},[8620],{"type":33,"value":8621}," python .\\app.py ",{"type":27,"tag":261,"props":8623,"children":8624},{"style":274},[8625],{"type":33,"value":8300},{"type":27,"tag":261,"props":8627,"children":8628},{"style":268},[8629],{"type":33,"value":8630},"trt_engine_path ",{"type":27,"tag":261,"props":8632,"children":8633},{"style":493},[8634],{"type":33,"value":8635},"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\"",{"type":27,"tag":261,"props":8637,"children":8638},{"style":274},[8639],{"type":33,"value":8464},{"type":27,"tag":261,"props":8641,"children":8642},{"style":268},[8643],{"type":33,"value":8644},"trt_engine_name llama_float16_tp1_rank0.engine ",{"type":27,"tag":261,"props":8646,"children":8647},{"style":274},[8648],{"type":33,"value":8300},{"type":27,"tag":261,"props":8650,"children":8651},{"style":268},[8652],{"type":33,"value":8653},"tokenizer_dir_path ",{"type":27,"tag":261,"props":8655,"children":8656},{"style":493},[8657],{"type":33,"value":8658},"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\\"",{"type":27,"tag":261,"props":8660,"children":8661},{"style":274},[8662],{"type":33,"value":8464},{"type":27,"tag":261,"props":8664,"children":8665},{"style":268},[8666],{"type":33,"value":8667},"port ",{"type":27,"tag":261,"props":8669,"children":8670},{"style":295},[8671],{"type":33,"value":8672},"5000",{"type":27,"tag":261,"props":8674,"children":8675},{"style":274},[8676],{"type":33,"value":8464},{"type":27,"tag":261,"props":8678,"children":8679},{"style":268},[8680],{"type":33,"value":8681},"host ",{"type":27,"tag":261,"props":8683,"children":8684},{"style":295},[8685],{"type":33,"value":8686},"0.0",{"type":27,"tag":261,"props":8688,"children":8689},{"style":268},[8690],{"type":33,"value":2970},{"type":27,"tag":261,"props":8692,"children":8693},{"style":295},[8694],{"type":33,"value":8695},"0.0\n",{"type":27,"tag":36,"props":8697,"children":8698},{},[8699,8701,8707],{"type":33,"value":8700},"Tip: Adding ",{"type":27,"tag":170,"props":8702,"children":8704},{"className":8703},[],[8705],{"type":33,"value":8706},"--host 0.0.0.0",{"type":33,"value":8708}," isn't required here, but it allows me to use the CodeLlama/TensorRT-LLM server with VSCode any computer on my local network using my PC's local IP address in the Continue.dev configuration.",{"type":27,"tag":36,"props":8710,"children":8711},{},[8712],{"type":33,"value":8713},"Using docker in WSL to start the Llama-2-13b-chat-hf LLM server:",{"type":27,"tag":238,"props":8715,"children":8717},{"code":8716},"root@0a5b5b75f079:/code/git/TensorRT-LLM/examples/server/flask# python3 app.py --trt_engine_path /llama/Llama-2-13b-chat-hf/trt_engines/1-gpu/ --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path /llama/Llama-2-13b-chat-hf/ --port 5001 --host 0.0.0.0\n",[8718],{"type":27,"tag":170,"props":8719,"children":8720},{"__ignoreMap":8},[8721],{"type":33,"value":8716},{"type":27,"tag":36,"props":8723,"children":8724},{},[8725,8727,8732],{"type":33,"value":8726},"Note: Here I also add ",{"type":27,"tag":170,"props":8728,"children":8730},{"className":8729},[],[8731],{"type":33,"value":8706},{"type":33,"value":8733},", but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows.",{"type":27,"tag":36,"props":8735,"children":8736},{},[8737],{"type":33,"value":8738},"BakkesMod includes a console window that came in handy for debugging errors during development.",{"type":27,"tag":36,"props":8740,"children":8741},{},[8742],{"type":33,"value":8743},"At the beginning of this developer contest on January 9, NVIDIA announced Chat with RTX. This is a demo program for Windows that automates a lots of the processes needed to set up a TensorRT-LLM-powered LLM running on your PC. Keep an eye on this project as it may become the best way to install and manage large language models on Windows PCs.",{"type":27,"tag":36,"props":8745,"children":8746},{},[8747],{"type":27,"tag":84,"props":8748,"children":8751},{"alt":8749,"src":8750},"Chat with RTX image","/static/rlbc/chat_with_rtx.jpeg",[],{"type":27,"tag":28,"props":8753,"children":8754},{"id":7318},[8755],{"type":33,"value":7321},{"type":27,"tag":36,"props":8757,"children":8758},{},[8759,8761,8766],{"type":33,"value":8760},"Here's a quick look at key parts of the plugin source code (",{"type":27,"tag":61,"props":8762,"children":8764},{"href":7520,"rel":8763},[65],[8765],{"type":33,"value":7520},{"type":33,"value":8767},").",{"type":27,"tag":47,"props":8769,"children":8771},{"id":8770},"hooking-events",[8772],{"type":33,"value":8773},"Hooking events",{"type":27,"tag":36,"props":8775,"children":8776},{},[8777,8779,8785],{"type":33,"value":8778},"Hooking events is the core of how this plugin works. ",{"type":27,"tag":170,"props":8780,"children":8782},{"className":8781},[],[8783],{"type":33,"value":8784},"StatTickerMessage",{"type":33,"value":8786}," events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car.",{"type":27,"tag":238,"props":8788,"children":8790},{"code":8789,"language":8075,"meta":8,"className":8076,"style":8},"    // Hooks different types of events that are handled in onStatTickerMessage\n    // See https://wiki.bakkesplugins.com/functions/stat_events/\n    gameWrapper->HookEventWithCallerPost\u003CServerWrapper>(\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\",\n        [this](ServerWrapper caller, void* params, std::string eventname) {\n            onStatTickerMessage(params);\n        });\n",[8791],{"type":27,"tag":170,"props":8792,"children":8793},{"__ignoreMap":8},[8794,8802,8810,8845,8920,8933],{"type":27,"tag":261,"props":8795,"children":8796},{"class":263,"line":264},[8797],{"type":27,"tag":261,"props":8798,"children":8799},{"style":478},[8800],{"type":33,"value":8801},"    // Hooks different types of events that are handled in onStatTickerMessage\n",{"type":27,"tag":261,"props":8803,"children":8804},{"class":263,"line":343},[8805],{"type":27,"tag":261,"props":8806,"children":8807},{"style":478},[8808],{"type":33,"value":8809},"    // See https://wiki.bakkesplugins.com/functions/stat_events/\n",{"type":27,"tag":261,"props":8811,"children":8812},{"class":263,"line":379},[8813,8818,8823,8828,8832,8836,8841],{"type":27,"tag":261,"props":8814,"children":8815},{"style":268},[8816],{"type":33,"value":8817},"    gameWrapper->HookEventWithCallerPost",{"type":27,"tag":261,"props":8819,"children":8820},{"style":274},[8821],{"type":33,"value":8822},"\u003C",{"type":27,"tag":261,"props":8824,"children":8825},{"style":268},[8826],{"type":33,"value":8827},"ServerWrapper",{"type":27,"tag":261,"props":8829,"children":8830},{"style":274},[8831],{"type":33,"value":2490},{"type":27,"tag":261,"props":8833,"children":8834},{"style":268},[8835],{"type":33,"value":740},{"type":27,"tag":261,"props":8837,"children":8838},{"style":493},[8839],{"type":33,"value":8840},"\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\"",{"type":27,"tag":261,"props":8842,"children":8843},{"style":268},[8844],{"type":33,"value":1211},{"type":27,"tag":261,"props":8846,"children":8847},{"class":263,"line":537},[8848,8853,8858,8863,8867,8872,8876,8881,8886,8891,8895,8900,8905,8910,8915],{"type":27,"tag":261,"props":8849,"children":8850},{"style":268},[8851],{"type":33,"value":8852},"        [",{"type":27,"tag":261,"props":8854,"children":8855},{"style":873},[8856],{"type":33,"value":8857},"this",{"type":27,"tag":261,"props":8859,"children":8860},{"style":268},[8861],{"type":33,"value":8862},"](",{"type":27,"tag":261,"props":8864,"children":8865},{"style":732},[8866],{"type":33,"value":8827},{"type":27,"tag":261,"props":8868,"children":8869},{"style":285},[8870],{"type":33,"value":8871}," caller",{"type":27,"tag":261,"props":8873,"children":8874},{"style":268},[8875],{"type":33,"value":303},{"type":27,"tag":261,"props":8877,"children":8878},{"style":487},[8879],{"type":33,"value":8880},"void",{"type":27,"tag":261,"props":8882,"children":8883},{"style":274},[8884],{"type":33,"value":8885},"*",{"type":27,"tag":261,"props":8887,"children":8888},{"style":285},[8889],{"type":33,"value":8890}," params",{"type":27,"tag":261,"props":8892,"children":8893},{"style":268},[8894],{"type":33,"value":303},{"type":27,"tag":261,"props":8896,"children":8897},{"style":732},[8898],{"type":33,"value":8899},"std",{"type":27,"tag":261,"props":8901,"children":8902},{"style":268},[8903],{"type":33,"value":8904},"::",{"type":27,"tag":261,"props":8906,"children":8907},{"style":732},[8908],{"type":33,"value":8909},"string",{"type":27,"tag":261,"props":8911,"children":8912},{"style":285},[8913],{"type":33,"value":8914}," eventname",{"type":27,"tag":261,"props":8916,"children":8917},{"style":268},[8918],{"type":33,"value":8919},") {\n",{"type":27,"tag":261,"props":8921,"children":8922},{"class":263,"line":561},[8923,8928],{"type":27,"tag":261,"props":8924,"children":8925},{"style":820},[8926],{"type":33,"value":8927},"            onStatTickerMessage",{"type":27,"tag":261,"props":8929,"children":8930},{"style":268},[8931],{"type":33,"value":8932},"(params);\n",{"type":27,"tag":261,"props":8934,"children":8935},{"class":263,"line":785},[8936],{"type":27,"tag":261,"props":8937,"children":8938},{"style":268},[8939],{"type":33,"value":8940},"        });\n",{"type":27,"tag":47,"props":8942,"children":8944},{"id":8943},"handling-events-and-building-the-prompt",[8945],{"type":33,"value":8946},"Handling events and building the prompt",{"type":27,"tag":36,"props":8948,"children":8949},{},[8950,8952,8958],{"type":33,"value":8951},"We can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the ",{"type":27,"tag":170,"props":8953,"children":8955},{"className":8954},[],[8956],{"type":33,"value":8957},"appendToPrompt",{"type":33,"value":8959}," method.",{"type":27,"tag":238,"props":8961,"children":8963},{"code":8962,"language":8075,"meta":8,"className":8076,"style":8},"    // handle different events like scoring a goal or making a save\n    if (statEvent.GetEventName() == \"Goal\") {\n\n        // was the goal scored by the human player or the bot?\n        if (playerPRI.memory_address == receiver.memory_address) {\n            appendToPrompt(\"Your human opponent just scored a goal against you! \" + score_sentence, \"user\");\n        }\n        else {\n            appendToPrompt(\"You just scored a goal against the human player! \" + score_sentence, \"user\");\n        }\n    }\n",[8964],{"type":27,"tag":170,"props":8965,"children":8966},{"__ignoreMap":8},[8967,8975,9011,9018,9026,9048,9084,9092,9105,9137,9144],{"type":27,"tag":261,"props":8968,"children":8969},{"class":263,"line":264},[8970],{"type":27,"tag":261,"props":8971,"children":8972},{"style":478},[8973],{"type":33,"value":8974},"    // handle different events like scoring a goal or making a save\n",{"type":27,"tag":261,"props":8976,"children":8977},{"class":263,"line":343},[8978,8983,8988,8993,8998,9002,9007],{"type":27,"tag":261,"props":8979,"children":8980},{"style":274},[8981],{"type":33,"value":8982},"    if",{"type":27,"tag":261,"props":8984,"children":8985},{"style":268},[8986],{"type":33,"value":8987}," (statEvent.",{"type":27,"tag":261,"props":8989,"children":8990},{"style":820},[8991],{"type":33,"value":8992},"GetEventName",{"type":27,"tag":261,"props":8994,"children":8995},{"style":268},[8996],{"type":33,"value":8997},"() ",{"type":27,"tag":261,"props":8999,"children":9000},{"style":274},[9001],{"type":33,"value":2005},{"type":27,"tag":261,"props":9003,"children":9004},{"style":493},[9005],{"type":33,"value":9006}," \"Goal\"",{"type":27,"tag":261,"props":9008,"children":9009},{"style":268},[9010],{"type":33,"value":8919},{"type":27,"tag":261,"props":9012,"children":9013},{"class":263,"line":379},[9014],{"type":27,"tag":261,"props":9015,"children":9016},{"emptyLinePlaceholder":22},[9017],{"type":33,"value":766},{"type":27,"tag":261,"props":9019,"children":9020},{"class":263,"line":537},[9021],{"type":27,"tag":261,"props":9022,"children":9023},{"style":478},[9024],{"type":33,"value":9025},"        // was the goal scored by the human player or the bot?\n",{"type":27,"tag":261,"props":9027,"children":9028},{"class":263,"line":561},[9029,9034,9039,9043],{"type":27,"tag":261,"props":9030,"children":9031},{"style":274},[9032],{"type":33,"value":9033},"        if",{"type":27,"tag":261,"props":9035,"children":9036},{"style":268},[9037],{"type":33,"value":9038}," (playerPRI.memory_address ",{"type":27,"tag":261,"props":9040,"children":9041},{"style":274},[9042],{"type":33,"value":2005},{"type":27,"tag":261,"props":9044,"children":9045},{"style":268},[9046],{"type":33,"value":9047}," receiver.memory_address) {\n",{"type":27,"tag":261,"props":9049,"children":9050},{"class":263,"line":785},[9051,9056,9060,9065,9070,9075,9079],{"type":27,"tag":261,"props":9052,"children":9053},{"style":820},[9054],{"type":33,"value":9055},"            appendToPrompt",{"type":27,"tag":261,"props":9057,"children":9058},{"style":268},[9059],{"type":33,"value":740},{"type":27,"tag":261,"props":9061,"children":9062},{"style":493},[9063],{"type":33,"value":9064},"\"Your human opponent just scored a goal against you! \"",{"type":27,"tag":261,"props":9066,"children":9067},{"style":274},[9068],{"type":33,"value":9069}," +",{"type":27,"tag":261,"props":9071,"children":9072},{"style":268},[9073],{"type":33,"value":9074}," score_sentence, ",{"type":27,"tag":261,"props":9076,"children":9077},{"style":493},[9078],{"type":33,"value":1206},{"type":27,"tag":261,"props":9080,"children":9081},{"style":268},[9082],{"type":33,"value":9083},");\n",{"type":27,"tag":261,"props":9085,"children":9086},{"class":263,"line":794},[9087],{"type":27,"tag":261,"props":9088,"children":9089},{"style":268},[9090],{"type":33,"value":9091},"        }\n",{"type":27,"tag":261,"props":9093,"children":9094},{"class":263,"line":803},[9095,9100],{"type":27,"tag":261,"props":9096,"children":9097},{"style":274},[9098],{"type":33,"value":9099},"        else",{"type":27,"tag":261,"props":9101,"children":9102},{"style":268},[9103],{"type":33,"value":9104}," {\n",{"type":27,"tag":261,"props":9106,"children":9107},{"class":263,"line":811},[9108,9112,9116,9121,9125,9129,9133],{"type":27,"tag":261,"props":9109,"children":9110},{"style":820},[9111],{"type":33,"value":9055},{"type":27,"tag":261,"props":9113,"children":9114},{"style":268},[9115],{"type":33,"value":740},{"type":27,"tag":261,"props":9117,"children":9118},{"style":493},[9119],{"type":33,"value":9120},"\"You just scored a goal against the human player! \"",{"type":27,"tag":261,"props":9122,"children":9123},{"style":274},[9124],{"type":33,"value":9069},{"type":27,"tag":261,"props":9126,"children":9127},{"style":268},[9128],{"type":33,"value":9074},{"type":27,"tag":261,"props":9130,"children":9131},{"style":493},[9132],{"type":33,"value":1206},{"type":27,"tag":261,"props":9134,"children":9135},{"style":268},[9136],{"type":33,"value":9083},{"type":27,"tag":261,"props":9138,"children":9139},{"class":263,"line":860},[9140],{"type":27,"tag":261,"props":9141,"children":9142},{"style":268},[9143],{"type":33,"value":9091},{"type":27,"tag":261,"props":9145,"children":9146},{"class":263,"line":884},[9147],{"type":27,"tag":261,"props":9148,"children":9149},{"style":268},[9150],{"type":33,"value":9151},"    }\n",{"type":27,"tag":47,"props":9153,"children":9155},{"id":9154},"making-requests-and-handling-responses",[9156],{"type":33,"value":9157},"Making requests and handling responses",{"type":27,"tag":36,"props":9159,"children":9160},{},[9161],{"type":33,"value":9162},"The last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI's API.",{"type":27,"tag":238,"props":9164,"children":9166},{"code":9165,"language":8075,"meta":8,"className":8076,"style":8},"std::string message = response_json[\"choices\"][0][\"message\"][\"content\"];\n",[9167],{"type":27,"tag":170,"props":9168,"children":9169},{"__ignoreMap":8},[9170],{"type":27,"tag":261,"props":9171,"children":9172},{"class":263,"line":264},[9173,9177,9182,9186,9191,9196,9201,9206,9210,9215,9219,9223],{"type":27,"tag":261,"props":9174,"children":9175},{"style":732},[9176],{"type":33,"value":8899},{"type":27,"tag":261,"props":9178,"children":9179},{"style":268},[9180],{"type":33,"value":9181},"::string message ",{"type":27,"tag":261,"props":9183,"children":9184},{"style":274},[9185],{"type":33,"value":277},{"type":27,"tag":261,"props":9187,"children":9188},{"style":268},[9189],{"type":33,"value":9190}," response_json[",{"type":27,"tag":261,"props":9192,"children":9193},{"style":493},[9194],{"type":33,"value":9195},"\"choices\"",{"type":27,"tag":261,"props":9197,"children":9198},{"style":268},[9199],{"type":33,"value":9200},"][",{"type":27,"tag":261,"props":9202,"children":9203},{"style":295},[9204],{"type":33,"value":9205},"0",{"type":27,"tag":261,"props":9207,"children":9208},{"style":268},[9209],{"type":33,"value":9200},{"type":27,"tag":261,"props":9211,"children":9212},{"style":493},[9213],{"type":33,"value":9214},"\"message\"",{"type":27,"tag":261,"props":9216,"children":9217},{"style":268},[9218],{"type":33,"value":9200},{"type":27,"tag":261,"props":9220,"children":9221},{"style":493},[9222],{"type":33,"value":981},{"type":27,"tag":261,"props":9224,"children":9225},{"style":268},[9226],{"type":33,"value":9227},"];\n",{"type":27,"tag":36,"props":9229,"children":9230},{},[9231,9232,9238],{"type":33,"value":5353},{"type":27,"tag":170,"props":9233,"children":9235},{"className":9234},[],[9236],{"type":33,"value":9237},"LogToChatbox",{"type":33,"value":9239}," method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time.",{"type":27,"tag":238,"props":9241,"children":9243},{"code":9242,"language":8075,"meta":8,"className":8076,"style":8},"gameWrapper->LogToChatbox(messages[i], this->bot_name);\n",[9244],{"type":27,"tag":170,"props":9245,"children":9246},{"__ignoreMap":8},[9247],{"type":27,"tag":261,"props":9248,"children":9249},{"class":263,"line":264},[9250,9255,9259,9264,9268],{"type":27,"tag":261,"props":9251,"children":9252},{"style":268},[9253],{"type":33,"value":9254},"gameWrapper->",{"type":27,"tag":261,"props":9256,"children":9257},{"style":820},[9258],{"type":33,"value":9237},{"type":27,"tag":261,"props":9260,"children":9261},{"style":268},[9262],{"type":33,"value":9263},"(messages[i], ",{"type":27,"tag":261,"props":9265,"children":9266},{"style":873},[9267],{"type":33,"value":8857},{"type":27,"tag":261,"props":9269,"children":9270},{"style":268},[9271],{"type":33,"value":9272},"->bot_name);\n",{"type":27,"tag":36,"props":9274,"children":9275},{},[9276,9278,9284],{"type":33,"value":9277},"That's it! The code isn't that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages (",{"type":27,"tag":170,"props":9279,"children":9281},{"className":9280},[],[9282],{"type":33,"value":9283},"\u003C/s>",{"type":33,"value":9285},"). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt.",{"type":27,"tag":28,"props":9287,"children":9289},{"id":9288},"rocket-league-botchat-ui",[9290],{"type":33,"value":9291},"Rocket League BotChat UI",{"type":27,"tag":36,"props":9293,"children":9294},{},[9295],{"type":33,"value":9296},"Most BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here's what the UI for Rocket League BotChat looks like:",{"type":27,"tag":36,"props":9298,"children":9299},{},[9300],{"type":27,"tag":84,"props":9301,"children":9304},{"alt":9302,"src":9303},"Rocket League BotChat Plugin UI","/static/rlbc/rlbcui.png",[],{"type":27,"tag":47,"props":9306,"children":9308},{"id":9307},"system-prompt",[9309],{"type":33,"value":9310},"System prompt",{"type":27,"tag":36,"props":9312,"children":9313},{},[9314],{"type":33,"value":9315},"The system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here's the default prompt that I used:",{"type":27,"tag":238,"props":9317,"children":9319},{"code":9318,"language":8075,"meta":8,"className":8076,"style":8},"    std::string ai_player = \"You are an elite AI player in the car soccer game Rocket League. \";\n    std::string one_v_one = \"You are playing a 1v1 match against a human player. \";\n    std::string instructions = \"You will send short chat messages to your human opponent in response to what happens in the game. \";\n    std::string details = \"Respond to the human player with brief messages no more than 12 words long.\";\n    // initial system prompt\n    std::string initial_system_prompt = ai_player + one_v_one + instructions + details;\n",[9320],{"type":27,"tag":170,"props":9321,"children":9322},{"__ignoreMap":8},[9323,9350,9375,9400,9425,9433],{"type":27,"tag":261,"props":9324,"children":9325},{"class":263,"line":264},[9326,9331,9336,9340,9345],{"type":27,"tag":261,"props":9327,"children":9328},{"style":732},[9329],{"type":33,"value":9330},"    std",{"type":27,"tag":261,"props":9332,"children":9333},{"style":268},[9334],{"type":33,"value":9335},"::string ai_player ",{"type":27,"tag":261,"props":9337,"children":9338},{"style":274},[9339],{"type":33,"value":277},{"type":27,"tag":261,"props":9341,"children":9342},{"style":493},[9343],{"type":33,"value":9344}," \"You are an elite AI player in the car soccer game Rocket League. \"",{"type":27,"tag":261,"props":9346,"children":9347},{"style":268},[9348],{"type":33,"value":9349},";\n",{"type":27,"tag":261,"props":9351,"children":9352},{"class":263,"line":343},[9353,9357,9362,9366,9371],{"type":27,"tag":261,"props":9354,"children":9355},{"style":732},[9356],{"type":33,"value":9330},{"type":27,"tag":261,"props":9358,"children":9359},{"style":268},[9360],{"type":33,"value":9361},"::string one_v_one ",{"type":27,"tag":261,"props":9363,"children":9364},{"style":274},[9365],{"type":33,"value":277},{"type":27,"tag":261,"props":9367,"children":9368},{"style":493},[9369],{"type":33,"value":9370}," \"You are playing a 1v1 match against a human player. \"",{"type":27,"tag":261,"props":9372,"children":9373},{"style":268},[9374],{"type":33,"value":9349},{"type":27,"tag":261,"props":9376,"children":9377},{"class":263,"line":379},[9378,9382,9387,9391,9396],{"type":27,"tag":261,"props":9379,"children":9380},{"style":732},[9381],{"type":33,"value":9330},{"type":27,"tag":261,"props":9383,"children":9384},{"style":268},[9385],{"type":33,"value":9386},"::string instructions ",{"type":27,"tag":261,"props":9388,"children":9389},{"style":274},[9390],{"type":33,"value":277},{"type":27,"tag":261,"props":9392,"children":9393},{"style":493},[9394],{"type":33,"value":9395}," \"You will send short chat messages to your human opponent in response to what happens in the game. \"",{"type":27,"tag":261,"props":9397,"children":9398},{"style":268},[9399],{"type":33,"value":9349},{"type":27,"tag":261,"props":9401,"children":9402},{"class":263,"line":537},[9403,9407,9412,9416,9421],{"type":27,"tag":261,"props":9404,"children":9405},{"style":732},[9406],{"type":33,"value":9330},{"type":27,"tag":261,"props":9408,"children":9409},{"style":268},[9410],{"type":33,"value":9411},"::string details ",{"type":27,"tag":261,"props":9413,"children":9414},{"style":274},[9415],{"type":33,"value":277},{"type":27,"tag":261,"props":9417,"children":9418},{"style":493},[9419],{"type":33,"value":9420}," \"Respond to the human player with brief messages no more than 12 words long.\"",{"type":27,"tag":261,"props":9422,"children":9423},{"style":268},[9424],{"type":33,"value":9349},{"type":27,"tag":261,"props":9426,"children":9427},{"class":263,"line":561},[9428],{"type":27,"tag":261,"props":9429,"children":9430},{"style":478},[9431],{"type":33,"value":9432},"    // initial system prompt\n",{"type":27,"tag":261,"props":9434,"children":9435},{"class":263,"line":785},[9436,9440,9445,9449,9454,9458,9463,9467,9472,9476],{"type":27,"tag":261,"props":9437,"children":9438},{"style":732},[9439],{"type":33,"value":9330},{"type":27,"tag":261,"props":9441,"children":9442},{"style":268},[9443],{"type":33,"value":9444},"::string initial_system_prompt ",{"type":27,"tag":261,"props":9446,"children":9447},{"style":274},[9448],{"type":33,"value":277},{"type":27,"tag":261,"props":9450,"children":9451},{"style":268},[9452],{"type":33,"value":9453}," ai_player ",{"type":27,"tag":261,"props":9455,"children":9456},{"style":274},[9457],{"type":33,"value":8201},{"type":27,"tag":261,"props":9459,"children":9460},{"style":268},[9461],{"type":33,"value":9462}," one_v_one ",{"type":27,"tag":261,"props":9464,"children":9465},{"style":274},[9466],{"type":33,"value":8201},{"type":27,"tag":261,"props":9468,"children":9469},{"style":268},[9470],{"type":33,"value":9471}," instructions ",{"type":27,"tag":261,"props":9473,"children":9474},{"style":274},[9475],{"type":33,"value":8201},{"type":27,"tag":261,"props":9477,"children":9478},{"style":268},[9479],{"type":33,"value":9480}," details;\n",{"type":27,"tag":36,"props":9482,"children":9483},{},[9484,9486,9492,9494,9500],{"type":33,"value":9485},"The last part about ",{"type":27,"tag":170,"props":9487,"children":9489},{"className":9488},[],[9490],{"type":33,"value":9491},"no more than 12 words long",{"type":33,"value":9493}," was the most effective way of controlling the length responses from the LLM. I tried changing the ",{"type":27,"tag":170,"props":9495,"children":9497},{"className":9496},[],[9498],{"type":33,"value":9499},"max_output_len",{"type":33,"value":9501}," when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user. Changing the system prompt was a lot of fun to expirment with!",{"type":27,"tag":47,"props":9503,"children":9505},{"id":9504},"temperature-and-seed",[9506],{"type":33,"value":9507},"Temperature and Seed",{"type":27,"tag":36,"props":9509,"children":9510},{},[9511],{"type":33,"value":9512},"These values are included in the body of the request to the LLM, but I didn't have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn't really work.",{"type":27,"tag":47,"props":9514,"children":9515},{"id":8069},[9516],{"type":33,"value":9517},"Messages",{"type":27,"tag":36,"props":9519,"children":9520},{},[9521],{"type":33,"value":9522},"This section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the \"user\" (which are messages about game events) and the \"assistant\" (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt.",{"type":27,"tag":28,"props":9524,"children":9525},{"id":7345},[9526],{"type":33,"value":7348},{"type":27,"tag":9528,"props":9529,"children":9530},"rocket-league-bot-chat-video",{},[],{"type":27,"tag":36,"props":9532,"children":9533},{},[9534],{"type":33,"value":9535},"I used Blender's sequence editor to create a demo video for my contest submission. I don't edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here's how I approached creating the demo video for my project.",{"type":27,"tag":36,"props":9537,"children":9538},{},[9539],{"type":27,"tag":84,"props":9540,"children":9543},{"alt":9541,"src":9542},"Blender video sequence editor UI used to create my project video","/static/rlbc/blender.png",[],{"type":27,"tag":574,"props":9545,"children":9546},{},[9547,9552,9564,9586,9591],{"type":27,"tag":98,"props":9548,"children":9549},{},[9550],{"type":33,"value":9551},"Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action",{"type":27,"tag":98,"props":9553,"children":9554},{},[9555,9557],{"type":33,"value":9556},"Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used ",{"type":27,"tag":61,"props":9558,"children":9561},{"href":9559,"rel":9560},"https://open.spotify.com/track/68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51",[65],[9562],{"type":33,"value":9563},"\"Dads in Space\" by Steven Walking",{"type":27,"tag":98,"props":9565,"children":9566},{},[9567,9569,9575,9577,9584],{"type":33,"value":9568},"Get stock Rocket League footage from YouTube with ",{"type":27,"tag":170,"props":9570,"children":9572},{"className":9571},[],[9573],{"type":33,"value":9574},"youtube-dl",{"type":33,"value":9576}," (this is an amazing tool!). I mostly used footage from the ",{"type":27,"tag":61,"props":9578,"children":9581},{"href":9579,"rel":9580},"https://www.youtube.com/watch?v=e1tqWldCYOI&pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D",[65],[9582],{"type":33,"value":9583},"RLCS 2023 Winter Major Trailer",{"type":33,"value":9585},". This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps.",{"type":27,"tag":98,"props":9587,"children":9588},{},[9589],{"type":33,"value":9590},"Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!",{"type":27,"tag":98,"props":9592,"children":9593},{},[9594],{"type":33,"value":9595},"Use ComfyUI to animate some of the images used in the contest description and use these in my video",{"type":27,"tag":36,"props":9597,"children":9598},{},[9599],{"type":27,"tag":84,"props":9600,"children":9603},{"alt":9601,"src":9602},"ComfyUI workflow for animating images using img2vid model","/static/rlbc/comfyui.png",[],{"type":27,"tag":574,"props":9605,"children":9606},{},[9607],{"type":27,"tag":98,"props":9608,"children":9609},{},[9610],{"type":33,"value":9611},"Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice.",{"type":27,"tag":36,"props":9613,"children":9614},{},[9615],{"type":27,"tag":84,"props":9616,"children":9619},{"alt":9617,"src":9618},"Eleven Labs Voice Generation Web UI","/static/rlbc/elevenlabs.png",[],{"type":27,"tag":36,"props":9621,"children":9622},{},[9623],{"type":27,"tag":61,"props":9624,"children":9626},{"href":9625},"#",[9627],{"type":33,"value":9628},"Embed twitter video here",{"type":27,"tag":28,"props":9630,"children":9631},{"id":7365},[9632],{"type":33,"value":7368},{"type":27,"tag":36,"props":9634,"children":9635},{},[9636,9638,9643],{"type":33,"value":9637},"This plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in \"user\" and \"assistant\" prompts getting out of order which breaks assertions on the ",{"type":27,"tag":170,"props":9639,"children":9641},{"className":9640},[],[9642],{"type":33,"value":5751},{"type":33,"value":9644}," repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM.",{"type":27,"tag":36,"props":9646,"children":9647},{},[9648],{"type":33,"value":9649},"There are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like \"shot on goal\" to avoid prompt messages getting out of order. This could also be addressed with more code logic.",{"type":27,"tag":36,"props":9651,"children":9652},{},[9653],{"type":33,"value":9654},"Prompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example.",{"type":27,"tag":36,"props":9656,"children":9657},{},[9658,9660,9666],{"type":33,"value":9659},"I faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like ",{"type":27,"tag":170,"props":9661,"children":9663},{"className":9662},[],[9664],{"type":33,"value":9665},"SendMessage Good shot, bot!",{"type":33,"value":9667},", for example.",{"type":27,"tag":28,"props":9669,"children":9670},{"id":7429},[9671],{"type":33,"value":7432},{"type":27,"tag":36,"props":9673,"children":9674},{},[9675],{"type":33,"value":9676},"Participating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a \"Rocket League Coach\" plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc.",{"type":27,"tag":36,"props":9678,"children":9679},{},[9680],{"type":33,"value":9681},"I think the gaming industry's adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I'm excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM.",{"type":27,"tag":36,"props":9683,"children":9684},{},[9685],{"type":33,"value":9686},"Thanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!",{"type":27,"tag":5518,"props":9688,"children":9689},{},[9690],{"type":33,"value":5522},{"title":8,"searchDepth":343,"depth":343,"links":9692},[9693,9694,9695,9696,9697,9701,9706,9711,9712,9713],{"id":5593,"depth":343,"text":5596},{"id":7528,"depth":343,"text":7531},{"id":6064,"depth":343,"text":7577},{"id":6075,"depth":343,"text":6078},{"id":7236,"depth":343,"text":7239,"children":9698},[9699,9700],{"id":8244,"depth":379,"text":8247},{"id":8556,"depth":379,"text":8559},{"id":7318,"depth":343,"text":7321,"children":9702},[9703,9704,9705],{"id":8770,"depth":379,"text":8773},{"id":8943,"depth":379,"text":8946},{"id":9154,"depth":379,"text":9157},{"id":9288,"depth":343,"text":9291,"children":9707},[9708,9709,9710],{"id":9307,"depth":379,"text":9310},{"id":9504,"depth":379,"text":9507},{"id":8069,"depth":379,"text":9517},{"id":7345,"depth":343,"text":7348},{"id":7365,"depth":343,"text":7368},{"id":7429,"depth":343,"text":7432},"content:2024:02:17:rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest.md","2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest.md","2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",{"_path":9718,"_dir":9719,"_draft":7,"_partial":7,"_locale":8,"title":9720,"description":9721,"date":9722,"image":9723,"tags":9724,"external":9729,"comments":22,"body":9737,"_type":5561,"_id":13075,"_source":5563,"_file":13076,"_stem":13077,"_extension":5566},"/2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem","27","Python, Vue, Chinese-LLaMA-2 and The Three-Body Problem","Translating The Three-Body Problem book to English with Chinese LLMs, making visualizations with stable diffusion and running n-body simulations with CUDA","2023-11-23","/static/three-body-problem/cover.png",[9725,5580,17,16,20,254,14,9726,5578,5582,5575,9727,9728],"three-body-problem","cuda","vue","three.js",[9730,9733,9735],{"link":9731,"site":9732},"https://news.ycombinator.com/item?id=38393757","hn",{"link":9734,"site":7491},"https://www.reddit.com/r/threebodyproblem/comments/1823l5j/python_vue_chinesellama2_and_the_threebody_problem/",{"link":9736,"site":5588},"https://twitter.com/briancaffey/status/1727710878349332614",{"type":24,"children":9738,"toc":13058},[9739,9744,9749,9792,9797,9802,9808,9813,9821,9835,9840,9848,9853,9861,9866,9874,9879,9888,9893,9899,9904,9927,9932,9965,9971,9985,9998,10004,10018,10023,10050,10058,10064,10069,10074,10079,10084,10092,10097,10102,10107,10130,10136,10141,10368,10376,10389,10397,10405,10593,10605,10613,10618,10656,10662,10667,10675,10685,10693,10701,10706,10712,10717,10723,10728,11793,11798,11909,11914,12323,12349,12354,12527,12540,12553,12717,12722,12781,12793,12798,12859,12871,12876,12881,12886,12894,12900,12905,12910,12916,12921,12929,12934,12942,12947,12952,12957,12963,12976,12981,13032,13046,13054],{"type":27,"tag":9740,"props":9741,"children":9742},"h1",{"id":5593},[9743],{"type":33,"value":5596},{"type":27,"tag":36,"props":9745,"children":9746},{},[9747],{"type":33,"value":9748},"This articles brings together several of my interest, both old and new:",{"type":27,"tag":574,"props":9750,"children":9751},{},[9752,9757,9762,9767,9772,9777,9782,9787],{"type":27,"tag":98,"props":9753,"children":9754},{},[9755],{"type":33,"value":9756},"The Sci-Fi book series 'Three-Body Problem' by Liu Cixun",{"type":27,"tag":98,"props":9758,"children":9759},{},[9760],{"type":33,"value":9761},"Chinese language",{"type":27,"tag":98,"props":9763,"children":9764},{},[9765],{"type":33,"value":9766},"NLP techniques",{"type":27,"tag":98,"props":9768,"children":9769},{},[9770],{"type":33,"value":9771},"Large Language Models (LLMs)",{"type":27,"tag":98,"props":9773,"children":9774},{},[9775],{"type":33,"value":9776},"Stable Diffusion",{"type":27,"tag":98,"props":9778,"children":9779},{},[9780],{"type":33,"value":9781},"Data visualization and 3D graphics",{"type":27,"tag":98,"props":9783,"children":9784},{},[9785],{"type":33,"value":9786},"Mathematics",{"type":27,"tag":98,"props":9788,"children":9789},{},[9790],{"type":33,"value":9791},"NVIDIA / CUDA",{"type":27,"tag":36,"props":9793,"children":9794},{},[9795],{"type":33,"value":9796},"This is a linguistic, artistic and computational experiment with the two big AI algorithms of 2023: large language models (LLMs) and Stable Diffusion. I used the leading open-source LLMs from China’s tech sector to translate and summarize the text of Chinese author Liu Cixin’s award-winning science fiction novel: The Three-Body Problem. The book's storyline is based on a simple yet elusive problem from classical physics: predicting the movement of three gravitationally-attracted objects in space. I generated code for simulations and visualizations of this physics problem to present my own solutions to the three-body problem based on parallel computation. I also used Stable Diffusion to portray the imaginitive solutions to the three-body physics problem from one of the book’s main settings: an immersive virtual-reality game that spans centries of world history.",{"type":27,"tag":36,"props":9798,"children":9799},{},[9800],{"type":33,"value":9801},"I also share some of my experiences in China as an exchange student and research manager in the renewable energy technology sector. I wrote this article in English and translated it into Chinese using the same large language models I used to translate the Chinese text of the sci-fi novel into English. Warning: this article contains spoilers for the first book in the trilogy!",{"type":27,"tag":28,"props":9803,"children":9805},{"id":9804},"back-story",[9806],{"type":33,"value":9807},"Back story",{"type":27,"tag":36,"props":9809,"children":9810},{},[9811],{"type":33,"value":9812},"A few months ago my company announced that another round of layoffs was to come the following week. I'm on an engineering team that had already been impacted by a few rounds of layoffs in the past year, and I was expecting to be let go. On an impulse I bought a book at the top of my reading list from Amazon: \"Three-Body Problem\". It is an award-winning Sci-Fi trilogy written by Liu Cixin, a Chinese computer engineer who started writing the book as a series of essays that were published in China's \"World of Sci-Fi\" magazine.",{"type":27,"tag":36,"props":9814,"children":9815},{},[9816],{"type":27,"tag":84,"props":9817,"children":9820},{"alt":9818,"src":9819},"Images of Three Body Problem Book Series","/static/three-body-problem/books.png",[],{"type":27,"tag":36,"props":9822,"children":9823},{},[9824,9826,9833],{"type":33,"value":9825},"I started learning Chinese in college, adding a major in Chinese Language to the mathematics major I decided on in my freshman year after taking vector calculus and linear algebra. In my sophmore year I did a semester abroad at Fudan University's ",{"type":27,"tag":61,"props":9827,"children":9830},{"href":9828,"rel":9829},"https://ices.fudan.edu.cn/6628/list.htm",[65],[9831],{"type":33,"value":9832},"International Cultural Exchange School",{"type":33,"value":9834},". In 2007, living and studying Chinese in Shanghai as a 19 year old American was a really fun time. I was placed in an advanced-level course with a diverse group of students where English was not the lowest common linguistic denominator. We had a demanding cirriculum that emphasized reading, listening and speaking Chinese, but most of the language learning came through extracirricular activities: exploring Shanghai's food scene, bartering with vendors at the fabric markets, late night clubbing, walking around the Bund and the French Concession and chatting with my taxi cab drivers. It is hard to imagine how I did this without an iPhone, but I was able to get pretty far with an old Nokia 3310.",{"type":27,"tag":36,"props":9836,"children":9837},{},[9838],{"type":33,"value":9839},"At the end of one night of particularly heavy drinking, some of my classmates and I dropped in on an wangba (internet cafe) before heading back to the international dorm. Chinese internet cafes in 2007 were an expansive underground dens of computers, monitors, MMORPGs, FPSs, cigarets, and on-demand instant noodles delivered directly to your seat through an app on the desktop. That night our game of choice was Counter-Strike. In one of the lowest points of my gaming career, my classmates and I were crushed by our Chinese counterterrorist opponent.",{"type":27,"tag":36,"props":9841,"children":9842},{},[9843],{"type":27,"tag":84,"props":9844,"children":9847},{"alt":9845,"src":9846},"Chinese internet cafe","/static/three-body-problem/wangba.webp",[],{"type":27,"tag":36,"props":9849,"children":9850},{},[9851],{"type":33,"value":9852},"My favorite memory of that semester at Fudan University was travelling on an epic over-night sleeper train from Shanghai to Guangxi province with a school-sponsored class trip to see Guilin. Multiple games of sam-yuk-gu (3-6-9) ran in parallel across the matrix of 3-by-2 sleeper car bunk beds lining the train car like workloads distributed across multiple GPU cores. The rules of 3-6-9 are simple: a group of people go around in a circle counting up from 1. If your number contains a 3, 6 or 9, you clap once for each occurance of the number instead of saying your number. The first person to break the rules takes a drink. Then repeat indefinitely. The next morning we all boarded a boat cruise in a daze to see the Lijiang river's stunning limestone peaks featured on the 20 yuan note:",{"type":27,"tag":36,"props":9854,"children":9855},{},[9856],{"type":27,"tag":84,"props":9857,"children":9860},{"alt":9858,"src":9859},"20 yuan note with Guilin rock formations","/static/three-body-problem/twenty_small.gif",[],{"type":27,"tag":36,"props":9862,"children":9863},{},[9864],{"type":33,"value":9865},"My second job after college took me back to China where I specialized in the technologies, policies and applications of large scale battery projects as a research manager for China's energy storage industry association. The job exposed me to the power industry and cutting-edge battery projects, and also sharpened my technical Chinese as I was frequently reading, translating in a bi-linguagl environment. It was fun  I didn't realize it at the time, but that job was great preperation for reading Chinese Sci-Fi novels.",{"type":27,"tag":36,"props":9867,"children":9868},{},[9869],{"type":27,"tag":84,"props":9870,"children":9873},{"alt":9871,"src":9872},"State Grid HQ in Xi Cheng","/static/three-body-problem/invokeai/castles.png",[],{"type":27,"tag":36,"props":9875,"children":9876},{},[9877],{"type":33,"value":9878},"My first introduction to the 'Three-Body Problem' book came from one of my best friends from college. He lived at the inner-most leaf-node of one of Beijing's most labrythnian hutongs next to a family that trained racing pigeons. My friend and I bonded over our study of Chinese language, classical guitar and our experiences in Beijing. I strongly considered his recommendation to check out 三体 (Three Body), the Chinese Sci-Fi novel about alien life in a solar system with three stars as he described it, but I never had the chance to read the book.",{"type":27,"tag":9880,"props":9881,"children":9887},"iframe",{"width":9882,"height":9883,"src":9884,"title":9885,"frameBorder":9205,"allow":9886,"allowFullScreen":22},"100%",315,"https://www.youtube.com/embed/5lj99Uz1d50?si=TwrypbY4vTfeWGRf","YouTube video player","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",[],{"type":27,"tag":36,"props":9889,"children":9890},{},[9891],{"type":33,"value":9892},"Almost 10 years later I came across across a preview for the Netflix production of \"3 Body Problem\" scheduled to come out in early 2024. With the possibility of loosing my job weighing heavily on me, I picked up the books on Amazon hoping to have something to if I was going to be layed off. Over the weekend I was able to read a few of the chapters on my Kindle. I had not completely forgotten how to speak Chinese, and I could easily look up words and translate entire paragraphs with Google Translate.",{"type":27,"tag":28,"props":9894,"children":9896},{"id":9895},"chinese-in-numbers",[9897],{"type":33,"value":9898},"Chinese in numbers",{"type":27,"tag":36,"props":9900,"children":9901},{},[9902],{"type":33,"value":9903},"Here's a quick primer on the Chinese language from a mathematical perspective. This will be helpful before jumping into using NLP and LLMs with Chinese text later in this article.",{"type":27,"tag":36,"props":9905,"children":9906},{},[9907,9909,9916,9918,9925],{"type":33,"value":9908},"First, how many Chinese characters are there? This question isn't specific enough to have a single answer. A common rule of thumb that I have heard before says that there are over 50,000 characters in total with roughly 10,000 characters in use and about 3,000 characters frequently used in Chinese media and newspapers (",{"type":27,"tag":61,"props":9910,"children":9913},{"href":9911,"rel":9912},"https://en.wikipedia.org/wiki/Chinese_language#Vocabulary",[65],[9914],{"type":33,"value":9915},"source",{"type":33,"value":9917},"). ",{"type":27,"tag":61,"props":9919,"children":9922},{"href":9920,"rel":9921},"https://stackoverflow.com/a/1366113/6084948",[65],[9923],{"type":33,"value":9924},"This answer",{"type":33,"value":9926}," from StackOverflow's legendary #1 ranked user VonC gives a good answer based on the number of Unicode characters in the CJK Unified Ideographs block: 20,992.",{"type":27,"tag":36,"props":9928,"children":9929},{},[9930],{"type":33,"value":9931},"Here are some numbers and statistics to be better understand the text of the Three-Body Problem Chinese text:",{"type":27,"tag":574,"props":9933,"children":9934},{},[9935,9940,9945,9950,9955,9960],{"type":27,"tag":98,"props":9936,"children":9937},{},[9938],{"type":33,"value":9939},"188,380 total charactes in the book",{"type":27,"tag":98,"props":9941,"children":9942},{},[9943],{"type":33,"value":9944},"2,859 unique characters in the book",{"type":27,"tag":98,"props":9946,"children":9947},{},[9948],{"type":33,"value":9949},"36 chapters in the book",{"type":27,"tag":98,"props":9951,"children":9952},{},[9953],{"type":33,"value":9954},"average of 69.78 paragraphs per chapter",{"type":27,"tag":98,"props":9956,"children":9957},{},[9958],{"type":33,"value":9959},"total of 2,512 paragraphs in the book",{"type":27,"tag":98,"props":9961,"children":9962},{},[9963],{"type":33,"value":9964},"average of 74.99 characters per paragraph",{"type":27,"tag":47,"props":9966,"children":9968},{"id":9967},"character-frequency",[9969],{"type":33,"value":9970},"Character Frequency",{"type":27,"tag":36,"props":9972,"children":9973},{},[9974,9976,9983],{"type":33,"value":9975},"Let's look at how frequently each character in the book is used. We can also combine this with some data on the overall frequency of Chinese characters. The best measurement I found for overall character frequency is from ",{"type":27,"tag":61,"props":9977,"children":9980},{"href":9978,"rel":9979},"https://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO",[65],[9981],{"type":33,"value":9982},"Middle Tennessee State University",{"type":33,"value":9984},". Here's a visualization that shows of all of unique characters in the book. The height of a column represents how frequently a character occurs in the book, and the color represents the relatively frequency of the character in Chinese language overall.",{"type":27,"tag":9986,"props":9987,"children":9990},"div",{"className":9988},[9989],"wrap",[9991],{"type":27,"tag":9880,"props":9992,"children":9997},{"className":9993,"src":9995,"width":9882,"height":9996},[9994],"p-4","https://briancaffey.github.io/three-body-problem/tjs/load.html",550,[],{"type":27,"tag":28,"props":9999,"children":10001},{"id":10000},"meta-llms-and-grass-mud-horse",[10002],{"type":33,"value":10003},"Meta, LLMs, and Grass Mud Horse",{"type":27,"tag":36,"props":10005,"children":10006},{},[10007,10009,10016],{"type":33,"value":10008},"In recent months I have been following the development of big open source AI projects. Two projects in particular are InvokeAI, an image generation tool based on Stable Diffusion, and ",{"type":27,"tag":61,"props":10010,"children":10013},{"href":10011,"rel":10012},"https://ai.meta.com/llama/",[65],[10014],{"type":33,"value":10015},"LLaMA 2",{"type":33,"value":10017},", the latest generation of Meta's open source LLM. The name LLaMA stands for 'Large Language Model Meta AI', which happens to be the same spelling as the word for the domesticated South American camelid: llama. Before going deeper into LLMs we need a quick Chinese lesson.",{"type":27,"tag":36,"props":10019,"children":10020},{},[10021],{"type":33,"value":10022},"草泥马 is a non-technical word that referes to animals like Llama or Alpaca. It can be directly translated as \"Grass Mud Horse\" and it is phonetically similar to the most common Chinese profanity: 操你妈, which literally means \"f*** your mother\". The characters in these two words are nearly synonymous: the sounds of both words are \"cao ni ma\", but the tones are different, which in Chinese changes the meaning completely. The llama is basically a legendary Chinese internet meme subversive in the face of government censorship. 🦙 was approved as part of Unicode 11.0 in 2018. The extended version of this profanity is 草泥马戈壁 (Cǎonímǎ Gēbì: Grass Mud Horse Gobi), refering to the geographical origin of this mythical creature: the Gobi Dessert. This term is more explicit as it synonymous with \"f*** your mother's c***\". Coincidentally, the Gobi Desert is a region of Inner Mongolia which borders the mountainous region of Greater Khingan Range (大兴安岭), the location of Red Coast and Radar Peak in Three-Body Problem where Ye Wenjie makes first contact with the Trisolarians.",{"type":27,"tag":36,"props":10024,"children":10025},{},[10026,10028,10039,10041,10048],{"type":33,"value":10027},"I requested access to Meta's LLaMa 2 models as soon as they came out and I was able to get it to run on my NVIDIA RTX 4090 GPU. I also joined a subreddit called ",{"type":27,"tag":61,"props":10029,"children":10032},{"href":10030,"rel":10031},"https://www.reddit.com/r/LocalLLaMA/",[65],[10033],{"type":27,"tag":170,"props":10034,"children":10036},{"className":10035},[],[10037],{"type":33,"value":10038},"r/LocalLLaMa",{"type":33,"value":10040}," with over seventy thousand members discussing how to run large language models on consumer hardware. Another annoucement that caught my attention in July was the release of ",{"type":27,"tag":61,"props":10042,"children":10045},{"href":10043,"rel":10044},"https://github.com/ymcui/Chinese-LLaMA-Alpaca-2",[65],[10046],{"type":33,"value":10047},"Chinese LLaMa 2",{"type":33,"value":10049},", an open-source large language model trained on Chinese and English which does very well against Chinese Language LLM Benchmarks such as the CMMCU: Chinese Massive Multitask Language Understanding.",{"type":27,"tag":36,"props":10051,"children":10052},{},[10053],{"type":27,"tag":84,"props":10054,"children":10057},{"alt":10055,"src":10056},"image of CMMLU","/static/three-body-problem/cmmlu.jpeg",[],{"type":27,"tag":28,"props":10059,"children":10061},{"id":10060},"translation-in-and-of-the-three-body-problem",[10062],{"type":33,"value":10063},"Translation in and of The Three-Body Problem",{"type":27,"tag":36,"props":10065,"children":10066},{},[10067],{"type":33,"value":10068},"There are two important plot developments related to language translation in the Three-Body Problem novel, both of which involve book’s main female protagonist Ye Wenjie. First, copying the translation of Rachel Carson's 'Silent Spring' leads to her being relegated to the Red Coast project. At the Red Coast Ye Wenjie communicates with extraterrestrial life through a universal translation technology developed by the top-secret project.",{"type":27,"tag":36,"props":10070,"children":10071},{},[10072],{"type":33,"value":10073},"Ken Liu’s translation of the Three-Body Problem book from Chinese to English places the events during the Cultural Revolution at the beginning of the book rather than in the middle of the book. According to Liu, this was done in order to avoid attention of government censors, and his original intention was to tell the story in this way, starting with the events of the late 1960's in China.",{"type":27,"tag":36,"props":10075,"children":10076},{},[10077],{"type":33,"value":10078},"I tried translating the Chinese text of the Three-Body Problem book using LLMs. I started with the Chinese-LLaMA-2 model and then tried Qwen-7B-Chat, Baichuan-13B-Chat when these models came out. I found that the Qwen-7B-Chat model worked best for my translation tasks. Qwen is short for Qian Wen (千问, or \"one thousand questions\") and is developed by Alibaba Cloud.",{"type":27,"tag":36,"props":10080,"children":10081},{},[10082],{"type":33,"value":10083},"How do you get an LLM to translate text? Ultimately the quality of the translation returned by the LLM depends on the prompt and other parameters used for inference. I experimented with both chat and completion approaches and tried lots of different kinds of prompts. The models I worked with have a 4K context window (the number of tokens the model can take as input when generating responses), so for translation tasks I had the LLM work on one paragraph at a time. Here's the prompt I used with the Qwen-7B-Chat model:",{"type":27,"tag":238,"props":10085,"children":10087},{"code":10086},"\"你是一名翻译。请将每条消息从中文翻译成英文。\"\n(You are a translator. Please translate each message from Chinese to English.)\n",[10088],{"type":27,"tag":170,"props":10089,"children":10090},{"__ignoreMap":8},[10091],{"type":33,"value":10086},{"type":27,"tag":36,"props":10093,"children":10094},{},[10095],{"type":33,"value":10096},"I did some basic prompt engineering to get the LLM to translate the books in the Three-Body problem paragraph by paragraph. My computer was able to translate the first book overnight in under 500 minutes. Here are the results of my translation of Three-Body Problem with Qwen-7B-Chat model:",{"type":27,"tag":9880,"props":10098,"children":10101},{"className":10099,"src":10100,"width":9882,"height":9996},[9994],"https://briancaffey.github.io/three-body-problem/reader/?book=three_body&chapterNumber=1",[],{"type":27,"tag":36,"props":10103,"children":10104},{},[10105],{"type":33,"value":10106},"It was interesting to see the failure modes of translation tasks for the different models. Most of the time the LLM was able to provided accurate translations. Some of the failure modes I observed were:",{"type":27,"tag":574,"props":10108,"children":10109},{},[10110,10115,10120,10125],{"type":27,"tag":98,"props":10111,"children":10112},{},[10113],{"type":33,"value":10114},"a few Chinese characters would show up in the English translations",{"type":27,"tag":98,"props":10116,"children":10117},{},[10118],{"type":33,"value":10119},"a complete Chinese sentence would show up in an otherwise complete translation of a paragraph",{"type":27,"tag":98,"props":10121,"children":10122},{},[10123],{"type":33,"value":10124},"The LLM refused to translate certain paragraphs that included violent imagery, such as the violent scenes from the Cultural Revolution chapters",{"type":27,"tag":98,"props":10126,"children":10127},{},[10128],{"type":33,"value":10129},"If the sentence it was asked to translate was a question, the LLM would respond in Chinese to the question rather than providing a translation of the question itself",{"type":27,"tag":47,"props":10131,"children":10133},{"id":10132},"tokenization",[10134],{"type":33,"value":10135},"Tokenization",{"type":27,"tag":36,"props":10137,"children":10138},{},[10139],{"type":33,"value":10140},"When you feed a prompt to an LLM, it first puts the prompt through a process called tokenization. Tokenization takes a string of text and breaks it down into tokens (defined by the Large Language Model you are using). The process of tokenization is similar to the tokenization done by spaCy mentioned earlier. These tokens produced by LLM tokenization are numbers. Here's an example of tokenization in action using the Chinese-Llama-2 model:",{"type":27,"tag":238,"props":10142,"children":10144},{"code":10143,"language":254,"meta":8,"className":255,"style":8},"import json\nimport os\nfrom llama_cpp import Llama, LlamaTokenizer\n\nllm = Llama(\n    model_path=\"/path/to/models/ggml-model-q4_0.bin\",\n    n_ctx=4096,\n    n_gpu_layers=30\n)\n\ntokenizer = LlamaTokenizer(llama=llm)\n\nTEXT=\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"\ntokens = tokenizer.encode(TEXT)\n",[10145],{"type":27,"tag":170,"props":10146,"children":10147},{"__ignoreMap":8},[10148,10160,10172,10193,10200,10216,10237,10258,10275,10282,10289,10319,10326,10343],{"type":27,"tag":261,"props":10149,"children":10150},{"class":263,"line":264},[10151,10155],{"type":27,"tag":261,"props":10152,"children":10153},{"style":274},[10154],{"type":33,"value":1359},{"type":27,"tag":261,"props":10156,"children":10157},{"style":268},[10158],{"type":33,"value":10159}," json\n",{"type":27,"tag":261,"props":10161,"children":10162},{"class":263,"line":343},[10163,10167],{"type":27,"tag":261,"props":10164,"children":10165},{"style":274},[10166],{"type":33,"value":1359},{"type":27,"tag":261,"props":10168,"children":10169},{"style":268},[10170],{"type":33,"value":10171}," os\n",{"type":27,"tag":261,"props":10173,"children":10174},{"class":263,"line":379},[10175,10179,10184,10188],{"type":27,"tag":261,"props":10176,"children":10177},{"style":274},[10178],{"type":33,"value":1349},{"type":27,"tag":261,"props":10180,"children":10181},{"style":268},[10182],{"type":33,"value":10183}," llama_cpp ",{"type":27,"tag":261,"props":10185,"children":10186},{"style":274},[10187],{"type":33,"value":1359},{"type":27,"tag":261,"props":10189,"children":10190},{"style":268},[10191],{"type":33,"value":10192}," Llama, LlamaTokenizer\n",{"type":27,"tag":261,"props":10194,"children":10195},{"class":263,"line":537},[10196],{"type":27,"tag":261,"props":10197,"children":10198},{"emptyLinePlaceholder":22},[10199],{"type":33,"value":766},{"type":27,"tag":261,"props":10201,"children":10202},{"class":263,"line":561},[10203,10207,10211],{"type":27,"tag":261,"props":10204,"children":10205},{"style":268},[10206],{"type":33,"value":385},{"type":27,"tag":261,"props":10208,"children":10209},{"style":274},[10210],{"type":33,"value":277},{"type":27,"tag":261,"props":10212,"children":10213},{"style":268},[10214],{"type":33,"value":10215}," Llama(\n",{"type":27,"tag":261,"props":10217,"children":10218},{"class":263,"line":785},[10219,10224,10228,10233],{"type":27,"tag":261,"props":10220,"children":10221},{"style":285},[10222],{"type":33,"value":10223},"    model_path",{"type":27,"tag":261,"props":10225,"children":10226},{"style":274},[10227],{"type":33,"value":277},{"type":27,"tag":261,"props":10229,"children":10230},{"style":493},[10231],{"type":33,"value":10232},"\"/path/to/models/ggml-model-q4_0.bin\"",{"type":27,"tag":261,"props":10234,"children":10235},{"style":268},[10236],{"type":33,"value":1211},{"type":27,"tag":261,"props":10238,"children":10239},{"class":263,"line":794},[10240,10245,10249,10254],{"type":27,"tag":261,"props":10241,"children":10242},{"style":285},[10243],{"type":33,"value":10244},"    n_ctx",{"type":27,"tag":261,"props":10246,"children":10247},{"style":274},[10248],{"type":33,"value":277},{"type":27,"tag":261,"props":10250,"children":10251},{"style":295},[10252],{"type":33,"value":10253},"4096",{"type":27,"tag":261,"props":10255,"children":10256},{"style":268},[10257],{"type":33,"value":1211},{"type":27,"tag":261,"props":10259,"children":10260},{"class":263,"line":803},[10261,10266,10270],{"type":27,"tag":261,"props":10262,"children":10263},{"style":285},[10264],{"type":33,"value":10265},"    n_gpu_layers",{"type":27,"tag":261,"props":10267,"children":10268},{"style":274},[10269],{"type":33,"value":277},{"type":27,"tag":261,"props":10271,"children":10272},{"style":295},[10273],{"type":33,"value":10274},"30\n",{"type":27,"tag":261,"props":10276,"children":10277},{"class":263,"line":811},[10278],{"type":27,"tag":261,"props":10279,"children":10280},{"style":268},[10281],{"type":33,"value":340},{"type":27,"tag":261,"props":10283,"children":10284},{"class":263,"line":860},[10285],{"type":27,"tag":261,"props":10286,"children":10287},{"emptyLinePlaceholder":22},[10288],{"type":33,"value":766},{"type":27,"tag":261,"props":10290,"children":10291},{"class":263,"line":884},[10292,10297,10301,10306,10310,10314],{"type":27,"tag":261,"props":10293,"children":10294},{"style":268},[10295],{"type":33,"value":10296},"tokenizer ",{"type":27,"tag":261,"props":10298,"children":10299},{"style":274},[10300],{"type":33,"value":277},{"type":27,"tag":261,"props":10302,"children":10303},{"style":268},[10304],{"type":33,"value":10305}," LlamaTokenizer(",{"type":27,"tag":261,"props":10307,"children":10308},{"style":285},[10309],{"type":33,"value":5580},{"type":27,"tag":261,"props":10311,"children":10312},{"style":274},[10313],{"type":33,"value":277},{"type":27,"tag":261,"props":10315,"children":10316},{"style":268},[10317],{"type":33,"value":10318},"llm)\n",{"type":27,"tag":261,"props":10320,"children":10321},{"class":263,"line":902},[10322],{"type":27,"tag":261,"props":10323,"children":10324},{"emptyLinePlaceholder":22},[10325],{"type":33,"value":766},{"type":27,"tag":261,"props":10327,"children":10328},{"class":263,"line":911},[10329,10334,10338],{"type":27,"tag":261,"props":10330,"children":10331},{"style":295},[10332],{"type":33,"value":10333},"TEXT",{"type":27,"tag":261,"props":10335,"children":10336},{"style":274},[10337],{"type":33,"value":277},{"type":27,"tag":261,"props":10339,"children":10340},{"style":493},[10341],{"type":33,"value":10342},"\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"\n",{"type":27,"tag":261,"props":10344,"children":10345},{"class":263,"line":934},[10346,10351,10355,10360,10364],{"type":27,"tag":261,"props":10347,"children":10348},{"style":268},[10349],{"type":33,"value":10350},"tokens ",{"type":27,"tag":261,"props":10352,"children":10353},{"style":274},[10354],{"type":33,"value":277},{"type":27,"tag":261,"props":10356,"children":10357},{"style":268},[10358],{"type":33,"value":10359}," tokenizer.encode(",{"type":27,"tag":261,"props":10361,"children":10362},{"style":295},[10363],{"type":33,"value":10333},{"type":27,"tag":261,"props":10365,"children":10366},{"style":268},[10367],{"type":33,"value":340},{"type":27,"tag":238,"props":10369,"children":10371},{"code":10370},"print(str(tokens[:4]) + \" ...\")\n",[10372],{"type":27,"tag":170,"props":10373,"children":10374},{"__ignoreMap":8},[10375],{"type":33,"value":10370},{"type":27,"tag":2972,"props":10377,"children":10378},{},[10379],{"type":27,"tag":36,"props":10380,"children":10381},{},[10382,10387],{"type":27,"tag":261,"props":10383,"children":10384},{},[10385],{"type":33,"value":10386},"1, 30505, 32380, 36812",{"type":33,"value":10388}," ...",{"type":27,"tag":238,"props":10390,"children":10392},{"code":10391},"for token in tokens:\n    text = tokenizer.decode([token])\n    print(text, end=\" \")\n",[10393],{"type":27,"tag":170,"props":10394,"children":10395},{"__ignoreMap":8},[10396],{"type":33,"value":10391},{"type":27,"tag":2972,"props":10398,"children":10399},{},[10400],{"type":27,"tag":36,"props":10401,"children":10402},{},[10403],{"type":33,"value":10404},"在 那个 已被 忘 却 的日子 里 ， 它的 世界 颠覆 了 。 泥 土 飞 走 ， 出现了 一条 又 深 又 宽 的 峡谷 ， 然后 泥 土 又 轰 隆 隆 地 飞 回来 ， 峡谷 消失 了 ， 在 原来 峡谷 的 尽头 出现了 一座 黑色 的 孤 峰 。 其实 ， 在这 片 广阔 的 疆 域 上 ， 这种事 常常 发生 ， 泥 土 飞 走 又 飞 回 ， 峡谷 出现 又 消失 ， 然后 是 孤 峰 降临 ， 好像是 给 每次 灾 变 打 上 一个 醒目 的 标记 。 褐 蚁 和 几百 个 同 族 带着 幸 存 的 蚁 后 向 太阳 落 下的 方向 走了 一段 路 ， 建立了 新的 帝国 。",{"type":27,"tag":238,"props":10406,"children":10408},{"code":10407,"language":254,"meta":8,"className":255,"style":8},"english_text = \"This is an example of tokenization using a large language model.\"\nenglish_tokens = tokenizer.encode(english_text)\nprint(str(english_tokens[:4]) + \" ...\")\n\nfor token in english_tokens:\n    text = tokenizer.decode([token])\n    print(f\"'{text}'\", end=\" \")\n",[10409],{"type":27,"tag":170,"props":10410,"children":10411},{"__ignoreMap":8},[10412,10429,10446,10488,10495,10516,10533],{"type":27,"tag":261,"props":10413,"children":10414},{"class":263,"line":264},[10415,10420,10424],{"type":27,"tag":261,"props":10416,"children":10417},{"style":268},[10418],{"type":33,"value":10419},"english_text ",{"type":27,"tag":261,"props":10421,"children":10422},{"style":274},[10423],{"type":33,"value":277},{"type":27,"tag":261,"props":10425,"children":10426},{"style":493},[10427],{"type":33,"value":10428}," \"This is an example of tokenization using a large language model.\"\n",{"type":27,"tag":261,"props":10430,"children":10431},{"class":263,"line":343},[10432,10437,10441],{"type":27,"tag":261,"props":10433,"children":10434},{"style":268},[10435],{"type":33,"value":10436},"english_tokens ",{"type":27,"tag":261,"props":10438,"children":10439},{"style":274},[10440],{"type":33,"value":277},{"type":27,"tag":261,"props":10442,"children":10443},{"style":268},[10444],{"type":33,"value":10445}," tokenizer.encode(english_text)\n",{"type":27,"tag":261,"props":10447,"children":10448},{"class":263,"line":379},[10449,10453,10457,10461,10466,10470,10475,10479,10484],{"type":27,"tag":261,"props":10450,"children":10451},{"style":1520},[10452],{"type":33,"value":4157},{"type":27,"tag":261,"props":10454,"children":10455},{"style":268},[10456],{"type":33,"value":740},{"type":27,"tag":261,"props":10458,"children":10459},{"style":850},[10460],{"type":33,"value":853},{"type":27,"tag":261,"props":10462,"children":10463},{"style":268},[10464],{"type":33,"value":10465},"(english_tokens[:",{"type":27,"tag":261,"props":10467,"children":10468},{"style":295},[10469],{"type":33,"value":439},{"type":27,"tag":261,"props":10471,"children":10472},{"style":268},[10473],{"type":33,"value":10474},"]) ",{"type":27,"tag":261,"props":10476,"children":10477},{"style":274},[10478],{"type":33,"value":8201},{"type":27,"tag":261,"props":10480,"children":10481},{"style":493},[10482],{"type":33,"value":10483}," \" ...\"",{"type":27,"tag":261,"props":10485,"children":10486},{"style":268},[10487],{"type":33,"value":340},{"type":27,"tag":261,"props":10489,"children":10490},{"class":263,"line":537},[10491],{"type":27,"tag":261,"props":10492,"children":10493},{"emptyLinePlaceholder":22},[10494],{"type":33,"value":766},{"type":27,"tag":261,"props":10496,"children":10497},{"class":263,"line":561},[10498,10502,10507,10511],{"type":27,"tag":261,"props":10499,"children":10500},{"style":274},[10501],{"type":33,"value":1134},{"type":27,"tag":261,"props":10503,"children":10504},{"style":268},[10505],{"type":33,"value":10506}," token ",{"type":27,"tag":261,"props":10508,"children":10509},{"style":274},[10510],{"type":33,"value":553},{"type":27,"tag":261,"props":10512,"children":10513},{"style":268},[10514],{"type":33,"value":10515}," english_tokens:\n",{"type":27,"tag":261,"props":10517,"children":10518},{"class":263,"line":785},[10519,10524,10528],{"type":27,"tag":261,"props":10520,"children":10521},{"style":268},[10522],{"type":33,"value":10523},"    text ",{"type":27,"tag":261,"props":10525,"children":10526},{"style":274},[10527],{"type":33,"value":277},{"type":27,"tag":261,"props":10529,"children":10530},{"style":268},[10531],{"type":33,"value":10532}," tokenizer.decode([token])\n",{"type":27,"tag":261,"props":10534,"children":10535},{"class":263,"line":794},[10536,10541,10545,10549,10554,10558,10562,10566,10571,10575,10580,10584,10589],{"type":27,"tag":261,"props":10537,"children":10538},{"style":1520},[10539],{"type":33,"value":10540},"    print",{"type":27,"tag":261,"props":10542,"children":10543},{"style":268},[10544],{"type":33,"value":740},{"type":27,"tag":261,"props":10546,"children":10547},{"style":487},[10548],{"type":33,"value":1567},{"type":27,"tag":261,"props":10550,"children":10551},{"style":493},[10552],{"type":33,"value":10553},"\"'",{"type":27,"tag":261,"props":10555,"children":10556},{"style":295},[10557],{"type":33,"value":1577},{"type":27,"tag":261,"props":10559,"children":10560},{"style":268},[10561],{"type":33,"value":33},{"type":27,"tag":261,"props":10563,"children":10564},{"style":295},[10565],{"type":33,"value":1587},{"type":27,"tag":261,"props":10567,"children":10568},{"style":493},[10569],{"type":33,"value":10570},"'\"",{"type":27,"tag":261,"props":10572,"children":10573},{"style":268},[10574],{"type":33,"value":303},{"type":27,"tag":261,"props":10576,"children":10577},{"style":285},[10578],{"type":33,"value":10579},"end",{"type":27,"tag":261,"props":10581,"children":10582},{"style":274},[10583],{"type":33,"value":277},{"type":27,"tag":261,"props":10585,"children":10586},{"style":493},[10587],{"type":33,"value":10588},"\" \"",{"type":27,"tag":261,"props":10590,"children":10591},{"style":268},[10592],{"type":33,"value":340},{"type":27,"tag":2972,"props":10594,"children":10595},{},[10596],{"type":27,"tag":36,"props":10597,"children":10598},{},[10599,10604],{"type":27,"tag":261,"props":10600,"children":10601},{},[10602],{"type":33,"value":10603},"1, 4013, 338, 385",{"type":33,"value":10388},{"type":27,"tag":2972,"props":10606,"children":10607},{},[10608],{"type":27,"tag":36,"props":10609,"children":10610},{},[10611],{"type":33,"value":10612},"'' 'This' ' is' ' an' ' example' ' of' ' token' 'ization' ' using' ' a' ' large' ' language' ' model' '.'",{"type":27,"tag":36,"props":10614,"children":10615},{},[10616],{"type":33,"value":10617},"Here are some key differences between English and Chinese that have implications for how the language is tokenized by large language models:",{"type":27,"tag":574,"props":10619,"children":10620},{},[10621,10626,10631,10636,10641,10646,10651],{"type":27,"tag":98,"props":10622,"children":10623},{},[10624],{"type":33,"value":10625},"Chinese does not use spaces between words like English does",{"type":27,"tag":98,"props":10627,"children":10628},{},[10629],{"type":33,"value":10630},"Chinese words are typically formed from 2 or more characters",{"type":27,"tag":98,"props":10632,"children":10633},{},[10634],{"type":33,"value":10635},"Chinese verbs are not conjugated and do not have different tenses",{"type":27,"tag":98,"props":10637,"children":10638},{},[10639],{"type":33,"value":10640},"Chinese words don't have singular and plural variants",{"type":27,"tag":98,"props":10642,"children":10643},{},[10644],{"type":33,"value":10645},"Chinese grammar is very simple and is similar to English",{"type":27,"tag":98,"props":10647,"children":10648},{},[10649],{"type":33,"value":10650},"Chinese characters do not have capitization like ASCII characters",{"type":27,"tag":98,"props":10652,"children":10653},{},[10654],{"type":33,"value":10655},"The token represented by the number 1 encodes a starting token",{"type":27,"tag":28,"props":10657,"children":10659},{"id":10658},"imagining-scenes-from-three-body-problem-with-stable-diffusion",[10660],{"type":33,"value":10661},"Imagining scenes from Three-Body Problem with Stable Diffusion",{"type":27,"tag":36,"props":10663,"children":10664},{},[10665],{"type":33,"value":10666},"Here are some images I generated using Stable Diffusion with InvokeAI that depict scenes from the Three-Body Problem book. These scenes portray solutions to the Three-Body Problem that players in the Three-Body game devised. The first is a Confucian system of etiquette for predicting the movement of the three suns. The second is a human-powered computer that Qin Shi Huang used to try to predict the movement of the three suns.",{"type":27,"tag":2972,"props":10668,"children":10669},{},[10670],{"type":27,"tag":36,"props":10671,"children":10672},{},[10673],{"type":33,"value":10674},"Prompt: Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style",{"type":27,"tag":10676,"props":10677,"children":10678},"client-only",{},[10679],{"type":27,"tag":10680,"props":10681,"children":10684},"carousel",{":count":10682,"dir":10683},"8","confucius",[],{"type":27,"tag":2972,"props":10686,"children":10687},{},[10688],{"type":27,"tag":36,"props":10689,"children":10690},{},[10691],{"type":33,"value":10692},"array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle",{"type":27,"tag":10676,"props":10694,"children":10695},{},[10696],{"type":27,"tag":10680,"props":10697,"children":10700},{":count":10698,"dir":10699},"5","computer",[],{"type":27,"tag":36,"props":10702,"children":10703},{},[10704],{"type":33,"value":10705},"Congrats to the InvokeAI team on the 3.0 release. It has been awesome to use and the current docker compose setup is a huge improvement on the 2.x version.",{"type":27,"tag":28,"props":10707,"children":10709},{"id":10708},"n-body-simulations-cuda-and-threejs",[10710],{"type":33,"value":10711},"n-body simulations, CUDA and Three.js",{"type":27,"tag":36,"props":10713,"children":10714},{},[10715],{"type":33,"value":10716},"The nbody problem has no closed-form analytical solution, but it is possible to do a basic simulation of the three-body problem on consumer hardware and open source software, like NVIDIA and CUDA.",{"type":27,"tag":47,"props":10718,"children":10720},{"id":10719},"three-body-cuda-simulation",[10721],{"type":33,"value":10722},"Three-Body CUDA simulation",{"type":27,"tag":36,"props":10724,"children":10725},{},[10726],{"type":33,"value":10727},"I wrote a simple program with the help of ChatGPT for running nbody problem simulations. The program uses CuPy, a Python library that exposes APIs for doing matrix multiplication to predict the position of three bodies using Euclidian Integration. Here's the script:",{"type":27,"tag":238,"props":10729,"children":10733},{"code":10730,"language":10731,"meta":8,"className":10732,"style":8},"import numpy as np\nimport cupy as cp\nimport time\nimport json\n\n# Simulation parameters\nNUM_PARTICLES = 3\nDIMENSIONS = 3 # 3D space\nNUM_STEPS = 30\nDT = 0.1\n\n# Generate initial positions and velocities\nnp_positions = np.random.randn(NUM_PARTICLES, DIMENSIONS)\nnp_velocities = np.random.randn(NUM_PARTICLES, DIMENSIONS)\n\ncp_positions = cp.array(np_positions)\ncp_velocities = cp.array(np_velocities)\n\nnp_ticks = np.expand_dims(np_positions, axis=0)\ncp_ticks = cp.array(np_ticks)\n\n# nbody simulation loop\nstart_time = time.time()\nfor step in range(NUM_STEPS):\n\n    # this gets pairwise differences\n    diff = cp_positions[:, None, :] - cp_positions[None, :, :]\n    distances = cp.sqrt(cp.sum(diff**2, axis=2))\n\n    # avoid division by zero\n    epsilon = 1e-5\n    inv_distances = 1.0 / cp.maximum(distances, epsilon)\n\n    # calculate forces\n    cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n\n    # update velocities and positions\n    cp_velocities += DT * cp_forces\n    cp_positions += DT * cp_velocities\n    cp_ticks = cp.append(cp_ticks, cp.expand_dims(cp_positions, 0), 0)\n\nsim_time = time.time() - start_time\nprint(\"Simulation time:\", sim_time)\n\n\nclass NumpyArrayEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\n\nnp_ticks = cp_ticks.get()\n\n\n# this is data we can work with in python and write to a file\nwith open(\"ticks.json\", \"w\") as f:\n    f.write(json.dumps(np_ticks, cls=NumpyArrayEncoder))\n","py","language-py shiki shiki-themes github-light github-dark monokai",[10734],{"type":27,"tag":170,"props":10735,"children":10736},{"__ignoreMap":8},[10737,10758,10779,10791,10802,10809,10817,10834,10856,10873,10890,10897,10905,10938,10970,10977,10994,11011,11018,11052,11069,11076,11084,11101,11133,11140,11148,11192,11238,11245,11253,11270,11297,11304,11312,11368,11375,11384,11413,11439,11474,11482,11509,11531,11539,11547,11581,11615,11633,11647,11669,11677,11685,11702,11710,11718,11727,11770],{"type":27,"tag":261,"props":10738,"children":10739},{"class":263,"line":264},[10740,10744,10749,10753],{"type":27,"tag":261,"props":10741,"children":10742},{"style":274},[10743],{"type":33,"value":1359},{"type":27,"tag":261,"props":10745,"children":10746},{"style":268},[10747],{"type":33,"value":10748}," numpy ",{"type":27,"tag":261,"props":10750,"children":10751},{"style":274},[10752],{"type":33,"value":1611},{"type":27,"tag":261,"props":10754,"children":10755},{"style":268},[10756],{"type":33,"value":10757}," np\n",{"type":27,"tag":261,"props":10759,"children":10760},{"class":263,"line":343},[10761,10765,10770,10774],{"type":27,"tag":261,"props":10762,"children":10763},{"style":274},[10764],{"type":33,"value":1359},{"type":27,"tag":261,"props":10766,"children":10767},{"style":268},[10768],{"type":33,"value":10769}," cupy ",{"type":27,"tag":261,"props":10771,"children":10772},{"style":274},[10773],{"type":33,"value":1611},{"type":27,"tag":261,"props":10775,"children":10776},{"style":268},[10777],{"type":33,"value":10778}," cp\n",{"type":27,"tag":261,"props":10780,"children":10781},{"class":263,"line":379},[10782,10786],{"type":27,"tag":261,"props":10783,"children":10784},{"style":274},[10785],{"type":33,"value":1359},{"type":27,"tag":261,"props":10787,"children":10788},{"style":268},[10789],{"type":33,"value":10790}," time\n",{"type":27,"tag":261,"props":10792,"children":10793},{"class":263,"line":537},[10794,10798],{"type":27,"tag":261,"props":10795,"children":10796},{"style":274},[10797],{"type":33,"value":1359},{"type":27,"tag":261,"props":10799,"children":10800},{"style":268},[10801],{"type":33,"value":10159},{"type":27,"tag":261,"props":10803,"children":10804},{"class":263,"line":561},[10805],{"type":27,"tag":261,"props":10806,"children":10807},{"emptyLinePlaceholder":22},[10808],{"type":33,"value":766},{"type":27,"tag":261,"props":10810,"children":10811},{"class":263,"line":785},[10812],{"type":27,"tag":261,"props":10813,"children":10814},{"style":478},[10815],{"type":33,"value":10816},"# Simulation parameters\n",{"type":27,"tag":261,"props":10818,"children":10819},{"class":263,"line":794},[10820,10825,10829],{"type":27,"tag":261,"props":10821,"children":10822},{"style":295},[10823],{"type":33,"value":10824},"NUM_PARTICLES",{"type":27,"tag":261,"props":10826,"children":10827},{"style":274},[10828],{"type":33,"value":6259},{"type":27,"tag":261,"props":10830,"children":10831},{"style":295},[10832],{"type":33,"value":10833}," 3\n",{"type":27,"tag":261,"props":10835,"children":10836},{"class":263,"line":803},[10837,10842,10846,10851],{"type":27,"tag":261,"props":10838,"children":10839},{"style":295},[10840],{"type":33,"value":10841},"DIMENSIONS",{"type":27,"tag":261,"props":10843,"children":10844},{"style":274},[10845],{"type":33,"value":6259},{"type":27,"tag":261,"props":10847,"children":10848},{"style":295},[10849],{"type":33,"value":10850}," 3",{"type":27,"tag":261,"props":10852,"children":10853},{"style":478},[10854],{"type":33,"value":10855}," # 3D space\n",{"type":27,"tag":261,"props":10857,"children":10858},{"class":263,"line":811},[10859,10864,10868],{"type":27,"tag":261,"props":10860,"children":10861},{"style":295},[10862],{"type":33,"value":10863},"NUM_STEPS",{"type":27,"tag":261,"props":10865,"children":10866},{"style":274},[10867],{"type":33,"value":6259},{"type":27,"tag":261,"props":10869,"children":10870},{"style":295},[10871],{"type":33,"value":10872}," 30\n",{"type":27,"tag":261,"props":10874,"children":10875},{"class":263,"line":860},[10876,10881,10885],{"type":27,"tag":261,"props":10877,"children":10878},{"style":295},[10879],{"type":33,"value":10880},"DT",{"type":27,"tag":261,"props":10882,"children":10883},{"style":274},[10884],{"type":33,"value":6259},{"type":27,"tag":261,"props":10886,"children":10887},{"style":295},[10888],{"type":33,"value":10889}," 0.1\n",{"type":27,"tag":261,"props":10891,"children":10892},{"class":263,"line":884},[10893],{"type":27,"tag":261,"props":10894,"children":10895},{"emptyLinePlaceholder":22},[10896],{"type":33,"value":766},{"type":27,"tag":261,"props":10898,"children":10899},{"class":263,"line":902},[10900],{"type":27,"tag":261,"props":10901,"children":10902},{"style":478},[10903],{"type":33,"value":10904},"# Generate initial positions and velocities\n",{"type":27,"tag":261,"props":10906,"children":10907},{"class":263,"line":911},[10908,10913,10917,10922,10926,10930,10934],{"type":27,"tag":261,"props":10909,"children":10910},{"style":268},[10911],{"type":33,"value":10912},"np_positions ",{"type":27,"tag":261,"props":10914,"children":10915},{"style":274},[10916],{"type":33,"value":277},{"type":27,"tag":261,"props":10918,"children":10919},{"style":268},[10920],{"type":33,"value":10921}," np.random.randn(",{"type":27,"tag":261,"props":10923,"children":10924},{"style":295},[10925],{"type":33,"value":10824},{"type":27,"tag":261,"props":10927,"children":10928},{"style":268},[10929],{"type":33,"value":303},{"type":27,"tag":261,"props":10931,"children":10932},{"style":295},[10933],{"type":33,"value":10841},{"type":27,"tag":261,"props":10935,"children":10936},{"style":268},[10937],{"type":33,"value":340},{"type":27,"tag":261,"props":10939,"children":10940},{"class":263,"line":934},[10941,10946,10950,10954,10958,10962,10966],{"type":27,"tag":261,"props":10942,"children":10943},{"style":268},[10944],{"type":33,"value":10945},"np_velocities ",{"type":27,"tag":261,"props":10947,"children":10948},{"style":274},[10949],{"type":33,"value":277},{"type":27,"tag":261,"props":10951,"children":10952},{"style":268},[10953],{"type":33,"value":10921},{"type":27,"tag":261,"props":10955,"children":10956},{"style":295},[10957],{"type":33,"value":10824},{"type":27,"tag":261,"props":10959,"children":10960},{"style":268},[10961],{"type":33,"value":303},{"type":27,"tag":261,"props":10963,"children":10964},{"style":295},[10965],{"type":33,"value":10841},{"type":27,"tag":261,"props":10967,"children":10968},{"style":268},[10969],{"type":33,"value":340},{"type":27,"tag":261,"props":10971,"children":10972},{"class":263,"line":952},[10973],{"type":27,"tag":261,"props":10974,"children":10975},{"emptyLinePlaceholder":22},[10976],{"type":33,"value":766},{"type":27,"tag":261,"props":10978,"children":10979},{"class":263,"line":970},[10980,10985,10989],{"type":27,"tag":261,"props":10981,"children":10982},{"style":268},[10983],{"type":33,"value":10984},"cp_positions ",{"type":27,"tag":261,"props":10986,"children":10987},{"style":274},[10988],{"type":33,"value":277},{"type":27,"tag":261,"props":10990,"children":10991},{"style":268},[10992],{"type":33,"value":10993}," cp.array(np_positions)\n",{"type":27,"tag":261,"props":10995,"children":10996},{"class":263,"line":998},[10997,11002,11006],{"type":27,"tag":261,"props":10998,"children":10999},{"style":268},[11000],{"type":33,"value":11001},"cp_velocities ",{"type":27,"tag":261,"props":11003,"children":11004},{"style":274},[11005],{"type":33,"value":277},{"type":27,"tag":261,"props":11007,"children":11008},{"style":268},[11009],{"type":33,"value":11010}," cp.array(np_velocities)\n",{"type":27,"tag":261,"props":11012,"children":11013},{"class":263,"line":1038},[11014],{"type":27,"tag":261,"props":11015,"children":11016},{"emptyLinePlaceholder":22},[11017],{"type":33,"value":766},{"type":27,"tag":261,"props":11019,"children":11020},{"class":263,"line":1075},[11021,11026,11030,11035,11040,11044,11048],{"type":27,"tag":261,"props":11022,"children":11023},{"style":268},[11024],{"type":33,"value":11025},"np_ticks ",{"type":27,"tag":261,"props":11027,"children":11028},{"style":274},[11029],{"type":33,"value":277},{"type":27,"tag":261,"props":11031,"children":11032},{"style":268},[11033],{"type":33,"value":11034}," np.expand_dims(np_positions, ",{"type":27,"tag":261,"props":11036,"children":11037},{"style":285},[11038],{"type":33,"value":11039},"axis",{"type":27,"tag":261,"props":11041,"children":11042},{"style":274},[11043],{"type":33,"value":277},{"type":27,"tag":261,"props":11045,"children":11046},{"style":295},[11047],{"type":33,"value":9205},{"type":27,"tag":261,"props":11049,"children":11050},{"style":268},[11051],{"type":33,"value":340},{"type":27,"tag":261,"props":11053,"children":11054},{"class":263,"line":1083},[11055,11060,11064],{"type":27,"tag":261,"props":11056,"children":11057},{"style":268},[11058],{"type":33,"value":11059},"cp_ticks ",{"type":27,"tag":261,"props":11061,"children":11062},{"style":274},[11063],{"type":33,"value":277},{"type":27,"tag":261,"props":11065,"children":11066},{"style":268},[11067],{"type":33,"value":11068}," cp.array(np_ticks)\n",{"type":27,"tag":261,"props":11070,"children":11071},{"class":263,"line":1092},[11072],{"type":27,"tag":261,"props":11073,"children":11074},{"emptyLinePlaceholder":22},[11075],{"type":33,"value":766},{"type":27,"tag":261,"props":11077,"children":11078},{"class":263,"line":1100},[11079],{"type":27,"tag":261,"props":11080,"children":11081},{"style":478},[11082],{"type":33,"value":11083},"# nbody simulation loop\n",{"type":27,"tag":261,"props":11085,"children":11086},{"class":263,"line":1151},[11087,11092,11096],{"type":27,"tag":261,"props":11088,"children":11089},{"style":268},[11090],{"type":33,"value":11091},"start_time ",{"type":27,"tag":261,"props":11093,"children":11094},{"style":274},[11095],{"type":33,"value":277},{"type":27,"tag":261,"props":11097,"children":11098},{"style":268},[11099],{"type":33,"value":11100}," time.time()\n",{"type":27,"tag":261,"props":11102,"children":11103},{"class":263,"line":1173},[11104,11108,11113,11117,11121,11125,11129],{"type":27,"tag":261,"props":11105,"children":11106},{"style":274},[11107],{"type":33,"value":1134},{"type":27,"tag":261,"props":11109,"children":11110},{"style":268},[11111],{"type":33,"value":11112}," step ",{"type":27,"tag":261,"props":11114,"children":11115},{"style":274},[11116],{"type":33,"value":553},{"type":27,"tag":261,"props":11118,"children":11119},{"style":1520},[11120],{"type":33,"value":1523},{"type":27,"tag":261,"props":11122,"children":11123},{"style":268},[11124],{"type":33,"value":740},{"type":27,"tag":261,"props":11126,"children":11127},{"style":295},[11128],{"type":33,"value":10863},{"type":27,"tag":261,"props":11130,"children":11131},{"style":268},[11132],{"type":33,"value":750},{"type":27,"tag":261,"props":11134,"children":11135},{"class":263,"line":1182},[11136],{"type":27,"tag":261,"props":11137,"children":11138},{"emptyLinePlaceholder":22},[11139],{"type":33,"value":766},{"type":27,"tag":261,"props":11141,"children":11142},{"class":263,"line":1191},[11143],{"type":27,"tag":261,"props":11144,"children":11145},{"style":478},[11146],{"type":33,"value":11147},"    # this gets pairwise differences\n",{"type":27,"tag":261,"props":11149,"children":11150},{"class":263,"line":1214},[11151,11156,11160,11165,11169,11174,11178,11183,11187],{"type":27,"tag":261,"props":11152,"children":11153},{"style":268},[11154],{"type":33,"value":11155},"    diff ",{"type":27,"tag":261,"props":11157,"children":11158},{"style":274},[11159],{"type":33,"value":277},{"type":27,"tag":261,"props":11161,"children":11162},{"style":268},[11163],{"type":33,"value":11164}," cp_positions[:, ",{"type":27,"tag":261,"props":11166,"children":11167},{"style":295},[11168],{"type":33,"value":4413},{"type":27,"tag":261,"props":11170,"children":11171},{"style":268},[11172],{"type":33,"value":11173},", :] ",{"type":27,"tag":261,"props":11175,"children":11176},{"style":274},[11177],{"type":33,"value":8281},{"type":27,"tag":261,"props":11179,"children":11180},{"style":268},[11181],{"type":33,"value":11182}," cp_positions[",{"type":27,"tag":261,"props":11184,"children":11185},{"style":295},[11186],{"type":33,"value":4413},{"type":27,"tag":261,"props":11188,"children":11189},{"style":268},[11190],{"type":33,"value":11191},", :, :]\n",{"type":27,"tag":261,"props":11193,"children":11194},{"class":263,"line":1237},[11195,11200,11204,11209,11214,11218,11222,11226,11230,11234],{"type":27,"tag":261,"props":11196,"children":11197},{"style":268},[11198],{"type":33,"value":11199},"    distances ",{"type":27,"tag":261,"props":11201,"children":11202},{"style":274},[11203],{"type":33,"value":277},{"type":27,"tag":261,"props":11205,"children":11206},{"style":268},[11207],{"type":33,"value":11208}," cp.sqrt(cp.sum(diff",{"type":27,"tag":261,"props":11210,"children":11211},{"style":274},[11212],{"type":33,"value":11213},"**",{"type":27,"tag":261,"props":11215,"children":11216},{"style":295},[11217],{"type":33,"value":8314},{"type":27,"tag":261,"props":11219,"children":11220},{"style":268},[11221],{"type":33,"value":303},{"type":27,"tag":261,"props":11223,"children":11224},{"style":285},[11225],{"type":33,"value":11039},{"type":27,"tag":261,"props":11227,"children":11228},{"style":274},[11229],{"type":33,"value":277},{"type":27,"tag":261,"props":11231,"children":11232},{"style":295},[11233],{"type":33,"value":8314},{"type":27,"tag":261,"props":11235,"children":11236},{"style":268},[11237],{"type":33,"value":1035},{"type":27,"tag":261,"props":11239,"children":11240},{"class":263,"line":1268},[11241],{"type":27,"tag":261,"props":11242,"children":11243},{"emptyLinePlaceholder":22},[11244],{"type":33,"value":766},{"type":27,"tag":261,"props":11246,"children":11247},{"class":263,"line":1277},[11248],{"type":27,"tag":261,"props":11249,"children":11250},{"style":478},[11251],{"type":33,"value":11252},"    # avoid division by zero\n",{"type":27,"tag":261,"props":11254,"children":11255},{"class":263,"line":1286},[11256,11261,11265],{"type":27,"tag":261,"props":11257,"children":11258},{"style":268},[11259],{"type":33,"value":11260},"    epsilon ",{"type":27,"tag":261,"props":11262,"children":11263},{"style":274},[11264],{"type":33,"value":277},{"type":27,"tag":261,"props":11266,"children":11267},{"style":295},[11268],{"type":33,"value":11269}," 1e-5\n",{"type":27,"tag":261,"props":11271,"children":11272},{"class":263,"line":1295},[11273,11278,11282,11287,11292],{"type":27,"tag":261,"props":11274,"children":11275},{"style":268},[11276],{"type":33,"value":11277},"    inv_distances ",{"type":27,"tag":261,"props":11279,"children":11280},{"style":274},[11281],{"type":33,"value":277},{"type":27,"tag":261,"props":11283,"children":11284},{"style":295},[11285],{"type":33,"value":11286}," 1.0",{"type":27,"tag":261,"props":11288,"children":11289},{"style":274},[11290],{"type":33,"value":11291}," /",{"type":27,"tag":261,"props":11293,"children":11294},{"style":268},[11295],{"type":33,"value":11296}," cp.maximum(distances, epsilon)\n",{"type":27,"tag":261,"props":11298,"children":11299},{"class":263,"line":1304},[11300],{"type":27,"tag":261,"props":11301,"children":11302},{"emptyLinePlaceholder":22},[11303],{"type":33,"value":766},{"type":27,"tag":261,"props":11305,"children":11306},{"class":263,"line":1312},[11307],{"type":27,"tag":261,"props":11308,"children":11309},{"style":478},[11310],{"type":33,"value":11311},"    # calculate forces\n",{"type":27,"tag":261,"props":11313,"children":11314},{"class":263,"line":2102},[11315,11320,11324,11329,11333,11338,11342,11347,11352,11356,11360,11364],{"type":27,"tag":261,"props":11316,"children":11317},{"style":268},[11318],{"type":33,"value":11319},"    cp_forces ",{"type":27,"tag":261,"props":11321,"children":11322},{"style":274},[11323],{"type":33,"value":277},{"type":27,"tag":261,"props":11325,"children":11326},{"style":268},[11327],{"type":33,"value":11328}," cp.sum((diff.T ",{"type":27,"tag":261,"props":11330,"children":11331},{"style":274},[11332],{"type":33,"value":8885},{"type":27,"tag":261,"props":11334,"children":11335},{"style":268},[11336],{"type":33,"value":11337}," inv_distances",{"type":27,"tag":261,"props":11339,"children":11340},{"style":274},[11341],{"type":33,"value":11213},{"type":27,"tag":261,"props":11343,"children":11344},{"style":295},[11345],{"type":33,"value":11346},"3",{"type":27,"tag":261,"props":11348,"children":11349},{"style":268},[11350],{"type":33,"value":11351},").T, ",{"type":27,"tag":261,"props":11353,"children":11354},{"style":285},[11355],{"type":33,"value":11039},{"type":27,"tag":261,"props":11357,"children":11358},{"style":274},[11359],{"type":33,"value":277},{"type":27,"tag":261,"props":11361,"children":11362},{"style":295},[11363],{"type":33,"value":1532},{"type":27,"tag":261,"props":11365,"children":11366},{"style":268},[11367],{"type":33,"value":340},{"type":27,"tag":261,"props":11369,"children":11370},{"class":263,"line":2131},[11371],{"type":27,"tag":261,"props":11372,"children":11373},{"emptyLinePlaceholder":22},[11374],{"type":33,"value":766},{"type":27,"tag":261,"props":11376,"children":11378},{"class":263,"line":11377},37,[11379],{"type":27,"tag":261,"props":11380,"children":11381},{"style":478},[11382],{"type":33,"value":11383},"    # update velocities and positions\n",{"type":27,"tag":261,"props":11385,"children":11387},{"class":263,"line":11386},38,[11388,11393,11398,11403,11408],{"type":27,"tag":261,"props":11389,"children":11390},{"style":268},[11391],{"type":33,"value":11392},"    cp_velocities ",{"type":27,"tag":261,"props":11394,"children":11395},{"style":274},[11396],{"type":33,"value":11397},"+=",{"type":27,"tag":261,"props":11399,"children":11400},{"style":295},[11401],{"type":33,"value":11402}," DT",{"type":27,"tag":261,"props":11404,"children":11405},{"style":274},[11406],{"type":33,"value":11407}," *",{"type":27,"tag":261,"props":11409,"children":11410},{"style":268},[11411],{"type":33,"value":11412}," cp_forces\n",{"type":27,"tag":261,"props":11414,"children":11416},{"class":263,"line":11415},39,[11417,11422,11426,11430,11434],{"type":27,"tag":261,"props":11418,"children":11419},{"style":268},[11420],{"type":33,"value":11421},"    cp_positions ",{"type":27,"tag":261,"props":11423,"children":11424},{"style":274},[11425],{"type":33,"value":11397},{"type":27,"tag":261,"props":11427,"children":11428},{"style":295},[11429],{"type":33,"value":11402},{"type":27,"tag":261,"props":11431,"children":11432},{"style":274},[11433],{"type":33,"value":11407},{"type":27,"tag":261,"props":11435,"children":11436},{"style":268},[11437],{"type":33,"value":11438}," cp_velocities\n",{"type":27,"tag":261,"props":11440,"children":11442},{"class":263,"line":11441},40,[11443,11448,11452,11457,11461,11466,11470],{"type":27,"tag":261,"props":11444,"children":11445},{"style":268},[11446],{"type":33,"value":11447},"    cp_ticks ",{"type":27,"tag":261,"props":11449,"children":11450},{"style":274},[11451],{"type":33,"value":277},{"type":27,"tag":261,"props":11453,"children":11454},{"style":268},[11455],{"type":33,"value":11456}," cp.append(cp_ticks, cp.expand_dims(cp_positions, ",{"type":27,"tag":261,"props":11458,"children":11459},{"style":295},[11460],{"type":33,"value":9205},{"type":27,"tag":261,"props":11462,"children":11463},{"style":268},[11464],{"type":33,"value":11465},"), ",{"type":27,"tag":261,"props":11467,"children":11468},{"style":295},[11469],{"type":33,"value":9205},{"type":27,"tag":261,"props":11471,"children":11472},{"style":268},[11473],{"type":33,"value":340},{"type":27,"tag":261,"props":11475,"children":11477},{"class":263,"line":11476},41,[11478],{"type":27,"tag":261,"props":11479,"children":11480},{"emptyLinePlaceholder":22},[11481],{"type":33,"value":766},{"type":27,"tag":261,"props":11483,"children":11485},{"class":263,"line":11484},42,[11486,11491,11495,11500,11504],{"type":27,"tag":261,"props":11487,"children":11488},{"style":268},[11489],{"type":33,"value":11490},"sim_time ",{"type":27,"tag":261,"props":11492,"children":11493},{"style":274},[11494],{"type":33,"value":277},{"type":27,"tag":261,"props":11496,"children":11497},{"style":268},[11498],{"type":33,"value":11499}," time.time() ",{"type":27,"tag":261,"props":11501,"children":11502},{"style":274},[11503],{"type":33,"value":8281},{"type":27,"tag":261,"props":11505,"children":11506},{"style":268},[11507],{"type":33,"value":11508}," start_time\n",{"type":27,"tag":261,"props":11510,"children":11512},{"class":263,"line":11511},43,[11513,11517,11521,11526],{"type":27,"tag":261,"props":11514,"children":11515},{"style":1520},[11516],{"type":33,"value":4157},{"type":27,"tag":261,"props":11518,"children":11519},{"style":268},[11520],{"type":33,"value":740},{"type":27,"tag":261,"props":11522,"children":11523},{"style":493},[11524],{"type":33,"value":11525},"\"Simulation time:\"",{"type":27,"tag":261,"props":11527,"children":11528},{"style":268},[11529],{"type":33,"value":11530},", sim_time)\n",{"type":27,"tag":261,"props":11532,"children":11534},{"class":263,"line":11533},44,[11535],{"type":27,"tag":261,"props":11536,"children":11537},{"emptyLinePlaceholder":22},[11538],{"type":33,"value":766},{"type":27,"tag":261,"props":11540,"children":11542},{"class":263,"line":11541},45,[11543],{"type":27,"tag":261,"props":11544,"children":11545},{"emptyLinePlaceholder":22},[11546],{"type":33,"value":766},{"type":27,"tag":261,"props":11548,"children":11550},{"class":263,"line":11549},46,[11551,11555,11560,11564,11568,11572,11577],{"type":27,"tag":261,"props":11552,"children":11553},{"style":487},[11554],{"type":33,"value":729},{"type":27,"tag":261,"props":11556,"children":11557},{"style":732},[11558],{"type":33,"value":11559}," NumpyArrayEncoder",{"type":27,"tag":261,"props":11561,"children":11562},{"style":268},[11563],{"type":33,"value":740},{"type":27,"tag":261,"props":11565,"children":11566},{"style":743},[11567],{"type":33,"value":7899},{"type":27,"tag":261,"props":11569,"children":11570},{"style":268},[11571],{"type":33,"value":2970},{"type":27,"tag":261,"props":11573,"children":11574},{"style":743},[11575],{"type":33,"value":11576},"JSONEncoder",{"type":27,"tag":261,"props":11578,"children":11579},{"style":268},[11580],{"type":33,"value":750},{"type":27,"tag":261,"props":11582,"children":11584},{"class":263,"line":11583},47,[11585,11589,11594,11598,11602,11606,11611],{"type":27,"tag":261,"props":11586,"children":11587},{"style":487},[11588],{"type":33,"value":817},{"type":27,"tag":261,"props":11590,"children":11591},{"style":820},[11592],{"type":33,"value":11593}," default",{"type":27,"tag":261,"props":11595,"children":11596},{"style":268},[11597],{"type":33,"value":740},{"type":27,"tag":261,"props":11599,"children":11600},{"style":830},[11601],{"type":33,"value":833},{"type":27,"tag":261,"props":11603,"children":11604},{"style":268},[11605],{"type":33,"value":303},{"type":27,"tag":261,"props":11607,"children":11608},{"style":830},[11609],{"type":33,"value":11610},"obj",{"type":27,"tag":261,"props":11612,"children":11613},{"style":268},[11614],{"type":33,"value":750},{"type":27,"tag":261,"props":11616,"children":11618},{"class":263,"line":11617},48,[11619,11623,11628],{"type":27,"tag":261,"props":11620,"children":11621},{"style":274},[11622],{"type":33,"value":9033},{"type":27,"tag":261,"props":11624,"children":11625},{"style":1520},[11626],{"type":33,"value":11627}," isinstance",{"type":27,"tag":261,"props":11629,"children":11630},{"style":268},[11631],{"type":33,"value":11632},"(obj, np.ndarray):\n",{"type":27,"tag":261,"props":11634,"children":11636},{"class":263,"line":11635},49,[11637,11642],{"type":27,"tag":261,"props":11638,"children":11639},{"style":274},[11640],{"type":33,"value":11641},"            return",{"type":27,"tag":261,"props":11643,"children":11644},{"style":268},[11645],{"type":33,"value":11646}," obj.tolist()\n",{"type":27,"tag":261,"props":11648,"children":11650},{"class":263,"line":11649},50,[11651,11655,11660,11664],{"type":27,"tag":261,"props":11652,"children":11653},{"style":274},[11654],{"type":33,"value":1318},{"type":27,"tag":261,"props":11656,"children":11657},{"style":268},[11658],{"type":33,"value":11659}," json.JSONEncoder.default(",{"type":27,"tag":261,"props":11661,"children":11662},{"style":873},[11663],{"type":33,"value":833},{"type":27,"tag":261,"props":11665,"children":11666},{"style":268},[11667],{"type":33,"value":11668},", obj)\n",{"type":27,"tag":261,"props":11670,"children":11672},{"class":263,"line":11671},51,[11673],{"type":27,"tag":261,"props":11674,"children":11675},{"emptyLinePlaceholder":22},[11676],{"type":33,"value":766},{"type":27,"tag":261,"props":11678,"children":11680},{"class":263,"line":11679},52,[11681],{"type":27,"tag":261,"props":11682,"children":11683},{"emptyLinePlaceholder":22},[11684],{"type":33,"value":766},{"type":27,"tag":261,"props":11686,"children":11688},{"class":263,"line":11687},53,[11689,11693,11697],{"type":27,"tag":261,"props":11690,"children":11691},{"style":268},[11692],{"type":33,"value":11025},{"type":27,"tag":261,"props":11694,"children":11695},{"style":274},[11696],{"type":33,"value":277},{"type":27,"tag":261,"props":11698,"children":11699},{"style":268},[11700],{"type":33,"value":11701}," cp_ticks.get()\n",{"type":27,"tag":261,"props":11703,"children":11705},{"class":263,"line":11704},54,[11706],{"type":27,"tag":261,"props":11707,"children":11708},{"emptyLinePlaceholder":22},[11709],{"type":33,"value":766},{"type":27,"tag":261,"props":11711,"children":11713},{"class":263,"line":11712},55,[11714],{"type":27,"tag":261,"props":11715,"children":11716},{"emptyLinePlaceholder":22},[11717],{"type":33,"value":766},{"type":27,"tag":261,"props":11719,"children":11721},{"class":263,"line":11720},56,[11722],{"type":27,"tag":261,"props":11723,"children":11724},{"style":478},[11725],{"type":33,"value":11726},"# this is data we can work with in python and write to a file\n",{"type":27,"tag":261,"props":11728,"children":11730},{"class":263,"line":11729},57,[11731,11736,11740,11744,11749,11753,11758,11762,11766],{"type":27,"tag":261,"props":11732,"children":11733},{"style":274},[11734],{"type":33,"value":11735},"with",{"type":27,"tag":261,"props":11737,"children":11738},{"style":1520},[11739],{"type":33,"value":1558},{"type":27,"tag":261,"props":11741,"children":11742},{"style":268},[11743],{"type":33,"value":740},{"type":27,"tag":261,"props":11745,"children":11746},{"style":493},[11747],{"type":33,"value":11748},"\"ticks.json\"",{"type":27,"tag":261,"props":11750,"children":11751},{"style":268},[11752],{"type":33,"value":303},{"type":27,"tag":261,"props":11754,"children":11755},{"style":493},[11756],{"type":33,"value":11757},"\"w\"",{"type":27,"tag":261,"props":11759,"children":11760},{"style":268},[11761],{"type":33,"value":1606},{"type":27,"tag":261,"props":11763,"children":11764},{"style":274},[11765],{"type":33,"value":1611},{"type":27,"tag":261,"props":11767,"children":11768},{"style":268},[11769],{"type":33,"value":1616},{"type":27,"tag":261,"props":11771,"children":11773},{"class":263,"line":11772},58,[11774,11779,11784,11788],{"type":27,"tag":261,"props":11775,"children":11776},{"style":268},[11777],{"type":33,"value":11778},"    f.write(json.dumps(np_ticks, ",{"type":27,"tag":261,"props":11780,"children":11781},{"style":285},[11782],{"type":33,"value":11783},"cls",{"type":27,"tag":261,"props":11785,"children":11786},{"style":274},[11787],{"type":33,"value":277},{"type":27,"tag":261,"props":11789,"children":11790},{"style":268},[11791],{"type":33,"value":11792},"NumpyArrayEncoder))\n",{"type":27,"tag":36,"props":11794,"children":11795},{},[11796],{"type":33,"value":11797},"To better understand the matrix math here I walked through a simple example of what each step does:",{"type":27,"tag":238,"props":11799,"children":11801},{"code":11800,"language":10731,"meta":8,"className":10732,"style":8},"# particle coordinates (x,y,z) in 3D space\npositions = cp.array([[1,2.5,3], [4,5,6], [7,8,9]])\n",[11802],{"type":27,"tag":170,"props":11803,"children":11804},{"__ignoreMap":8},[11805,11813],{"type":27,"tag":261,"props":11806,"children":11807},{"class":263,"line":264},[11808],{"type":27,"tag":261,"props":11809,"children":11810},{"style":478},[11811],{"type":33,"value":11812},"# particle coordinates (x,y,z) in 3D space\n",{"type":27,"tag":261,"props":11814,"children":11815},{"class":263,"line":343},[11816,11821,11825,11830,11834,11839,11844,11848,11852,11857,11861,11865,11869,11873,11878,11882,11887,11891,11895,11899,11904],{"type":27,"tag":261,"props":11817,"children":11818},{"style":268},[11819],{"type":33,"value":11820},"positions ",{"type":27,"tag":261,"props":11822,"children":11823},{"style":274},[11824],{"type":33,"value":277},{"type":27,"tag":261,"props":11826,"children":11827},{"style":268},[11828],{"type":33,"value":11829}," cp.array([[",{"type":27,"tag":261,"props":11831,"children":11832},{"style":295},[11833],{"type":33,"value":1532},{"type":27,"tag":261,"props":11835,"children":11836},{"style":268},[11837],{"type":33,"value":11838},",",{"type":27,"tag":261,"props":11840,"children":11841},{"style":295},[11842],{"type":33,"value":11843},"2.5",{"type":27,"tag":261,"props":11845,"children":11846},{"style":268},[11847],{"type":33,"value":11838},{"type":27,"tag":261,"props":11849,"children":11850},{"style":295},[11851],{"type":33,"value":11346},{"type":27,"tag":261,"props":11853,"children":11854},{"style":268},[11855],{"type":33,"value":11856},"], [",{"type":27,"tag":261,"props":11858,"children":11859},{"style":295},[11860],{"type":33,"value":439},{"type":27,"tag":261,"props":11862,"children":11863},{"style":268},[11864],{"type":33,"value":11838},{"type":27,"tag":261,"props":11866,"children":11867},{"style":295},[11868],{"type":33,"value":10698},{"type":27,"tag":261,"props":11870,"children":11871},{"style":268},[11872],{"type":33,"value":11838},{"type":27,"tag":261,"props":11874,"children":11875},{"style":295},[11876],{"type":33,"value":11877},"6",{"type":27,"tag":261,"props":11879,"children":11880},{"style":268},[11881],{"type":33,"value":11856},{"type":27,"tag":261,"props":11883,"children":11884},{"style":295},[11885],{"type":33,"value":11886},"7",{"type":27,"tag":261,"props":11888,"children":11889},{"style":268},[11890],{"type":33,"value":11838},{"type":27,"tag":261,"props":11892,"children":11893},{"style":295},[11894],{"type":33,"value":10682},{"type":27,"tag":261,"props":11896,"children":11897},{"style":268},[11898],{"type":33,"value":11838},{"type":27,"tag":261,"props":11900,"children":11901},{"style":295},[11902],{"type":33,"value":11903},"9",{"type":27,"tag":261,"props":11905,"children":11906},{"style":268},[11907],{"type":33,"value":11908},"]])\n",{"type":27,"tag":36,"props":11910,"children":11911},{},[11912],{"type":33,"value":11913},"The first operation creates an array for pairwise distances for each dimension:",{"type":27,"tag":238,"props":11915,"children":11917},{"code":11916,"language":254,"meta":8,"className":255,"style":8},"diff = positions[None, :, :] - positions[:, None, :]\nprint(diff)\n\narray([[[ 0. ,  0. ,  0. ],\n        [ 3. ,  2.5,  3. ],\n        [ 6. ,  5.5,  6. ]],\n\n       [[-3. , -2.5, -3. ],\n        [ 0. ,  0. ,  0. ],\n        [ 3. ,  3. ,  3. ]],\n\n       [[-6. , -5.5, -6. ],\n        [-3. , -3. , -3. ],\n        [ 0. ,  0. ,  0. ]]])\n",[11918],{"type":27,"tag":170,"props":11919,"children":11920},{"__ignoreMap":8},[11921,11965,11977,11984,12018,12051,12084,12091,12136,12167,12198,12205,12248,12291],{"type":27,"tag":261,"props":11922,"children":11923},{"class":263,"line":264},[11924,11929,11933,11938,11942,11947,11951,11956,11960],{"type":27,"tag":261,"props":11925,"children":11926},{"style":268},[11927],{"type":33,"value":11928},"diff ",{"type":27,"tag":261,"props":11930,"children":11931},{"style":274},[11932],{"type":33,"value":277},{"type":27,"tag":261,"props":11934,"children":11935},{"style":268},[11936],{"type":33,"value":11937}," positions[",{"type":27,"tag":261,"props":11939,"children":11940},{"style":295},[11941],{"type":33,"value":4413},{"type":27,"tag":261,"props":11943,"children":11944},{"style":268},[11945],{"type":33,"value":11946},", :, :] ",{"type":27,"tag":261,"props":11948,"children":11949},{"style":274},[11950],{"type":33,"value":8281},{"type":27,"tag":261,"props":11952,"children":11953},{"style":268},[11954],{"type":33,"value":11955}," positions[:, ",{"type":27,"tag":261,"props":11957,"children":11958},{"style":295},[11959],{"type":33,"value":4413},{"type":27,"tag":261,"props":11961,"children":11962},{"style":268},[11963],{"type":33,"value":11964},", :]\n",{"type":27,"tag":261,"props":11966,"children":11967},{"class":263,"line":343},[11968,11972],{"type":27,"tag":261,"props":11969,"children":11970},{"style":1520},[11971],{"type":33,"value":4157},{"type":27,"tag":261,"props":11973,"children":11974},{"style":268},[11975],{"type":33,"value":11976},"(diff)\n",{"type":27,"tag":261,"props":11978,"children":11979},{"class":263,"line":379},[11980],{"type":27,"tag":261,"props":11981,"children":11982},{"emptyLinePlaceholder":22},[11983],{"type":33,"value":766},{"type":27,"tag":261,"props":11985,"children":11986},{"class":263,"line":537},[11987,11992,11996,12001,12005,12009,12013],{"type":27,"tag":261,"props":11988,"children":11989},{"style":268},[11990],{"type":33,"value":11991},"array([[[ ",{"type":27,"tag":261,"props":11993,"children":11994},{"style":295},[11995],{"type":33,"value":9205},{"type":27,"tag":261,"props":11997,"children":11998},{"style":268},[11999],{"type":33,"value":12000},". ,  ",{"type":27,"tag":261,"props":12002,"children":12003},{"style":295},[12004],{"type":33,"value":9205},{"type":27,"tag":261,"props":12006,"children":12007},{"style":268},[12008],{"type":33,"value":12000},{"type":27,"tag":261,"props":12010,"children":12011},{"style":295},[12012],{"type":33,"value":9205},{"type":27,"tag":261,"props":12014,"children":12015},{"style":268},[12016],{"type":33,"value":12017},". ],\n",{"type":27,"tag":261,"props":12019,"children":12020},{"class":263,"line":561},[12021,12026,12030,12034,12038,12043,12047],{"type":27,"tag":261,"props":12022,"children":12023},{"style":268},[12024],{"type":33,"value":12025},"        [ ",{"type":27,"tag":261,"props":12027,"children":12028},{"style":295},[12029],{"type":33,"value":11346},{"type":27,"tag":261,"props":12031,"children":12032},{"style":268},[12033],{"type":33,"value":12000},{"type":27,"tag":261,"props":12035,"children":12036},{"style":295},[12037],{"type":33,"value":11843},{"type":27,"tag":261,"props":12039,"children":12040},{"style":268},[12041],{"type":33,"value":12042},",  ",{"type":27,"tag":261,"props":12044,"children":12045},{"style":295},[12046],{"type":33,"value":11346},{"type":27,"tag":261,"props":12048,"children":12049},{"style":268},[12050],{"type":33,"value":12017},{"type":27,"tag":261,"props":12052,"children":12053},{"class":263,"line":785},[12054,12058,12062,12066,12071,12075,12079],{"type":27,"tag":261,"props":12055,"children":12056},{"style":268},[12057],{"type":33,"value":12025},{"type":27,"tag":261,"props":12059,"children":12060},{"style":295},[12061],{"type":33,"value":11877},{"type":27,"tag":261,"props":12063,"children":12064},{"style":268},[12065],{"type":33,"value":12000},{"type":27,"tag":261,"props":12067,"children":12068},{"style":295},[12069],{"type":33,"value":12070},"5.5",{"type":27,"tag":261,"props":12072,"children":12073},{"style":268},[12074],{"type":33,"value":12042},{"type":27,"tag":261,"props":12076,"children":12077},{"style":295},[12078],{"type":33,"value":11877},{"type":27,"tag":261,"props":12080,"children":12081},{"style":268},[12082],{"type":33,"value":12083},". ]],\n",{"type":27,"tag":261,"props":12085,"children":12086},{"class":263,"line":794},[12087],{"type":27,"tag":261,"props":12088,"children":12089},{"emptyLinePlaceholder":22},[12090],{"type":33,"value":766},{"type":27,"tag":261,"props":12092,"children":12093},{"class":263,"line":803},[12094,12099,12103,12107,12112,12116,12120,12124,12128,12132],{"type":27,"tag":261,"props":12095,"children":12096},{"style":268},[12097],{"type":33,"value":12098},"       [[",{"type":27,"tag":261,"props":12100,"children":12101},{"style":274},[12102],{"type":33,"value":8281},{"type":27,"tag":261,"props":12104,"children":12105},{"style":295},[12106],{"type":33,"value":11346},{"type":27,"tag":261,"props":12108,"children":12109},{"style":268},[12110],{"type":33,"value":12111},". , ",{"type":27,"tag":261,"props":12113,"children":12114},{"style":274},[12115],{"type":33,"value":8281},{"type":27,"tag":261,"props":12117,"children":12118},{"style":295},[12119],{"type":33,"value":11843},{"type":27,"tag":261,"props":12121,"children":12122},{"style":268},[12123],{"type":33,"value":303},{"type":27,"tag":261,"props":12125,"children":12126},{"style":274},[12127],{"type":33,"value":8281},{"type":27,"tag":261,"props":12129,"children":12130},{"style":295},[12131],{"type":33,"value":11346},{"type":27,"tag":261,"props":12133,"children":12134},{"style":268},[12135],{"type":33,"value":12017},{"type":27,"tag":261,"props":12137,"children":12138},{"class":263,"line":811},[12139,12143,12147,12151,12155,12159,12163],{"type":27,"tag":261,"props":12140,"children":12141},{"style":268},[12142],{"type":33,"value":12025},{"type":27,"tag":261,"props":12144,"children":12145},{"style":295},[12146],{"type":33,"value":9205},{"type":27,"tag":261,"props":12148,"children":12149},{"style":268},[12150],{"type":33,"value":12000},{"type":27,"tag":261,"props":12152,"children":12153},{"style":295},[12154],{"type":33,"value":9205},{"type":27,"tag":261,"props":12156,"children":12157},{"style":268},[12158],{"type":33,"value":12000},{"type":27,"tag":261,"props":12160,"children":12161},{"style":295},[12162],{"type":33,"value":9205},{"type":27,"tag":261,"props":12164,"children":12165},{"style":268},[12166],{"type":33,"value":12017},{"type":27,"tag":261,"props":12168,"children":12169},{"class":263,"line":860},[12170,12174,12178,12182,12186,12190,12194],{"type":27,"tag":261,"props":12171,"children":12172},{"style":268},[12173],{"type":33,"value":12025},{"type":27,"tag":261,"props":12175,"children":12176},{"style":295},[12177],{"type":33,"value":11346},{"type":27,"tag":261,"props":12179,"children":12180},{"style":268},[12181],{"type":33,"value":12000},{"type":27,"tag":261,"props":12183,"children":12184},{"style":295},[12185],{"type":33,"value":11346},{"type":27,"tag":261,"props":12187,"children":12188},{"style":268},[12189],{"type":33,"value":12000},{"type":27,"tag":261,"props":12191,"children":12192},{"style":295},[12193],{"type":33,"value":11346},{"type":27,"tag":261,"props":12195,"children":12196},{"style":268},[12197],{"type":33,"value":12083},{"type":27,"tag":261,"props":12199,"children":12200},{"class":263,"line":884},[12201],{"type":27,"tag":261,"props":12202,"children":12203},{"emptyLinePlaceholder":22},[12204],{"type":33,"value":766},{"type":27,"tag":261,"props":12206,"children":12207},{"class":263,"line":902},[12208,12212,12216,12220,12224,12228,12232,12236,12240,12244],{"type":27,"tag":261,"props":12209,"children":12210},{"style":268},[12211],{"type":33,"value":12098},{"type":27,"tag":261,"props":12213,"children":12214},{"style":274},[12215],{"type":33,"value":8281},{"type":27,"tag":261,"props":12217,"children":12218},{"style":295},[12219],{"type":33,"value":11877},{"type":27,"tag":261,"props":12221,"children":12222},{"style":268},[12223],{"type":33,"value":12111},{"type":27,"tag":261,"props":12225,"children":12226},{"style":274},[12227],{"type":33,"value":8281},{"type":27,"tag":261,"props":12229,"children":12230},{"style":295},[12231],{"type":33,"value":12070},{"type":27,"tag":261,"props":12233,"children":12234},{"style":268},[12235],{"type":33,"value":303},{"type":27,"tag":261,"props":12237,"children":12238},{"style":274},[12239],{"type":33,"value":8281},{"type":27,"tag":261,"props":12241,"children":12242},{"style":295},[12243],{"type":33,"value":11877},{"type":27,"tag":261,"props":12245,"children":12246},{"style":268},[12247],{"type":33,"value":12017},{"type":27,"tag":261,"props":12249,"children":12250},{"class":263,"line":911},[12251,12255,12259,12263,12267,12271,12275,12279,12283,12287],{"type":27,"tag":261,"props":12252,"children":12253},{"style":268},[12254],{"type":33,"value":8852},{"type":27,"tag":261,"props":12256,"children":12257},{"style":274},[12258],{"type":33,"value":8281},{"type":27,"tag":261,"props":12260,"children":12261},{"style":295},[12262],{"type":33,"value":11346},{"type":27,"tag":261,"props":12264,"children":12265},{"style":268},[12266],{"type":33,"value":12111},{"type":27,"tag":261,"props":12268,"children":12269},{"style":274},[12270],{"type":33,"value":8281},{"type":27,"tag":261,"props":12272,"children":12273},{"style":295},[12274],{"type":33,"value":11346},{"type":27,"tag":261,"props":12276,"children":12277},{"style":268},[12278],{"type":33,"value":12111},{"type":27,"tag":261,"props":12280,"children":12281},{"style":274},[12282],{"type":33,"value":8281},{"type":27,"tag":261,"props":12284,"children":12285},{"style":295},[12286],{"type":33,"value":11346},{"type":27,"tag":261,"props":12288,"children":12289},{"style":268},[12290],{"type":33,"value":12017},{"type":27,"tag":261,"props":12292,"children":12293},{"class":263,"line":934},[12294,12298,12302,12306,12310,12314,12318],{"type":27,"tag":261,"props":12295,"children":12296},{"style":268},[12297],{"type":33,"value":12025},{"type":27,"tag":261,"props":12299,"children":12300},{"style":295},[12301],{"type":33,"value":9205},{"type":27,"tag":261,"props":12303,"children":12304},{"style":268},[12305],{"type":33,"value":12000},{"type":27,"tag":261,"props":12307,"children":12308},{"style":295},[12309],{"type":33,"value":9205},{"type":27,"tag":261,"props":12311,"children":12312},{"style":268},[12313],{"type":33,"value":12000},{"type":27,"tag":261,"props":12315,"children":12316},{"style":295},[12317],{"type":33,"value":9205},{"type":27,"tag":261,"props":12319,"children":12320},{"style":268},[12321],{"type":33,"value":12322},". ]]])\n",{"type":27,"tag":36,"props":12324,"children":12325},{},[12326,12328,12333,12334,12340,12341,12347],{"type":33,"value":12327},"The rows of zeros correspond to a particle's ",{"type":27,"tag":170,"props":12329,"children":12331},{"className":12330},[],[12332],{"type":33,"value":5588},{"type":33,"value":303},{"type":27,"tag":170,"props":12335,"children":12337},{"className":12336},[],[12338],{"type":33,"value":12339},"y",{"type":33,"value":4901},{"type":27,"tag":170,"props":12342,"children":12344},{"className":12343},[],[12345],{"type":33,"value":12346},"z",{"type":33,"value":12348}," distances to itself, which are all zero by axioms of Euclidian vector spaces.",{"type":27,"tag":36,"props":12350,"children":12351},{},[12352],{"type":33,"value":12353},"The next operation calculates the distance between each particle:",{"type":27,"tag":238,"props":12355,"children":12357},{"code":12356,"language":254,"meta":8,"className":255,"style":8},"distances = cp.sqrt(cp.sum(diff**2, axis=2))\nprint(distances)\n\narray([[ 0.        ,  4.9244289 , 10.11187421],\n       [ 4.9244289 ,  0.        ,  5.19615242],\n       [10.11187421,  5.19615242,  0.        ]])\n",[12358],{"type":27,"tag":170,"props":12359,"children":12360},{"__ignoreMap":8},[12361,12405,12417,12424,12460,12494],{"type":27,"tag":261,"props":12362,"children":12363},{"class":263,"line":264},[12364,12369,12373,12377,12381,12385,12389,12393,12397,12401],{"type":27,"tag":261,"props":12365,"children":12366},{"style":268},[12367],{"type":33,"value":12368},"distances ",{"type":27,"tag":261,"props":12370,"children":12371},{"style":274},[12372],{"type":33,"value":277},{"type":27,"tag":261,"props":12374,"children":12375},{"style":268},[12376],{"type":33,"value":11208},{"type":27,"tag":261,"props":12378,"children":12379},{"style":274},[12380],{"type":33,"value":11213},{"type":27,"tag":261,"props":12382,"children":12383},{"style":295},[12384],{"type":33,"value":8314},{"type":27,"tag":261,"props":12386,"children":12387},{"style":268},[12388],{"type":33,"value":303},{"type":27,"tag":261,"props":12390,"children":12391},{"style":285},[12392],{"type":33,"value":11039},{"type":27,"tag":261,"props":12394,"children":12395},{"style":274},[12396],{"type":33,"value":277},{"type":27,"tag":261,"props":12398,"children":12399},{"style":295},[12400],{"type":33,"value":8314},{"type":27,"tag":261,"props":12402,"children":12403},{"style":268},[12404],{"type":33,"value":1035},{"type":27,"tag":261,"props":12406,"children":12407},{"class":263,"line":343},[12408,12412],{"type":27,"tag":261,"props":12409,"children":12410},{"style":1520},[12411],{"type":33,"value":4157},{"type":27,"tag":261,"props":12413,"children":12414},{"style":268},[12415],{"type":33,"value":12416},"(distances)\n",{"type":27,"tag":261,"props":12418,"children":12419},{"class":263,"line":379},[12420],{"type":27,"tag":261,"props":12421,"children":12422},{"emptyLinePlaceholder":22},[12423],{"type":33,"value":766},{"type":27,"tag":261,"props":12425,"children":12426},{"class":263,"line":537},[12427,12432,12436,12441,12446,12451,12456],{"type":27,"tag":261,"props":12428,"children":12429},{"style":268},[12430],{"type":33,"value":12431},"array([[ ",{"type":27,"tag":261,"props":12433,"children":12434},{"style":295},[12435],{"type":33,"value":9205},{"type":27,"tag":261,"props":12437,"children":12438},{"style":268},[12439],{"type":33,"value":12440},".        ,  ",{"type":27,"tag":261,"props":12442,"children":12443},{"style":295},[12444],{"type":33,"value":12445},"4.9244289",{"type":27,"tag":261,"props":12447,"children":12448},{"style":268},[12449],{"type":33,"value":12450}," , ",{"type":27,"tag":261,"props":12452,"children":12453},{"style":295},[12454],{"type":33,"value":12455},"10.11187421",{"type":27,"tag":261,"props":12457,"children":12458},{"style":268},[12459],{"type":33,"value":4561},{"type":27,"tag":261,"props":12461,"children":12462},{"class":263,"line":561},[12463,12468,12472,12477,12481,12485,12490],{"type":27,"tag":261,"props":12464,"children":12465},{"style":268},[12466],{"type":33,"value":12467},"       [ ",{"type":27,"tag":261,"props":12469,"children":12470},{"style":295},[12471],{"type":33,"value":12445},{"type":27,"tag":261,"props":12473,"children":12474},{"style":268},[12475],{"type":33,"value":12476}," ,  ",{"type":27,"tag":261,"props":12478,"children":12479},{"style":295},[12480],{"type":33,"value":9205},{"type":27,"tag":261,"props":12482,"children":12483},{"style":268},[12484],{"type":33,"value":12440},{"type":27,"tag":261,"props":12486,"children":12487},{"style":295},[12488],{"type":33,"value":12489},"5.19615242",{"type":27,"tag":261,"props":12491,"children":12492},{"style":268},[12493],{"type":33,"value":4561},{"type":27,"tag":261,"props":12495,"children":12496},{"class":263,"line":785},[12497,12502,12506,12510,12514,12518,12522],{"type":27,"tag":261,"props":12498,"children":12499},{"style":268},[12500],{"type":33,"value":12501},"       [",{"type":27,"tag":261,"props":12503,"children":12504},{"style":295},[12505],{"type":33,"value":12455},{"type":27,"tag":261,"props":12507,"children":12508},{"style":268},[12509],{"type":33,"value":12042},{"type":27,"tag":261,"props":12511,"children":12512},{"style":295},[12513],{"type":33,"value":12489},{"type":27,"tag":261,"props":12515,"children":12516},{"style":268},[12517],{"type":33,"value":12042},{"type":27,"tag":261,"props":12519,"children":12520},{"style":295},[12521],{"type":33,"value":9205},{"type":27,"tag":261,"props":12523,"children":12524},{"style":268},[12525],{"type":33,"value":12526},".        ]])\n",{"type":27,"tag":36,"props":12528,"children":12529},{},[12530,12532,12538],{"type":33,"value":12531},"The diagonal or zeros represents that fact that a particle ",{"type":27,"tag":170,"props":12533,"children":12535},{"className":12534},[],[12536],{"type":33,"value":12537},"n",{"type":33,"value":12539}," has a distance of zero to iself.",{"type":27,"tag":36,"props":12541,"children":12542},{},[12543,12545,12551],{"type":33,"value":12544},"The next step calculates inverse distances and uses a small ",{"type":27,"tag":170,"props":12546,"children":12548},{"className":12547},[],[12549],{"type":33,"value":12550},"epsilon",{"type":33,"value":12552}," value to avoid division by 0:",{"type":27,"tag":238,"props":12554,"children":12556},{"code":12555,"language":254,"meta":8,"className":255,"style":8},"epsilon = 1e-5\ninv_distances = 1.0 / cp.maximum(distances, epsilon)\nprint(inv_distances)\n\narray([[1.00000000e+05, 2.03069233e-01, 9.88936353e-02],\n       [2.03069233e-01, 1.00000000e+05, 1.92450090e-01],\n       [9.88936353e-02, 1.92450090e-01, 1.00000000e+05]])\n",[12557],{"type":27,"tag":170,"props":12558,"children":12559},{"__ignoreMap":8},[12560,12576,12600,12612,12619,12654,12686],{"type":27,"tag":261,"props":12561,"children":12562},{"class":263,"line":264},[12563,12568,12572],{"type":27,"tag":261,"props":12564,"children":12565},{"style":268},[12566],{"type":33,"value":12567},"epsilon ",{"type":27,"tag":261,"props":12569,"children":12570},{"style":274},[12571],{"type":33,"value":277},{"type":27,"tag":261,"props":12573,"children":12574},{"style":295},[12575],{"type":33,"value":11269},{"type":27,"tag":261,"props":12577,"children":12578},{"class":263,"line":343},[12579,12584,12588,12592,12596],{"type":27,"tag":261,"props":12580,"children":12581},{"style":268},[12582],{"type":33,"value":12583},"inv_distances ",{"type":27,"tag":261,"props":12585,"children":12586},{"style":274},[12587],{"type":33,"value":277},{"type":27,"tag":261,"props":12589,"children":12590},{"style":295},[12591],{"type":33,"value":11286},{"type":27,"tag":261,"props":12593,"children":12594},{"style":274},[12595],{"type":33,"value":11291},{"type":27,"tag":261,"props":12597,"children":12598},{"style":268},[12599],{"type":33,"value":11296},{"type":27,"tag":261,"props":12601,"children":12602},{"class":263,"line":379},[12603,12607],{"type":27,"tag":261,"props":12604,"children":12605},{"style":1520},[12606],{"type":33,"value":4157},{"type":27,"tag":261,"props":12608,"children":12609},{"style":268},[12610],{"type":33,"value":12611},"(inv_distances)\n",{"type":27,"tag":261,"props":12613,"children":12614},{"class":263,"line":537},[12615],{"type":27,"tag":261,"props":12616,"children":12617},{"emptyLinePlaceholder":22},[12618],{"type":33,"value":766},{"type":27,"tag":261,"props":12620,"children":12621},{"class":263,"line":561},[12622,12627,12632,12636,12641,12645,12650],{"type":27,"tag":261,"props":12623,"children":12624},{"style":268},[12625],{"type":33,"value":12626},"array([[",{"type":27,"tag":261,"props":12628,"children":12629},{"style":295},[12630],{"type":33,"value":12631},"1.00000000e+05",{"type":27,"tag":261,"props":12633,"children":12634},{"style":268},[12635],{"type":33,"value":303},{"type":27,"tag":261,"props":12637,"children":12638},{"style":295},[12639],{"type":33,"value":12640},"2.03069233e-01",{"type":27,"tag":261,"props":12642,"children":12643},{"style":268},[12644],{"type":33,"value":303},{"type":27,"tag":261,"props":12646,"children":12647},{"style":295},[12648],{"type":33,"value":12649},"9.88936353e-02",{"type":27,"tag":261,"props":12651,"children":12652},{"style":268},[12653],{"type":33,"value":4561},{"type":27,"tag":261,"props":12655,"children":12656},{"class":263,"line":785},[12657,12661,12665,12669,12673,12677,12682],{"type":27,"tag":261,"props":12658,"children":12659},{"style":268},[12660],{"type":33,"value":12501},{"type":27,"tag":261,"props":12662,"children":12663},{"style":295},[12664],{"type":33,"value":12640},{"type":27,"tag":261,"props":12666,"children":12667},{"style":268},[12668],{"type":33,"value":303},{"type":27,"tag":261,"props":12670,"children":12671},{"style":295},[12672],{"type":33,"value":12631},{"type":27,"tag":261,"props":12674,"children":12675},{"style":268},[12676],{"type":33,"value":303},{"type":27,"tag":261,"props":12678,"children":12679},{"style":295},[12680],{"type":33,"value":12681},"1.92450090e-01",{"type":27,"tag":261,"props":12683,"children":12684},{"style":268},[12685],{"type":33,"value":4561},{"type":27,"tag":261,"props":12687,"children":12688},{"class":263,"line":794},[12689,12693,12697,12701,12705,12709,12713],{"type":27,"tag":261,"props":12690,"children":12691},{"style":268},[12692],{"type":33,"value":12501},{"type":27,"tag":261,"props":12694,"children":12695},{"style":295},[12696],{"type":33,"value":12649},{"type":27,"tag":261,"props":12698,"children":12699},{"style":268},[12700],{"type":33,"value":303},{"type":27,"tag":261,"props":12702,"children":12703},{"style":295},[12704],{"type":33,"value":12681},{"type":27,"tag":261,"props":12706,"children":12707},{"style":268},[12708],{"type":33,"value":303},{"type":27,"tag":261,"props":12710,"children":12711},{"style":295},[12712],{"type":33,"value":12631},{"type":27,"tag":261,"props":12714,"children":12715},{"style":268},[12716],{"type":33,"value":11908},{"type":27,"tag":36,"props":12718,"children":12719},{},[12720],{"type":33,"value":12721},"The next step is the most elegant part of the simulation and really flexes the GPU's parallel compute capabilities:",{"type":27,"tag":238,"props":12723,"children":12725},{"code":12724,"language":254,"meta":8,"className":255,"style":8},"cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n",[12726],{"type":27,"tag":170,"props":12727,"children":12728},{"__ignoreMap":8},[12729],{"type":27,"tag":261,"props":12730,"children":12731},{"class":263,"line":264},[12732,12737,12741,12745,12749,12753,12757,12761,12765,12769,12773,12777],{"type":27,"tag":261,"props":12733,"children":12734},{"style":268},[12735],{"type":33,"value":12736},"cp_forces ",{"type":27,"tag":261,"props":12738,"children":12739},{"style":274},[12740],{"type":33,"value":277},{"type":27,"tag":261,"props":12742,"children":12743},{"style":268},[12744],{"type":33,"value":11328},{"type":27,"tag":261,"props":12746,"children":12747},{"style":274},[12748],{"type":33,"value":8885},{"type":27,"tag":261,"props":12750,"children":12751},{"style":268},[12752],{"type":33,"value":11337},{"type":27,"tag":261,"props":12754,"children":12755},{"style":274},[12756],{"type":33,"value":11213},{"type":27,"tag":261,"props":12758,"children":12759},{"style":295},[12760],{"type":33,"value":11346},{"type":27,"tag":261,"props":12762,"children":12763},{"style":268},[12764],{"type":33,"value":11351},{"type":27,"tag":261,"props":12766,"children":12767},{"style":285},[12768],{"type":33,"value":11039},{"type":27,"tag":261,"props":12770,"children":12771},{"style":274},[12772],{"type":33,"value":277},{"type":27,"tag":261,"props":12774,"children":12775},{"style":295},[12776],{"type":33,"value":1532},{"type":27,"tag":261,"props":12778,"children":12779},{"style":268},[12780],{"type":33,"value":340},{"type":27,"tag":36,"props":12782,"children":12783},{},[12784,12785,12791],{"type":33,"value":5353},{"type":27,"tag":170,"props":12786,"children":12788},{"className":12787},[],[12789],{"type":33,"value":12790},".T",{"type":33,"value":12792}," operation transposes a matrix, multiplies by the cube of inverse distances, then transposes the matrix again before summing along the first axis. Transposing a matrix basically swaps rows and columns.",{"type":27,"tag":36,"props":12794,"children":12795},{},[12796],{"type":33,"value":12797},"The next two steps are also pretty elegant:",{"type":27,"tag":238,"props":12799,"children":12801},{"code":12800,"language":254,"meta":8,"className":255,"style":8},"# update velocities and positions\ncp_velocities += DT * cp_forces\ncp_positions += DT * cp_velocities\n",[12802],{"type":27,"tag":170,"props":12803,"children":12804},{"__ignoreMap":8},[12805,12813,12836],{"type":27,"tag":261,"props":12806,"children":12807},{"class":263,"line":264},[12808],{"type":27,"tag":261,"props":12809,"children":12810},{"style":478},[12811],{"type":33,"value":12812},"# update velocities and positions\n",{"type":27,"tag":261,"props":12814,"children":12815},{"class":263,"line":343},[12816,12820,12824,12828,12832],{"type":27,"tag":261,"props":12817,"children":12818},{"style":268},[12819],{"type":33,"value":11001},{"type":27,"tag":261,"props":12821,"children":12822},{"style":274},[12823],{"type":33,"value":11397},{"type":27,"tag":261,"props":12825,"children":12826},{"style":295},[12827],{"type":33,"value":11402},{"type":27,"tag":261,"props":12829,"children":12830},{"style":274},[12831],{"type":33,"value":11407},{"type":27,"tag":261,"props":12833,"children":12834},{"style":268},[12835],{"type":33,"value":11412},{"type":27,"tag":261,"props":12837,"children":12838},{"class":263,"line":379},[12839,12843,12847,12851,12855],{"type":27,"tag":261,"props":12840,"children":12841},{"style":268},[12842],{"type":33,"value":10984},{"type":27,"tag":261,"props":12844,"children":12845},{"style":274},[12846],{"type":33,"value":11397},{"type":27,"tag":261,"props":12848,"children":12849},{"style":295},[12850],{"type":33,"value":11402},{"type":27,"tag":261,"props":12852,"children":12853},{"style":274},[12854],{"type":33,"value":11407},{"type":27,"tag":261,"props":12856,"children":12857},{"style":268},[12858],{"type":33,"value":11438},{"type":27,"tag":36,"props":12860,"children":12861},{},[12862,12864,12869],{"type":33,"value":12863},"In the last step I append the updated positions to an array that holds every \"tick\" (the positions of each particle between each time interval, ",{"type":27,"tag":170,"props":12865,"children":12867},{"className":12866},[],[12868],{"type":33,"value":10880},{"type":33,"value":12870}," - \"delta time\")",{"type":27,"tag":36,"props":12872,"children":12873},{},[12874],{"type":33,"value":12875},"Here's the formula for the mathematical equation used to calculate the force on any given body in an n-body system:",{"type":27,"tag":9880,"props":12877,"children":12880},{"src":12878,"width":9882,"height":12879},"https://briancaffey.github.io/three-body-problem/iframe/formula.html",110,[],{"type":27,"tag":36,"props":12882,"children":12883},{},[12884],{"type":33,"value":12885},"To test that the simulation was working correctly I used ChatGPT again to construct a 3D scene in Blender with a Python script:",{"type":27,"tag":36,"props":12887,"children":12888},{},[12889],{"type":27,"tag":84,"props":12890,"children":12893},{"alt":12891,"src":12892},"Blender Animation","/static/three-body-problem/blender.png",[],{"type":27,"tag":47,"props":12895,"children":12897},{"id":12896},"threejs",[12898],{"type":33,"value":12899},"Three.js",{"type":27,"tag":36,"props":12901,"children":12902},{},[12903],{"type":33,"value":12904},"Imagine that we are working for a Chinese startup called the Qin Dynasty. The founder, Qin Shi Huang, is a brutal tyrant with an obsessive fear of assassination. Let's put on our product hat for a minute and think about how we can impress him with a clean solution to the three-body problem. A recent attempt involved building a 30,000-person analog computer that was destroyed in tri-solar syzygy. Failing to accurately predict the movement of the suns could mean execution by live-burial, a fiery death or worse. Using CUDA and Blender is a good MVP but doesn't make for the best technical demo since it involves so many different steps: running the simulation in CUDA, exporting data to JSON, loading data into a visualuzation and then finally rendering a video of the simulation. With a popular Javascript library called Three.js we can run an interactive three-body problem simulation in real-time right in the browser. Here's the three-body simulation I also co-authored with ChatGPT-4 using Three.js:",{"type":27,"tag":9880,"props":12906,"children":12909},{"src":12907,"width":9882,"height":12908},"https://briancaffey.github.io/three-body-problem/three/",350,[],{"type":27,"tag":28,"props":12911,"children":12913},{"id":12912},"screen-adaptations-the-gaming-industry-and-the-ccp",[12914],{"type":33,"value":12915},"Screen adaptations, the gaming industry and the CCP",{"type":27,"tag":36,"props":12917,"children":12918},{},[12919],{"type":33,"value":12920},"Dream of the Red Chamber is one of China's Four Great Classical Novels and is often seen as the pinacle of Chinese fiction. It was written in the mid 18th century and first published in 1791. It is a long saga that totals 960,000 characters in length, on similar scale to the length of the Three-Body Problem trilogy. Sun Wen, a Qing dynasty artist, spent 36 years of his life doing a series of 230 paintings depicting scenes from the Dream of the Red Chamber: dream sequences, demons, goddesses, nuns, nobles, beggars, raging fires, landscapes, interiors, wildlife, gardens, temples, funerals, battles, processions, banquets, trials, operas, marriages.",{"type":27,"tag":36,"props":12922,"children":12923},{},[12924],{"type":27,"tag":84,"props":12925,"children":12928},{"alt":12926,"src":12927},"Sun Wen paintings sample","/static/three-body-problem/dorc.png",[],{"type":27,"tag":36,"props":12930,"children":12931},{},[12932],{"type":33,"value":12933},"Following in this tradition of celebrating great literature, Tencent Video and China Central Television produced a 30-episode adaptation of the Three-Body Problem that was released in Feburary 2023. It is a surprisingly faithful reproduction of the book that is worth checking out. The portrayal of Shi Qiang (Da Shi) was easily my favorite part of the series. I was also impressed by how the Three-Body VR game scenes were done with computer graphics. It got me thinking about how China is represented in some of the worlds most popular video games.",{"type":27,"tag":36,"props":12935,"children":12936},{},[12937],{"type":27,"tag":84,"props":12938,"children":12941},{"alt":12939,"src":12940},"games","/static/three-body-problem/game.png",[],{"type":27,"tag":36,"props":12943,"children":12944},{},[12945],{"type":33,"value":12946},"This is Rocket League, a competitive vehicular soccer game where players, like in the Three-Body game, must master the laws of gravity. The Chinese-themed Forbiden Temple arena shown here is one of many virtual international venues in the game. Epic Games (creator of Fortnite) bought Rocket League in 2019 for an estimated $250 to $300 million.",{"type":27,"tag":36,"props":12948,"children":12949},{},[12950],{"type":33,"value":12951},"Like Rocket League, Overwatch is a highly-competitive eSport on a global scale. It features a large roster of 38 players from all over the world. Dr. Mei-Ling Zhou (周美灵) is a Chinese climatologist who uses ice both to attack opponents and to defend herself. Mei became controversial in China due to her adoption as a symbolic figure in the 2019 Hong Kong protests.",{"type":27,"tag":36,"props":12953,"children":12954},{},[12955],{"type":33,"value":12956},"Three college friends combined their interest in anime, comics and games (ACG) and literature to publish one of the most successful games created by a Chinese company and arguably one of China's most important cultural exports: Genshin Impact. miHoYo, the Shanghai-based company that develops Genshin Impact, grossed $4 billion of revenue globally in the game's first year setting a new record in the gaming industry. Like other companies of its size, miHoYo has a party committee under the Chinese Communist Party that influences the company's operations.",{"type":27,"tag":28,"props":12958,"children":12960},{"id":12959},"ai-and-layoffs-in-the-tech-industry",[12961],{"type":33,"value":12962},"AI and layoffs in the tech industry",{"type":27,"tag":36,"props":12964,"children":12965},{},[12966,12968,12975],{"type":33,"value":12967},"I have a positive attitude toward AI and its ability to supercharge the creative work we do, but I also think that replacing humans and AI-related layoffs should should be a part of the conversation. I wasn't impacted by the recent round of layoffs at my company, and I'm grateful to have the opportunity to work with a talented team on interesting problems in the health tech industry. I do have some solid references for folks in DevOps, product and backend and frontend engineering, and I’m happy to ",{"type":27,"tag":61,"props":12969,"children":12972},{"href":12970,"rel":12971},"https://www.linkedin.com/in/brian-caffey-06b22a18/",[65],[12973],{"type":33,"value":12974},"connect and share via LinkedIn",{"type":33,"value":2970},{"type":27,"tag":36,"props":12977,"children":12978},{},[12979],{"type":33,"value":12980},"Layoffs in both China and the U.S. have been pummled the tech sector over the last two years. New graduates in China are also facing a difficult job market. There are some popular expressions that paint a picture of the job market in China:",{"type":27,"tag":574,"props":12982,"children":12983},{},[12984,12989,12994,13006,13020],{"type":27,"tag":98,"props":12985,"children":12986},{},[12987],{"type":33,"value":12988},"躺平 Lying flat: avoiding relentless work",{"type":27,"tag":98,"props":12990,"children":12991},{},[12992],{"type":33,"value":12993},"九九六 996 Work culture: describes working from 9AM to 9PM, 6 days a week, a common work schedule for many Chinese employees",{"type":27,"tag":98,"props":12995,"children":12996},{},[12997,12999],{"type":33,"value":12998},"全职儿女 Full-time Children: ",{"type":27,"tag":61,"props":13000,"children":13003},{"href":13001,"rel":13002},"https://www.cnn.com/2023/07/26/economy/china-youth-unemployment-intl-hnk/index.html",[65],[13004],{"type":33,"value":13005},"Young Chinese are getting paid to be ‘full-time children’ as jobs become harder to find",{"type":27,"tag":98,"props":13007,"children":13008},{},[13009,13011,13018],{"type":33,"value":13010},"35岁诅咒 The curse of 35: ",{"type":27,"tag":61,"props":13012,"children":13015},{"href":13013,"rel":13014},"https://www.marketplace.org/shows/marketplace-tech/chinas-tech-workers-ageism-the-curse-of-35/",[65],[13016],{"type":33,"value":13017},"Ageism in China’s tech sector has workers fearing the “curse of 35”",{"type":33,"value":13019},". Shout out to my fellow Year of the Dragon 35 year olds! 🐲",{"type":27,"tag":98,"props":13021,"children":13022},{},[13023,13025],{"type":33,"value":13024},"吃苦 Eat Bitterness: ",{"type":27,"tag":61,"props":13026,"children":13029},{"href":13027,"rel":13028},"https://www.nytimes.com/2023/05/30/business/china-youth-unemployment.html",[65],[13030],{"type":33,"value":13031},"China’s Young People Can’t Find Jobs. Xi Jinping Says to ‘Eat Bitterness.’",{"type":27,"tag":36,"props":13033,"children":13034},{},[13035,13037,13044],{"type":33,"value":13036},"It is an exciting time for AI. Elon Musk and Kaifu Lee have both recently released open-source large language models: Grok and Yi. Sam Altman was fired as CEO of OpenAI, then came back. Stable Diffusion just released a text to video model. AGI might already be here. In the U.S., we are going into our first presidential election cycle with AI fully turned on. Here's a ",{"type":27,"tag":61,"props":13038,"children":13041},{"href":13039,"rel":13040},"https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032",[65],[13042],{"type":33,"value":13043},"link to The Three-Body Problem book on Amazon",{"type":33,"value":13045},". Thanks for reading and Happy Thanksgiving!",{"type":27,"tag":36,"props":13047,"children":13048},{},[13049],{"type":27,"tag":84,"props":13050,"children":13053},{"alt":13051,"src":13052},"Happy Thanksgiving","/static/three-body-problem/thanksgiving.png",[],{"type":27,"tag":5518,"props":13055,"children":13056},{},[13057],{"type":33,"value":5522},{"title":8,"searchDepth":343,"depth":343,"links":13059},[13060,13061,13064,13065,13068,13069,13073,13074],{"id":9804,"depth":343,"text":9807},{"id":9895,"depth":343,"text":9898,"children":13062},[13063],{"id":9967,"depth":379,"text":9970},{"id":10000,"depth":343,"text":10003},{"id":10060,"depth":343,"text":10063,"children":13066},[13067],{"id":10132,"depth":379,"text":10135},{"id":10658,"depth":343,"text":10661},{"id":10708,"depth":343,"text":10711,"children":13070},[13071,13072],{"id":10719,"depth":379,"text":10722},{"id":12896,"depth":379,"text":12899},{"id":12912,"depth":343,"text":12915},{"id":12959,"depth":343,"text":12962},"content:2023:08:27:python-vue-chinese-llama-2-and-the-three-body-problem.md","2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem.md","2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem",{"_path":13079,"_dir":13080,"_draft":7,"_partial":7,"_locale":8,"title":13081,"description":13082,"layout":13083,"date":13084,"comments":22,"image":13085,"tags":13086,"body":13090,"_type":5561,"_id":13699,"_source":5563,"_file":13700,"_stem":13701,"_extension":5566},"/2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux","19","Installing the GPU version of Tensorflow with Docker on Arch Linux","A walkthrough of Tensorflow setup and usage on Arch Linux with docker","post","2017-11-19T00:00:00.000Z","/static/trump.png",[13087,13088,13089,14,254],"arch-linux","tensorflow","docker",{"type":24,"children":13091,"toc":13693},[13092,13097,13120,13126,13131,13137,13150,13155,13163,13168,13176,13181,13189,13217,13225,13230,13235,13243,13249,13263,13268,13281,13289,13301,13314,13322,13327,13335,13340,13348,13360,13368,13373,13381,13386,13394,13399,13407,13412,13560,13580,13600,13605,13613,13618,13626,13634,13639,13644,13652,13657,13670,13677,13682,13689],{"type":27,"tag":36,"props":13093,"children":13094},{},[13095],{"type":33,"value":13096},"I've tried installing the GPU version of Tensorflow a few times before and failed. There seems to be lots of confusion about the build process, of which there are many. Also, over the last few years there have been many new versions of the software needed to support the GPU version of Tensorflow as well as the first official release of Tensorflow itself (which is now on version 1.4), such as CUDA and cudnn, and different version of python. This is one more attempt at installing the GPU version of Tensor Flow on my Desktop PC that is currently dual booting with Arch Linux and Windows 10. I've decided to try going the docker route because it should eliminate some of the headache of missing depedencies. Here are the specs for my computer:",{"type":27,"tag":574,"props":13098,"children":13099},{},[13100,13105,13110,13115],{"type":27,"tag":98,"props":13101,"children":13102},{},[13103],{"type":33,"value":13104},"i7-6700K",{"type":27,"tag":98,"props":13106,"children":13107},{},[13108],{"type":33,"value":13109},"NVIDIA GTX 1080",{"type":27,"tag":98,"props":13111,"children":13112},{},[13113],{"type":33,"value":13114},"Asus Hero VIII motherboard",{"type":27,"tag":98,"props":13116,"children":13117},{},[13118],{"type":33,"value":13119},"Arch Linux on a 128 GB SSD (Windows 10 is installed on a separate SSD)",{"type":27,"tag":28,"props":13121,"children":13123},{"id":13122},"installing-cuda-and-cudnn",[13124],{"type":33,"value":13125},"Installing CUDA and cudnn",{"type":27,"tag":36,"props":13127,"children":13128},{},[13129],{"type":33,"value":13130},"We don't need to install these when installing Tensorflow with Docker. Read to the bottom for more info.",{"type":27,"tag":28,"props":13132,"children":13134},{"id":13133},"installing-docker",[13135],{"type":33,"value":13136},"Installing Docker",{"type":27,"tag":36,"props":13138,"children":13139},{},[13140,13142,13149],{"type":33,"value":13141},"To install docker on our machine, let's start with the ",{"type":27,"tag":61,"props":13143,"children":13146},{"href":13144,"rel":13145},"https://wiki.archlinux.org/index.php/Docker",[65],[13147],{"type":33,"value":13148},"Arch Wiki article on docker",{"type":33,"value":2970},{"type":27,"tag":36,"props":13151,"children":13152},{},[13153],{"type":33,"value":13154},"We need to add the Loopback module to the Linux Kernel, so we run:",{"type":27,"tag":238,"props":13156,"children":13158},{"code":13157},"# tee /etc/modules-load.d/loop.conf \u003C\u003C\u003C \"loop\"\n# modprobe loop\n$ reboot\n",[13159],{"type":27,"tag":170,"props":13160,"children":13161},{"__ignoreMap":8},[13162],{"type":33,"value":13157},{"type":27,"tag":36,"props":13164,"children":13165},{},[13166],{"type":33,"value":13167},"Ater rebooting we can install docker:",{"type":27,"tag":238,"props":13169,"children":13171},{"code":13170},"yaourt -S docker\n",[13172],{"type":27,"tag":170,"props":13173,"children":13174},{"__ignoreMap":8},[13175],{"type":33,"value":13170},{"type":27,"tag":36,"props":13177,"children":13178},{},[13179],{"type":33,"value":13180},"Now we want to add ourself to the docker group with the following command:",{"type":27,"tag":238,"props":13182,"children":13184},{"code":13183},"$ sudo gpasswd -a brian docker\n[sudo] password for brian:\nAdding user brian to group docker\n",[13185],{"type":27,"tag":170,"props":13186,"children":13187},{"__ignoreMap":8},[13188],{"type":33,"value":13183},{"type":27,"tag":36,"props":13190,"children":13191},{},[13192,13194,13200,13202,13208,13210,13215],{"type":33,"value":13193},"If you run ",{"type":27,"tag":170,"props":13195,"children":13197},{"className":13196},[],[13198],{"type":33,"value":13199},"groups",{"type":33,"value":13201},", you won't see docker listed in the groups you (brian) belong to. Run ",{"type":27,"tag":170,"props":13203,"children":13205},{"className":13204},[],[13206],{"type":33,"value":13207},"newgrp docker",{"type":33,"value":13209}," and then re-run docker and you should see ",{"type":27,"tag":170,"props":13211,"children":13213},{"className":13212},[],[13214],{"type":33,"value":13089},{"type":33,"value":13216}," listed with any other groups you belong to:",{"type":27,"tag":238,"props":13218,"children":13220},{"code":13219},"[brian@a1arch ~]$ groups\nwheel storage power users\n[brian@a1arch ~]$ newgrp docker\n                   -`                    brian@a1arch\n                  .o+`                   ------------\n                 `ooo/                   OS: Arch Linux x86_64\n                `+oooo:                  Kernel: 4.12.8-2-ARCH\n               `+oooooo:                 Uptime: 6 mins\n               -+oooooo+:                Packages: 1127\n             `/:-:++oooo+:               Shell: bash 4.4.12\n            `/++++/+++++++:              Resolution: 1920x1080\n           `/++++++++++++++:             WM: i3\n          `/+++ooooooooooooo/`           Theme: Adwaita [GTK2]\n         ./ooosssso++osssssso+`          Icons: Adwaita [GTK2]\n        .oossssso-````/ossssss+`         Terminal: urxvt\n       -osssssso.      :ssssssso.        Terminal Font: Inconsolata-12\n      :osssssss/        osssso+++.       CPU: Intel i7-6700K (8) @ 4.200GHz\n     /ossssssss/        +ssssooo/-       GPU: NVIDIA GeForce GTX 1080\n   `/ossssso+/:-        -:/+osssso+-     Memory: 3289MiB / 15975MiB\n  `+sso+:-`                 `.-/+oso:\n `++:.                           `-/+/\n .`                                 `/\n\n[brian@a1arch ~]$ groups\ndocker wheel storage power users\n",[13221],{"type":27,"tag":170,"props":13222,"children":13223},{"__ignoreMap":8},[13224],{"type":33,"value":13219},{"type":27,"tag":36,"props":13226,"children":13227},{},[13228],{"type":33,"value":13229},"Doing this prevents us from having to write sudo each time we run docker.",{"type":27,"tag":36,"props":13231,"children":13232},{},[13233],{"type":33,"value":13234},"Next we need to start the docker daemon.",{"type":27,"tag":238,"props":13236,"children":13238},{"code":13237},"$ systemctl start docker\n==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ====\nAuthentication is required to start 'docker.service'.\nAuthenticating as: brian\nPassword:\n==== AUTHENTICATION COMPLETE ====\n$\n",[13239],{"type":27,"tag":170,"props":13240,"children":13241},{"__ignoreMap":8},[13242],{"type":33,"value":13237},{"type":27,"tag":47,"props":13244,"children":13246},{"id":13245},"side-note",[13247],{"type":33,"value":13248},"Side note",{"type":27,"tag":36,"props":13250,"children":13251},{},[13252,13254,13261],{"type":33,"value":13253},"There seems to be an ",{"type":27,"tag":61,"props":13255,"children":13258},{"href":13256,"rel":13257},"https://github.com/moby/moby/issues/23289",[65],[13259],{"type":33,"value":13260},"Arch Linux-specific bug",{"type":33,"value":13262}," which prevents us from enabling docker (and nvidia-docker which we will get next). There is a solution to downgrade to an older version of docker, or you can just start the docker service and the nvidia-docker service when you want to use them. I have found it faster to first start nvidia-docker and then start docker services.",{"type":27,"tag":36,"props":13264,"children":13265},{},[13266],{"type":33,"value":13267},"So far so good. Next let's look at the Tensorflow documentation for installing Tensorflow with docker.",{"type":27,"tag":36,"props":13269,"children":13270},{},[13271,13273,13279],{"type":33,"value":13272},"We need to install ",{"type":27,"tag":170,"props":13274,"children":13276},{"className":13275},[],[13277],{"type":33,"value":13278},"nvidia-docker",{"type":33,"value":13280},":",{"type":27,"tag":238,"props":13282,"children":13284},{"code":13283},"$ yaourt -S nvidia-docker\n[...]\n[sudo] password for brian:\nloading packages...\nresolving dependencies...\nlooking for conflicting packages...\n\nPackages (1) nvidia-docker-1.0.1-1\n\nTotal Installed Size:  13.34 MiB\n\n:: Proceed with installation? [Y/n]\n(1/1) checking keys in keyring                                 [##################################] 100%\n(1/1) checking package integrity                               [##################################] 100%\n(1/1) loading package files                                    [##################################] 100%\n(1/1) checking for file conflicts                              [##################################] 100%\n(1/1) checking available disk space                            [##################################] 100%\n:: Processing package changes...\n(1/1) installing nvidia-docker                                 [##################################] 100%\n=> Prior to running 'CUDA'-containers, ensure that the nvidia-docker-plugin\n   is loaded. -> https://github.com/NVIDIA/nvidia-docker#other-distributions\n\n*) manually; sudo -b nohup nvidia-docker-plugin > /tmp/nvidia-docker.log\n\n*) automatically at startup; systemctl enable nvidia-docker.service\nOptional dependencies for nvidia-docker\n    cuda [installed]\n    nvidia [installed]\n    opencl-nvidia [installed]\n:: Running post-transaction hooks...\n(1/1) Arming ConditionNeedsUpdate...\n",[13285],{"type":27,"tag":170,"props":13286,"children":13287},{"__ignoreMap":8},[13288],{"type":33,"value":13283},{"type":27,"tag":36,"props":13290,"children":13291},{},[13292,13294,13300],{"type":33,"value":13293},"Next it says: Launch a Docker container that contains one of the TensorFlow binary images. Those images are available ",{"type":27,"tag":61,"props":13295,"children":13298},{"href":13296,"rel":13297},"https://hub.docker.com/r/tensorflow/tensorflow/tags/",[65],[13299],{"type":33,"value":5608},{"type":33,"value":2970},{"type":27,"tag":36,"props":13302,"children":13303},{},[13304,13306,13312],{"type":33,"value":13305},"Next I pulled the container with the ",{"type":27,"tag":170,"props":13307,"children":13309},{"className":13308},[],[13310],{"type":33,"value":13311},"gpu-latest",{"type":33,"value":13313}," tag and it started to download the container:",{"type":27,"tag":238,"props":13315,"children":13317},{"code":13316},"$ docker pull tensorflow/tensorflow:gpu-latest\n[sudo] password for brian:\nlatest-gpu: Pulling from tensorflow/tensorflow\nae79f2514705: Pull complete\nc59d01a7e4ca: Pull complete\n41ba73a9054d: Pull complete\nf1bbfd495cc1: Pull complete\n0c346f7223e2: Pull complete\n5dcd01667896: Pull complete\nca677f607487: Downloading  180.7MB/453MB\nb4637619a887: Download complete\n8c644ff287da: Downloading    224MB/465.6MB\n119c5f576e79: Download complete\n009f82e71a7c: Download complete\ndbc0fb5872c7: Downloading  17.83MB/66.54MB\n5ef01389c5b2: Waiting\n04f824004b76: Waiting\n5861b82f52e5: Waiting\na495a3b4e6e1: Waiting\n3a0a25b1bbaf: Pulling fs layer\nb76a0afeb1e1: Waiting\n",[13318],{"type":27,"tag":170,"props":13319,"children":13320},{"__ignoreMap":8},[13321],{"type":33,"value":13316},{"type":27,"tag":36,"props":13323,"children":13324},{},[13325],{"type":33,"value":13326},"It finished after several minutes:",{"type":27,"tag":238,"props":13328,"children":13330},{"code":13329},"ca677f607487: Pull complete\nb4637619a887: Pull complete\n8c644ff287da: Pull complete\n119c5f576e79: Pull complete\n009f82e71a7c: Pull complete\ndbc0fb5872c7: Pull complete\n5ef01389c5b2: Pull complete\n04f824004b76: Pull complete\n5861b82f52e5: Pull complete\na495a3b4e6e1: Pull complete\n3a0a25b1bbaf: Pull complete\nb76a0afeb1e1: Pull complete\nDigest: sha256:90e27448121b321c5ec66069fb2c718301df2ddaf25ba916b6f53719141572b0\nStatus: Downloaded newer image for tensorflow/tensorflow:latest-gpu\n$\n",[13331],{"type":27,"tag":170,"props":13332,"children":13333},{"__ignoreMap":8},[13334],{"type":33,"value":13329},{"type":27,"tag":36,"props":13336,"children":13337},{},[13338],{"type":33,"value":13339},"Let's verify that it has the image:",{"type":27,"tag":238,"props":13341,"children":13343},{"code":13342},"$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\ntensorflow/tensorflow   latest-gpu          2f243a16ff63        13 days ago         3.36GB\n",[13344],{"type":27,"tag":170,"props":13345,"children":13346},{"__ignoreMap":8},[13347],{"type":33,"value":13342},{"type":27,"tag":36,"props":13349,"children":13350},{},[13351,13353,13358],{"type":33,"value":13352},"Next let's start the ",{"type":27,"tag":170,"props":13354,"children":13356},{"className":13355},[],[13357],{"type":33,"value":13278},{"type":33,"value":13359}," service:",{"type":27,"tag":238,"props":13361,"children":13363},{"code":13362},"$ systemctl start nvidia-docker\n==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ====\nAuthentication is required to start 'nvidia-docker.service'.\nAuthenticating as: brian\nPassword:\n==== AUTHENTICATION COMPLETE ====\n$\n",[13364],{"type":27,"tag":170,"props":13365,"children":13366},{"__ignoreMap":8},[13367],{"type":33,"value":13362},{"type":27,"tag":36,"props":13369,"children":13370},{},[13371],{"type":33,"value":13372},"OK, we should be ready to launch the image:",{"type":27,"tag":238,"props":13374,"children":13376},{"code":13375},"$ nvidia-docker run -it tensorflow/tensorflow:latest-gpu bash\nroot@761a62c1cff1:/notebooks#\n",[13377],{"type":27,"tag":170,"props":13378,"children":13379},{"__ignoreMap":8},[13380],{"type":33,"value":13375},{"type":27,"tag":36,"props":13382,"children":13383},{},[13384],{"type":33,"value":13385},"This is looking good. Let's try to start python:",{"type":27,"tag":238,"props":13387,"children":13389},{"code":13388},"root@761a62c1cff1:/notebooks# python\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>>\n",[13390],{"type":27,"tag":170,"props":13391,"children":13392},{"__ignoreMap":8},[13393],{"type":33,"value":13388},{"type":27,"tag":36,"props":13395,"children":13396},{},[13397],{"type":33,"value":13398},"That works! Let's try out the classic MNIST hand-written digit classification problem that comes packaged as a notebook with the container image:",{"type":27,"tag":238,"props":13400,"children":13402},{"code":13401},"$ nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu\n[sudo] password for brian:\n[I 21:54:26.671 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n[W 21:54:26.689 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\n[I 21:54:26.693 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 21:54:26.693 NotebookApp] 0 active kernels\n[I 21:54:26.693 NotebookApp] The Jupyter Notebook is running at:\n[I 21:54:26.693 NotebookApp] http://[all ip addresses on your system]:8888/?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd\n[I 21:54:26.693 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 21:54:26.693 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd\n[I 21:54:34.489 NotebookApp] 302 GET /?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd (172.17.0.1) 0.32ms\n[I 21:54:59.019 NotebookApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\n[W 21:54:59.023 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n[W 21:54:59.049 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20171119215426 (172.17.0.1) 4.38ms referer=http://localhost:8888/notebooks/3_mnist_from_scratch.ipynb\n[I 21:54:59.813 NotebookApp] Kernel started: 00027a3e-59ae-47ce-90a5-752a9d1fe075\n[I 21:55:00.199 NotebookApp] Adapting to protocol v5.1 for kernel 00027a3e-59ae-47ce-90a5-752a9d1fe075\n[I 21:56:59.815 NotebookApp] Saving file at /3_mnist_from_scratch.ipynb\n[W 21:56:59.816 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n2017-11-19 21:57:03.988627: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 21:57:04.070873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 21:57:04.071129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.44GiB\n2017-11-19 21:57:04.071143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",[13403],{"type":27,"tag":170,"props":13404,"children":13405},{"__ignoreMap":8},[13406],{"type":33,"value":13401},{"type":27,"tag":36,"props":13408,"children":13409},{},[13410],{"type":33,"value":13411},"I was only able to get the entire notebook to run after making a few small configuration tweaks to the tensorflow Interactive Session to fix some memory issues:",{"type":27,"tag":238,"props":13413,"children":13415},{"code":13414,"language":254,"meta":8,"className":255,"style":8},"gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.75)\n\ns = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n\n# Use our newly created session as the default for\n# subsequent operations.\ns.as_default()\n\n# Initialize all the variables we defined above.\ntf.global_variables_initializer().run()\n",[13416],{"type":27,"tag":170,"props":13417,"children":13418},{"__ignoreMap":8},[13419,13454,13461,13506,13513,13521,13529,13537,13544,13552],{"type":27,"tag":261,"props":13420,"children":13421},{"class":263,"line":264},[13422,13427,13431,13436,13441,13445,13450],{"type":27,"tag":261,"props":13423,"children":13424},{"style":268},[13425],{"type":33,"value":13426},"gpu_options ",{"type":27,"tag":261,"props":13428,"children":13429},{"style":274},[13430],{"type":33,"value":277},{"type":27,"tag":261,"props":13432,"children":13433},{"style":268},[13434],{"type":33,"value":13435}," tf.GPUOptions(",{"type":27,"tag":261,"props":13437,"children":13438},{"style":285},[13439],{"type":33,"value":13440},"per_process_gpu_memory_fraction",{"type":27,"tag":261,"props":13442,"children":13443},{"style":274},[13444],{"type":33,"value":277},{"type":27,"tag":261,"props":13446,"children":13447},{"style":295},[13448],{"type":33,"value":13449},"0.75",{"type":27,"tag":261,"props":13451,"children":13452},{"style":268},[13453],{"type":33,"value":340},{"type":27,"tag":261,"props":13455,"children":13456},{"class":263,"line":343},[13457],{"type":27,"tag":261,"props":13458,"children":13459},{"emptyLinePlaceholder":22},[13460],{"type":33,"value":766},{"type":27,"tag":261,"props":13462,"children":13463},{"class":263,"line":379},[13464,13469,13473,13478,13483,13487,13492,13497,13501],{"type":27,"tag":261,"props":13465,"children":13466},{"style":268},[13467],{"type":33,"value":13468},"s ",{"type":27,"tag":261,"props":13470,"children":13471},{"style":274},[13472],{"type":33,"value":277},{"type":27,"tag":261,"props":13474,"children":13475},{"style":268},[13476],{"type":33,"value":13477}," tf.InteractiveSession(",{"type":27,"tag":261,"props":13479,"children":13480},{"style":285},[13481],{"type":33,"value":13482},"config",{"type":27,"tag":261,"props":13484,"children":13485},{"style":274},[13486],{"type":33,"value":277},{"type":27,"tag":261,"props":13488,"children":13489},{"style":268},[13490],{"type":33,"value":13491},"tf.ConfigProto(",{"type":27,"tag":261,"props":13493,"children":13494},{"style":285},[13495],{"type":33,"value":13496},"gpu_options",{"type":27,"tag":261,"props":13498,"children":13499},{"style":274},[13500],{"type":33,"value":277},{"type":27,"tag":261,"props":13502,"children":13503},{"style":268},[13504],{"type":33,"value":13505},"gpu_options))\n",{"type":27,"tag":261,"props":13507,"children":13508},{"class":263,"line":537},[13509],{"type":27,"tag":261,"props":13510,"children":13511},{"emptyLinePlaceholder":22},[13512],{"type":33,"value":766},{"type":27,"tag":261,"props":13514,"children":13515},{"class":263,"line":561},[13516],{"type":27,"tag":261,"props":13517,"children":13518},{"style":478},[13519],{"type":33,"value":13520},"# Use our newly created session as the default for\n",{"type":27,"tag":261,"props":13522,"children":13523},{"class":263,"line":785},[13524],{"type":27,"tag":261,"props":13525,"children":13526},{"style":478},[13527],{"type":33,"value":13528},"# subsequent operations.\n",{"type":27,"tag":261,"props":13530,"children":13531},{"class":263,"line":794},[13532],{"type":27,"tag":261,"props":13533,"children":13534},{"style":268},[13535],{"type":33,"value":13536},"s.as_default()\n",{"type":27,"tag":261,"props":13538,"children":13539},{"class":263,"line":803},[13540],{"type":27,"tag":261,"props":13541,"children":13542},{"emptyLinePlaceholder":22},[13543],{"type":33,"value":766},{"type":27,"tag":261,"props":13545,"children":13546},{"class":263,"line":811},[13547],{"type":27,"tag":261,"props":13548,"children":13549},{"style":478},[13550],{"type":33,"value":13551},"# Initialize all the variables we defined above.\n",{"type":27,"tag":261,"props":13553,"children":13554},{"class":263,"line":860},[13555],{"type":27,"tag":261,"props":13556,"children":13557},{"style":268},[13558],{"type":33,"value":13559},"tf.global_variables_initializer().run()\n",{"type":27,"tag":36,"props":13561,"children":13562},{},[13563,13565,13570,13572,13579],{"type":33,"value":13564},"Without setting ",{"type":27,"tag":170,"props":13566,"children":13568},{"className":13567},[],[13569],{"type":33,"value":13496},{"type":33,"value":13571},", Tensorflow allocates 95% of available GPU memory (according to ",{"type":27,"tag":61,"props":13573,"children":13576},{"href":13574,"rel":13575},"https://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu",[65],[13577],{"type":33,"value":13578},"this SO question",{"type":33,"value":8767},{"type":27,"tag":36,"props":13581,"children":13582},{},[13583,13585,13591,13593,13598],{"type":33,"value":13584},"Setting it to ",{"type":27,"tag":170,"props":13586,"children":13588},{"className":13587},[],[13589],{"type":33,"value":13590},"0.333",{"type":33,"value":13592}," was too low and didn't allow for training to complete, but setting it to ",{"type":27,"tag":170,"props":13594,"children":13596},{"className":13595},[],[13597],{"type":33,"value":13449},{"type":33,"value":13599}," seemed to work just fine.",{"type":27,"tag":36,"props":13601,"children":13602},{},[13603],{"type":33,"value":13604},"You can monitor GPU memory usage on NVIDIA cards with the following command:",{"type":27,"tag":238,"props":13606,"children":13608},{"code":13607},"$ nvidia-smi\nSun Nov 19 17:03:03 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\n| 27%   32C    P8    10W / 180W |   6707MiB /  8105MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0       350    C   /usr/bin/python                               6365MiB |\n|    0       554    G   /usr/lib/xorg-server/Xorg                       19MiB |\n|    0       588    G   /usr/bin/gnome-shell                            28MiB |\n|    0       853    G   /usr/lib/xorg-server/Xorg                      186MiB |\n|    0       873    G   compton                                          2MiB |\n|    0      1114    G   ...el-token=A50C2F183DB4F79482A2D8768ED1B285    64MiB |\n|    0      2190    G   ...el-token=1AC796A35DBDCDBE07AEC2FC1E8026C4    35MiB |\n+-----------------------------------------------------------------------------+\n",[13609],{"type":27,"tag":170,"props":13610,"children":13611},{"__ignoreMap":8},[13612],{"type":33,"value":13607},{"type":27,"tag":36,"props":13614,"children":13615},{},[13616],{"type":33,"value":13617},"I think this was a success! I'm fairly certain that we were leveraging the GPU to run the MNIST hand-written digit notebook. I didn't see messages that CUDNN loaded, but I can find versions of both CUDNN and CUDA in the docker image:",{"type":27,"tag":238,"props":13619,"children":13621},{"code":13620},"root@80f65a971e9a:/# ls /usr/include/x86_64-linux-gnu/\na.out.h  bits  cudnn_v6.h      fpu_control.h  gnu        python2.7\nasm      c++   expat_config.h  freetype2      ieee754.h  sys\n",[13622],{"type":27,"tag":170,"props":13623,"children":13624},{"__ignoreMap":8},[13625],{"type":33,"value":13620},{"type":27,"tag":238,"props":13627,"children":13629},{"code":13628},"root@80f65a971e9a:/# nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Tue_Jan_10_13:22:03_CST_2017\nCuda compilation tools, release 8.0, V8.0.61\n",[13630],{"type":27,"tag":170,"props":13631,"children":13632},{"__ignoreMap":8},[13633],{"type":33,"value":13628},{"type":27,"tag":36,"props":13635,"children":13636},{},[13637],{"type":33,"value":13638},"In previous attempts I had to register for an NVIDIA developer account and install these packages, but they seem to be packaged with the container.",{"type":27,"tag":36,"props":13640,"children":13641},{},[13642],{"type":33,"value":13643},"Finally, we can check the installed python packages:",{"type":27,"tag":238,"props":13645,"children":13647},{"code":13646},"root@80f65a971e9a:~# pip freeze | grep tensorflow\ntensorflow-gpu==1.4.0\ntensorflow-tensorboard==0.4.0rc2\nroot@80f65a971e9a:~#\n",[13648],{"type":27,"tag":170,"props":13649,"children":13650},{"__ignoreMap":8},[13651],{"type":33,"value":13646},{"type":27,"tag":36,"props":13653,"children":13654},{},[13655],{"type":33,"value":13656},"This looks good, but I'm still not 100% sure that everything was done properly. I would like to learn more about Tensorflow and also play around with some examples using Tensorboard. Let me know if you have any questions or comments about this setup, I'm still learning! Thanks for reading.",{"type":27,"tag":36,"props":13658,"children":13659},{},[13660,13662,13669],{"type":33,"value":13661},"Just for fun, here's a DeepDream rendering of a famous Donald Trump picture using Google's pre-trained ",{"type":27,"tag":61,"props":13663,"children":13666},{"href":13664,"rel":13665},"https://github.com/google/inception",[65],[13667],{"type":33,"value":13668},"Inception model",{"type":33,"value":13280},{"type":27,"tag":36,"props":13671,"children":13672},{},[13673],{"type":27,"tag":84,"props":13674,"children":13676},{"alt":13675,"src":13085},"png",[],{"type":27,"tag":36,"props":13678,"children":13679},{},[13680],{"type":33,"value":13681},"For comparison, here is the original image:",{"type":27,"tag":36,"props":13683,"children":13684},{},[13685],{"type":27,"tag":84,"props":13686,"children":13688},{"alt":13675,"src":13687},"/static/trump_original.jpg",[],{"type":27,"tag":5518,"props":13690,"children":13691},{},[13692],{"type":33,"value":5522},{"title":8,"searchDepth":343,"depth":343,"links":13694},[13695,13696],{"id":13122,"depth":343,"text":13125},{"id":13133,"depth":343,"text":13136,"children":13697},[13698],{"id":13245,"depth":379,"text":13248},"content:2017:11:19:tensorflow-gpu-setup-with-docker-on-arch-linux.md","2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux.md","2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux",1731145355689]