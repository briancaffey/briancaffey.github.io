[{"data":1,"prerenderedAt":68639},["ShallowReactive",2],{"/blog/tags/cms/":3},[4,886,2209,4080,6315,8176,8460,9193,10064,11235,15300,18783,20111,21164,21241,21537,24510,27055,27089,27561,28174,28499,28641,29020,29819,30168,32542,34498,36981,37049,38010,41616,42207,42592,43512,43798,44511,45126,45899,46089,46822,46895,47251,48233,49125,49354,50818,53391,53434,53475,57255,58632,60506,68367],{"id":5,"title":6,"body":7,"comments":315,"date":870,"description":871,"draft":872,"extension":873,"external":874,"image":875,"meta":876,"navigation":315,"path":877,"seo":878,"stem":879,"tags":880,"__hash__":885},"blog/2025/07/21/upgrading-my-static-nuxt-blog-from-nuxt-3-to-nuxt-4.md","Upgrading my Blog to Nuxt 4",{"type":8,"value":9,"toc":858},"minimark",[10,22,25,36,39,45,48,54,57,63,69,76,81,87,90,96,99,106,112,118,124,131,137,143,149,155,158,167,172,179,201,205,208,225,229,255,259,275,279,286,335,348,351,353,359,364,367,373,376,382,389,395,401,407,410,419,422,428,435,441,452,458,461,467,473,482,488,495,501,508,515,521,526,532,535,542,545,551,554,559,562,700,706,722,728,734,738,753,759,769,775,789,796,802,808,819,827,830,836,839,845,854],[11,12,13,14,21],"p",{},"This article will share my experience upgrading my Nuxt blog from Nuxt 3 to Nuxt 4. I'm following the official ",[15,16,20],"a",{"href":17,"rel":18},"https://nuxt.com/docs/4.x/getting-started/upgrade",[19],"nofollow","Upgrade Guide"," from Nuxt. It will be mostly unfiltered and I'll try to document things as they happen. Let's go!",[11,23,24],{},"First, I'll do this on a new branch:",[26,27,32],"pre",{"className":28,"code":30,"language":31},[29],"language-text","git branch -b nuxt4-upgrade\n","text",[33,34,30],"code",{"__ignoreMap":35},"",[11,37,38],{},"The first step in the migration guide says to run:",[26,40,43],{"className":41,"code":42,"language":31},[29],"yarn nuxt upgrade\n",[33,44,42],{"__ignoreMap":35},[11,46,47],{},"This gave me an error.",[26,49,52],{"className":50,"code":51,"language":31},[29],"error @nuxt/vite-builder@4.0.0: The engine \"node\" is incompatible with this module. Expected version \"^20.19.0 || >=22.12.0\". Got \"20.18.0\"\nerror Found incompatible module.\ninfo Visit https://yarnpkg.com/en/docs/cli/add for documentation about this command.\n\n ERROR  Command failed: yarn add -D nuxt                                                  3:41:22 PM\n\n  at genericNodeError (node:internal/errors:984:15)\n  at wrappedFn (node:internal/errors:538:14)\n  at checkExecSyncError (node:child_process:891:11)\n  at execSync (node:child_process:963:15)\n  at Object.run (node_modules/nuxi/dist/chunks/upgrade.mjs:99:5)\n  at async runCommand$1 (node_modules/nuxi/dist/shared/nuxi.6aad497e.mjs:1648:16)\n  at async runCommand$1 (node_modules/nuxi/dist/shared/nuxi.6aad497e.mjs:1639:11)\n  at async runMain$1 (node_modules/nuxi/dist/shared/nuxi.6aad497e.mjs:1777:7)\n\n\n\n ERROR  Command failed: yarn add -D nuxt                                                  3:41:22 PM\n\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n",[33,53,51],{"__ignoreMap":35},[11,55,56],{},"First, I need to upgrade node. I use nvm, so this is easy. Let's see what node versions I have installed:",[26,58,61],{"className":59,"code":60,"language":31},[29],"~/git/briancaffey.github.io$ nvm list\n->     v20.18.0\n       v22.15.0\ndefault -> 20 (-> v20.18.0)\niojs -> N/A (default)\nunstable -> N/A (default)\nnode -> stable (-> v22.15.0) (default)\nstable -> 22.15 (-> v22.15.0) (default)\nlts/* -> lts/jod (-> v22.15.0)\nlts/argon -> v4.9.1 (-> N/A)\nlts/boron -> v6.17.1 (-> N/A)\nlts/carbon -> v8.17.0 (-> N/A)\nlts/dubnium -> v10.24.1 (-> N/A)\nlts/erbium -> v12.22.12 (-> N/A)\nlts/fermium -> v14.21.3 (-> N/A)\nlts/gallium -> v16.20.2 (-> N/A)\nlts/hydrogen -> v18.20.8 (-> N/A)\nlts/iron -> v20.19.1 (-> N/A)\nlts/jod -> v22.15.0\n",[33,62,60],{"__ignoreMap":35},[26,64,67],{"className":65,"code":66,"language":31},[29],"~/git/briancaffey.github.io$ nvm install 22.17.1\nDownloading and installing node v22.17.1...\nDownloading https://nodejs.org/dist/v22.17.1/node-v22.17.1-darwin-arm64.tar.xz...\n###################################################################################################################################################################################### 100.0%\nComputing checksum with sha256sum\nChecksums matched!\nNow using node v22.17.1 (npm v10.9.2)\n",[33,68,66],{"__ignoreMap":35},[11,70,71,72,75],{},"OK, great, now let's run that ",[33,73,74],{},"yarn nuxt upgrade"," again:",[26,77,79],{"className":78,"code":42,"language":31},[29],[33,80,42],{"__ignoreMap":35},[26,82,85],{"className":83,"code":84,"language":31},[29],"~/git/briancaffey.github.io$ yarn nuxt upgrade\nyarn run v1.22.22\nerror Command \"nuxt\" not found.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n",[33,86,84],{"__ignoreMap":35},[11,88,89],{},"Hmm, I'll try adding nuxt again:",[26,91,94],{"className":92,"code":93,"language":31},[29],"yarn add nuxt@^4.0.0\n\n...\n✨  Done in 36.03s.\n",[33,95,93],{"__ignoreMap":35},[11,97,98],{},"OK, great! Now I have Nuxt 4 installed.",[11,100,101,102,105],{},"The next section is about ",[33,103,104],{},"Migrating Using Codemods",". I'll try running this:",[107,108,109],"blockquote",{},[11,110,111],{},"This command will execute all codemods in sequence, with the option to deselect any that you do not wish to run. Each codemod is also listed below alongside its respective change and can be executed independently.",[26,113,116],{"className":114,"code":115,"language":31},[29],"yarn dlx codemod@latest nuxt/4/migration-recipe\n",[33,117,115],{"__ignoreMap":35},[26,119,122],{"className":120,"code":121,"language":31},[29],"~/git/briancaffey.github.io$ yarn dlx codemod@latest nuxt/4/migration-recipe\nyarn run v1.22.22\nerror Command \"dlx\" not found.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n",[33,123,121],{"__ignoreMap":35},[11,125,126,127,130],{},"OK, I guess I need to add ",[33,128,129],{},"dlx"," with yarn first:",[26,132,135],{"className":133,"code":134,"language":31},[29],"yarn add dlx\n",[33,136,134],{"__ignoreMap":35},[26,138,141],{"className":139,"code":140,"language":31},[29],"success Saved lockfile.\nsuccess Saved 2 new dependencies.\ninfo Direct dependencies\n└─ dlx@0.2.1\ninfo All dependencies\n├─ dlx@0.2.1\n└─ underscore@1.13.7\n✨  Done in 2.52s.\n",[33,142,140],{"__ignoreMap":35},[11,144,145,146,148],{},"But I still got the same error about ",[33,147,129],{},", so I asked cursor:",[26,150,153],{"className":151,"code":152,"language":31},[29],"I'm trying to run this yarn command:\n\nyarn dlx codemod@latest nuxt/4/migration-recipe\n\n\nBut I'm getting an error:\n\nyarn dlx codemod@latest nuxt/4/migration-recipe\nyarn run v1.22.22\nerror Command \"dlx\" not found.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n\nHow can I fix this?\n",[33,154,152],{"__ignoreMap":35},[156,157],"hr",{},[11,159,160,161,163,164,166],{},"The issue is that you're using Yarn v1.22.22, which doesn't have the ",[33,162,129],{}," command. The ",[33,165,129],{}," command was introduced in Yarn v2+ (Berry). Here are several ways to fix this:",[168,169,171],"h2",{"id":170},"option-1-use-npx-recommended","Option 1: Use npx (Recommended)",[11,173,174,175,178],{},"Since you're using Yarn v1, you can use ",[33,176,177],{},"npx"," instead:",[26,180,184],{"className":181,"code":182,"language":183,"meta":35,"style":35},"language-bash shiki shiki-themes github-light github-dark","npx codemod@latest nuxt/4/migration-recipe\n","bash",[33,185,186],{"__ignoreMap":35},[187,188,191,194,198],"span",{"class":189,"line":190},"line",1,[187,192,177],{"class":193},"sScJk",[187,195,197],{"class":196},"sZZnC"," codemod@latest",[187,199,200],{"class":196}," nuxt/4/migration-recipe\n",[168,202,204],{"id":203},"option-2-use-yarn-create-alternative","Option 2: Use yarn create (Alternative)",[11,206,207],{},"You can try using yarn's create command:",[26,209,211],{"className":181,"code":210,"language":183,"meta":35,"style":35},"yarn create codemod@latest nuxt/4/migration-recipe\n",[33,212,213],{"__ignoreMap":35},[187,214,215,218,221,223],{"class":189,"line":190},[187,216,217],{"class":193},"yarn",[187,219,220],{"class":196}," create",[187,222,197],{"class":196},[187,224,200],{"class":196},[168,226,228],{"id":227},"option-3-install-codemod-globally-first","Option 3: Install codemod globally first",[26,230,232],{"className":181,"code":231,"language":183,"meta":35,"style":35},"yarn global add codemod\ncodemod@latest nuxt/4/migration-recipe\n",[33,233,234,247],{"__ignoreMap":35},[187,235,236,238,241,244],{"class":189,"line":190},[187,237,217],{"class":193},[187,239,240],{"class":196}," global",[187,242,243],{"class":196}," add",[187,245,246],{"class":196}," codemod\n",[187,248,250,253],{"class":189,"line":249},2,[187,251,252],{"class":193},"codemod@latest",[187,254,200],{"class":196},[168,256,258],{"id":257},"option-4-use-the-full-path","Option 4: Use the full path",[26,260,262],{"className":181,"code":261,"language":183,"meta":35,"style":35},"yarn exec codemod@latest nuxt/4/migration-recipe\n",[33,263,264],{"__ignoreMap":35},[187,265,266,268,271,273],{"class":189,"line":190},[187,267,217],{"class":193},[187,269,270],{"class":196}," exec",[187,272,197],{"class":196},[187,274,200],{"class":196},[168,276,278],{"id":277},"option-5-upgrade-to-yarn-v2-optional","Option 5: Upgrade to Yarn v2+ (Optional)",[11,280,281,282,285],{},"If you want to use ",[33,283,284],{},"yarn dlx",", you could upgrade to Yarn v2+:",[26,287,289],{"className":181,"code":288,"language":183,"meta":35,"style":35},"# Enable Yarn v2+ for this project\nyarn set version berry\n\n# Then you can use dlx\nyarn dlx codemod@latest nuxt/4/migration-recipe\n",[33,290,291,297,310,317,323],{"__ignoreMap":35},[187,292,293],{"class":189,"line":190},[187,294,296],{"class":295},"sJ8bj","# Enable Yarn v2+ for this project\n",[187,298,299,301,304,307],{"class":189,"line":249},[187,300,217],{"class":193},[187,302,303],{"class":196}," set",[187,305,306],{"class":196}," version",[187,308,309],{"class":196}," berry\n",[187,311,313],{"class":189,"line":312},3,[187,314,316],{"emptyLinePlaceholder":315},true,"\n",[187,318,320],{"class":189,"line":319},4,[187,321,322],{"class":295},"# Then you can use dlx\n",[187,324,326,328,331,333],{"class":189,"line":325},5,[187,327,217],{"class":193},[187,329,330],{"class":196}," dlx",[187,332,197],{"class":196},[187,334,200],{"class":196},[11,336,337,344,345,347],{},[338,339,340,341,343],"strong",{},"I recommend using Option 1 (",[33,342,177],{},")"," since it's the most straightforward and doesn't require changing your package manager setup. The ",[33,346,177],{}," command is available by default with Node.js and will work perfectly for running the codemod.",[11,349,350],{},"After running the codemod, you can continue with the manual upgrade steps I outlined in the previous plan.",[156,352],{},[11,354,355,356,358],{},"OK, I'll try upgrading yarn later, and for now I'll go with using ",[33,357,177],{},":",[26,360,362],{"className":361,"code":182,"language":31},[29],[33,363,182],{"__ignoreMap":35},[11,365,366],{},"OK, that worked, now I got the list of options to choose from for my Nuxt 4 upgrade:",[26,368,371],{"className":369,"code":370,"language":31},[29],"✔ Successfully downloaded \"nuxt/4/migration-recipe\" from the registry.\n? Press Enter to run the selected codemods in order. You can deselect anything you don’t want. (Press\n\u003Cspace> to select, \u003Ca> to toggle all, \u003Ci> to invert selection, and \u003Center> to proceed)\n❯◉ nuxt/4/absolute-watch-path\n ◉ nuxt/4/default-data-error-value\n ◉ nuxt/4/deprecated-dedupe-value\n ◉ nuxt/4/file-structure\n ◉ nuxt/4/shallow-function-reactivity\n ◉ nuxt/4/template-compilation-changes\n",[33,372,370],{"__ignoreMap":35},[11,374,375],{},"OK! That seems to work! It ran each codemod, for example:",[26,377,380],{"className":378,"code":379,"language":31},[29],"╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\n│                                                                                                  │\n│                                                                                                  │\n│      Codemod: nuxt/4/absolute-watch-path@1.0.3                                                   │\n│      Target: /Users/brian/git/briancaffey.github.io                                              │\n│                                                                                                  │\n│      Using paths provided by codemod settings                                                    │\n│      Included patterns: **/*.js, **/*.jsx, **/*.ts, **/*.tsx, **/*.vue                           │\n│      Patterns excluded by default: **/*.d.ts, **/node_modules/**/*.*, **/.next/**/*.*,           │\n│      **/dist/**/*.*, **/build/**/*.*, **/.git/**/*.*,                                            │\n│      **/.svn/**/*.*, **/.hg/**/*.*, **/.bzr/**/*.*,                                              │\n│      **/_darcs/**/*.*, **/_MTN/**/*.*, **/_FOSSIL_, **/.fslckout,                                │\n│      **/.view/**/*.*                                                                             │\n│      Patterns excluded from gitignore: **/.vscode/**/*.*, **/node_modules, /logs, **/*.log,      │\n│      **/npm-debug.log*, **/yarn-debug.log*, **/yarn-error.log*,                                  │\n│      **/pids, **/*.pid, **/*.seed, **/*.pid.lock, **/lib-cov,                                    │\n│      **/coverage, **/.nyc_output, **/.grunt, **/bower_components,                                │\n│      **/.lock-wscript, **/build/Release, **/node_modules/**/*.*,                                 │\n│      **/jspm_packages/**/*.*, **/typings/**/*.*, **/.npm,                                        │\n│      **/.eslintcache, **/.node_repl_history, **/*.tgz,                                           │\n│      **/.yarn-integrity, **/.env, **/.cache, **/.next, **/.nuxt,                                 │\n│      **/dist, **/.vuepress/dist, **/.serverless, **/.idea,                                       │\n│      **/sw.*, **/.DS_Store, **/*.swp, **/notes, **/.output                                       │\n│                                                                                                  │\n│      Running in 4 threads                                                                        │\n│      File formatting disabled                                                                    │\n│                                                                                                  │\n│                                                                                                  │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",[33,381,379],{"__ignoreMap":35},[11,383,384,385,388],{},"The big change at this point is that most of the code was moved to the ",[33,386,387],{},"/app"," directory:",[26,390,393],{"className":391,"code":392,"language":31},[29],"Untracked files:\n  (use \"git add \u003Cfile>...\" to include in what will be committed)\n        app/\n        content/2025/07/21/\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n",[33,394,392],{"__ignoreMap":35},[11,396,397,398],{},"Is that it? Let's try running ",[33,399,400],{},"yarn dev",[26,402,405],{"className":403,"code":404,"language":31},[29],"yarn dev\n...\n ERROR  Cannot start nuxt:  ENOENT: no such file or directory, open '/Users/brian/git/briancaffey.github.io/app/i18n/en-US.js'                           4:12:36 PM\n",[33,406,404],{"__ignoreMap":35},[11,408,409],{},"OK, we have an error related to i18n. I have struggled with i18n a fair bit when doing major changes to my site.",[11,411,412,413,415,416,418],{},"It looks like Nuxt 4 expects the i18n folder to be under tha new ",[33,414,387],{}," directory, but it is still in the root directory, so let's move that and try running ",[33,417,400],{}," again.",[11,420,421],{},"Cool, no errors:",[26,423,426],{"className":424,"code":425,"language":31},[29],"~/git/briancaffey.github.io$ yarn dev\nyarn run v1.22.22\n$ nuxi dev --host\nNuxt 4.0.0 with Nitro 2.12.0                                              nuxi 4:16:03 PM\n                                                                               4:16:03 PM\n\n              █▀▀▀▀▀▀▀██▀▀▀█▀██▀█▀▀▀▀▀▀▀█\n              █ █▀▀▀█ ██▀▀ ▄▄██▀█ █▀▀▀█ █\n              █ █   █ █ █ ▄▀▄ ▄▄█ █   █ █\n              █ ▀▀▀▀▀ █▀▄ █▀▄▀█ █ ▀▀▀▀▀ █\n              █▀▀███▀▀▀█ ▀█ ▀█▀ ███▀▀████\n              ██ ▄ ▄▀▀▀█▀▄▀▀▀▀ ▄▄▀▀▄ ▄ ▀█\n              █▀█▄▀▀ ▀▀ ▀ ▄▀█▀▄ ▀▀█▄▄██ █\n              █ ▄██ █▀█▀▀▄█▄ ▄ █▀█▀█▀█ ▀█\n              █ █▀█  ▀▄▄█▄ ▀▀▄█ ▀▀▀ █ █▄█\n              █▀▀▀▀▀▀▀█ ▀▄  █ ▄ █▀█ ▀▄█▀█\n              █ █▀▀▀█ █▄ ▀█▄▀▄▀ ▀▀▀ ▀▀███\n              █ █   █ ██▄▀▄ ████▀▄▄█▄▀▄ █\n              █ ▀▀▀▀▀ █ ▀ ▀▄█▀  █ ▄▄▀██ █\n              ▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\n\n  ➜ Local:    http://localhost:3000/\n  ➜ Network:  http://192.168.5.226:3000/ [QR code]\n\nℹ Using default Tailwind CSS file                            nuxt:tailwindcss 4:16:04 PM\n\n[4:16:04 PM]  WARN  Locales en-US, fr-FR, zh-ZH, ru-RU, ja-JP, hi-IN uses deprecated iso property, this will be replaced with language in v9\n\n  ➜ DevTools: press Shift + Option + D in the browser (v2.6.2)                 4:16:04 PM\n\n✔ Vite client built in 45ms                                                   4:16:05 PM\n✔ Vite server built in 30ms                                                   4:16:05 PM\n✔ Nuxt Nitro server built in 1071ms                                     nitro 4:16:07 PM\nℹ Vite client warmed up in 1ms                                                4:16:07 PM\nℹ Vite server warmed up in 63ms                                               4:16:07 PM\n[4:16:25 PM] ℹ ✨ new dependencies optimized: @vue/devtools-core, @vue/devtools-kit, vue-disqus, vue3-apexcharts, pinia, @intlify/shared, is-https, @intlify/core-base\nℹ ✨ optimized dependencies changed. reloading                                4:16:25 PM\n",[33,427,425],{"__ignoreMap":35},[11,429,430,431,434],{},"But I am getting a big ",[33,432,433],{},"500"," error in the browser:",[26,436,439],{"className":437,"code":438,"language":31},[29],"500\norgTransform.apply is not a function\n\nCustomize this page\nat createError (/Users/brian/git/briancaffey.github.io/node_modules/h3/dist/index.mjs:71:15)\nat /Users/brian/git/briancaffey.github.io/node_modules/@nuxt/vite-builder/dist/index.mjs:403:21)\nat async processMessage (/Users/brian/git/briancaffey.github.io/node_modules/@nuxt/vite-builder/dist/index.mjs:386:30)\n",[33,440,438],{"__ignoreMap":35},[11,442,443,444,447,448,451],{},"Searching GitHub's code, I it looks like ",[33,445,446],{},"orgTransform.apply"," comes from ",[33,449,450],{},"rdhainaut/unplugin-vue-i18n · lib/index.mjs",", so this is likely another i18n issue.",[26,453,456],{"className":454,"code":455,"language":31},[29],"\"@nuxtjs/i18n@^8.3.3\":\n  version \"8.5.6\"\n  resolved \"https://registry.yarnpkg.com/@nuxtjs/i18n/-/i18n-8.5.6.tgz#d65b375fba5244d83fc6833a604e2b532a64bef2\"\n  integrity sha512-L+g+LygKNoaS/AXExk7tzS9wSNn9QdP1T9VdTjjEGYftpeFgv2U8AQsY0dQAhgPIbXXhIAkNYxTk4YcINj9CfA==\n  dependencies:\n    \"@intlify/h3\" \"^0.5.0\"\n    \"@intlify/shared\" \"^9.14.1\"\n    \"@intlify/unplugin-vue-i18n\" \"^3.0.1\"\n",[33,457,455],{"__ignoreMap":35},[11,459,460],{},"Let's make sure that I'm on the latest version of Nuxt's i18n:",[26,462,465],{"className":463,"code":464,"language":31},[29],"npx nuxi module add i18n\n",[33,466,464],{"__ignoreMap":35},[26,468,471],{"className":469,"code":470,"language":31},[29],"yarn add @nuxtjs/i18n@^10.0.0\n",[33,472,470],{"__ignoreMap":35},[11,474,475,476,479,480,358],{},"After upgrading ",[33,477,478],{},"@nuxtjs/i18n"," to v10.0.0 I got another i18n error when running ",[33,481,400],{},[26,483,486],{"className":484,"code":485,"language":31},[29],"[4:26:12 PM]  ERROR  Cannot start nuxt:  ENOENT: no such file or directory, open '/Users/brian/git/briancaffey.github.io/i18n/locales/i18n/en-US.js'\n\n    at readFileSync (node:fs:441:20)\n    at getLocaleType (node_modules/@nuxtjs/i18n/dist/module.mjs:143:34)\n    at resolveLocales (node_modules/@nuxtjs/i18n/dist/module.mjs:128:20)\n    at resolveLocaleInfo (node_modules/@nuxtjs/i18n/dist/module.mjs:1511:20)\n    at async node_modules/@nuxtjs/i18n/dist/module.mjs:1926:7\n    at async initNuxt (node_modules/nuxt/dist/index.mjs:5613:3)\n    at async NuxtDevServer._load (node_modules/@nuxt/cli/dist/chunks/index.mjs:211:5)\n    at async NuxtDevServer.load (node_modules/@nuxt/cli/dist/chunks/index.mjs:139:7)\n    at async NuxtDevServer.init (node_modules/@nuxt/cli/dist/chunks/index.mjs:130:5)\n    at async initialize (node_modules/@nuxt/cli/dist/chunks/index.mjs:426:3)\n",[33,487,485],{"__ignoreMap":35},[11,489,490,491,494],{},"OK! So I rarranged my ",[33,492,493],{},"i18n"," folder to match watch it was looking for:",[26,496,499],{"className":497,"code":498,"language":31},[29],"~/git/briancaffey.github.io$ tree i18n -L 3\ni18n\n├── i18n.config.js\n└── locales\n    └── i18n\n        ├── en-US.js\n        ├── fr-FR.js\n        ├── hi-IN.js\n        ├── jp-JP.js\n        ├── ru-RU.js\n        └── zh-ZH.js\n",[33,500,498],{"__ignoreMap":35},[11,502,503,504,507],{},"Now I have my language files duplicated, however. But at least I'm able to visit my site in the browser! And I can see in the NuxtDevTools window that I am using ",[33,505,506],{},"v4.0.0","!",[11,509,510],{},[511,512],"img",{"alt":513,"src":514},"NuxtDevTools","/static/nuxt4/nuxt-dev-tools.png",[26,516,519],{"className":517,"code":518,"language":31},[29],"  ➜ Local:    http://localhost:3000/\n  ➜ Network:  http://192.168.5.226:3000/ [QR code]\n\nℹ Using default Tailwind CSS file                                                    nuxt:tailwindcss 4:31:24 PM\n  ➜ DevTools: press Shift + Option + D in the browser (v2.6.2)                                         4:31:24 PM\n\nℹ Re-optimizing dependencies because lockfile has changed                                             4:31:25 PM\n✔ Vite client built in 61ms                                                                           4:31:25 PM\n✔ Vite server built in 42ms                                                                           4:31:26 PM\n✔ Nuxt Nitro server built in 1422ms                                                             nitro 4:31:27 PM\nℹ Vite client warmed up in 1ms                                                                        4:31:27 PM\nℹ Vite server warmed up in 164ms                                                                      4:31:27 PM\n[4:31:38 PM]  WARN  [@nuxtjs/mdc] Language \"Dockerfile\" is not loaded to the Shiki highlighter, fallback to plain text. Add the language to \"mdc.highlight.langs\" to fix this.\n[4:31:38 PM]  WARN  [@nuxtjs/mdc] Language \"Dockerfile\" is not loaded to the Shiki highlighter, fallback to plain text. Add the language to \"mdc.highlight.langs\" to fix this.\n[4:31:38 PM]  WARN  [@nuxtjs/mdc] Language \"vue\" is not loaded to the Shiki highlighter, fallback to plain text. Add the language to \"mdc.highlight.langs\" to fix this.\n[4:31:48 PM] ℹ ✨ new dependencies optimized: @vue/devtools-core, @vue/devtools-kit, vue-disqus, vue3-apexcharts, pinia\nℹ ✨ optimized dependencies changed. reloading                                                        4:31:48 PM\nℹ ✨ new dependencies optimized: @vueuse/components                                                   4:31:50 PM\nℹ ✨ optimized dependencies changed. reloading                                                        4:31:50 PM\n",[33,520,518],{"__ignoreMap":35},[11,522,523,524,358],{},"I went through each of my project dependencies and updated them to the latest versions. After doing this and resolving some small issues, I got the following when running ",[33,525,400],{},[26,527,530],{"className":528,"code":529,"language":31},[29],"[@nuxt/content 7:10:17 PM]  WARN  No content configuration found, falling back to default collection. In order to have full control over your collections, create the config file in project root. See: https://content.nuxt.com/docs/getting-started/installation\n\nℹ Using default Tailwind CSS file                                                                 nuxt:tailwindcss 7:10:17 PM\n  ➜ DevTools: press Shift + Option + D in the browser (v2.6.2)                                                      7:10:17 PM\n\n\n ERROR  Nuxt Content requires better-sqlite3 module to operate.                                       @nuxt/content 7:10:18 PM\n\n                                                                                                                    7:10:18 PM\n                                                                                                                    7:10:18 PM\n❯ Do you want to install better-sqlite3 package?\n● Yes / ○ No\n",[33,531,529],{"__ignoreMap":35},[11,533,534],{},"After installing this I got another error!",[11,536,537,538],{},"Upgrading from Nuxt Content v2 to v3 introduced some breaking changes, so I had to work through those. ",[15,539,540],{"href":540,"rel":541},"https://content.nuxt.com/docs/getting-started/migration",[19],[11,543,544],{},"This is one of the errors I got after upgrading Nuxt Content from v2 to v3.",[26,546,549],{"className":547,"code":548,"language":31},[29]," ERROR  [request error] [unhandled] [GET] http://localhost:3000/blog/1                                                                                                                      7:14:09 PM\n\n\nℹ Error: Cannot read properties of undefined (reading 'length')\n\n ⁃ at _sfc_ssrRender (app/pages/blog/[number].vue:78:62)\n\n   37 ┃\n   38 ┃  const { data: paginatedItems } = await useAsyncData(route.path, () =>\n   39 ┃    queryCollection(\"/\")\n   40 ┃      .where({ draft: { $ne: true } })\n   41 ┃      .sort({'date': -1})\n   42 ┃      .limit(10)\n   43 ┃      .skip(9 * (pageNo - 1))\n   44 ┃      .find()\n   45 ┃  )\n   46 ┃  \u003C/script>\n   47 ┃\n",[33,550,548],{"__ignoreMap":35},[11,552,553],{},"The API changed and it uses a SQL-like query language. The migration docs mentioned this:",[107,555,556],{},[11,557,558],{},"The new API is backed by SQL and content queries happens within a specific collection.",[11,560,561],{},"Now the above query is written more like this with the new version of Nuxt Content:",[26,563,567],{"className":564,"code":565,"language":566,"meta":35,"style":35},"language-js shiki shiki-themes github-light github-dark","const { data: paginatedItems } = await useAsyncData(route.path, () =>\n  queryCollection(\"blog\")\n    .order(\"date\", \"DESC\")\n    .limit(10)\n    .skip(9 * (pageNo - 1))\n    .all()\n)\n","js",[33,568,569,608,622,643,657,684,695],{"__ignoreMap":35},[187,570,571,575,579,583,586,590,593,596,599,602,605],{"class":189,"line":190},[187,572,574],{"class":573},"szBVR","const",[187,576,578],{"class":577},"sVt8B"," { ",[187,580,582],{"class":581},"s4XuR","data",[187,584,585],{"class":577},": ",[187,587,589],{"class":588},"sj4cs","paginatedItems",[187,591,592],{"class":577}," } ",[187,594,595],{"class":573},"=",[187,597,598],{"class":573}," await",[187,600,601],{"class":193}," useAsyncData",[187,603,604],{"class":577},"(route.path, () ",[187,606,607],{"class":573},"=>\n",[187,609,610,613,616,619],{"class":189,"line":249},[187,611,612],{"class":193},"  queryCollection",[187,614,615],{"class":577},"(",[187,617,618],{"class":196},"\"blog\"",[187,620,621],{"class":577},")\n",[187,623,624,627,630,632,635,638,641],{"class":189,"line":312},[187,625,626],{"class":577},"    .",[187,628,629],{"class":193},"order",[187,631,615],{"class":577},[187,633,634],{"class":196},"\"date\"",[187,636,637],{"class":577},", ",[187,639,640],{"class":196},"\"DESC\"",[187,642,621],{"class":577},[187,644,645,647,650,652,655],{"class":189,"line":319},[187,646,626],{"class":577},[187,648,649],{"class":193},"limit",[187,651,615],{"class":577},[187,653,654],{"class":588},"10",[187,656,621],{"class":577},[187,658,659,661,664,666,669,672,675,678,681],{"class":189,"line":325},[187,660,626],{"class":577},[187,662,663],{"class":193},"skip",[187,665,615],{"class":577},[187,667,668],{"class":588},"9",[187,670,671],{"class":573}," *",[187,673,674],{"class":577}," (pageNo ",[187,676,677],{"class":573},"-",[187,679,680],{"class":588}," 1",[187,682,683],{"class":577},"))\n",[187,685,687,689,692],{"class":189,"line":686},6,[187,688,626],{"class":577},[187,690,691],{"class":193},"all",[187,693,694],{"class":577},"()\n",[187,696,698],{"class":189,"line":697},7,[187,699,621],{"class":577},[168,701,703],{"id":702},"yarn-generate",[33,704,705],{},"yarn generate",[11,707,708,709,711,712,714,715,718,719,721],{},"After fixing the Nuxt Content query syntax for Nuxt Content v3, I tried running ",[33,710,705],{}," to build my static site. This can often uncover issues that you won't see when just running ",[33,713,400],{},". Right away I found an issue with my Pinia Store. It couldn't find the file for my store. The codemod migration tool did not move my ",[33,716,717],{},"store"," folder into the ",[33,720,387],{}," directory, so I had to do that manually.",[11,723,724,725,727],{},"OK! After fixing the Pinia Store issues I was able to run ",[33,726,705],{}," successfully!",[26,729,732],{"className":730,"code":731,"language":31},[29],"ℹ Prerendered 785 routes in 93.991 seconds\n✔ Generated public .output/public\n✔ You can preview this build using npx serve .output/public\n✔ You can now deploy .output/public to any static hosting!\n✨  Done in 105.98s.\n",[33,733,731],{"__ignoreMap":35},[168,735,737],{"id":736},"cicd","CI/CD",[11,739,740,741,744,745,748,749,752],{},"To deploy my blog to ",[33,742,743],{},"briancaffey.github.io"," I just have to push my working changes to the ",[33,746,747],{},"master"," branch of my GitHub repository. GitHub Actions builds the site and pushes the build assets to GitHub Pages for me. To avoid any issues I need to make sure I'm using the correct version of ",[33,750,751],{},"node",".",[26,754,757],{"className":755,"code":756,"language":31},[29],"- name: Setup Node\n  uses: actions/setup-node@v4\n  with:\n    node-version: \"20\"\n",[33,758,756],{"__ignoreMap":35},[11,760,761,762,765,766],{},"I updated ",[33,763,764],{},"\"20\""," to ",[33,767,768],{},"\"22.17.1\"",[168,770,772],{"id":771},"datacontentcontentssqlite",[33,773,774],{},".data/content/contents.sqlite",[11,776,777,778,765,781,784,785,343],{},"Something else I noticed is this new sqlite file. This is just for debugging, so we can add ",[33,779,780],{},".data",[33,782,783],{},".gitignore"," (",[15,786,787],{"href":787,"rel":788},"https://content.nuxt.com/docs/advanced/tools#locate-your-sqlite-database",[19],[168,790,792,795],{"id":791},"yarn-lint-error",[33,793,794],{},"yarn lint"," error",[11,797,798,799,801],{},"I forgot to see if the code would properly lint, so on pushing to the master branch I got a failed deployment pipeline. Running ",[33,800,794],{}," from my Mac I get the same error:",[26,803,806],{"className":804,"code":805,"language":31},[29],"~/git/briancaffey.github.io$ yarn lint\nyarn run v1.22.22\n$ eslint .\n\nOops! Something went wrong! :(\n\nESLint: 9.31.0\n\nTypeError [ERR_INVALID_MODULE_SPECIFIER]: Invalid module \".nuxt/eslint.config.mjs\" is not a valid package name imported from /Users/brian/git/briancaffey.github.io/eslint.config.mjs\n    at parsePackageName (node:internal/modules/package_json_reader:211:11)\n    at Object.getPackageJSONURL (node:internal/modules/package_json_reader:222:53)\n    at packageResolve (node:internal/modules/esm/resolve:768:81)\n    at moduleResolve (node:internal/modules/esm/resolve:854:18)\n    at defaultResolve (node:internal/modules/esm/resolve:984:11)\n    at ModuleLoader.defaultResolve (node:internal/modules/esm/loader:780:12)\n    at #cachedDefaultResolve (node:internal/modules/esm/loader:704:25)\n    at ModuleLoader.resolve (node:internal/modules/esm/loader:687:38)\n    at ModuleLoader.getModuleJobForImport (node:internal/modules/esm/loader:305:38)\n    at ModuleJob._link (node:internal/modules/esm/module_job:175:49)\nerror Command failed with exit code 2.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n",[33,807,805],{"__ignoreMap":35},[11,809,810,811,814,815,818],{},"The error seems to be related to the file ",[33,812,813],{},"eslint.config.mjs",". I'm not familiar with what ",[33,816,817],{},".mjs"," files are, so I asked cursor:",[107,820,821,824],{},[11,822,823],{},"What are .mjs files?",[11,825,826],{},".mjs stands for \"Module JavaScript\" and is a file extension that explicitly tells Node.js to treat the file as an ES module, regardless of your project's configuration.",[11,828,829],{},"I was able to fix the error by making a small change to the input line that was throwing the error:",[26,831,834],{"className":832,"code":833,"language":31},[29],"import withNuxt from '.nuxt/eslint.config.mjs'\n",[33,835,833],{"__ignoreMap":35},[11,837,838],{},"changed to",[26,840,843],{"className":841,"code":842,"language":31},[29],"import withNuxt from './.nuxt/eslint.config.mjs'\n",[33,844,842],{"__ignoreMap":35},[11,846,847,848,850,851,853],{},"Now running ",[33,849,794],{}," gave me small error in one of my components. Fixing that resulted in a successful ",[33,852,794],{},"! OK, it should work this time!",[855,856,857],"style",{},"html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .s4XuR, html code.shiki .s4XuR{--shiki-default:#E36209;--shiki-dark:#FFAB70}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}",{"title":35,"searchDepth":249,"depth":249,"links":859},[860,861,862,863,864,865,866,867,868],{"id":170,"depth":249,"text":171},{"id":203,"depth":249,"text":204},{"id":227,"depth":249,"text":228},{"id":257,"depth":249,"text":258},{"id":277,"depth":249,"text":278},{"id":702,"depth":249,"text":705},{"id":736,"depth":249,"text":737},{"id":771,"depth":249,"text":774},{"id":791,"depth":249,"text":869},"yarn lint error","2025-07-21","This article details my process of upgrading my static GitHub Pages Blog from Nuxt 3 to Nuxt 4, and an upgrade of the headless CMS that powers my site, Nuxt Content, from v2 to v3",false,"md",null,"/static/nuxt4/nuxt4.png",{},"/2025/07/21/upgrading-my-static-nuxt-blog-from-nuxt-3-to-nuxt-4",{"title":6,"description":871},"2025/07/21/upgrading-my-static-nuxt-blog-from-nuxt-3-to-nuxt-4",[881,882,883,884],"nuxt","vue","blogging","cms","QjWGr_nCs32DuBRXFBVhsJkA3SQG9NFz-dzeNpy1YEk",{"id":887,"title":888,"body":889,"comments":315,"date":2191,"description":2192,"draft":872,"extension":873,"external":2193,"image":2197,"meta":2198,"navigation":315,"path":2199,"seo":2200,"stem":2201,"tags":2202,"__hash__":2208},"blog/2025/07/17/flux-plugin-for-project-g-assist-hackathon.md","Flux Plug-in for Project G-Assist",{"type":8,"value":890,"toc":2149},[891,894,897,900,903,906,910,915,943,947,973,977,1009,1013,1039,1043,1060,1064,1067,1084,1091,1095,1099,1102,1105,1109,1116,1158,1162,1180,1184,1198,1310,1314,1323,1355,1359,1374,1378,1381,1438,1448,1452,1455,1458,1461,1464,1467,1471,1478,1484,1487,1493,1496,1502,1505,1509,1520,1524,1690,1701,1705,1711,1714,1721,1725,1728,1730,1738,1742,1745,1751,1754,1758,1761,1781,1787,1790,1796,1813,1817,1823,1828,1831,1834,1838,1852,1856,1867,1871,1885,1889,1892,1908,1912,1915,2062,2066,2073,2087,2091,2117,2121,2128,2132,2139,2143,2146],[11,892,893],{},"This article will discuss my submission for the Project G-Assist Hackathon: Flux Plug-in for Project G-Assist. This plugin allows RTX AI PC users to tap into their GPU's image generation capabilities through the Flux.1-dev NVIDIA NIM. G-Assist now generates images from your commands on demand!",[895,896],"flux-plugin-tweet",{},[11,898,899],{},"The Flux Plugin for G-Assist is a plugin developed for NVIDIA’s Project G-Assist that brings real-time AI image generation directly to the desktop through natural language commands. It allows users to create high-quality images using the Flux family of models from Black Forest Labs, seamlessly integrated with the G-Assist interface. Users can simply type or speak prompts such as “a futuristic robot painter in a neon-lit workshop”, and the plugin handles the entire process—from submitting the prompt to generating and displaying the image—all without leaving the G-Assist chat window.",[11,901,902],{},"The plugin supports multiple deployment backends for inference, including Flux NIMs running locally via WSL, on the cloud via build.nvidia.com, or through InvokeAI using the Flux Kontext model for image-to-image and screenshot-based transformations. The plugin includes additional tools for managing the inference service, such as checking status and turning the NIM service on or off, enabling a user-friendly experience for both beginners and power users.",[11,904,905],{},"The goal of this plugin is to make generative image workflows faster, more accessible, and more fun—leveraging the strengths of NVIDIA’s hardware, AI models, and desktop ecosystem. By combining the power of FLUX with the voice-enabled G-Assist interface, the plugin turns any PC into a hands-free creative studio.",[168,907,909],{"id":908},"what-can-it-do","What Can It Do?",[911,912,914],"h3",{"id":913},"image-generation","Image Generation",[916,917,918,925,931,937],"ul",{},[919,920,921,924],"li",{},[338,922,923],{},"Generate images from text prompts"," using Flux AI model",[919,926,927,930],{},[338,928,929],{},"Support for multiple backends",": Local NIM servers, NVIDIA hosted services, or InvokeAI",[919,932,933,936],{},[338,934,935],{},"Automatic desktop background setting"," - generated images can be set as your wallpaper",[919,938,939,942],{},[338,940,941],{},"High-quality output"," with customizable parameters (resolution, steps, CFG scale)",[911,944,946],{"id":945},"nim-server-management","NIM Server Management",[916,948,949,955,961,967],{},[919,950,951,954],{},[338,952,953],{},"Start/stop local Flux NIM servers"," using WSL and Podman",[919,956,957,960],{},[338,958,959],{},"Check NIM server status"," to see if the service is running",[919,962,963,966],{},[338,964,965],{},"Health endpoint testing"," for local servers",[919,968,969,972],{},[338,970,971],{},"Automatic configuration"," using NGC API keys and Hugging Face tokens",[911,974,976],{"id":975},"invokeai-integration","InvokeAI Integration",[916,978,979,985,991,997,1003],{},[919,980,981,984],{},[338,982,983],{},"Upload screenshots to InvokeAI"," for image-to-image workflows",[919,986,987,990],{},[338,988,989],{},"Flux Kontext generation"," using uploaded images as reference",[919,992,993,996],{},[338,994,995],{},"Processor control"," - pause and resume InvokeAI processing queues",[919,998,999,1002],{},[338,1000,1001],{},"VRAM management"," - empty model cache to free up memory",[919,1004,1005,1008],{},[338,1006,1007],{},"Status monitoring"," - check InvokeAI service health and version",[911,1010,1012],{"id":1011},"smart-configuration","Smart Configuration",[916,1014,1015,1021,1027,1033],{},[919,1016,1017,1020],{},[338,1018,1019],{},"Flexible URL configuration"," - works with local servers or NVIDIA hosted endpoints",[919,1022,1023,1026],{},[338,1024,1025],{},"Automatic API key validation"," - ensures proper NVIDIA API key format",[919,1028,1029,1032],{},[338,1030,1031],{},"Configurable output directories"," for generated images",[919,1034,1035,1038],{},[338,1036,1037],{},"Board management"," for InvokeAI gallery organization",[911,1040,1042],{"id":1041},"example-commands","Example Commands",[916,1044,1045,1048,1051,1054,1057],{},[919,1046,1047],{},"\"hey flux, generate an image of a cyberpunk city at night\"",[919,1049,1050],{},"\"hey flux, start the Flux NIM server\"",[919,1052,1053],{},"\"hey flux, use kontext to make it a cartoon style\" (does image-to-image generation using latest screenshot taken with NVIDIA screenshot shortcut)",[919,1055,1056],{},"\"hey flux, empty the InvokeAI model cache to free up VRAM\"",[919,1058,1059],{},"\"hey flux, Check if the NIM server is running\"",[168,1061,1063],{"id":1062},"before-you-start","Before You Start",[11,1065,1066],{},"Make sure you have:",[916,1068,1069,1072,1075,1078,1081],{},[919,1070,1071],{},"Windows PC",[919,1073,1074],{},"Python 3.12 or higher",[919,1076,1077],{},"G-Assist installed on your system",[919,1079,1080],{},"pywin32 >= 223",[919,1082,1083],{},"Basic knowledge of Python",[11,1085,1086,1087,1090],{},"💡 ",[338,1088,1089],{},"Tip",": Use a virtual environment to keep your plugin dependencies isolated from other Python projects!",[168,1092,1094],{"id":1093},"installation-guide","Installation Guide",[911,1096,1098],{"id":1097},"step-1-get-the-files","Step 1: Get the Files",[11,1100,1101],{},"Clone this repository",[11,1103,1104],{},"This downloads the plugin code and all necessary files to your computer.",[911,1106,1108],{"id":1107},"step-2-set-up-python-environment","Step 2: Set Up Python Environment",[11,1110,1111,1112,1115],{},"Run the ",[33,1113,1114],{},"setup.bat"," file. This will take care of creating a Python virtual environment will install project dependencies.",[26,1117,1119],{"className":181,"code":1118,"language":183,"meta":35,"style":35},"python -m venv .venv\n.venv\\Scripts\\activate\npython -m pip install -r requirements.txt\n",[33,1120,1121,1135,1140],{"__ignoreMap":35},[187,1122,1123,1126,1129,1132],{"class":189,"line":190},[187,1124,1125],{"class":193},"python",[187,1127,1128],{"class":588}," -m",[187,1130,1131],{"class":196}," venv",[187,1133,1134],{"class":196}," .venv\n",[187,1136,1137],{"class":189,"line":249},[187,1138,1139],{"class":193},".venv\\Scripts\\activate\n",[187,1141,1142,1144,1146,1149,1152,1155],{"class":189,"line":312},[187,1143,1125],{"class":193},[187,1145,1128],{"class":588},[187,1147,1148],{"class":196}," pip",[187,1150,1151],{"class":196}," install",[187,1153,1154],{"class":588}," -r",[187,1156,1157],{"class":196}," requirements.txt\n",[911,1159,1161],{"id":1160},"step-3-build-the-project","Step 3: Build the project",[11,1163,1164,1165,1168,1169,1172,1173,1176,1177,752],{},"Run ",[33,1166,1167],{},"build.bat"," to build the project. This script will also place the ",[33,1170,1171],{},"g-assist-plugin-flux.exe"," and ",[33,1174,1175],{},"manifest.json"," files in ",[33,1178,1179],{},"%PROGRAMDATA%\\NVIDIA Corporation\\nvtopps\\rise\\plugins\\flux",[911,1181,1183],{"id":1182},"step-4-configuration","Step 4: Configuration",[11,1185,1186,1187,1190,1191,1194,1195,1197],{},"Copy the ",[33,1188,1189],{},"config.example.json"," file and rename it as ",[33,1192,1193],{},"config.json"," and place it in ",[33,1196,1179],{},". Customize it with the appropriate configuration values (see below for configuration details). Here is an example configuration:",[26,1199,1203],{"className":1200,"code":1201,"language":1202,"meta":35,"style":35},"language-json shiki shiki-themes github-light github-dark","{\n    \"GALLERY_DIRECTORY\": \"E:\\\\NVIDIA\",\n    \"FLUX_NIM_URL\": \"http://localhost:8000\",\n    \"NGC_API_KEY\": \"xxxxxxxx\",\n    \"HF_TOKEN\": \"hf_xxxxxxxx\",\n    \"LOCAL_NIM_CACHE\": \"~/.cache/nim\",\n    \"INVOKEAI_URL\": \"http://localhost:9090\",\n    \"OUTPUT_DIRECTORY\": \"E:\\\\Flux\"\n}\n","json",[33,1204,1205,1210,1229,1241,1253,1265,1277,1289,1304],{"__ignoreMap":35},[187,1206,1207],{"class":189,"line":190},[187,1208,1209],{"class":577},"{\n",[187,1211,1212,1215,1217,1220,1223,1226],{"class":189,"line":249},[187,1213,1214],{"class":588},"    \"GALLERY_DIRECTORY\"",[187,1216,585],{"class":577},[187,1218,1219],{"class":196},"\"E:",[187,1221,1222],{"class":588},"\\\\",[187,1224,1225],{"class":196},"NVIDIA\"",[187,1227,1228],{"class":577},",\n",[187,1230,1231,1234,1236,1239],{"class":189,"line":312},[187,1232,1233],{"class":588},"    \"FLUX_NIM_URL\"",[187,1235,585],{"class":577},[187,1237,1238],{"class":196},"\"http://localhost:8000\"",[187,1240,1228],{"class":577},[187,1242,1243,1246,1248,1251],{"class":189,"line":319},[187,1244,1245],{"class":588},"    \"NGC_API_KEY\"",[187,1247,585],{"class":577},[187,1249,1250],{"class":196},"\"xxxxxxxx\"",[187,1252,1228],{"class":577},[187,1254,1255,1258,1260,1263],{"class":189,"line":325},[187,1256,1257],{"class":588},"    \"HF_TOKEN\"",[187,1259,585],{"class":577},[187,1261,1262],{"class":196},"\"hf_xxxxxxxx\"",[187,1264,1228],{"class":577},[187,1266,1267,1270,1272,1275],{"class":189,"line":686},[187,1268,1269],{"class":588},"    \"LOCAL_NIM_CACHE\"",[187,1271,585],{"class":577},[187,1273,1274],{"class":196},"\"~/.cache/nim\"",[187,1276,1228],{"class":577},[187,1278,1279,1282,1284,1287],{"class":189,"line":697},[187,1280,1281],{"class":588},"    \"INVOKEAI_URL\"",[187,1283,585],{"class":577},[187,1285,1286],{"class":196},"\"http://localhost:9090\"",[187,1288,1228],{"class":577},[187,1290,1292,1295,1297,1299,1301],{"class":189,"line":1291},8,[187,1293,1294],{"class":588},"    \"OUTPUT_DIRECTORY\"",[187,1296,585],{"class":577},[187,1298,1219],{"class":196},[187,1300,1222],{"class":588},[187,1302,1303],{"class":196},"Flux\"\n",[187,1305,1307],{"class":189,"line":1306},9,[187,1308,1309],{"class":577},"}\n",[911,1311,1313],{"id":1312},"step-5-set-up-flux","Step 5: Set up Flux",[11,1315,1316,1317,1322],{},"Follow instructions ",[15,1318,1321],{"href":1319,"rel":1320},"https://build.nvidia.com/black-forest-labs/flux_1-dev/deploy?environment=wsl2.md",[19],"here"," for installing the FLUX.1-dev model from Black Forest Labs using NVIDIA NIM on WSL. Be sure to do the following:",[916,1324,1325,1333,1342,1345],{},[919,1326,1327,1328,1332],{},"follow instructions here: ",[15,1329,1330],{"href":1330,"rel":1331},"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html",[19]," for getting started with WSL",[919,1334,1335,1336,1341],{},"use the ",[15,1337,1340],{"href":1338,"rel":1339},"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html#use-the-nvidia-nim-wsl2-installer-recommended",[19],"NVIDIA NIM WSL2 Installer"," for configuring a new WSL environment configured with all of the required NVIDIA dependencies",[919,1343,1344],{},"In your Hugging Face account read and accept FLUX.1-dev, FLUX.1-Canny-dev, FLUX.1-Depth-dev and FLUX.1-dev-onnx License Agreements and Acceptable Use Policy. You must accept the agreements/policies for all of the models even though this plugin does not directly use the Canny or Depth modes.",[919,1346,1347,1348,1351,1352,1354],{},"Make sure to map port ",[33,1349,1350],{},"8000"," in the NIM to port ",[33,1353,1350],{}," on the WSL host as shown in the setup link above.",[911,1356,1358],{"id":1357},"step-6-install-invokeai-optional","Step 6: Install InvokeAI (optional)",[11,1360,1361,1362,1366,1367,1370,1371,752],{},"InvokeAI has a Windows installer that can be found here: ",[15,1363,1364],{"href":1364,"rel":1365},"https://invoke-ai.github.io/InvokeAI/installation/quick_start/#step-2-download",[19],". Download the Flux models including the ",[338,1368,1369],{},"FLUX.1 Kontext dev (Quantized)"," model for using Flux Kontext in the Flux Plug-in for G-Assist. By default InvokeAI runs on ",[33,1372,1373],{},"http://localhost:9090",[911,1375,1377],{"id":1376},"step-7-start-the-nvidia-nim","Step 7: Start the NVIDIA NIM",[11,1379,1380],{},"Ask flux if this NIM is running. If it is not running, ask flux to start the NIM. This will run a command to star the NIM container in WSL using podman:",[26,1382,1385],{"className":1383,"code":1384,"language":1125,"meta":35,"style":35},"language-python shiki shiki-themes github-light github-dark","podman_cmd = [\n    'wsl', '-d', 'NVIDIA-Workbench',\n    'podman', 'run', '-d', '--rm', '--name=nim-server',\n    '--device', 'nvidia.com/gpu=all',\n    '-e', f'NGC_API_KEY={NGC_API_KEY}',\n    '-e', f'HF_TOKEN={HF_TOKEN}',\n    '-p', '8000:8000',\n    '-v', f'{LOCAL_NIM_CACHE}:/opt/nim/.cache/',\n    'nvcr.io/nim/black-forest-labs/flux.1-dev:1.0.0'\n]\n",[33,1386,1387,1392,1397,1402,1407,1412,1417,1422,1427,1432],{"__ignoreMap":35},[187,1388,1389],{"class":189,"line":190},[187,1390,1391],{},"podman_cmd = [\n",[187,1393,1394],{"class":189,"line":249},[187,1395,1396],{},"    'wsl', '-d', 'NVIDIA-Workbench',\n",[187,1398,1399],{"class":189,"line":312},[187,1400,1401],{},"    'podman', 'run', '-d', '--rm', '--name=nim-server',\n",[187,1403,1404],{"class":189,"line":319},[187,1405,1406],{},"    '--device', 'nvidia.com/gpu=all',\n",[187,1408,1409],{"class":189,"line":325},[187,1410,1411],{},"    '-e', f'NGC_API_KEY={NGC_API_KEY}',\n",[187,1413,1414],{"class":189,"line":686},[187,1415,1416],{},"    '-e', f'HF_TOKEN={HF_TOKEN}',\n",[187,1418,1419],{"class":189,"line":697},[187,1420,1421],{},"    '-p', '8000:8000',\n",[187,1423,1424],{"class":189,"line":1291},[187,1425,1426],{},"    '-v', f'{LOCAL_NIM_CACHE}:/opt/nim/.cache/',\n",[187,1428,1429],{"class":189,"line":1306},[187,1430,1431],{},"    'nvcr.io/nim/black-forest-labs/flux.1-dev:1.0.0'\n",[187,1433,1435],{"class":189,"line":1434},10,[187,1436,1437],{},"]\n",[11,1439,1440,1441,1172,1444,1447],{},"Then ask if the NIM is ready. This will check the ",[33,1442,1443],{},"/v1/health/live",[33,1445,1446],{},"/v1/health/ready"," endpoints of the Flux NIM.",[911,1449,1451],{"id":1450},"step-8-generete-ai-images-using-the-flux-plug-in-in-the-g-assist-chat-window","Step 8: Generete AI images using the Flux Plug-in in the G-Assist chat window",[11,1453,1454],{},"Send a message to G-Assist:",[11,1456,1457],{},"\"hey flux, generate a cat piloting a spaceship\"",[11,1459,1460],{},"The Flux Plug-in will respond with:",[11,1462,1463],{},"flux> Your image generation request is in progress! Prompt: \"a cat piloting a spaceship\"",[11,1465,1466],{},"When the image generation is complete you will find the image on your Desktop background, and the image will be saved to the output directory specified in your configuration file.",[911,1468,1470],{"id":1469},"step-9-transform-a-screenshot-with-flux-kontext","Step 9: Transform a screenshot with Flux Kontext",[11,1472,1473,1474,1477],{},"Take a screenshot using the NVIDIA Screenshot hotkey (usually ",[33,1475,1476],{},"Alt + F1","), and then ask the Flux Plug-in to transform it to any style using Kontext.",[11,1479,1480],{},[511,1481],{"alt":1482,"src":1483},"Cat piloting spaceship","/static/flux/cat_spaceship.png",[11,1485,1486],{},"hey flux, use kontext with the prompt: cartoon style",[11,1488,1489],{},[511,1490],{"alt":1491,"src":1492},"Cat Spaceship Cartoon style with Kontext","/static/flux/cat_spaceship_cartoon.png",[11,1494,1495],{},"Flux does this by triggering an InvokeAI graph workflow. The generated image and the workflow can both be viewed in the InvokeAI UI:",[11,1497,1498],{},[511,1499],{"alt":1500,"src":1501},"InvokeAI Flux Kontext Workflow","/static/flux/invokeai_workflow.png",[11,1503,1504],{},"You can ask the Flux Plug-in to pause/resume InvokeAI processing to avoid running Flux Kontext image generation while your GPU is busy with other tasks. Also you can ask flux to empty the model cache in order to free up VRAM on your GPU.",[168,1506,1508],{"id":1507},"configuration","Configuration",[11,1510,1511,1512,1514,1515,765,1517,1519],{},"The Flux plugin uses a ",[33,1513,1193],{}," file to manage all settings. Copy ",[33,1516,1189],{},[33,1518,1193],{}," and customize the values for your setup.",[911,1521,1523],{"id":1522},"configuration-options","Configuration Options",[1525,1526,1527,1543],"table",{},[1528,1529,1530],"thead",{},[1531,1532,1533,1537,1540],"tr",{},[1534,1535,1536],"th",{},"Option",[1534,1538,1539],{},"Example Values",[1534,1541,1542],{},"Required",[1544,1545,1546,1565,1580,1597,1612,1626,1640,1656,1673],"tbody",{},[1531,1547,1548,1554,1562],{},[1549,1550,1551],"td",{},[33,1552,1553],{},"GALLERY_DIRECTORY",[1549,1555,1556,637,1559],{},[33,1557,1558],{},"\"D:\\\\Screenshots\"",[33,1560,1561],{},"\"C:\\\\NVIDIA\"",[1549,1563,1564],{},"No",[1531,1566,1567,1572,1577],{},[1549,1568,1569],{},[33,1570,1571],{},"NVIDIA_API_KEY",[1549,1573,1574],{},[33,1575,1576],{},"\"nvapi-your-key-here\"",[1549,1578,1579],{},"No*",[1531,1581,1582,1587,1594],{},[1549,1583,1584],{},[33,1585,1586],{},"FLUX_NIM_URL",[1549,1588,1589,637,1591],{},[33,1590,1238],{},[33,1592,1593],{},"\"http://192.168.1.100:8000\"",[1549,1595,1596],{},"Yes",[1531,1598,1599,1604,1609],{},[1549,1600,1601],{},[33,1602,1603],{},"NGC_API_KEY",[1549,1605,1606],{},[33,1607,1608],{},"\"your-ngc-key\"",[1549,1610,1611],{},"Yes**",[1531,1613,1614,1619,1624],{},[1549,1615,1616],{},[33,1617,1618],{},"HF_TOKEN",[1549,1620,1621],{},[33,1622,1623],{},"\"hf_your-token\"",[1549,1625,1611],{},[1531,1627,1628,1633,1638],{},[1549,1629,1630],{},[33,1631,1632],{},"LOCAL_NIM_CACHE",[1549,1634,1635],{},[33,1636,1637],{},"~/.cache/nim",[1549,1639,1611],{},[1531,1641,1642,1647,1654],{},[1549,1643,1644],{},[33,1645,1646],{},"INVOKEAI_URL",[1549,1648,1649,637,1651],{},[33,1650,1286],{},[33,1652,1653],{},"\"http://192.168.1.100:9090\"",[1549,1655,1564],{},[1531,1657,1658,1663,1671],{},[1549,1659,1660],{},[33,1661,1662],{},"BOARD_ID",[1549,1664,1665,637,1668],{},[33,1666,1667],{},"\"my-gallery-board\"",[33,1669,1670],{},"\"flux-gallery\"",[1549,1672,1564],{},[1531,1674,1675,1680,1688],{},[1549,1676,1677],{},[33,1678,1679],{},"OUTPUT_DIRECTORY",[1549,1681,1682,637,1685],{},[33,1683,1684],{},"\"C:\\\\GeneratedImages\"",[33,1686,1687],{},"\"D:\\\\flux-output\"",[1549,1689,1564],{},[11,1691,1692,1693,1695,1696,1700],{},"*Required only when using NVIDIA hosted Flux service (",[33,1694,1586],{}," starts with \"",[15,1697,1698],{"href":1698,"rel":1699},"https://ai.api.nvidia.com",[19],"\")\n**Required only when using local NIM server",[168,1702,1704],{"id":1703},"using-the-flux1-dev-nvidia-nim-for-text-to-image-generation","Using the Flux.1-dev NVIDIA NIM for text-to-image generation",[11,1706,1707],{},[511,1708],{"alt":1709,"src":1710},"Desert Nomad","/static/flux/desert_nomad.png",[11,1712,1713],{},"On NVIDIA GeForce RTX AI PCs, the best way to do AI image inference is by using NVIDIA NIMs. Windows currently has beta support for running NVIDIA NIMs in WSL with Podman (a program for running containers, similar to Docker).",[11,1715,1716,1717,1720],{},"NVIDIA provides an installer that installs a WSL distribution with all dependencies installed. You can find those resources here: ",[15,1718,1330],{"href":1330,"rel":1719},[19]," (WSL2 is required for hosting any NIM. Refer to the official NVIDIA NIM on WSL2 documentation for setup instructions.)",[911,1722,1724],{"id":1723},"how-it-works","How It Works",[11,1726,1727],{},"You can request an image to be generated by simply saying something like:",[11,1729,1457],{},[11,1731,1732,1733,1737],{},"The Flux Plugin will make an API request to the Flux NIM URL (configured in your config.json, defaults to ",[15,1734,1735],{"href":1735,"rel":1736},"http://localhost:8000",[19],").",[911,1739,1741],{"id":1740},"asynchronous-processing","Asynchronous Processing",[11,1743,1744],{},"The G-Assist chat assistant has a timeout of 10 seconds, so the chat assistant returns immediately with:",[26,1746,1749],{"className":1747,"code":1748,"language":31},[29],"flux> Your image generation request is in progress! Prompt: \"a cat piloting a spaceship\"\n",[33,1750,1748],{"__ignoreMap":35},[11,1752,1753],{},"The image generation request runs on a separate thread, since the image generation process with the NVIDIA NIM can take up to 30 seconds (depending on the number of steps, 50 steps is used in the plugin for best results). The plugin then creates an image file from the base64 encoded image in the response from the Flux NIM server, and it sets this image as your desktop background image.",[911,1755,1757],{"id":1756},"nim-management","NIM Management",[11,1759,1760],{},"There are also commands for starting and stopping the Flux NIM, which runs Podman commands inside of the NVIDIA-Workbench WSL distribution:",[916,1762,1763,1769,1775],{},[919,1764,1765,1768],{},[338,1766,1767],{},"Start NIM",": \"hey flux, start the Flux NIM server\"",[919,1770,1771,1774],{},[338,1772,1773],{},"Stop NIM",": \"hey flux, stop the Flux NIM server\"",[919,1776,1777,1780],{},[338,1778,1779],{},"Check Status",": \"hey flux, check if the NIM server is running\"",[11,1782,1783],{},[511,1784],{"alt":1785,"src":1786},"Flux Plug-in controls","/static/flux/controls.png",[911,1788,1508],{"id":1789},"configuration-1",[11,1791,1792,1793,1795],{},"Make sure your ",[33,1794,1193],{}," includes the necessary credentials:",[916,1797,1798,1803,1808],{},[919,1799,1800,1802],{},[33,1801,1603],{},": Your NVIDIA NGC API key for downloading models",[919,1804,1805,1807],{},[33,1806,1618],{},": Your Hugging Face token for model access",[919,1809,1810,1812],{},[33,1811,1632],{},": Path to your local NIM cache directory",[168,1814,1816],{"id":1815},"image-to-image-generation-with-flux-kontext-and-invokeai","Image-to-image generation with Flux Kontext and InvokeAI",[11,1818,1819],{},[511,1820],{"alt":1821,"src":1822},"Helicopter over NYC","/static/flux/nyc_heli.png",[11,1824,1825],{},[511,1826],{"alt":1821,"src":1827},"/static/flux/nyc_heli_watercolor.png",[11,1829,1830],{},"The Flux Plug-in supports image-to-image generation using an open source image generation tool called InvokeAI. This tool is similar to ComfyUI and it has solid API support. Currently there is no NVIDIA NIM for Flux Kontext but the NVIDIA blog mentioned that this might be released as soon as May 2025.",[11,1832,1833],{},"You can interact with the InvokeAI program in a few different ways:",[911,1835,1837],{"id":1836},"screenshot-based-image-generation","Screenshot-based Image Generation",[916,1839,1840,1846,1849],{},[919,1841,1842,1845],{},[338,1843,1844],{},"Upload your latest screenshot"," and perform image-to-image generation using Flux Kontext. This allows you to apply any type of manipulation to your screenshot (for example, you can say \"hey flux, use kontext to make it in the style of a cartoon\")",[919,1847,1848],{},"The plugin automatically finds your most recent screenshot and uploads it to InvokeAI",[919,1850,1851],{},"You can provide custom prompts to guide the transformation process",[911,1853,1855],{"id":1854},"processing-control","Processing Control",[916,1857,1858,1864],{},[919,1859,1860,1863],{},[338,1861,1862],{},"Pause or resume processing",": This is useful if you are playing a GPU intensive game. You can pause processing, but still submit image-to-image generation tasks using Flux Kontext. The tasks will be queued and they can be resumed later when your GPU is not busy with other tasks.",[919,1865,1866],{},"Monitor the processing queue status and control when generation happens",[911,1868,1870],{"id":1869},"memory-management","Memory Management",[916,1872,1873,1879,1882],{},[919,1874,1875,1878],{},[338,1876,1877],{},"Empty the model cache",": InvokeAI keeps models cached between generation, but you can empty the model cache by simply telling it to do so",[919,1880,1881],{},"This helps free up VRAM when you're not actively using InvokeAI",[919,1883,1884],{},"Useful for switching between different AI workloads or when playing games",[911,1886,1888],{"id":1887},"setup-requirements","Setup Requirements",[11,1890,1891],{},"To use the image-to-image features, you'll need:",[916,1893,1894,1899,1902],{},[919,1895,1896,1897,343],{},"InvokeAI installed and running locally (typically on ",[33,1898,1373],{},[919,1900,1901],{},"Flux Kontext model loaded in InvokeAI",[919,1903,1904,1905,1907],{},"Proper configuration in your ",[33,1906,1193],{}," file",[168,1909,1911],{"id":1910},"supported-commands","Supported Commands",[11,1913,1914],{},"The Flux Plug-in for G-Assist supports the following commands:",[1525,1916,1917,1930],{},[1528,1918,1919],{},[1531,1920,1921,1924,1927],{},[1534,1922,1923],{},"Function",[1534,1925,1926],{},"Description",[1534,1928,1929],{},"Example",[1544,1931,1932,1945,1958,1971,1984,1997,2010,2023,2036,2049],{},[1531,1933,1934,1939,1942],{},[1549,1935,1936],{},[33,1937,1938],{},"flux_nim_ready_check",[1549,1940,1941],{},"Tests health endpoints of the Flux NIM server",[1549,1943,1944],{},"\"hey flux, check if the flux nim server is ready\"",[1531,1946,1947,1952,1955],{},[1549,1948,1949],{},[33,1950,1951],{},"check_nim_status",[1549,1953,1954],{},"Checks if the Flux NIM server is running",[1549,1956,1957],{},"\"hey flux, check if the nim server is running\"",[1531,1959,1960,1965,1968],{},[1549,1961,1962],{},[33,1963,1964],{},"stop_nim",[1549,1966,1967],{},"Stops the Flux NIM server",[1549,1969,1970],{},"\"hey flux, stop the flux nim server\"",[1531,1972,1973,1978,1981],{},[1549,1974,1975],{},[33,1976,1977],{},"start_nim",[1549,1979,1980],{},"Starts the Flux NIM server",[1549,1982,1983],{},"\"hey flux, start the flux nim server\"",[1531,1985,1986,1991,1994],{},[1549,1987,1988],{},[33,1989,1990],{},"generate_image",[1549,1992,1993],{},"Generates an image from text prompt using Flux",[1549,1995,1996],{},"\"hey flux, generate an image of a cyberpunk city\"",[1531,1998,1999,2004,2007],{},[1549,2000,2001],{},[33,2002,2003],{},"generate_image_using_kontext",[1549,2005,2006],{},"Performs image-to-image generation using Flux Kontext",[1549,2008,2009],{},"\"hey flux, use kontext to make it a cartoon style\"",[1531,2011,2012,2017,2020],{},[1549,2013,2014],{},[33,2015,2016],{},"invokeai_status",[1549,2018,2019],{},"Checks the status of the InvokeAI service",[1549,2021,2022],{},"\"hey flux, check invokeai status\"",[1531,2024,2025,2030,2033],{},[1549,2026,2027],{},[33,2028,2029],{},"pause_invokeai_processor",[1549,2031,2032],{},"Pauses the InvokeAI processing queue",[1549,2034,2035],{},"\"hey flux, pause the invokeai processor\"",[1531,2037,2038,2043,2046],{},[1549,2039,2040],{},[33,2041,2042],{},"resume_invokeai_processor",[1549,2044,2045],{},"Resumes the InvokeAI processing queue",[1549,2047,2048],{},"\"hey flux, resume the invokeai processor\"",[1531,2050,2051,2056,2059],{},[1549,2052,2053],{},[33,2054,2055],{},"invokeai_empty_model_cache",[1549,2057,2058],{},"Empties the InvokeAI model cache to free VRAM",[1549,2060,2061],{},"\"hey flux, empty the invokeai model cache\"",[168,2063,2065],{"id":2064},"logging","Logging",[11,2067,2068,2069,2072],{},"Your plugin automatically logs to ",[33,2070,2071],{},"flux_plugin.log"," in your user's profile directory. It tracks:",[916,2074,2075,2078,2081,2084],{},[919,2076,2077],{},"Plugin startup and shutdown",[919,2079,2080],{},"Command reception and processing",[919,2082,2083],{},"Error conditions",[919,2085,2086],{},"Function execution details",[168,2088,2090],{"id":2089},"troubleshooting-tips","Troubleshooting Tips",[916,2092,2093,2099,2105,2111],{},[919,2094,2095,2098],{},[338,2096,2097],{},"Plugin not starting?"," Check if Python 3.12+ is installed and in PATH",[919,2100,2101,2104],{},[338,2102,2103],{},"Communication errors?"," Verify pywin32 is installed correctly",[919,2106,2107,2110],{},[338,2108,2109],{},"Commands not working?"," Double-check your command registration",[919,2112,2113,2116],{},[338,2114,2115],{},"Missing logs?"," Ensure write permissions in user profile directory",[168,2118,2120],{"id":2119},"want-to-contribute","Want to Contribute?",[11,2122,2123,2124,2127],{},"We'd love your help making this template even better! Check out ",[33,2125,2126],{},"CONTRIBUTING.md"," for guidelines on how to contribute.",[168,2129,2131],{"id":2130},"license","License",[11,2133,2134,2135,2138],{},"This project is licensed under the Apache License 2.0 - see the ",[33,2136,2137],{},"LICENSE"," file for details.",[168,2140,2142],{"id":2141},"hardware","Hardware",[11,2144,2145],{},"The Flux Plug-in for G-Assist was developed and tested on a PC with a GeForce RTX 4090 GPU.",[855,2147,2148],{},"html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}",{"title":35,"searchDepth":249,"depth":249,"links":2150},[2151,2158,2159,2170,2173,2179,2185,2186,2187,2188,2189,2190],{"id":908,"depth":249,"text":909,"children":2152},[2153,2154,2155,2156,2157],{"id":913,"depth":312,"text":914},{"id":945,"depth":312,"text":946},{"id":975,"depth":312,"text":976},{"id":1011,"depth":312,"text":1012},{"id":1041,"depth":312,"text":1042},{"id":1062,"depth":249,"text":1063},{"id":1093,"depth":249,"text":1094,"children":2160},[2161,2162,2163,2164,2165,2166,2167,2168,2169],{"id":1097,"depth":312,"text":1098},{"id":1107,"depth":312,"text":1108},{"id":1160,"depth":312,"text":1161},{"id":1182,"depth":312,"text":1183},{"id":1312,"depth":312,"text":1313},{"id":1357,"depth":312,"text":1358},{"id":1376,"depth":312,"text":1377},{"id":1450,"depth":312,"text":1451},{"id":1469,"depth":312,"text":1470},{"id":1507,"depth":249,"text":1508,"children":2171},[2172],{"id":1522,"depth":312,"text":1523},{"id":1703,"depth":249,"text":1704,"children":2174},[2175,2176,2177,2178],{"id":1723,"depth":312,"text":1724},{"id":1740,"depth":312,"text":1741},{"id":1756,"depth":312,"text":1757},{"id":1789,"depth":312,"text":1508},{"id":1815,"depth":249,"text":1816,"children":2180},[2181,2182,2183,2184],{"id":1836,"depth":312,"text":1837},{"id":1854,"depth":312,"text":1855},{"id":1869,"depth":312,"text":1870},{"id":1887,"depth":312,"text":1888},{"id":1910,"depth":249,"text":1911},{"id":2064,"depth":249,"text":2065},{"id":2089,"depth":249,"text":2090},{"id":2119,"depth":249,"text":2120},{"id":2130,"depth":249,"text":2131},{"id":2141,"depth":249,"text":2142},"2025-07-20","A Plug-in for Project G-Assist that puts the power of AI image generation right at your fingertips",[2194],{"link":2195,"site":2196},"https://x.com/briancaffey/status/1946684573095497992","x","/static/flux/flux_plugin_for_project_g_assist.png",{},"/2025/07/17/flux-plugin-for-project-g-assist-hackathon",{"title":888,"description":2192},"2025/07/17/flux-plugin-for-project-g-assist-hackathon",[2203,2204,2205,2206,2207],"nvidia","ai","rtx","nim","flux","F7BA3NruhsLBf1Fq-RTPWsDqurhsKpRssWplTlyJF9M",{"id":2210,"title":2211,"body":2212,"comments":315,"date":4066,"description":4067,"draft":872,"extension":873,"external":4068,"image":4070,"meta":4071,"navigation":315,"path":4072,"seo":4073,"stem":4074,"tags":4075,"__hash__":4079},"blog/2025/05/27/mediation-simulator-project-for-nvidia-agent-intelligence-toolkit.md","Mediation Simulator: My submission for the NVIDIA Agent Intelligence Toolkit Hackathon",{"type":8,"value":2213,"toc":4047},[2214,2219,2226,2235,2238,2242,2245,2248,2268,2272,2275,2296,2299,2308,2314,2323,2329,2333,2336,2382,2386,2389,2457,2464,2467,2473,2477,2480,2483,2488,2495,2499,2502,2505,3363,3369,3373,3376,3382,3385,3391,3395,3398,3402,3405,3409,3412,3588,3595,3601,3604,3699,3703,3706,3710,3713,3754,3757,3761,3764,3767,3770,3989,3992,3998,4001,4004,4007,4027,4030,4034,4037,4041,4044],[2215,2216,2218],"h1",{"id":2217},"building-mediation-simulator-for-the-nvidia-agent-intelligence-toolkit-hackathon","Building Mediation Simulator for the NVIDIA Agent Intelligence Toolkit Hackathon!",[11,2220,2221,2222,2225],{},"I'm excited to share a project I've been building for the NVIDIA Agent Intelligence Toolkit Hackathon: ",[338,2223,2224],{},"Mediation Simulator","! The NVIDIA Agent Intelligence Toolkit (or AgentIQ Toolkit, as it's often called) is a powerful open-source library designed for connecting, evaluating, and accelerating teams of AI agents. My goal? To see if I could leverage this toolkit to build AI agent teams capable of simulating the entire, complex process of a law school mediation competition.",[11,2227,2228,2229,2234],{},"Check out my ",[15,2230,2233],{"href":2231,"rel":2232},"https://x.com/briancaffey/status/1926036369597510117",[19],"𝕏 Post"," that introduces the project:",[2236,2237],"mediation-simulator-tweet",{},[168,2239,2241],{"id":2240},"what-is-mediation-simulator-and-why-mediation","What is Mediation Simulator? And Why Mediation?",[11,2243,2244],{},"At its core, Mediation Simulator is my attempt to model the nuanced, semi-structured, three-way conversations that happen in mediation. Law school mediation tournaments are events where students practice negotiation and dispute resolution skills. This seemed like a really interesting challenge! How do you get language models to effectively navigate such a dynamic environment?",[11,2246,2247],{},"Mediation competitions also present some unique data challenges that are perfect for AI:",[916,2249,2250,2256,2262],{},[919,2251,2252,2255],{},[338,2253,2254],{},"Layered Information:"," There are \"common facts\" known to everyone, and \"confidential facts\" privy only to one party (and sometimes shared strategically with the mediator or the other side).",[919,2257,2258,2261],{},[338,2259,2260],{},"Dynamic Conversations:"," The main discussion involves all three parties (mediator and two disputants), but it also features \"caucuses\"—private, two-way conversations between the mediator and one party.",[919,2263,2264,2267],{},[338,2265,2266],{},"Creative Scenarios:"," The cases are entirely fictional, often involving made-up companies, fictional countries, and even fictional currencies! This is a deliberate choice to help students avoid real-world biases. And guess what? AI is really good at generating fake data!",[911,2269,2271],{"id":2270},"what-i-built","What I built",[11,2273,2274],{},"Mediation Simulator consists of three main components:",[2276,2277,2278,2284,2290],"ol",{},[919,2279,2280,2283],{},[338,2281,2282],{},"Case Generation Workflow",": A CLI tool that uses LLMs to generate realistic mediation case scenarios, complete with common facts, confidential information for each party, and supporting documents. This workflow creates the foundation for all mediation simulations. The data for the case scenarios is saved in both YAML files and in my Redis database. I'll talk about why I did chose to store data in local files and on Redis later in this article.",[919,2285,2286,2289],{},[338,2287,2288],{},"Automated Mediation Workflow",": Another CLI tool that orchestrates a full mediation session between three AI agents (mediator, requesting party, and responding party). This workflow simulates the entire mediation process, from opening statements through negotiation to conclusion, with the AI agents engaging in realistic dialogue based on their roles and the case information. A clerk agent helps to guide the converstaion, controlling who the next speaker should be based on a summary of what has already been said by different parties.",[919,2291,2292,2295],{},[338,2293,2294],{},"Interactive Mediation API",": A REST API that allows a human user to participate in a mediation session by taking on the role of either the requesting or responding party. The API manages the session state and coordinates the interaction between the human participant and the AI mediator and opposing party. The conversation history for the mediation session is stored in Redis using a memory backend that I implemented with NVIDIA Agent Intelligence Toolkit.",[11,2297,2298],{},"To make the results of these workflows easily viewable, I also built two web interfaces:",[916,2300,2301],{},[919,2302,2303,2304,2307],{},"A ",[338,2305,2306],{},"Viewer Interface"," that displays the full three-party dialogue from automated mediation sessions, making it easy to review and analyze the AI agents' interactions.",[11,2309,2310],{},[511,2311],{"alt":2312,"src":2313},"Mediation Simulator Viewer","/static/mediation-simulator/mediation_simulator_viewer.png",[916,2315,2316],{},[919,2317,2318,2319,2322],{},"An ",[338,2320,2321],{},"Interactive Interface"," that provides a chat-like experience for human participants in the interactive mediation mode, with real-time updates and a clean, intuitive design.",[11,2324,2325],{},[511,2326],{"alt":2327,"src":2328},"Mediation Simulator Interactive","/static/mediation-simulator/interactive_mediation_screenshot.png",[911,2330,2332],{"id":2331},"the-genesis-from-idea-to-data","The Genesis: From Idea to Data",[11,2334,2335],{},"The first major hurdle was generating the foundational case data. Here's how that unfolded:",[2276,2337,2338,2344,2350,2356,2376],{},[919,2339,2340,2343],{},[338,2341,2342],{},"Deep Dive Research:"," I started by brainstorming with an LLM (OpenAI's o3, in this case) to get a comprehensive understanding of law school mediation competitions. I wanted to know everything: the rules, structure, participants, judging criteria and different types of cases.",[919,2345,2346,2349],{},[338,2347,2348],{},"Prompt Engineering for Cases:"," With that knowledge, I tasked another LLM (GPT-4o) with generating prompts to be used for creating diverse and realistic (albeit fictional) mediation case scenarios.",[919,2351,2352,2355],{},[338,2353,2354],{},"Case Generation:"," Using these prompts, I generated sets of distinct cases facts and related documents.",[919,2357,2358,2361,2362],{},[338,2359,2360],{},"Structuring with LangGraph:"," To manage the data for each case, I used LangGraph. I designed a state object to encapsulate all crucial elements:\n",[916,2363,2364,2367,2370],{},[919,2365,2366],{},"Common facts.",[919,2368,2369],{},"Confidential facts for both the requesting and responding parties.",[919,2371,2372,2375],{},[338,2373,2374],{},"Related Documents!"," This was my own little twist. I wanted to test RAG (Retrieval Augmented Generation) integration within the agentic workflow. Could parties use tools to search for information in these documents to bolster their arguments during mediation? Ultimately I couldn't really get this to work. I'll share more on why later in this article.",[919,2377,2378,2381],{},[338,2379,2380],{},"Data Persistence:"," With the data structured, I saved it in accessible formats: the LangGraph state was serialized to YAML, and the case details (like facts and documents) were stored in Markdown files. I also stored same LangGraph state to Redis using a simple JSON string.",[911,2383,2385],{"id":2384},"orchestrating-the-simulation","Orchestrating the Simulation",[11,2387,2388],{},"With the case data ready, the next step was to build out the mediation simulation itself:",[2276,2390,2391,2414,2441,2447],{},[919,2392,2393,2396,2397],{},[338,2394,2395],{},"Defining the Flow:"," I broke down the mediation process into its typical phases:\n",[916,2398,2399,2402,2405,2408,2411],{},[919,2400,2401],{},"Opening Statements",[919,2403,2404],{},"Information Gathering",[919,2406,2407],{},"Caucuses (for each party)",[919,2409,2410],{},"Negotiation",[919,2412,2413],{},"Conclusion",[919,2415,2416,2419,2420],{},[338,2417,2418],{},"Assembling the Agent Team:"," I set up a LangGraph graph consisting of:\n",[916,2421,2422,2428,2435],{},[919,2423,2303,2424,2427],{},[338,2425,2426],{},"Mediator"," agent",[919,2429,2430,2431,2434],{},"Two ",[338,2432,2433],{},"Party"," agents (Requesting and Responding parties)",[919,2436,2303,2437,2440],{},[338,2438,2439],{},"Clerk"," agent, whose job is to help manage the conversation flow and transition the simulation between the different phases.",[919,2442,2443,2446],{},[338,2444,2445],{},"Dynamic Prompting:"," The prompts for the mediator and the parties (both when initiating a statement and when responding) change significantly based on the current phase of the mediation. Critically, these prompts also dynamically include a summary log of what has already been said, providing context.",[919,2448,2449,2452,2453,2456],{},[338,2450,2451],{},"Message Logging:"," Each time a party speaks, I store the message using my Redis backend and also store additional metadata in the ",[33,2454,2455],{},"additional_kwargs"," section of each message, such as the speaker, the current phase of mediation and a summary of the message (the summary is generated by another LLM call that just summarizes the response.)",[11,2458,2459,2460,2463],{},"Getting to a functional mediation workflow was crucial! It allowed me to see the actual dialogue unfold and immediately highlighted areas for improvement. For instance, I realized that prompts needed to guide parties to ask one clear question at a time, directed at a specific participant, rather than posing multiple questions to several people at once. This really helps keep the simulated conversation straightforward and more realistic. I also had to instruct the LLM to use the names of the differnt parties, and I had to provide the names of the parties to the prompt. Without this instruction the LLM would give responses like this: \"Hello ",[187,2461,2462],{},"Requesting Party Name",", thank you for sharing your opinion.\"",[11,2465,2466],{},"Here's a look at the main workflow generated from the LangGraph code:",[11,2468,2469],{},[511,2470],{"alt":2471,"src":2472},"Mediation Simulator Workflow with LangGraph","/static/mediation-simulator/mediation_workflow.png",[911,2474,2476],{"id":2475},"bringing-it-to-life-the-vibe-coding-web-viewer","Bringing it to Life: The \"Vibe Coding\" Web Viewer!",[11,2478,2479],{},"Reading through raw Markdown files or terminal output to follow a complex, multi-turn mediation isn't ideal. I needed a better way to visualize the results! And this is where a bit of \"vibe coding\" came in incredibly handy.",[11,2481,2482],{},"I prompted an LLM to generate a single HTML page using Vue.js and Tailwind CSS. My requirements were simple: list all generated mediation cases, and when a case is selected, display the full dialogue. The amazing part? I never actually looked at the generated code in detail! I was able to make incremental improvements by simply describing changes or new features I wanted, and the LLM iterated until it was almost exactly what I envisioned. This was super easy and fast! I also made a page that lists all of the different mediation sessions with cover images that I generated with the NVIDIA Flux.1 Dev NIM:",[11,2484,2485],{},[511,2486],{"alt":2312,"src":2487},"/static/mediation-simulator/viewer.png",[11,2489,2490,2491,2494],{},"Having this simple web interface has been a game-changer for reviewing simulations. Plus, keeping it in my project repository means I can easily host it on GitHub Pages at ",[33,2492,2493],{},"briancaffey.github.io/mediation-simulator"," to share the results of my project. It's just so much better than staring at text files, and took just a few minutes to put together.",[168,2496,2498],{"id":2497},"a-deeper-look-into-the-nvidia-agent-intelligence-toolkit","A deeper look into the NVIDIA Agent Intelligence Toolkit",[11,2500,2501],{},"Now, I know what some developers think about LLM frameworks like LangChain/LangGraph, LlamaIndex, and CrewAI—there's often a bit of \"framework fatigue.\" But hear me out! The NVIDIA Agent Intelligence Toolkit brings all of these Frameworks together in a manageable way and it makes it really easy to not only write an agentic program, but it also makes it really easy to read other programs written with the framework.",[11,2503,2504],{},"The key to understanding the AIQ Toolkit is the config files. These are YAML files that neatly list out all of the dependencies of your agentic application. Let's take a look at the config file I made for mediation simulator. It's a long file, so I'll share it and then break down the important sections:",[26,2506,2510],{"className":2507,"code":2508,"language":2509,"meta":35,"style":35},"language-yaml shiki shiki-themes github-light github-dark","general:\n  use_uvloop: true\n  front_end:\n    _type: fastapi\n    cors:\n      allow_origins: ['*']\n      allow_methods:\n        - GET\n        - POST\n        - OPTIONS\n    endpoints:\n      - path: /case/{case_id}\n        method: GET\n        description: Gets the mediation case for the given case ID.\n        function_name: get_mediation_case\n      - path: /case/{case_id}/session/{session_id}\n        method: GET\n        description: Gets the mediation session data for the given case ID and session ID.\n        function_name: get_mediation_session\n      - path: /case/{case_id}/session/{session_id}/send\n        method: POST\n        description: Sends a message to the mediation session for the given case ID and session ID.\n        function_name: send_message_to_mediation_session\n  telemetry:\n    enabled: false\n    tracing:\n      phoenix:\n        _type: phoenix\n        endpoint: http://localhost:6006/v1/traces\n        project: default\n\nretrievers:\n  milvus_retriever:\n    _type: milvus_retriever\n    uri: \"http://localhost:19530\"\n    embedding_model: \"nv-embedqa-e5-v5\"\n    collection_name: \"aiq_case_documents\"\n    vector_field: \"embedding\"\n    search_params:\n      metric_type: \"IP\" # works best with nv-embedqa-e5-v5\n\nllms:\n  nim_llm:\n    _type: nim\n    base_url: http://192.168.5.96:1234/v1\n    model_name: qwen3-8b\n    max_tokens: 10000\n    temperature: 0.7\n  mediation_llm:\n    _type: nim\n    base_url: http://192.168.5.96:1234/v1\n    model_name: qwen3-8b\n    max_tokens: 10000\n    temperature: 0.7\n\nmemory:\n  redis_memory:\n    _type: redis_memory\n    connection_url: redis://localhost:6379/0\n\nfunctions:\n  case_document_rag:\n    _type: case_document_rag\n    retriever: milvus_retriever\n    llm_name: nim_llm\n    collection_name: \"mediation_simulator_case_documents\"\n    top_k: 5\n  case_query_agent:\n    _type: case_query_agent\n    llm_name: nim_llm\n    tool_names:\n      - case_document_rag\n    verbose: true\n    max_iterations: 5\n\n  # server route functions\n  get_mediation_case:\n    _type: server/get_mediation_case\n  get_mediation_session:\n    _type: server/get_mediation_session\n  send_message_to_mediation_session:\n    _type: mediation\n\nembedders:\n  nv-embedqa-e5-v5:\n    _type: nim\n    base_url: http://192.168.5.96:8000/v1\n    model_name: nvidia/nv-embedqa-e5-v5\n\nworkflow:\n  _type: mediation\n  llm: mediation_llm\n  data_dir: ./data\n","yaml",[33,2511,2512,2521,2531,2538,2548,2555,2568,2575,2583,2590,2597,2605,2619,2629,2640,2651,2663,2672,2682,2692,2704,2713,2723,2733,2741,2752,2760,2768,2779,2790,2801,2806,2814,2822,2832,2843,2854,2865,2876,2884,2898,2903,2911,2919,2929,2940,2951,2962,2973,2981,2990,2999,3008,3017,3026,3031,3039,3047,3057,3068,3073,3081,3089,3099,3109,3120,3130,3141,3149,3159,3168,3176,3183,3193,3203,3208,3214,3222,3232,3240,3250,3258,3268,3273,3281,3289,3298,3308,3318,3323,3331,3341,3352],{"__ignoreMap":35},[187,2513,2514,2518],{"class":189,"line":190},[187,2515,2517],{"class":2516},"s9eBZ","general",[187,2519,2520],{"class":577},":\n",[187,2522,2523,2526,2528],{"class":189,"line":249},[187,2524,2525],{"class":2516},"  use_uvloop",[187,2527,585],{"class":577},[187,2529,2530],{"class":588},"true\n",[187,2532,2533,2536],{"class":189,"line":312},[187,2534,2535],{"class":2516},"  front_end",[187,2537,2520],{"class":577},[187,2539,2540,2543,2545],{"class":189,"line":319},[187,2541,2542],{"class":2516},"    _type",[187,2544,585],{"class":577},[187,2546,2547],{"class":196},"fastapi\n",[187,2549,2550,2553],{"class":189,"line":325},[187,2551,2552],{"class":2516},"    cors",[187,2554,2520],{"class":577},[187,2556,2557,2560,2563,2566],{"class":189,"line":686},[187,2558,2559],{"class":2516},"      allow_origins",[187,2561,2562],{"class":577},": [",[187,2564,2565],{"class":196},"'*'",[187,2567,1437],{"class":577},[187,2569,2570,2573],{"class":189,"line":697},[187,2571,2572],{"class":2516},"      allow_methods",[187,2574,2520],{"class":577},[187,2576,2577,2580],{"class":189,"line":1291},[187,2578,2579],{"class":577},"        - ",[187,2581,2582],{"class":196},"GET\n",[187,2584,2585,2587],{"class":189,"line":1306},[187,2586,2579],{"class":577},[187,2588,2589],{"class":196},"POST\n",[187,2591,2592,2594],{"class":189,"line":1434},[187,2593,2579],{"class":577},[187,2595,2596],{"class":196},"OPTIONS\n",[187,2598,2600,2603],{"class":189,"line":2599},11,[187,2601,2602],{"class":2516},"    endpoints",[187,2604,2520],{"class":577},[187,2606,2608,2611,2614,2616],{"class":189,"line":2607},12,[187,2609,2610],{"class":577},"      - ",[187,2612,2613],{"class":2516},"path",[187,2615,585],{"class":577},[187,2617,2618],{"class":196},"/case/{case_id}\n",[187,2620,2622,2625,2627],{"class":189,"line":2621},13,[187,2623,2624],{"class":2516},"        method",[187,2626,585],{"class":577},[187,2628,2582],{"class":196},[187,2630,2632,2635,2637],{"class":189,"line":2631},14,[187,2633,2634],{"class":2516},"        description",[187,2636,585],{"class":577},[187,2638,2639],{"class":196},"Gets the mediation case for the given case ID.\n",[187,2641,2643,2646,2648],{"class":189,"line":2642},15,[187,2644,2645],{"class":2516},"        function_name",[187,2647,585],{"class":577},[187,2649,2650],{"class":196},"get_mediation_case\n",[187,2652,2654,2656,2658,2660],{"class":189,"line":2653},16,[187,2655,2610],{"class":577},[187,2657,2613],{"class":2516},[187,2659,585],{"class":577},[187,2661,2662],{"class":196},"/case/{case_id}/session/{session_id}\n",[187,2664,2666,2668,2670],{"class":189,"line":2665},17,[187,2667,2624],{"class":2516},[187,2669,585],{"class":577},[187,2671,2582],{"class":196},[187,2673,2675,2677,2679],{"class":189,"line":2674},18,[187,2676,2634],{"class":2516},[187,2678,585],{"class":577},[187,2680,2681],{"class":196},"Gets the mediation session data for the given case ID and session ID.\n",[187,2683,2685,2687,2689],{"class":189,"line":2684},19,[187,2686,2645],{"class":2516},[187,2688,585],{"class":577},[187,2690,2691],{"class":196},"get_mediation_session\n",[187,2693,2695,2697,2699,2701],{"class":189,"line":2694},20,[187,2696,2610],{"class":577},[187,2698,2613],{"class":2516},[187,2700,585],{"class":577},[187,2702,2703],{"class":196},"/case/{case_id}/session/{session_id}/send\n",[187,2705,2707,2709,2711],{"class":189,"line":2706},21,[187,2708,2624],{"class":2516},[187,2710,585],{"class":577},[187,2712,2589],{"class":196},[187,2714,2716,2718,2720],{"class":189,"line":2715},22,[187,2717,2634],{"class":2516},[187,2719,585],{"class":577},[187,2721,2722],{"class":196},"Sends a message to the mediation session for the given case ID and session ID.\n",[187,2724,2726,2728,2730],{"class":189,"line":2725},23,[187,2727,2645],{"class":2516},[187,2729,585],{"class":577},[187,2731,2732],{"class":196},"send_message_to_mediation_session\n",[187,2734,2736,2739],{"class":189,"line":2735},24,[187,2737,2738],{"class":2516},"  telemetry",[187,2740,2520],{"class":577},[187,2742,2744,2747,2749],{"class":189,"line":2743},25,[187,2745,2746],{"class":2516},"    enabled",[187,2748,585],{"class":577},[187,2750,2751],{"class":588},"false\n",[187,2753,2755,2758],{"class":189,"line":2754},26,[187,2756,2757],{"class":2516},"    tracing",[187,2759,2520],{"class":577},[187,2761,2763,2766],{"class":189,"line":2762},27,[187,2764,2765],{"class":2516},"      phoenix",[187,2767,2520],{"class":577},[187,2769,2771,2774,2776],{"class":189,"line":2770},28,[187,2772,2773],{"class":2516},"        _type",[187,2775,585],{"class":577},[187,2777,2778],{"class":196},"phoenix\n",[187,2780,2782,2785,2787],{"class":189,"line":2781},29,[187,2783,2784],{"class":2516},"        endpoint",[187,2786,585],{"class":577},[187,2788,2789],{"class":196},"http://localhost:6006/v1/traces\n",[187,2791,2793,2796,2798],{"class":189,"line":2792},30,[187,2794,2795],{"class":2516},"        project",[187,2797,585],{"class":577},[187,2799,2800],{"class":196},"default\n",[187,2802,2804],{"class":189,"line":2803},31,[187,2805,316],{"emptyLinePlaceholder":315},[187,2807,2809,2812],{"class":189,"line":2808},32,[187,2810,2811],{"class":2516},"retrievers",[187,2813,2520],{"class":577},[187,2815,2817,2820],{"class":189,"line":2816},33,[187,2818,2819],{"class":2516},"  milvus_retriever",[187,2821,2520],{"class":577},[187,2823,2825,2827,2829],{"class":189,"line":2824},34,[187,2826,2542],{"class":2516},[187,2828,585],{"class":577},[187,2830,2831],{"class":196},"milvus_retriever\n",[187,2833,2835,2838,2840],{"class":189,"line":2834},35,[187,2836,2837],{"class":2516},"    uri",[187,2839,585],{"class":577},[187,2841,2842],{"class":196},"\"http://localhost:19530\"\n",[187,2844,2846,2849,2851],{"class":189,"line":2845},36,[187,2847,2848],{"class":2516},"    embedding_model",[187,2850,585],{"class":577},[187,2852,2853],{"class":196},"\"nv-embedqa-e5-v5\"\n",[187,2855,2857,2860,2862],{"class":189,"line":2856},37,[187,2858,2859],{"class":2516},"    collection_name",[187,2861,585],{"class":577},[187,2863,2864],{"class":196},"\"aiq_case_documents\"\n",[187,2866,2868,2871,2873],{"class":189,"line":2867},38,[187,2869,2870],{"class":2516},"    vector_field",[187,2872,585],{"class":577},[187,2874,2875],{"class":196},"\"embedding\"\n",[187,2877,2879,2882],{"class":189,"line":2878},39,[187,2880,2881],{"class":2516},"    search_params",[187,2883,2520],{"class":577},[187,2885,2887,2890,2892,2895],{"class":189,"line":2886},40,[187,2888,2889],{"class":2516},"      metric_type",[187,2891,585],{"class":577},[187,2893,2894],{"class":196},"\"IP\"",[187,2896,2897],{"class":295}," # works best with nv-embedqa-e5-v5\n",[187,2899,2901],{"class":189,"line":2900},41,[187,2902,316],{"emptyLinePlaceholder":315},[187,2904,2906,2909],{"class":189,"line":2905},42,[187,2907,2908],{"class":2516},"llms",[187,2910,2520],{"class":577},[187,2912,2914,2917],{"class":189,"line":2913},43,[187,2915,2916],{"class":2516},"  nim_llm",[187,2918,2520],{"class":577},[187,2920,2922,2924,2926],{"class":189,"line":2921},44,[187,2923,2542],{"class":2516},[187,2925,585],{"class":577},[187,2927,2928],{"class":196},"nim\n",[187,2930,2932,2935,2937],{"class":189,"line":2931},45,[187,2933,2934],{"class":2516},"    base_url",[187,2936,585],{"class":577},[187,2938,2939],{"class":196},"http://192.168.5.96:1234/v1\n",[187,2941,2943,2946,2948],{"class":189,"line":2942},46,[187,2944,2945],{"class":2516},"    model_name",[187,2947,585],{"class":577},[187,2949,2950],{"class":196},"qwen3-8b\n",[187,2952,2954,2957,2959],{"class":189,"line":2953},47,[187,2955,2956],{"class":2516},"    max_tokens",[187,2958,585],{"class":577},[187,2960,2961],{"class":588},"10000\n",[187,2963,2965,2968,2970],{"class":189,"line":2964},48,[187,2966,2967],{"class":2516},"    temperature",[187,2969,585],{"class":577},[187,2971,2972],{"class":588},"0.7\n",[187,2974,2976,2979],{"class":189,"line":2975},49,[187,2977,2978],{"class":2516},"  mediation_llm",[187,2980,2520],{"class":577},[187,2982,2984,2986,2988],{"class":189,"line":2983},50,[187,2985,2542],{"class":2516},[187,2987,585],{"class":577},[187,2989,2928],{"class":196},[187,2991,2993,2995,2997],{"class":189,"line":2992},51,[187,2994,2934],{"class":2516},[187,2996,585],{"class":577},[187,2998,2939],{"class":196},[187,3000,3002,3004,3006],{"class":189,"line":3001},52,[187,3003,2945],{"class":2516},[187,3005,585],{"class":577},[187,3007,2950],{"class":196},[187,3009,3011,3013,3015],{"class":189,"line":3010},53,[187,3012,2956],{"class":2516},[187,3014,585],{"class":577},[187,3016,2961],{"class":588},[187,3018,3020,3022,3024],{"class":189,"line":3019},54,[187,3021,2967],{"class":2516},[187,3023,585],{"class":577},[187,3025,2972],{"class":588},[187,3027,3029],{"class":189,"line":3028},55,[187,3030,316],{"emptyLinePlaceholder":315},[187,3032,3034,3037],{"class":189,"line":3033},56,[187,3035,3036],{"class":2516},"memory",[187,3038,2520],{"class":577},[187,3040,3042,3045],{"class":189,"line":3041},57,[187,3043,3044],{"class":2516},"  redis_memory",[187,3046,2520],{"class":577},[187,3048,3050,3052,3054],{"class":189,"line":3049},58,[187,3051,2542],{"class":2516},[187,3053,585],{"class":577},[187,3055,3056],{"class":196},"redis_memory\n",[187,3058,3060,3063,3065],{"class":189,"line":3059},59,[187,3061,3062],{"class":2516},"    connection_url",[187,3064,585],{"class":577},[187,3066,3067],{"class":196},"redis://localhost:6379/0\n",[187,3069,3071],{"class":189,"line":3070},60,[187,3072,316],{"emptyLinePlaceholder":315},[187,3074,3076,3079],{"class":189,"line":3075},61,[187,3077,3078],{"class":2516},"functions",[187,3080,2520],{"class":577},[187,3082,3084,3087],{"class":189,"line":3083},62,[187,3085,3086],{"class":2516},"  case_document_rag",[187,3088,2520],{"class":577},[187,3090,3092,3094,3096],{"class":189,"line":3091},63,[187,3093,2542],{"class":2516},[187,3095,585],{"class":577},[187,3097,3098],{"class":196},"case_document_rag\n",[187,3100,3102,3105,3107],{"class":189,"line":3101},64,[187,3103,3104],{"class":2516},"    retriever",[187,3106,585],{"class":577},[187,3108,2831],{"class":196},[187,3110,3112,3115,3117],{"class":189,"line":3111},65,[187,3113,3114],{"class":2516},"    llm_name",[187,3116,585],{"class":577},[187,3118,3119],{"class":196},"nim_llm\n",[187,3121,3123,3125,3127],{"class":189,"line":3122},66,[187,3124,2859],{"class":2516},[187,3126,585],{"class":577},[187,3128,3129],{"class":196},"\"mediation_simulator_case_documents\"\n",[187,3131,3133,3136,3138],{"class":189,"line":3132},67,[187,3134,3135],{"class":2516},"    top_k",[187,3137,585],{"class":577},[187,3139,3140],{"class":588},"5\n",[187,3142,3144,3147],{"class":189,"line":3143},68,[187,3145,3146],{"class":2516},"  case_query_agent",[187,3148,2520],{"class":577},[187,3150,3152,3154,3156],{"class":189,"line":3151},69,[187,3153,2542],{"class":2516},[187,3155,585],{"class":577},[187,3157,3158],{"class":196},"case_query_agent\n",[187,3160,3162,3164,3166],{"class":189,"line":3161},70,[187,3163,3114],{"class":2516},[187,3165,585],{"class":577},[187,3167,3119],{"class":196},[187,3169,3171,3174],{"class":189,"line":3170},71,[187,3172,3173],{"class":2516},"    tool_names",[187,3175,2520],{"class":577},[187,3177,3179,3181],{"class":189,"line":3178},72,[187,3180,2610],{"class":577},[187,3182,3098],{"class":196},[187,3184,3186,3189,3191],{"class":189,"line":3185},73,[187,3187,3188],{"class":2516},"    verbose",[187,3190,585],{"class":577},[187,3192,2530],{"class":588},[187,3194,3196,3199,3201],{"class":189,"line":3195},74,[187,3197,3198],{"class":2516},"    max_iterations",[187,3200,585],{"class":577},[187,3202,3140],{"class":588},[187,3204,3206],{"class":189,"line":3205},75,[187,3207,316],{"emptyLinePlaceholder":315},[187,3209,3211],{"class":189,"line":3210},76,[187,3212,3213],{"class":295},"  # server route functions\n",[187,3215,3217,3220],{"class":189,"line":3216},77,[187,3218,3219],{"class":2516},"  get_mediation_case",[187,3221,2520],{"class":577},[187,3223,3225,3227,3229],{"class":189,"line":3224},78,[187,3226,2542],{"class":2516},[187,3228,585],{"class":577},[187,3230,3231],{"class":196},"server/get_mediation_case\n",[187,3233,3235,3238],{"class":189,"line":3234},79,[187,3236,3237],{"class":2516},"  get_mediation_session",[187,3239,2520],{"class":577},[187,3241,3243,3245,3247],{"class":189,"line":3242},80,[187,3244,2542],{"class":2516},[187,3246,585],{"class":577},[187,3248,3249],{"class":196},"server/get_mediation_session\n",[187,3251,3253,3256],{"class":189,"line":3252},81,[187,3254,3255],{"class":2516},"  send_message_to_mediation_session",[187,3257,2520],{"class":577},[187,3259,3261,3263,3265],{"class":189,"line":3260},82,[187,3262,2542],{"class":2516},[187,3264,585],{"class":577},[187,3266,3267],{"class":196},"mediation\n",[187,3269,3271],{"class":189,"line":3270},83,[187,3272,316],{"emptyLinePlaceholder":315},[187,3274,3276,3279],{"class":189,"line":3275},84,[187,3277,3278],{"class":2516},"embedders",[187,3280,2520],{"class":577},[187,3282,3284,3287],{"class":189,"line":3283},85,[187,3285,3286],{"class":2516},"  nv-embedqa-e5-v5",[187,3288,2520],{"class":577},[187,3290,3292,3294,3296],{"class":189,"line":3291},86,[187,3293,2542],{"class":2516},[187,3295,585],{"class":577},[187,3297,2928],{"class":196},[187,3299,3301,3303,3305],{"class":189,"line":3300},87,[187,3302,2934],{"class":2516},[187,3304,585],{"class":577},[187,3306,3307],{"class":196},"http://192.168.5.96:8000/v1\n",[187,3309,3311,3313,3315],{"class":189,"line":3310},88,[187,3312,2945],{"class":2516},[187,3314,585],{"class":577},[187,3316,3317],{"class":196},"nvidia/nv-embedqa-e5-v5\n",[187,3319,3321],{"class":189,"line":3320},89,[187,3322,316],{"emptyLinePlaceholder":315},[187,3324,3326,3329],{"class":189,"line":3325},90,[187,3327,3328],{"class":2516},"workflow",[187,3330,2520],{"class":577},[187,3332,3334,3337,3339],{"class":189,"line":3333},91,[187,3335,3336],{"class":2516},"  _type",[187,3338,585],{"class":577},[187,3340,3267],{"class":196},[187,3342,3344,3347,3349],{"class":189,"line":3343},92,[187,3345,3346],{"class":2516},"  llm",[187,3348,585],{"class":577},[187,3350,3351],{"class":196},"mediation_llm\n",[187,3353,3355,3358,3360],{"class":189,"line":3354},93,[187,3356,3357],{"class":2516},"  data_dir",[187,3359,585],{"class":577},[187,3361,3362],{"class":196},"./data\n",[11,3364,3365,3366,3368],{},"Let's start at the top with ",[33,3367,2517],{}," key",[911,3370,3371],{"id":2517},[33,3372,2517],{},[11,3374,3375],{},"This section mainly defines the API routes for the FastAPI integration and the telemetry options I set up to view all of my programs traces. When building applications with LLMs, instrumenting for observability is key! Agent Intelligence Toolkit makes it really easy to hook up not just one observability tool, but really any number of observability tools! It all works through asynchronous calls, so it doesn't slow down the application.",[11,3377,3378],{},[511,3379],{"alt":3380,"src":3381},"LLM observability","/static/mediation-simulator/Phoenix.png",[11,3383,3384],{},"Defining the API routes was pretty straightforward. You define an AIQ Toolkit function that handles the route's behavior. These routes are also automatically added to the API's documentation page using OpenAPI/Swagger:",[11,3386,3387],{},[511,3388],{"alt":3389,"src":3390},"API documentation","/static/mediation-simulator/fastapi.png",[911,3392,3393],{"id":2811},[33,3394,2811],{},[11,3396,3397],{},"This section allows you to define different vector storage databases that your application uses. I used Milvus to store embeddings of case documents. Ultimately I wasn't able to incorporate these embeddings into my application.",[911,3399,3400],{"id":2908},[33,3401,2908],{},[11,3403,3404],{},"The LLMs section allows you to plug in to any LLM. I used a combination of LM Studio and NVIDIA NIMs to test my application. You can pretty much use any LLM that provides an OpenAI API interface. Local models have come a long way! New models like Qwen3 and Llama 3.1 have massive context windows (130k tokens!) which is a total game changer. These models are also getting a lot smarter. I was really impressed with how well these models followed my prompts. Using local models is nice because you will not be rate limited. I processed about 2 million prompt tokens and generated about 1.5 million completion tokens during the development of Mediation Simulator. As amazing as these new frontier models are now, I'm still bullish on the capabilities of (small) large language models that can run on consumer hardware like NVIDIA RTX GPUs.",[911,3406,3407],{"id":3036},[33,3408,3036],{},[11,3410,3411],{},"Figure out how memory works in the AIQ toolkit was a big \"ah-ha!\" moment for me. It allows for persisting chat messages between generations, and also storing arbitrary data that you can use in your workflows. I decided to use Redis (Redis Stack) to build a memory backend. Here's a quick look at what that code looks like:",[26,3413,3415],{"className":1383,"code":3414,"language":1125,"meta":35,"style":35},"@register_memory(config_type=RedisMemoryConfig)\nasync def redis_memory(config: RedisMemoryConfig, builder: Builder):\n\n    class RedisMemoryEditor(MemoryEditor):\n        def __init__(self, config: RedisMemoryConfig):\n            self._conn_url = config.connection_url\n            self.redis = Redis.from_url(self._conn_url)\n\n        async def get_client(self, session_id: str) -> RedisChatMessageHistory:\n            conn = RedisChatMessageHistory(\n                session_id=session_id, redis_url=self._conn_url\n            )\n            return conn\n\n        # mediation session state management\n        async def add_messages(\n            self, items: Sequence[BaseMessage], session_id: str\n        ) -> None:\n            client = await self.get_client(session_id)\n            await client.aadd_messages(items)\n\n        async def get_messages(self, session_id: str) -> Sequence[BaseMessage]:\n            client = await self.get_client(session_id)\n            messages = await client.aget_messages()\n            return messages\n\n        # case generation state management\n        async def save_case_description(\n            self, case_description: str, case_id: str\n        ) -> None:\n            \"\"\"\n            sets the case description using the \u003Ccase_id>_case_description as the redis key\n            \"\"\"\n            self.redis.set(f\"{case_id}_case_description\", case_description)\n\n        ...\n",[33,3416,3417,3422,3427,3431,3436,3441,3446,3451,3455,3460,3465,3470,3475,3480,3484,3489,3494,3499,3504,3509,3514,3518,3523,3527,3532,3537,3541,3546,3551,3556,3560,3565,3570,3574,3579,3583],{"__ignoreMap":35},[187,3418,3419],{"class":189,"line":190},[187,3420,3421],{},"@register_memory(config_type=RedisMemoryConfig)\n",[187,3423,3424],{"class":189,"line":249},[187,3425,3426],{},"async def redis_memory(config: RedisMemoryConfig, builder: Builder):\n",[187,3428,3429],{"class":189,"line":312},[187,3430,316],{"emptyLinePlaceholder":315},[187,3432,3433],{"class":189,"line":319},[187,3434,3435],{},"    class RedisMemoryEditor(MemoryEditor):\n",[187,3437,3438],{"class":189,"line":325},[187,3439,3440],{},"        def __init__(self, config: RedisMemoryConfig):\n",[187,3442,3443],{"class":189,"line":686},[187,3444,3445],{},"            self._conn_url = config.connection_url\n",[187,3447,3448],{"class":189,"line":697},[187,3449,3450],{},"            self.redis = Redis.from_url(self._conn_url)\n",[187,3452,3453],{"class":189,"line":1291},[187,3454,316],{"emptyLinePlaceholder":315},[187,3456,3457],{"class":189,"line":1306},[187,3458,3459],{},"        async def get_client(self, session_id: str) -> RedisChatMessageHistory:\n",[187,3461,3462],{"class":189,"line":1434},[187,3463,3464],{},"            conn = RedisChatMessageHistory(\n",[187,3466,3467],{"class":189,"line":2599},[187,3468,3469],{},"                session_id=session_id, redis_url=self._conn_url\n",[187,3471,3472],{"class":189,"line":2607},[187,3473,3474],{},"            )\n",[187,3476,3477],{"class":189,"line":2621},[187,3478,3479],{},"            return conn\n",[187,3481,3482],{"class":189,"line":2631},[187,3483,316],{"emptyLinePlaceholder":315},[187,3485,3486],{"class":189,"line":2642},[187,3487,3488],{},"        # mediation session state management\n",[187,3490,3491],{"class":189,"line":2653},[187,3492,3493],{},"        async def add_messages(\n",[187,3495,3496],{"class":189,"line":2665},[187,3497,3498],{},"            self, items: Sequence[BaseMessage], session_id: str\n",[187,3500,3501],{"class":189,"line":2674},[187,3502,3503],{},"        ) -> None:\n",[187,3505,3506],{"class":189,"line":2684},[187,3507,3508],{},"            client = await self.get_client(session_id)\n",[187,3510,3511],{"class":189,"line":2694},[187,3512,3513],{},"            await client.aadd_messages(items)\n",[187,3515,3516],{"class":189,"line":2706},[187,3517,316],{"emptyLinePlaceholder":315},[187,3519,3520],{"class":189,"line":2715},[187,3521,3522],{},"        async def get_messages(self, session_id: str) -> Sequence[BaseMessage]:\n",[187,3524,3525],{"class":189,"line":2725},[187,3526,3508],{},[187,3528,3529],{"class":189,"line":2735},[187,3530,3531],{},"            messages = await client.aget_messages()\n",[187,3533,3534],{"class":189,"line":2743},[187,3535,3536],{},"            return messages\n",[187,3538,3539],{"class":189,"line":2754},[187,3540,316],{"emptyLinePlaceholder":315},[187,3542,3543],{"class":189,"line":2762},[187,3544,3545],{},"        # case generation state management\n",[187,3547,3548],{"class":189,"line":2770},[187,3549,3550],{},"        async def save_case_description(\n",[187,3552,3553],{"class":189,"line":2781},[187,3554,3555],{},"            self, case_description: str, case_id: str\n",[187,3557,3558],{"class":189,"line":2792},[187,3559,3503],{},[187,3561,3562],{"class":189,"line":2803},[187,3563,3564],{},"            \"\"\"\n",[187,3566,3567],{"class":189,"line":2808},[187,3568,3569],{},"            sets the case description using the \u003Ccase_id>_case_description as the redis key\n",[187,3571,3572],{"class":189,"line":2816},[187,3573,3564],{},[187,3575,3576],{"class":189,"line":2824},[187,3577,3578],{},"            self.redis.set(f\"{case_id}_case_description\", case_description)\n",[187,3580,3581],{"class":189,"line":2834},[187,3582,316],{"emptyLinePlaceholder":315},[187,3584,3585],{"class":189,"line":2845},[187,3586,3587],{},"        ...\n",[11,3589,3590,3591,3594],{},"I found that LangChain has a ",[33,3592,3593],{},"RedisChatMessageHistory"," class that made putting this backend together almost trivial. Redis Stack also ships with a web viewer which really came in handy for debugging my memory backend:",[11,3596,3597],{},[511,3598],{"alt":3599,"src":3600},"Reis Memory Backend","/static/mediation-simulator/redis.png",[11,3602,3603],{},"For storing other types of data, I was able to implement my own methods and store things like case data or other metadata for a mediation simulator session for things like current_speaker, number of session, current session, etc. I love Redis! The setup is also really easy, I just added a docker compose file:",[26,3605,3607],{"className":2507,"code":3606,"language":2509,"meta":35,"style":35},"services:\n  redis:\n    image: redis/redis-stack:latest\n    volumes:\n      - redis-data:/data\n    container_name: redis\n    ports:\n      - 6379:6379\n      - 8001:8001  # RedisInsight port\n\nvolumes:\n  redis-data:\n",[33,3608,3609,3616,3623,3633,3640,3647,3657,3664,3671,3681,3685,3692],{"__ignoreMap":35},[187,3610,3611,3614],{"class":189,"line":190},[187,3612,3613],{"class":2516},"services",[187,3615,2520],{"class":577},[187,3617,3618,3621],{"class":189,"line":249},[187,3619,3620],{"class":2516},"  redis",[187,3622,2520],{"class":577},[187,3624,3625,3628,3630],{"class":189,"line":312},[187,3626,3627],{"class":2516},"    image",[187,3629,585],{"class":577},[187,3631,3632],{"class":196},"redis/redis-stack:latest\n",[187,3634,3635,3638],{"class":189,"line":319},[187,3636,3637],{"class":2516},"    volumes",[187,3639,2520],{"class":577},[187,3641,3642,3644],{"class":189,"line":325},[187,3643,2610],{"class":577},[187,3645,3646],{"class":196},"redis-data:/data\n",[187,3648,3649,3652,3654],{"class":189,"line":686},[187,3650,3651],{"class":2516},"    container_name",[187,3653,585],{"class":577},[187,3655,3656],{"class":196},"redis\n",[187,3658,3659,3662],{"class":189,"line":697},[187,3660,3661],{"class":2516},"    ports",[187,3663,2520],{"class":577},[187,3665,3666,3668],{"class":189,"line":1291},[187,3667,2610],{"class":577},[187,3669,3670],{"class":196},"6379:6379\n",[187,3672,3673,3675,3678],{"class":189,"line":1306},[187,3674,2610],{"class":577},[187,3676,3677],{"class":196},"8001:8001",[187,3679,3680],{"class":295},"  # RedisInsight port\n",[187,3682,3683],{"class":189,"line":1434},[187,3684,316],{"emptyLinePlaceholder":315},[187,3686,3687,3690],{"class":189,"line":2599},[187,3688,3689],{"class":2516},"volumes",[187,3691,2520],{"class":577},[187,3693,3694,3697],{"class":189,"line":2607},[187,3695,3696],{"class":2516},"  redis-data",[187,3698,2520],{"class":577},[911,3700,3701],{"id":3078},[33,3702,3078],{},[11,3704,3705],{},"Functions are the building blocks of the AIQ Toolkit. You need to register the functions in your config file, then you can use them for different things, like the function that handles an API route, or the function that handles an agentic workflow. I defined some functions for RAG to allow my agents to look up case data, but I wasn't able to fully implement this in my main mediation simualator workflow. But the setup was easy!",[911,3707,3708],{"id":3278},[33,3709,3278],{},[11,3711,3712],{},"Embedders is a section that allows you to define embedding models that you would use together with RAG (for converting text to a vector embedding). Since I needed to make a lot of embeddings for all of the documents I generated for case facts, I used a locally hosted NVIDIA NIM:",[26,3714,3716],{"className":2507,"code":3715,"language":2509,"meta":35,"style":35},"embedders:\n  nv-embedqa-e5-v5:\n    _type: nim\n    base_url: http://192.168.5.96:8000/v1\n    model_name: nvidia/nv-embedqa-e5-v5\n",[33,3717,3718,3724,3730,3738,3746],{"__ignoreMap":35},[187,3719,3720,3722],{"class":189,"line":190},[187,3721,3278],{"class":2516},[187,3723,2520],{"class":577},[187,3725,3726,3728],{"class":189,"line":249},[187,3727,3286],{"class":2516},[187,3729,2520],{"class":577},[187,3731,3732,3734,3736],{"class":189,"line":312},[187,3733,2542],{"class":2516},[187,3735,585],{"class":577},[187,3737,2928],{"class":196},[187,3739,3740,3742,3744],{"class":189,"line":319},[187,3741,2934],{"class":2516},[187,3743,585],{"class":577},[187,3745,3307],{"class":196},[187,3747,3748,3750,3752],{"class":189,"line":325},[187,3749,2945],{"class":2516},[187,3751,585],{"class":577},[187,3753,3317],{"class":196},[11,3755,3756],{},"I would have run into rate limits if I was using the hosted version, so being able to run this locally was important for my use case.",[911,3758,3759],{"id":3328},[33,3760,3328],{},[11,3762,3763],{},"The workflow is the main \"application\" part of the config file. It is the entrypoint for your application. In my case, the workflow invokes a LangGraph that does my simulation. First it loads data from my memory backend and when I'm using the interactive mode it gathers information from the request like path parameters so it knows what data fetch from memory (like the case id and session id).",[11,3765,3766],{},"My mediation workflow code is a little bit messy. I tried to keep all of my prompting logic in separate files for simplicity. The trickiest part for me was serializing data between different formats: langgraph state, YAML files and Redis memory. I'm happy to have something now that is functional, but there are a lot of improvements and further refactoring that would make the code easier to read and maintain.",[11,3768,3769],{},"That wraps up the tour of my main config file for mediation simulator! I also had another smaller config file for case generation. Here's a quick look at that:",[26,3771,3773],{"className":2507,"code":3772,"language":2509,"meta":35,"style":35},"general:\n  use_uvloop: true\n  telemetry:\n    enabled: false\n    tracing:\n      phoenix:\n        _type: phoenix\n        endpoint: http://localhost:6006/v1/traces\n        project: mediation-simulator\n\nllms:\n  nim_llm:\n    _type: nim\n    base_url: http://192.168.5.96:1234/v1\n    model_name: qwen3-8b\n    max_tokens: 10000\n    temperature: 0.7\n  # nim_llm:\n  #   _type: nim\n  #   model_name: meta/llama-3.1-70b-instruct\n  #   max_tokens: 10000\n  #   temperature: 0.7\n\nmemory:\n  redis_memory:\n    _type: redis_memory\n    connection_url: redis://localhost:6379/0\n\nworkflow:\n  _type: case_generation\n  llm_name: nim_llm\n  data_dir: ./data\n",[33,3774,3775,3781,3789,3795,3803,3809,3815,3823,3831,3840,3844,3850,3856,3864,3872,3880,3888,3896,3901,3906,3911,3916,3921,3925,3931,3937,3945,3953,3957,3963,3972,3981],{"__ignoreMap":35},[187,3776,3777,3779],{"class":189,"line":190},[187,3778,2517],{"class":2516},[187,3780,2520],{"class":577},[187,3782,3783,3785,3787],{"class":189,"line":249},[187,3784,2525],{"class":2516},[187,3786,585],{"class":577},[187,3788,2530],{"class":588},[187,3790,3791,3793],{"class":189,"line":312},[187,3792,2738],{"class":2516},[187,3794,2520],{"class":577},[187,3796,3797,3799,3801],{"class":189,"line":319},[187,3798,2746],{"class":2516},[187,3800,585],{"class":577},[187,3802,2751],{"class":588},[187,3804,3805,3807],{"class":189,"line":325},[187,3806,2757],{"class":2516},[187,3808,2520],{"class":577},[187,3810,3811,3813],{"class":189,"line":686},[187,3812,2765],{"class":2516},[187,3814,2520],{"class":577},[187,3816,3817,3819,3821],{"class":189,"line":697},[187,3818,2773],{"class":2516},[187,3820,585],{"class":577},[187,3822,2778],{"class":196},[187,3824,3825,3827,3829],{"class":189,"line":1291},[187,3826,2784],{"class":2516},[187,3828,585],{"class":577},[187,3830,2789],{"class":196},[187,3832,3833,3835,3837],{"class":189,"line":1306},[187,3834,2795],{"class":2516},[187,3836,585],{"class":577},[187,3838,3839],{"class":196},"mediation-simulator\n",[187,3841,3842],{"class":189,"line":1434},[187,3843,316],{"emptyLinePlaceholder":315},[187,3845,3846,3848],{"class":189,"line":2599},[187,3847,2908],{"class":2516},[187,3849,2520],{"class":577},[187,3851,3852,3854],{"class":189,"line":2607},[187,3853,2916],{"class":2516},[187,3855,2520],{"class":577},[187,3857,3858,3860,3862],{"class":189,"line":2621},[187,3859,2542],{"class":2516},[187,3861,585],{"class":577},[187,3863,2928],{"class":196},[187,3865,3866,3868,3870],{"class":189,"line":2631},[187,3867,2934],{"class":2516},[187,3869,585],{"class":577},[187,3871,2939],{"class":196},[187,3873,3874,3876,3878],{"class":189,"line":2642},[187,3875,2945],{"class":2516},[187,3877,585],{"class":577},[187,3879,2950],{"class":196},[187,3881,3882,3884,3886],{"class":189,"line":2653},[187,3883,2956],{"class":2516},[187,3885,585],{"class":577},[187,3887,2961],{"class":588},[187,3889,3890,3892,3894],{"class":189,"line":2665},[187,3891,2967],{"class":2516},[187,3893,585],{"class":577},[187,3895,2972],{"class":588},[187,3897,3898],{"class":189,"line":2674},[187,3899,3900],{"class":295},"  # nim_llm:\n",[187,3902,3903],{"class":189,"line":2684},[187,3904,3905],{"class":295},"  #   _type: nim\n",[187,3907,3908],{"class":189,"line":2694},[187,3909,3910],{"class":295},"  #   model_name: meta/llama-3.1-70b-instruct\n",[187,3912,3913],{"class":189,"line":2706},[187,3914,3915],{"class":295},"  #   max_tokens: 10000\n",[187,3917,3918],{"class":189,"line":2715},[187,3919,3920],{"class":295},"  #   temperature: 0.7\n",[187,3922,3923],{"class":189,"line":2725},[187,3924,316],{"emptyLinePlaceholder":315},[187,3926,3927,3929],{"class":189,"line":2735},[187,3928,3036],{"class":2516},[187,3930,2520],{"class":577},[187,3932,3933,3935],{"class":189,"line":2743},[187,3934,3044],{"class":2516},[187,3936,2520],{"class":577},[187,3938,3939,3941,3943],{"class":189,"line":2754},[187,3940,2542],{"class":2516},[187,3942,585],{"class":577},[187,3944,3056],{"class":196},[187,3946,3947,3949,3951],{"class":189,"line":2762},[187,3948,3062],{"class":2516},[187,3950,585],{"class":577},[187,3952,3067],{"class":196},[187,3954,3955],{"class":189,"line":2770},[187,3956,316],{"emptyLinePlaceholder":315},[187,3958,3959,3961],{"class":189,"line":2781},[187,3960,3328],{"class":2516},[187,3962,2520],{"class":577},[187,3964,3965,3967,3969],{"class":189,"line":2792},[187,3966,3336],{"class":2516},[187,3968,585],{"class":577},[187,3970,3971],{"class":196},"case_generation\n",[187,3973,3974,3977,3979],{"class":189,"line":2803},[187,3975,3976],{"class":2516},"  llm_name",[187,3978,585],{"class":577},[187,3980,3119],{"class":196},[187,3982,3983,3985,3987],{"class":189,"line":2808},[187,3984,3357],{"class":2516},[187,3986,585],{"class":577},[187,3988,3362],{"class":196},[11,3990,3991],{},"I added a lot of logging to my workflow in order to keep an eye on how the workflows progressed. Here's a sample of the logs from the CLI invocation of the mediation simulator program:",[26,3993,3996],{"className":3994,"code":3995,"language":31},[29],"12:08:48 mediation.register INFO   ⚖️ [MEDIATOR]: Mediator node called\n12:08:56 mediation.register INFO   👤 [CLERK] Starting clerk node - Phase: JOINT_DISCUSSION_INFO_GATHERING, Turn: 3\n12:08:56 mediation.register INFO   🤔 [CLERK] Using LLM to decide next speaker\n12:09:12 mediation.register INFO   🎯 [CLERK] LLM selected next speaker: RESPONDING_PARTY\n12:09:12 mediation.register INFO   📊 [CLERK] Updated counters - Turn: 4, Phase turns: 4\n12:09:12 mediation.register INFO   🌚 Responding party node called\n12:09:20 mediation.register INFO   👤 [CLERK] Starting clerk node - Phase: JOINT_DISCUSSION_INFO_GATHERING, Turn: 4\n12:09:20 mediation.register INFO   🤔 [CLERK] Using LLM to decide next speaker\n12:09:23 mediation.register INFO   🎯 [CLERK] LLM selected next speaker: REQUESTING_PARTY\n12:09:23 mediation.register INFO   📊 [CLERK] Updated counters - Turn: 5, Phase turns: 5\n12:09:23 mediation.register INFO   🌝 Requesting party node called\n12:09:31 mediation.register INFO   👤 [CLERK] Starting clerk node - Phase: JOINT_DISCUSSION_INFO_GATHERING, Turn: 5\n12:09:31 mediation.register INFO   ⏰ [CLERK] Max turns (5) reached for current phase\n12:09:31 mediation.register INFO   🔄 [CLERK] Transitioning from joint discussion to negotiation\n12:09:31 mediation.register INFO   👨‍⚖️ [CLERK] Mediator will start the new phase\n12:09:31 mediation.register INFO   ⚖️ [MEDIATOR]: Mediator node called\n12:09:40 mediation.register INFO   👤 [CLERK] Starting clerk node - Phase: NEGOTIATION_BARGAINING, Turn: 5\n12:09:40 mediation.register INFO   🤔 [CLERK] Using LLM to decide next speaker\n12:09:48 mediation.register INFO   🎯 [CLERK] LLM selected next speaker: REQUESTING_PARTY\n12:09:48 mediation.register INFO   📊 [CLERK] Updated counters - Turn: 6, Phase turns: 1\n12:09:48 mediation.register INFO   🌝 Requesting party node called\n12:09:56 mediation.register INFO   👤 [CLERK] Starting clerk node - Phase: NEGOTIATION_BARGAINING, Turn: 6\n12:09:56 mediation.register INFO   🤔 [CLERK] Using LLM to decide next speaker\n12:09:58 mediation.register INFO   🎯 [CLERK] LLM selected next speaker: RESPONDING_PARTY\n12:09:58 mediation.register INFO   📊 [CLERK] Updated counters - Turn: 7, Phase turns: 2\n",[33,3997,3995],{"__ignoreMap":35},[11,3999,4000],{},"Config files can be as simple or as complex as they need to be depending on your workflow.",[11,4002,4003],{},"The AgentIQ Toolkit is doing something really valuable by bringing patterns and components from these (and other) frameworks into a cohesive system. This allows for some truly interesting and powerful combinations. The examples provided are excellent and taught me a lot about new patterns for building agentic workflows. ReWOO agents, for instance, were a new concept for me, as was seeing how to effectively combine LangGraph with LlamaIndex.",[11,4005,4006],{},"What I particularly appreciated was:",[916,4008,4009,4015,4021],{},[919,4010,4011,4014],{},[338,4012,4013],{},"Standardized Patterns:"," AgentIQ promotes good practices for crucial aspects of AI development, like evaluations and telemetry/tracing.",[919,4016,4017,4020],{},[338,4018,4019],{},"YAML Configuration:"," I really like using YAML for configuring development environments, similar to Docker Compose. It standardizes things and vastly improves readability. The way AgentIQ allows registering functions that can be included in workflows via YAML config files is a great example of this.",[919,4022,4023,4026],{},[338,4024,4025],{},"A Learning Goldmine:"," Working through as many examples as possible was incredibly beneficial. Reading the code helped me grasp the patterns underpinning AgentIQ. You are pretty much guaranteed to learn something new!",[11,4028,4029],{},"I'm so glad I took the plunge and got my feet wet with the AgentIQ Toolkit. It's been a fantastic learning resource.",[911,4031,4033],{"id":4032},"mediation-competitions-but-for-llms","Mediation Competitions, But for LLMs?",[11,4035,4036],{},"One of the fun, forward-looking ideas this project sparks is the concept of \"mediation competitions, but for LLMs.\" Imagine pitting different LLMs against each other, representing the requesting and responding parties, to see how they fare in these complex negotiation scenarios!",[911,4038,4040],{"id":4039},"whats-next","What's Next?",[11,4042,4043],{},"This hackathon project has been an incredible learning journey. Building Mediation Simulator has not only been a fun technical challenge but has also opened my eyes to the potential of AI agents in simulating complex human interactions. I'm excited to continue refining it and exploring more possibilities with the AgentIQ Toolkit!",[855,4045,4046],{},"html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":4048},[4049,4055],{"id":2240,"depth":249,"text":2241,"children":4050},[4051,4052,4053,4054],{"id":2270,"depth":312,"text":2271},{"id":2331,"depth":312,"text":2332},{"id":2384,"depth":312,"text":2385},{"id":2475,"depth":312,"text":2476},{"id":2497,"depth":249,"text":2498,"children":4056},[4057,4058,4059,4060,4061,4062,4063,4064,4065],{"id":2517,"depth":312,"text":2517},{"id":2811,"depth":312,"text":2811},{"id":2908,"depth":312,"text":2908},{"id":3036,"depth":312,"text":3036},{"id":3078,"depth":312,"text":3078},{"id":3278,"depth":312,"text":3278},{"id":3328,"depth":312,"text":3328},{"id":4032,"depth":312,"text":4033},{"id":4039,"depth":312,"text":4040},"2025-05-27","Mediation Simulator is an AI application designed to help legal students and professionals build dispute resolution skills through simulated mediation sessions",[4069],{"link":2231,"site":2196},"/static/mediation-simulator/mediation_simulator_title_image.png",{},"/2025/05/27/mediation-simulator-project-for-nvidia-agent-intelligence-toolkit",{"title":2211,"description":4067},"2025/05/27/mediation-simulator-project-for-nvidia-agent-intelligence-toolkit",[2203,2204,4076,4077,2206,4078],"agents","llm","mediation","Udh9dNGW_rT7IqtVY7OCxuVjWrktYpixcFRElMcmS5Q",{"id":4081,"title":4082,"body":4083,"comments":315,"date":6297,"description":6298,"draft":872,"extension":873,"external":6299,"image":4129,"meta":6304,"navigation":315,"path":6305,"seo":6306,"stem":6307,"tags":6308,"__hash__":6314},"blog/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest.md","RedLM: My submission for the NVIDIA and LlamaIndex Developer Contest",{"type":8,"value":4084,"toc":6259},[4085,4089,4092,4095,4099,4102,4117,4121,4124,4130,4133,4144,4148,4151,4156,4164,4167,4171,4174,4177,4184,4188,4191,4197,4200,4203,4209,4222,4228,4231,4251,4254,4283,4286,4303,4322,4326,4329,4335,4343,4348,4351,4357,4364,4371,4541,4545,4548,4726,4733,4737,4740,4746,4757,4763,4772,4776,4779,4827,4837,4931,4937,4940,4950,4954,4957,4966,4980,4986,4989,4992,4995,5001,5004,5031,5041,5058,5061,5090,5093,5096,5101,5104,5109,5112,5116,5119,5122,5128,5131,5156,5162,5165,5168,5174,5177,5209,5212,5321,5324,5327,5330,5333,5450,5453,5457,5460,5463,5473,5479,5485,5494,5499,5504,5507,5512,5517,5521,5524,5563,5569,5578,5594,5707,5713,5717,5723,5740,5746,5752,5755,5758,5764,5767,5772,5775,5781,5785,5791,5796,5805,5811,5814,5818,5821,5827,5830,5835,5838,5842,5848,5861,5864,5873,5879,5886,5892,5899,5904,5907,5911,5914,5922,5941,5950,5956,5959,5965,5968,5973,5976,5985,5996,6001,6011,6021,6028,6033,6037,6040,6043,6049,6052,6056,6062,6065,6068,6087,6090,6096,6099,6105,6108,6112,6115,6120,6123,6129,6150,6154,6157,6163,6166,6169,6181,6187,6195,6201,6204,6210,6219,6224,6227,6233,6236,6239,6245,6251,6256],[168,4086,4088],{"id":4087},"tddr","td;dr",[11,4090,4091],{},"RedLM is a new way to study art and literature powered by artificial intelligence. It is an application that applies LLMs to the study of one of China’s most famous literary works: Dream of the Red Chamber. It uses leading language and vision models from Chinese AI groups including Alibaba’s Qwen, Baichuan Intelligence Technology and 01.AI. RedLM uses tools, techniques and services from NVIDIA and LlamaIndex including NVIDIA NIMs, Retrieval Augmented Generation and Multi-Modal RAG with vision language models. This project is my submission for the NVIDIA and LlamaIndex Developer Contest.",[11,4093,4094],{},"This article will cover how I built the project, challenges I faced and some of the lessons I learned while working with NVIDIA and LlamaIndex technologies.",[911,4096,4098],{"id":4097},"links","Links",[4100,4101],"red-lm-tweet",{},[916,4103,4104,4110],{},[919,4105,4106],{},[15,4107,2233],{"href":4108,"rel":4109},"https://x.com/briancaffey/status/1855186768452321330",[19],[919,4111,4112],{},[15,4113,4116],{"href":4114,"rel":4115},"https://github.com/briancaffey/RedLM",[19],"RedLM GitHub repository",[168,4118,4120],{"id":4119},"what-is-redlm","What is RedLM?",[11,4122,4123],{},"RedLM is a combination of the word “Red” and LM, an abbreviation for “language model”. Dream of the Red Chamber is such an important book in Chinese literature that it has its own field of study called 红学 (literally “the study of red”), or Redology. So, RedLM is an application that uses language models for the study of Redology.",[11,4125,4126],{},[511,4127],{"alt":4128,"src":4129},"RedLM","/static/redlm/title.png",[11,4131,4132],{},"In this project I focused on three applications of language models:",[2276,4134,4135,4138,4141],{},[919,4136,4137],{},"Summary and translation of the source text",[919,4139,4140],{},"A Q&A bot that can answer questions about the book providing references to the specific paragraphs used to give answers",[919,4142,4143],{},"An image-based Q&A bot that can answer questions about sections of paintings that depict scenes from each of the book’s chapters.",[168,4145,4147],{"id":4146},"notebooklm","NotebookLM",[11,4149,4150],{},"I used this article to create a \"Deep Dive\" podcast episode for RedLM using Google's NotebookLM.",[11,4152,4153],{},[511,4154],{"alt":4147,"src":4155},"/static/redlm/notebooklm.png",[11,4157,4158,4159,752],{},"You can ",[15,4160,4163],{"href":4161,"rel":4162},"https://x.com/briancaffey/status/1855186771409244491",[19],"listen to this podcast episode here on 𝕏",[4165,4166],"red-lm-deep-dive-video",{},[168,4168,4170],{"id":4169},"how-i-built-redlm","How I built RedLM",[11,4172,4173],{},"RedLM consists of two parts: a web UI built with Vue 3 using the Nuxt Framework and a backend API built with Python, FastAPI and LlamaIndex. There are lots of great tools for building full-stack AI applications such as Gradio and Streamlit, but I wanted to build with the web tools that I’m most familiar with and that provide the most flexibility. These frameworks (Nuxt and FastAPI) are simple and effective and they allowed me to develop quickly.",[11,4175,4176],{},"Most of the code for this project was written by AI. I used OpenAI’s ChatGPT 4o, Anthropic’s Claude 3.5 Sonnet and 01.AI’s Yi-1.5-9B-Chat model. In my development process with AI, I prompted for one logical piece of the application at a time, such as one API route, one Vue component, one pinia store or one utility function, for example. In this article I'll share some of the prompts I used in my development workflow.",[11,4178,4179,4180,4183],{},"This project embraces a hybrid AI inference model, meaning that the AI inference can be done either on local RTX PCs or using NVIDIA’s Cloud APIs from ",[33,4181,4182],{},"build.nvidia.com"," depending on configuration via environment variables. I used PCs with NVIDIA GeForce RTX 4090 GPUs to do inference with language and vision models, and with a change of configuration, I was able to do similar inference using NVIDIA’s API endpoints. This allowed me to develop the project both on powerful RTX desktop workstations and Mac laptops.",[168,4185,4187],{"id":4186},"translating-dream-of-the-red-chamber-with-tensorrt-llm","Translating Dream of the Red Chamber with TensorRT-LLM",[11,4189,4190],{},"Translation is often mentioned as one of the capabilities of bilingual LLMs from China. I wanted to try translating this book from Chinese to English, but I also wanted to better understand the meaning of the original text written in vernacular Chinese. Written vernacular Chinese is essentially a form of Chinese that closely resembles the way Chinese was spoken in imperial China by common people. The use of vernacular Chinese (Baihua) in literary works marked a significant cultural shift that started to make literature and education more accessible. Before the emergence of written vernacular Chinese, Chinese literature was dominated by Classical Chinese (Wenyanwen) which is a more concise, ambiguous and specialized for of languages that assumes an understanding of ancient texts and Confucian classics. The difference between vernacular Chinese and modern Mandarin Chinese is somewhat analogous to the different between Shakespearian English (Early Modern English) and Modern English.",[11,4192,4193],{},[511,4194],{"alt":4195,"src":4196},"Baihua, Mandarin and English","/static/redlm/translations.png",[11,4198,4199],{},"Chinese large language models are well versed in Classical Chinese, written Chinese vernacular and modern Mandarin Chinese. I decided to rewrite the original vernacular text in simple, modern Mandarin Chinese and then using this new modern Mandarin version, translate the story into English.",[11,4201,4202],{},"Dream of the Red Chamber is a large book. It is composed of over 800,000 Chinese characters, using 4303 unique Chinese characters. It has 120 chapters and a total of 3996 paragraphs. Here is a histogram showing the number of characters per paragraph.",[11,4204,4205],{},[511,4206],{"alt":4207,"src":4208},"Paragraph lengths","/static/redlm/paragraphs.png",[11,4210,4211,4212,4217,4218,4221],{},"I rented a large multi-GPU instance from AWS using some of the credits I get as a member of the AWS Community Builders program. The g5.12xlarge instance I selected has 4 A10G Tensor Core GPUs. The TensorRT-LLM LLM API is a relatively new part of the TensorRT-LLM library. It provides a very simple, high-level interface for doing inference. Following the ",[15,4213,4216],{"href":4214,"rel":4215},"https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_inference_distributed.html",[19],"LLM Generate Distributed example"," from the TensorRT-LLM documentation, I was able to translate the entire book into simple Mandarin and then from Mandarin into English in about an hour and 15 minutes. The ",[33,4219,4220],{},"tensor_parallel_size"," option in the LLM API allows for distributed inference, this meant that up to 4 paragraphs could be translated at the same time on different GPUs on the same EC2 instance.",[26,4223,4226],{"className":4224,"code":4225,"language":31},[29],"Translating: data/book/22.json\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:15\u003C00:00,  2.41it/s]\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:24\u003C00:00,  1.54it/s]\nTranslated: data/book/22.json\nTranslating: data/book/114.json\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:11\u003C00:00,  1.81it/s]\nProcessed requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:12\u003C00:00,  1.58it/s]\nTranslated: data/book/114.json\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n[TensorRT-LLM][INFO] Refreshed the MPI local session\n\nreal    74m1.578s\nuser    0m45.936s\nsys 0m36.283s\n",[33,4227,4225],{"__ignoreMap":35},[11,4229,4230],{},"Getting good results required a bit of experimentation with parameters. The LLM API makes this very easy. The following code configures settings and builds the inference engine that can be used for doing completions:",[26,4232,4234],{"className":1383,"code":4233,"language":1125,"meta":35,"style":35},"sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=256)\nbuild_config = BuildConfig(max_seq_len=2048)\nllm = LLM(model=MODEL, build_config=build_config, tensor_parallel_size=4)\n",[33,4235,4236,4241,4246],{"__ignoreMap":35},[187,4237,4238],{"class":189,"line":190},[187,4239,4240],{},"sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=256)\n",[187,4242,4243],{"class":189,"line":249},[187,4244,4245],{},"build_config = BuildConfig(max_seq_len=2048)\n",[187,4247,4248],{"class":189,"line":312},[187,4249,4250],{},"llm = LLM(model=MODEL, build_config=build_config, tensor_parallel_size=4)\n",[11,4252,4253],{},"I used the following prompts to rewrite each paragraph of the original text in simple, modern Mandarin Chinese:",[26,4255,4257],{"className":1383,"code":4256,"language":1125,"meta":35,"style":35},"bai_prompts = [\n    # Here are examples of how to rewrite Chinese vernacular into simple modern Mandarin.\\n\\nChinese vernacular:\\n\\n{p}\\n\\nSimple modern Mandarin\n    f\"以下是如何将中国白话改写为简单的现代普通话的示例。\\n\\n中文白话：\\n\\n{p}\\n\\n简单的现代普通话：\\n\\n\"\n    for p in flat_bai\n]\n",[33,4258,4259,4264,4269,4274,4279],{"__ignoreMap":35},[187,4260,4261],{"class":189,"line":190},[187,4262,4263],{},"bai_prompts = [\n",[187,4265,4266],{"class":189,"line":249},[187,4267,4268],{},"    # Here are examples of how to rewrite Chinese vernacular into simple modern Mandarin.\\n\\nChinese vernacular:\\n\\n{p}\\n\\nSimple modern Mandarin\n",[187,4270,4271],{"class":189,"line":312},[187,4272,4273],{},"    f\"以下是如何将中国白话改写为简单的现代普通话的示例。\\n\\n中文白话：\\n\\n{p}\\n\\n简单的现代普通话：\\n\\n\"\n",[187,4275,4276],{"class":189,"line":319},[187,4277,4278],{},"    for p in flat_bai\n",[187,4280,4281],{"class":189,"line":325},[187,4282,1437],{},[11,4284,4285],{},"It was difficult to get good results consistently. Here are some observations I had:",[916,4287,4288,4291,4294,4297,4300],{},[919,4289,4290],{},"Some of the translated paragraphs were perfect",[919,4292,4293],{},"some translated paragraphs would randomly hallucinate the same phrase over and over again",[919,4295,4296],{},"Some requests to translate text to English would reply in Mandarin Chinese rather than in English",[919,4298,4299],{},"Sometimes I would even see computer code generated when asking for a translation",[919,4301,4302],{},"The names of characters were sometimes translated inconsistently, sometimes literally and sometimes using differing versions of pinyin, the Romanization system for transcribing the sounds of Mandarin Chinese",[11,4304,4305,4306,4309,4310,4313,4314,4317,4318,4321],{},"I found that ChatGPT 4o could handle any Chinese translation task flawlessly, but the ",[33,4307,4308],{},"Qwen2-7B"," model I used had mixed results! The change that I made that seemed to have the biggest impact on translation quality was setting ",[33,4311,4312],{},"*max_tokens*=256"," in ",[33,4315,4316],{},"SamplingParams",". I probably could have used a dynamic value for ",[33,4319,4320],{},"max_tokens"," based on the size of the current paragraph being translated. I also would have like to set up side-by-side comparisons of translations using different sized models, but rather than spend time and AWS credits on optimizing translation with TensorRT-LLM, I wanted to focus on the main part of this project: retrieval augmented generation (RAG) with LlamaIndex.",[168,4323,4325],{"id":4324},"building-qa-bots-with-rag-using-llamaindex","Building Q&A bots with RAG using LlamaIndex",[11,4327,4328],{},"My primary objective with this project was to implement a simple chat bot that responds to questions about the book with references to the specific paragraphs used in the response. The following shows images of the UI I built with one of the examples I included in the video I made for this project.",[11,4330,4331],{},[511,4332],{"alt":4333,"src":4334},"RAG Example","/static/redlm/rag_example.png",[11,4336,4337,4338,4342],{},"I haven't read that much of the book before working on this project, but I have read a lot ",[4339,4340,4341],"em",{},"about"," this book's characters, major themes and plot. This Q&A bot was a very interesting entrypoint to explore specific passages of the book starting with questions coming from my knowledge about the book. The question in the screenshots above is: “What does Jia Baoyu’s father think about him?” The response includes references to paragraphs where Jia Zheng (Baoyu’s father) is discussing his son. I was pretty amazed that the RAG query was able to pull out these two paragraphs.",[11,4344,4345],{},[4339,4346,4347],{},"In Dream of the Red Chamber, the relationship between protagonist Jia Baoyu and his father, Jia Zheng, is complex and fraught with tension. Jia Zheng, a strict, traditional Confucian patriarch, embodies values of discipline, scholarly rigor, and duty. He expects his son to excel in his studies and uphold the family’s honor by pursuing an official career in government. Baoyu, however, is sensitive, imaginative, and inclined toward poetry and the company of women, especially his cousins Lin Daiyu and Xue Baochai. This preference clashes with Jia Zheng’s expectations, leading to frequent misunderstandings and disappointment.",[11,4349,4350],{},"By default, LlamaIndex uses cosine similarity as the distance metric for finding the vectors representing the documents (paragraphs) that are “closest” to the vector representing the user query. This is the central mechanism by which RAG works. LlamaIndex provides an abstraction of this process, hiding the implementation details and allowing rapid development of retrieval systems.",[11,4352,4353],{},[511,4354],{"alt":4355,"src":4356},"Cosine Similarity","/static/redlm/cosine_similarity.png",[11,4358,4359,4360],{},"Source: ",[15,4361,4362],{"href":4362,"rel":4363},"https://medium.com/@kbdhunga/a-beginners-guide-to-similarity-search-vector-indexing-part-one-9cf5e9171976",[19],[11,4365,4366,4367,4370],{},"Here is some of the code I wrote for the text-based Q&A bot using LlamaIndex’s ",[33,4368,4369],{},"CustomQueryEngine"," class to fetch the nodes from which I get the referenced paragraph text, chapter number and paragraph number.",[26,4372,4374],{"className":1383,"code":4373,"language":1125,"meta":35,"style":35},"class QAndAQueryEngine(CustomQueryEngine):\n    \"\"\"RAG Completion Query Engine optimized for Q&A\"\"\"\n\n    retriever: BaseRetriever\n    response_synthesizer: BaseSynthesizer\n    llm: OpenAILike\n    qa_prompt: PromptTemplate\n\n    def custom_query(self, query_str: str):\n        nodes = self.retriever.retrieve(query_str)\n        metadata = []\n        # Collect the metadata into a list of dicts so that it can be sent to UI for references\n        for node in nodes:\n            metadata_dict = {}\n            node_metadata = node.node.metadata\n            metadata_dict[\"content\"] = node.node.text\n            metadata_dict[\"chapter\"] = int(node_metadata.get(\"chapter\"))\n            metadata_dict[\"paragraph\"] = int(node_metadata.get(\"paragraph\"))\n\n            metadata.append(metadata_dict)\n\n        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n        response = self.llm.chat(\n            [\n                ChatMessage(\n                    role=\"user\",\n                    content=q_and_a_prompt.format( # the English and Chinese prompt templates are discussed below\n                        context_str=context_str, query_str=query_str\n                    ),\n                )\n            ]\n        )\n\n        return response, metadata\n",[33,4375,4376,4381,4386,4390,4395,4400,4405,4410,4414,4419,4424,4429,4434,4439,4444,4449,4454,4459,4464,4468,4473,4477,4482,4487,4492,4497,4502,4507,4512,4517,4522,4527,4532,4536],{"__ignoreMap":35},[187,4377,4378],{"class":189,"line":190},[187,4379,4380],{},"class QAndAQueryEngine(CustomQueryEngine):\n",[187,4382,4383],{"class":189,"line":249},[187,4384,4385],{},"    \"\"\"RAG Completion Query Engine optimized for Q&A\"\"\"\n",[187,4387,4388],{"class":189,"line":312},[187,4389,316],{"emptyLinePlaceholder":315},[187,4391,4392],{"class":189,"line":319},[187,4393,4394],{},"    retriever: BaseRetriever\n",[187,4396,4397],{"class":189,"line":325},[187,4398,4399],{},"    response_synthesizer: BaseSynthesizer\n",[187,4401,4402],{"class":189,"line":686},[187,4403,4404],{},"    llm: OpenAILike\n",[187,4406,4407],{"class":189,"line":697},[187,4408,4409],{},"    qa_prompt: PromptTemplate\n",[187,4411,4412],{"class":189,"line":1291},[187,4413,316],{"emptyLinePlaceholder":315},[187,4415,4416],{"class":189,"line":1306},[187,4417,4418],{},"    def custom_query(self, query_str: str):\n",[187,4420,4421],{"class":189,"line":1434},[187,4422,4423],{},"        nodes = self.retriever.retrieve(query_str)\n",[187,4425,4426],{"class":189,"line":2599},[187,4427,4428],{},"        metadata = []\n",[187,4430,4431],{"class":189,"line":2607},[187,4432,4433],{},"        # Collect the metadata into a list of dicts so that it can be sent to UI for references\n",[187,4435,4436],{"class":189,"line":2621},[187,4437,4438],{},"        for node in nodes:\n",[187,4440,4441],{"class":189,"line":2631},[187,4442,4443],{},"            metadata_dict = {}\n",[187,4445,4446],{"class":189,"line":2642},[187,4447,4448],{},"            node_metadata = node.node.metadata\n",[187,4450,4451],{"class":189,"line":2653},[187,4452,4453],{},"            metadata_dict[\"content\"] = node.node.text\n",[187,4455,4456],{"class":189,"line":2665},[187,4457,4458],{},"            metadata_dict[\"chapter\"] = int(node_metadata.get(\"chapter\"))\n",[187,4460,4461],{"class":189,"line":2674},[187,4462,4463],{},"            metadata_dict[\"paragraph\"] = int(node_metadata.get(\"paragraph\"))\n",[187,4465,4466],{"class":189,"line":2684},[187,4467,316],{"emptyLinePlaceholder":315},[187,4469,4470],{"class":189,"line":2694},[187,4471,4472],{},"            metadata.append(metadata_dict)\n",[187,4474,4475],{"class":189,"line":2706},[187,4476,316],{"emptyLinePlaceholder":315},[187,4478,4479],{"class":189,"line":2715},[187,4480,4481],{},"        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",[187,4483,4484],{"class":189,"line":2725},[187,4485,4486],{},"        response = self.llm.chat(\n",[187,4488,4489],{"class":189,"line":2735},[187,4490,4491],{},"            [\n",[187,4493,4494],{"class":189,"line":2743},[187,4495,4496],{},"                ChatMessage(\n",[187,4498,4499],{"class":189,"line":2754},[187,4500,4501],{},"                    role=\"user\",\n",[187,4503,4504],{"class":189,"line":2762},[187,4505,4506],{},"                    content=q_and_a_prompt.format( # the English and Chinese prompt templates are discussed below\n",[187,4508,4509],{"class":189,"line":2770},[187,4510,4511],{},"                        context_str=context_str, query_str=query_str\n",[187,4513,4514],{"class":189,"line":2781},[187,4515,4516],{},"                    ),\n",[187,4518,4519],{"class":189,"line":2792},[187,4520,4521],{},"                )\n",[187,4523,4524],{"class":189,"line":2803},[187,4525,4526],{},"            ]\n",[187,4528,4529],{"class":189,"line":2808},[187,4530,4531],{},"        )\n",[187,4533,4534],{"class":189,"line":2816},[187,4535,316],{"emptyLinePlaceholder":315},[187,4537,4538],{"class":189,"line":2824},[187,4539,4540],{},"        return response, metadata\n",[911,4542,4544],{"id":4543},"indexing-the-book-data","Indexing the book data",[11,4546,4547],{},"In the indexing process, embedding models are used to translate chunks of text (paragraphs) into high-dimensional vectors that represent the relationships between the tokens in a chunk of text. These are the vectors stored in the \"Vector Database\" used by LlamaIndex. The chapter number, paragraph number and version (original, Mandarin Chinese and English) of each paragraph are added to the database entry as metadata during the indexing step which runs via a script before starting the FastAPI server. Here's how I indexed the original text and translations with LlamaIndex:",[26,4549,4551],{"className":1383,"code":4550,"language":1125,"meta":35,"style":35},"from llama_index.core import Document, VectorStoreIndex\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\nen_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nzh_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-zh-v1.5\")\n\ndef persist_index():\n    documents = []\n    for chapter in range(1, 121):\n        with open(f\"data/book/{chapter}.json\", \"r\") as f:\n            data = json.load(f)\n            paragraphs = data[\"paragraphs\"]\n\n        for i, p in enumerate(paragraphs):\n            for lang in [\"original\", \"chinese\", \"english\"]:\n                document = Document(\n                    text=p[lang],\n                    metadata={\n                        \"chapter\": str(chapter),\n                        \"paragraph\": str(i),\n                        \"language\": lang,\n                    },\n                    metadata_seperator=\"::\",\n                    metadata_template=\"{key}=>{value}\",\n                    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n                    embed_model=(\n                        en_embedding_model if lang == \"english\" else zh_embedding_model\n                    ),\n                )\n                documents.append(document)\n\n    index = VectorStoreIndex.from_documents(documents)\n    index.storage_context.persist(persist_dir=\"storage\")\n\nif __name__ == \"__main__\":\n    persist_index()\n",[33,4552,4553,4558,4563,4567,4572,4577,4581,4586,4591,4596,4601,4606,4611,4615,4620,4625,4630,4635,4640,4645,4650,4655,4660,4665,4670,4675,4680,4685,4689,4693,4698,4702,4707,4712,4716,4721],{"__ignoreMap":35},[187,4554,4555],{"class":189,"line":190},[187,4556,4557],{},"from llama_index.core import Document, VectorStoreIndex\n",[187,4559,4560],{"class":189,"line":249},[187,4561,4562],{},"from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",[187,4564,4565],{"class":189,"line":312},[187,4566,316],{"emptyLinePlaceholder":315},[187,4568,4569],{"class":189,"line":319},[187,4570,4571],{},"en_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",[187,4573,4574],{"class":189,"line":325},[187,4575,4576],{},"zh_embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-zh-v1.5\")\n",[187,4578,4579],{"class":189,"line":686},[187,4580,316],{"emptyLinePlaceholder":315},[187,4582,4583],{"class":189,"line":697},[187,4584,4585],{},"def persist_index():\n",[187,4587,4588],{"class":189,"line":1291},[187,4589,4590],{},"    documents = []\n",[187,4592,4593],{"class":189,"line":1306},[187,4594,4595],{},"    for chapter in range(1, 121):\n",[187,4597,4598],{"class":189,"line":1434},[187,4599,4600],{},"        with open(f\"data/book/{chapter}.json\", \"r\") as f:\n",[187,4602,4603],{"class":189,"line":2599},[187,4604,4605],{},"            data = json.load(f)\n",[187,4607,4608],{"class":189,"line":2607},[187,4609,4610],{},"            paragraphs = data[\"paragraphs\"]\n",[187,4612,4613],{"class":189,"line":2621},[187,4614,316],{"emptyLinePlaceholder":315},[187,4616,4617],{"class":189,"line":2631},[187,4618,4619],{},"        for i, p in enumerate(paragraphs):\n",[187,4621,4622],{"class":189,"line":2642},[187,4623,4624],{},"            for lang in [\"original\", \"chinese\", \"english\"]:\n",[187,4626,4627],{"class":189,"line":2653},[187,4628,4629],{},"                document = Document(\n",[187,4631,4632],{"class":189,"line":2665},[187,4633,4634],{},"                    text=p[lang],\n",[187,4636,4637],{"class":189,"line":2674},[187,4638,4639],{},"                    metadata={\n",[187,4641,4642],{"class":189,"line":2684},[187,4643,4644],{},"                        \"chapter\": str(chapter),\n",[187,4646,4647],{"class":189,"line":2694},[187,4648,4649],{},"                        \"paragraph\": str(i),\n",[187,4651,4652],{"class":189,"line":2706},[187,4653,4654],{},"                        \"language\": lang,\n",[187,4656,4657],{"class":189,"line":2715},[187,4658,4659],{},"                    },\n",[187,4661,4662],{"class":189,"line":2725},[187,4663,4664],{},"                    metadata_seperator=\"::\",\n",[187,4666,4667],{"class":189,"line":2735},[187,4668,4669],{},"                    metadata_template=\"{key}=>{value}\",\n",[187,4671,4672],{"class":189,"line":2743},[187,4673,4674],{},"                    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",[187,4676,4677],{"class":189,"line":2754},[187,4678,4679],{},"                    embed_model=(\n",[187,4681,4682],{"class":189,"line":2762},[187,4683,4684],{},"                        en_embedding_model if lang == \"english\" else zh_embedding_model\n",[187,4686,4687],{"class":189,"line":2770},[187,4688,4516],{},[187,4690,4691],{"class":189,"line":2781},[187,4692,4521],{},[187,4694,4695],{"class":189,"line":2792},[187,4696,4697],{},"                documents.append(document)\n",[187,4699,4700],{"class":189,"line":2803},[187,4701,316],{"emptyLinePlaceholder":315},[187,4703,4704],{"class":189,"line":2808},[187,4705,4706],{},"    index = VectorStoreIndex.from_documents(documents)\n",[187,4708,4709],{"class":189,"line":2816},[187,4710,4711],{},"    index.storage_context.persist(persist_dir=\"storage\")\n",[187,4713,4714],{"class":189,"line":2824},[187,4715,316],{"emptyLinePlaceholder":315},[187,4717,4718],{"class":189,"line":2834},[187,4719,4720],{},"if __name__ == \"__main__\":\n",[187,4722,4723],{"class":189,"line":2845},[187,4724,4725],{},"    persist_index()\n",[11,4727,4728,4729,4732],{},"For the embedding models, I used the small BAAI General Embedding models (BGE) for English and Chinese. BAAI is the Beijing Academy of Artificial Intelligence, and I learned about this organization through some of the examples on the LlamaIndex site that use BAAI embeddings. There are multi-lingual embedding models (e.g. ",[33,4730,4731],{},"BAAI/bge-m3","), but setting the embedding model on a per-document basis is possible and in some cases it might be preferable to using a single embedding model for all documents.",[911,4734,4736],{"id":4735},"milvus-vector-database","Milvus Vector Database",[11,4738,4739],{},"I did most of the development for this project using the in-memory VectorIndexStore provided by LlamaIndex. This worked well, but making any changes to the FastAPI server required the data to be reloaded into memory which took several seconds each time. This can really hinder a good development flow, so I looked into using an external service for the vector database instead of running it in memory.",[11,4741,4742],{},[511,4743],{"alt":4744,"src":4745},"Vector Database Options","/static/redlm/vectordbs.png",[11,4747,4748,4749,4756],{},"There are a LOT of options to consider when picking a vector database for a RAG application. Milvus has a highly decoupled architecture, it is fully open source and I had seen it in some examples in the ",[15,4750,4753],{"href":4751,"rel":4752},"https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RAG/examples/advanced_rag/multimodal_rag",[19],[33,4754,4755],{},"NVIDIA/GenerativeAIExamples"," repo, so I decided to give it a try.",[11,4758,4759],{},[511,4760],{"alt":4761,"src":4762},"Milvus Vector Database Architecture","/static/redlm/milvus.png",[11,4764,4765,4766,4771],{},"Using the ",[15,4767,4770],{"href":4768,"rel":4769},"https://milvus.io/docs/v2.0.x/install_standalone-docker.md",[19],"Milvus docker compose example"," I was able to set up an external vector database based on etcd and minio. Milvus also provides a Helm chart for running their vector database, this would be helpful if I was going to be running everything in Kubernetes (inference, vector database and application containers).",[911,4773,4775],{"id":4774},"other-examples-of-rag-with-english-questions","Other examples of RAG with English questions",[11,4777,4778],{},"One interesting design question I faced was how to support answering questions in both English and Chinese. I initially built the Q&A bot with only Chinese language support. Later, I added a simple helper function to determine if the input text is Chinese:",[26,4780,4782],{"className":1383,"code":4781,"language":1125,"meta":35,"style":35},"def is_chinese_text(text: str) -> bool:\n    \"\"\"\n    This is a simple helper function that is used to determine which prompt to use\n    depending on the language of the original user query\n    \"\"\"\n    chinese_count = sum(1 for char in text if '\\u4e00' \u003C= char \u003C= '\\u9fff')\n    english_count = sum(1 for char in text if 'a' \u003C= char.lower() \u003C= 'z')\n\n    return chinese_count > english_count\n",[33,4783,4784,4789,4794,4799,4804,4808,4813,4818,4822],{"__ignoreMap":35},[187,4785,4786],{"class":189,"line":190},[187,4787,4788],{},"def is_chinese_text(text: str) -> bool:\n",[187,4790,4791],{"class":189,"line":249},[187,4792,4793],{},"    \"\"\"\n",[187,4795,4796],{"class":189,"line":312},[187,4797,4798],{},"    This is a simple helper function that is used to determine which prompt to use\n",[187,4800,4801],{"class":189,"line":319},[187,4802,4803],{},"    depending on the language of the original user query\n",[187,4805,4806],{"class":189,"line":325},[187,4807,4793],{},[187,4809,4810],{"class":189,"line":686},[187,4811,4812],{},"    chinese_count = sum(1 for char in text if '\\u4e00' \u003C= char \u003C= '\\u9fff')\n",[187,4814,4815],{"class":189,"line":697},[187,4816,4817],{},"    english_count = sum(1 for char in text if 'a' \u003C= char.lower() \u003C= 'z')\n",[187,4819,4820],{"class":189,"line":1291},[187,4821,316],{"emptyLinePlaceholder":315},[187,4823,4824],{"class":189,"line":1306},[187,4825,4826],{},"    return chinese_count > english_count\n",[11,4828,4829,4830,4832,4833,4836],{},"This boolean value would then be used in the ",[33,4831,4369],{}," to use either the Chinese or English ",[33,4834,4835],{},"PromptTemplate",". This allowed the Q&A bot to answer questions in either Chinese or English, and it does not require translating back and forth between Chinese and English. However, this method relies on high-quality translations, so I don't expect English language questions to be answered as accurately as Chinese language questions. Here are the Chinese and English prompts that I used for the text-based Q&A bot, as well as some examples of the Q&A bot answering questions in English. The referenced materials include paragraphs from the English translation.",[26,4838,4840],{"className":1383,"code":4839,"language":1125,"meta":35,"style":35},"# Chinese prompt for text-based Q&A bot\nq_and_a_prompt = PromptTemplate(\n    \"这是相关的参考资料：\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\" # context_str contains Chinese paragraphs retrieved via RAG query\n    \"---------------------\\n\"\n    \"根据上述的参考资料，回答下面的问题\\n\"\n    \"问题：{user_question}\\n\"\n)\n\n# English prompt for text-based Q&A bot\nq_and_a_prompt_english = PromptTemplate(\n    \"This is some related reference material:\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\" # context_str contains English paragraphs retrieved via RAG query\n    \"---------------------\\n\"\n    \"Based on the above material, answer the following question:\\n\"\n    \"Question: {user_question}\\n\"\n)\n",[33,4841,4842,4847,4852,4857,4862,4867,4871,4876,4881,4885,4889,4894,4899,4904,4908,4913,4917,4922,4927],{"__ignoreMap":35},[187,4843,4844],{"class":189,"line":190},[187,4845,4846],{},"# Chinese prompt for text-based Q&A bot\n",[187,4848,4849],{"class":189,"line":249},[187,4850,4851],{},"q_and_a_prompt = PromptTemplate(\n",[187,4853,4854],{"class":189,"line":312},[187,4855,4856],{},"    \"这是相关的参考资料：\\n\"\n",[187,4858,4859],{"class":189,"line":319},[187,4860,4861],{},"    \"---------------------\\n\"\n",[187,4863,4864],{"class":189,"line":325},[187,4865,4866],{},"    \"{context_str}\\n\" # context_str contains Chinese paragraphs retrieved via RAG query\n",[187,4868,4869],{"class":189,"line":686},[187,4870,4861],{},[187,4872,4873],{"class":189,"line":697},[187,4874,4875],{},"    \"根据上述的参考资料，回答下面的问题\\n\"\n",[187,4877,4878],{"class":189,"line":1291},[187,4879,4880],{},"    \"问题：{user_question}\\n\"\n",[187,4882,4883],{"class":189,"line":1306},[187,4884,621],{},[187,4886,4887],{"class":189,"line":1434},[187,4888,316],{"emptyLinePlaceholder":315},[187,4890,4891],{"class":189,"line":2599},[187,4892,4893],{},"# English prompt for text-based Q&A bot\n",[187,4895,4896],{"class":189,"line":2607},[187,4897,4898],{},"q_and_a_prompt_english = PromptTemplate(\n",[187,4900,4901],{"class":189,"line":2621},[187,4902,4903],{},"    \"This is some related reference material:\\n\"\n",[187,4905,4906],{"class":189,"line":2631},[187,4907,4861],{},[187,4909,4910],{"class":189,"line":2642},[187,4911,4912],{},"    \"{context_str}\\n\" # context_str contains English paragraphs retrieved via RAG query\n",[187,4914,4915],{"class":189,"line":2653},[187,4916,4861],{},[187,4918,4919],{"class":189,"line":2665},[187,4920,4921],{},"    \"Based on the above material, answer the following question:\\n\"\n",[187,4923,4924],{"class":189,"line":2674},[187,4925,4926],{},"    \"Question: {user_question}\\n\"\n",[187,4928,4929],{"class":189,"line":2684},[187,4930,621],{},[11,4932,4933],{},[511,4934],{"alt":4935,"src":4936},"Multi-modal Q&A example 1","/static/redlm/qa_example_01.png",[11,4938,4939],{},"Asking random questions like this one is a fun way to explore the many scenes of Dream of the Red Chamber.",[11,4941,4942,4946],{},[511,4943],{"alt":4944,"src":4945},"RAG Flower Pedal Example","/static/redlm/qa_example_flower_pedals.png",[511,4947],{"alt":4948,"src":4949},"RAG Flower Pedal Example with Reference","/static/redlm/qa_example_flower_pedals_a.png",[168,4951,4953],{"id":4952},"redlm-rag-evaluation","RedLM RAG Evaluation",[11,4955,4956],{},"Examinations have long been a cornerstone of Chinese society, shaping individual aspirations, cultural values, and even government structures. This legacy began with the imperial civil service exams, kējǔ (科举), established during the Sui and Tang dynasties, and carries through in Modern China with the gaokao (高考) college entrance examination, both of which have allowed for unprecedented meritocratic routes to power and prestige. Given how widely this novel is studied in China, I was not surprised to find a wealth of examination questions written for students studying Dream of the Red Chamber.",[11,4958,4959,4960,4965],{},"I used ",[15,4961,4964],{"href":4962,"rel":4963},"https://www.examcoo.com/editor/do/view/id/246401",[19],"a set of 1000 multiple choice questions about Dream of the Red Chamber on examcoo.com"," to evaluate the effectiveness of the RAG system I built with LlamaIndex. I wrote a script to parse the questions from the website HTML using ChatGPT (parsing HTML is one of my favorite use cases of LLMs!) I filtered the list of 1000 questions down to 877 questions based on the following criteria:",[916,4967,4968,4974],{},[919,4969,4970,4973],{},[338,4971,4972],{},"Four answer choices",": some of the questions had more than four answer choices. I filtered questions with more than four answer choices to keep the evaluation simple. This would allow me to assume that random answer choices would have a 25% chance of being correct.",[919,4975,4976,4979],{},[338,4977,4978],{},"Only one answer",": For some questions the correct answer required selecting multiple answer choices. This would also help keep the evaluation logic simple.",[11,4981,4982],{},[511,4983],{"alt":4984,"src":4985},"Multiple Choice Questions from Dream of the Red Chamber Test","/static/redlm/hlm_mcq.png",[11,4987,4988],{},"Multiple choice questions from a Dream of the Red Chamber test (examcoo.com)",[11,4990,4991],{},"To run the evaluation I set up two scripts. The first script would prompt the LLM to answer the question without any additional information from the RAG system. This served as a baseline to see how well the LLM could do at answering multiple choice questions about the book. The script simply checks to see if the LLM response contains the letter (A, B, C or D) of the correct answer and keeps track of the number of questions answered correctly.",[11,4993,4994],{},"Another script was used to take the test using large language models with RAG. In this script, the prompt sent to the LLM included relevant paragraphs from the book based on how similar the query is to each paragraph in the book based on the cosine similarity metric mentioned earlier.",[11,4996,4997],{},[511,4998],{"alt":4999,"src":5000},"RAG evaluation","/static/redlm/rag_eval.png",[11,5002,5003],{},"Here are some results and other observations from this experiment:",[916,5005,5006,5009,5012,5018,5021,5028],{},[919,5007,5008],{},"LLMs alone scored in the mid 30% range (36%)",[919,5010,5011],{},"LLMs using retrieval augmented generation with the set of questions score in the mid 40% range (44%)",[919,5013,5014,5015,5017],{},"I used the completion API rather than the chat API and set the ",[33,5016,4320],{}," to 16. This was done to ensure that the LLM only gave a short response with a valid answer choice rather than giving a long response with an explanation.",[919,5019,5020],{},"The evaluation took longer for LLM + RAG test because of the time required for making the RAG query and the longer prompt (including both the original multiple-choice question and the referenced paragraphs).",[919,5022,5023,5024,5027],{},"I used the ",[33,5025,5026],{},"01-ai/Yi-1.5-9B-Chat"," model for this test, but I probably should have used the base model rather than the chat model.",[919,5029,5030],{},"Some questions would not be capable of being answered by RAG. For example, some of the questions are about film renditions of the novel. Most of the questions seemed relevant to the content of the book, so I didn’t bother to filter out the questions that were not directly related to the book’s content.",[11,5032,5033,5034,5037,5038,752],{},"Here is an example of a question that the LLM test script answered ",[4339,5035,5036],{},"incorrectly"," and the LLM + RAG test script answered ",[338,5039,5040],{},"correctly",[107,5042,5043,5046,5049,5052,5055],{},[11,5044,5045],{},"秦钟的父亲是如何死的？",[11,5047,5048],{},"A、外感风寒、风毒之症",[11,5050,5051],{},"B、被智能儿气死的",[11,5053,5054],{},"C、生气引发旧病加重",[11,5056,5057],{},"D、生气而诱发中风而死",[11,5059,5060],{},"Translation:",[107,5062,5063,5066,5072,5078,5084],{},[11,5064,5065],{},"How did Qin Zhong's father die?",[11,5067,5068,5071],{},[338,5069,5070],{},"A."," He caught a cold and developed wind-related illnesses.",[11,5073,5074,5077],{},[338,5075,5076],{},"B."," He was angered to death by Zhineng'er (a character).",[11,5079,5080,5083],{},[338,5081,5082],{},"C."," His old illness worsened due to anger.",[11,5085,5086,5089],{},[338,5087,5088],{},"D."," He had a stroke induced by anger and died.",[11,5091,5092],{},"Here is the paragraphs that the RAG query returned along with the English translation:",[11,5094,5095],{},"Original",[107,5097,5098],{},[11,5099,5100],{},"荣两处上下内外人等莫不欢天喜地，独有宝玉置若罔闻。你道什么缘故？原来近日水月庵的智能私逃入城，来找秦钟，不意被秦邦业知觉，将智能逐出，将秦钟打了一顿，自己气的老病发了，三五日便呜呼哀哉了。秦钟本自怯弱，又带病未痊，受了笞杖，今见老父气死，悔痛无及，又添了许多病症。因此，宝玉心中怅怅不乐。虽有元春晋封之事，那解得他的愁闷？贾母等如何谢恩，如何回家，亲友如何来庆贺，宁荣两府近日如何热闹，众人如何得意，独他一个皆视有如无，毫不介意。因此，众人嘲他越发呆了。",[11,5102,5103],{},"English",[107,5105,5106],{},[11,5107,5108],{},"Everyone in the Rong and Ning households, both inside and outside, were extremely happy, except for Baoyu, who seemed indifferent. Do you want to know why? It turns out that recently, the nun Zhineng from Shuiyue Temple secretly ran into the city to find Qin Zhong. Unexpectedly, she was discovered by Qin Zhong's father, Qin Banger. Qin Banger not only drove Zhineng away but also gave Qin Zhong a beating. This made Qin Banger so angry that his old illness relapsed, and within three to five days, he passed away. Qin Zhong had always been weak and hadn't fully recovered from a previous illness. After being beaten and seeing his father die in anger, he was overwhelmed with regret and sorrow, which worsened his condition. As a result, Baoyu felt very melancholic. Although the promotion of Yuan Chun to imperial concubine was a joyful event, it couldn't alleviate the gloom in his heart. While Grandmother Jia and others were busy expressing their gratitude and returning home, and relatives and friends came to celebrate, and the Rong and Ning households were bustling with excitement, Baoyu alone remained completely indifferent to it all. Consequently, everyone started to mock him for becoming more and more absent-minded.",[11,5110,5111],{},"The correct answer for this question is C.",[168,5113,5115],{"id":5114},"multi-modal-rag-for-visual-reasoning","Multi-modal RAG for visual reasoning",[11,5117,5118],{},"Qwen2-VL is a new AI model that was released in late August 2024. Qwen is the name of Alibaba’s AI Lab, and it is an abbreviation of the Chinese characters: 千问 (\"qian wen\", meaning 1000 questions). VL stands for vision-language, meaning that the model is capable of understanding both text and images. I had tested out the previous version of Qwen’s vision-language model and was very impressed by how it could accurately describe the contents of images and also answer general questions about images.",[11,5120,5121],{},"Sun Wen was a Qing-era painter who spent 36 years of his life creating a series of 230 paintings capturing scenes from Dream of the Red Chamber. The paintings are incredibly detailed and often contain repeated figures in a temporal sequence. If you asked a Qwen-VL model to describe one of the images, it might return lengthy description that doesn't fully capture the full detail of the scene. It might also be difficult for a language model to \"focus\" on a portion of the whole image.",[11,5123,5124],{},[511,5125],{"alt":5126,"src":5127},"Dream of the Red Chamber Painting 131","/static/redlm/painting_131.png",[11,5129,5130],{},"This sparked the idea to create a feature where users can click and drag over an image to select part of a painting, then ask questions specifically about the selected portion. I knew that while this could be achieved with tools like HTML canvas, writing it on my own would be quite time-consuming. It took me just a few minutes to write out the prompt, and Claude 3.5 Sonnet generated a perfect prototype of this feature in under a minute. Here’s the prompt I used:",[107,5132,5133,5136,5139,5150,5153],{},[11,5134,5135],{},"I'm going to describe a Vue component and I want you to write it using Vue 3 to the best of your ability.",[11,5137,5138],{},"write a simple single-file vue component using Vue 3 that does the following:",[916,5140,5141,5144,5147],{},[919,5142,5143],{},"displays an image",[919,5145,5146],{},"allows the users to click and drag to select a subsection of the image",[919,5148,5149],{},"the subsection of the image is saved as a base64-encoded data url to a variable that is displayed below the image",[11,5151,5152],{},"The solution should make use of HTML canvas. When you click down on the image you begin selecting the subsection. You then move the mouse to make your subsection on the image, and when you mouse-up the subsection is selected and the data url is updated. Then the subsection is displayed at the very bottom of the page as a \"preview\" image using the base 64 image string as the image source.",[11,5154,5155],{},"The selection box should be a dashed red line",[11,5157,5158],{},[511,5159],{"alt":5160,"src":5161},"RedLM Image Q&A","/static/redlm/image-qa.png",[11,5163,5164],{},"This shows the final result of the UI I built for the image Q&A feature in RedLM. It uses a similar chat layout that the text-based Q&A feature uses, with the addition of the image preview included in the chat log. The user query in this example just says “Please describe the contents of the image”. This was the first image that I tested when building the image Q&A feature to see if the correct passage can be referenced based on the description of an image. This pulled the exact passage and the answer provides details about what happened (a fire broke out) where it happened (at the Gourd Temple) and why it happened (a Monk accidentally set an oil pot on fire).",[11,5166,5167],{},"Here is a diagram showing the overall flow of data in the image Q&A feature:",[11,5169,5170],{},[511,5171],{"alt":5172,"src":5173},"Diagram of RedLM Image Q&A with RAG and Vision Language Models","/static/redlm/redlm.drawio.png",[11,5175,5176],{},"This flow chart shows how the image Q&A feature works.",[2276,5178,5179,5186,5197,5200,5206],{},[919,5180,5181,5182,5185],{},"The user selects part of an image and writes a question. This data is then sent to the RedLM API as a post request to the ",[33,5183,5184],{},"/mm-q-and-a"," endpoint (multi-modal Q&A).",[919,5187,5188,5189,5192,5193,5196],{},"Vision language models are used to get a description of the image. Depending on the application configuration, this query can use models such as ",[33,5190,5191],{},"Qwen/Qwen2-VL-2B-Instruct"," on RTX PCs or using the NVIDIA API Catalog using larger models such as ",[33,5194,5195],{},"meta/llama-3.2-90b-vision-instruct",". Not all vision language models have the same interface, so I added some logic to handle different model formats.",[919,5198,5199],{},"The image description is used to fetch relevant documents from the Vector Database",[919,5201,5202,5203,5205],{},"The full prompt with the image description and relevant documents is sent to the LLM. Again, inference for this step is done either with RTX PCs or using models from the ",[33,5204,4182],{}," API catalog.",[919,5207,5208],{},"The response from the LLM is sent back to the browser and is displayed to the user as a chat message.",[11,5210,5211],{},"Here is the prompt I used for the image Q&A feature:",[26,5213,5215],{"className":1383,"code":5214,"language":1125,"meta":35,"style":35},"# Chinese prompt for image-based Q&A bot\nmm_q_and_a_prompt = PromptTemplate(\n    \"这是书中相关的内容：\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"下面是场景的描述：\\n\"\n    \"---------------------\\n\"\n    \"{image_description}\\n\"\n    \"---------------------\\n\"\n    \"根据上述的信息，尽量解释上说的场景和书的关系。\"\n)\n\n# English prompt for image-based Q&A bot\nmm_q_and_a_prompt_english = PromptTemplate(\n    \"Here is relevant content from the book:\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Below is the description of a scene:\\n\"\n    \"---------------------\\n\"\n    \"{image_description}\\n\"\n    \"---------------------\\n\"\n    \"Based on the information provided above, try to explain the relationship between the described scene and the book content.\"\n)\n",[33,5216,5217,5222,5227,5232,5237,5241,5246,5250,5255,5259,5264,5268,5272,5277,5282,5287,5291,5295,5300,5304,5308,5312,5317],{"__ignoreMap":35},[187,5218,5219],{"class":189,"line":190},[187,5220,5221],{},"# Chinese prompt for image-based Q&A bot\n",[187,5223,5224],{"class":189,"line":249},[187,5225,5226],{},"mm_q_and_a_prompt = PromptTemplate(\n",[187,5228,5229],{"class":189,"line":312},[187,5230,5231],{},"    \"这是书中相关的内容：\\n\"\n",[187,5233,5234],{"class":189,"line":319},[187,5235,5236],{},"    \"{context_str}\\n\"\n",[187,5238,5239],{"class":189,"line":325},[187,5240,4861],{},[187,5242,5243],{"class":189,"line":686},[187,5244,5245],{},"    \"下面是场景的描述：\\n\"\n",[187,5247,5248],{"class":189,"line":697},[187,5249,4861],{},[187,5251,5252],{"class":189,"line":1291},[187,5253,5254],{},"    \"{image_description}\\n\"\n",[187,5256,5257],{"class":189,"line":1306},[187,5258,4861],{},[187,5260,5261],{"class":189,"line":1434},[187,5262,5263],{},"    \"根据上述的信息，尽量解释上说的场景和书的关系。\"\n",[187,5265,5266],{"class":189,"line":2599},[187,5267,621],{},[187,5269,5270],{"class":189,"line":2607},[187,5271,316],{"emptyLinePlaceholder":315},[187,5273,5274],{"class":189,"line":2621},[187,5275,5276],{},"# English prompt for image-based Q&A bot\n",[187,5278,5279],{"class":189,"line":2631},[187,5280,5281],{},"mm_q_and_a_prompt_english = PromptTemplate(\n",[187,5283,5284],{"class":189,"line":2642},[187,5285,5286],{},"    \"Here is relevant content from the book:\\n\"\n",[187,5288,5289],{"class":189,"line":2653},[187,5290,5236],{},[187,5292,5293],{"class":189,"line":2665},[187,5294,4861],{},[187,5296,5297],{"class":189,"line":2674},[187,5298,5299],{},"    \"Below is the description of a scene:\\n\"\n",[187,5301,5302],{"class":189,"line":2684},[187,5303,4861],{},[187,5305,5306],{"class":189,"line":2694},[187,5307,5254],{},[187,5309,5310],{"class":189,"line":2706},[187,5311,4861],{},[187,5313,5314],{"class":189,"line":2715},[187,5315,5316],{},"    \"Based on the information provided above, try to explain the relationship between the described scene and the book content.\"\n",[187,5318,5319],{"class":189,"line":2725},[187,5320,621],{},[11,5322,5323],{},"The prompt engineering for this feature was tricky. I was able to get some awesome results that would give me detailed and accurate responses, and then sometimes the LLM would seem confused about my query and tell me that there was no relationship between the scene description and the book content. Sometimes it would give me an accurate description of the scene, but then proceed to tell me that the book content is not related to the scene at all.",[11,5325,5326],{},"There is another important concept from LlamaIndex that I used to build the image Q&A feature: metadata filtering. Metadata filtering is an important concept in RAG systems  because it helps you focus your query on relevant documents in a precise way. A very simple example might be a RAG system that indexes news articles and stores the associated date as metadata. You could allow a user to set a date range for their query and only include articles that match the given date range.",[11,5328,5329],{},"For my image Q&A system, I have a mapping between the paintings and their associated chapters. When I ask a question about a painting, I want to use the description of the image to find similar paragraphs, but only the paragraphs that occur in the painting’s associated chapter. What I ended up doing was filtering the entire index before making the query. The alternative would be filtering the returned nodes after making the query, but this would have the possibility of not returning any nodes.",[11,5331,5332],{},"Here’s what some of the metadata filtering code looks like:",[26,5334,5336],{"className":1383,"code":5335,"language":1125,"meta":35,"style":35},"# main.py\n# filter by chapters associated with the queried image\nfilters = MetadataFilters(\n    filters=[ExactMatchFilter(key=\"chapter\", value=str(req_data.chapter))]\n)\nquery_engine = get_query_engine_for_multi_modal(filters)\n\n# rag.py\n# utility function that returns the query engine use for image Q&A\n# the index is filtered to only include nodes associated with the image being queried\ndef get_query_engine_for_multi_modal(filters):\n    retriever = index.as_retriever(filters=filters)\n    synthesizer = get_response_synthesizer(response_mode=\"compact\")\n    try:\n        query_engine = QAndAQueryEngine(\n            retriever=retriever,\n            response_synthesizer=synthesizer,\n            llm=model,\n            qa_prompt=mm_q_and_a_prompt,\n        )\n    except Exception as e:\n        print(e)\n    return query_engine\n",[33,5337,5338,5343,5348,5353,5358,5362,5367,5371,5376,5381,5386,5391,5396,5401,5406,5411,5416,5421,5426,5431,5435,5440,5445],{"__ignoreMap":35},[187,5339,5340],{"class":189,"line":190},[187,5341,5342],{},"# main.py\n",[187,5344,5345],{"class":189,"line":249},[187,5346,5347],{},"# filter by chapters associated with the queried image\n",[187,5349,5350],{"class":189,"line":312},[187,5351,5352],{},"filters = MetadataFilters(\n",[187,5354,5355],{"class":189,"line":319},[187,5356,5357],{},"    filters=[ExactMatchFilter(key=\"chapter\", value=str(req_data.chapter))]\n",[187,5359,5360],{"class":189,"line":325},[187,5361,621],{},[187,5363,5364],{"class":189,"line":686},[187,5365,5366],{},"query_engine = get_query_engine_for_multi_modal(filters)\n",[187,5368,5369],{"class":189,"line":697},[187,5370,316],{"emptyLinePlaceholder":315},[187,5372,5373],{"class":189,"line":1291},[187,5374,5375],{},"# rag.py\n",[187,5377,5378],{"class":189,"line":1306},[187,5379,5380],{},"# utility function that returns the query engine use for image Q&A\n",[187,5382,5383],{"class":189,"line":1434},[187,5384,5385],{},"# the index is filtered to only include nodes associated with the image being queried\n",[187,5387,5388],{"class":189,"line":2599},[187,5389,5390],{},"def get_query_engine_for_multi_modal(filters):\n",[187,5392,5393],{"class":189,"line":2607},[187,5394,5395],{},"    retriever = index.as_retriever(filters=filters)\n",[187,5397,5398],{"class":189,"line":2621},[187,5399,5400],{},"    synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",[187,5402,5403],{"class":189,"line":2631},[187,5404,5405],{},"    try:\n",[187,5407,5408],{"class":189,"line":2642},[187,5409,5410],{},"        query_engine = QAndAQueryEngine(\n",[187,5412,5413],{"class":189,"line":2653},[187,5414,5415],{},"            retriever=retriever,\n",[187,5417,5418],{"class":189,"line":2665},[187,5419,5420],{},"            response_synthesizer=synthesizer,\n",[187,5422,5423],{"class":189,"line":2674},[187,5424,5425],{},"            llm=model,\n",[187,5427,5428],{"class":189,"line":2684},[187,5429,5430],{},"            qa_prompt=mm_q_and_a_prompt,\n",[187,5432,5433],{"class":189,"line":2694},[187,5434,4531],{},[187,5436,5437],{"class":189,"line":2706},[187,5438,5439],{},"    except Exception as e:\n",[187,5441,5442],{"class":189,"line":2715},[187,5443,5444],{},"        print(e)\n",[187,5446,5447],{"class":189,"line":2725},[187,5448,5449],{},"    return query_engine\n",[11,5451,5452],{},"This seemed to work well for my use case, but it might not be a best practice, and it might not be efficient at a bigger scale.",[911,5454,5456],{"id":5455},"multi-modal-qa-examples","Multi-modal Q&A examples",[11,5458,5459],{},"Here are some more examples of results from different types of questions from the multi-modal Q&A bot.",[11,5461,5462],{},"The response to the following query did a good job of combining information gathered from the image description and image from related passages.",[11,5464,5465,5469],{},[511,5466],{"alt":5467,"src":5468},"Multi-modal Q&A example 2","/static/redlm/qa_example_02.png",[511,5470],{"alt":5471,"src":5472},"Multi-modal Q&A example 3","/static/redlm/qa_example_03.png",[11,5474,5475],{},[511,5476],{"alt":5477,"src":5478},"Q&A Example with Carriage","/static/redlm/qa_example_carriage.png",[11,5480,5481],{},[511,5482],{"alt":5483,"src":5484},"Ou Guan Example","/static/redlm/qa_example_ou_guan.png",[11,5486,5487,5488,5493],{},"This is one of my favorite examples of the RedLM image Q&A bot in action. The query here in Chinese says: \"What are these two people doing\"? The answer incorporates a description of what is happening in the story (Jia Baoyu comes across Ou Guan while visiting a temple) and also describes the significance of what is happening (",[15,5489,5492],{"href":5490,"rel":5491},"https://en.wikipedia.org/wiki/Joss_paper",[19],"burning paper money as a form of Chinese ancestral worship","). But this answer is not perfect and it demonstrates some of the difficulties I had with the prompt engineering for this project. The first part makes a lot of sense:",[107,5495,5496],{},[11,5497,5498],{},"这段参考资料来自中国古典小说《红楼梦》，讲述了贾宝玉在探访寺庙时发现藕官在河边烧纸钱的情景。藕官是贾府中的一个戏子，她的行为可能代表了对去世亲人的哀悼或祭奥。贾宝玉询问她烧纸的原因，藕官透露了自己有私下的情感问题，并且提到了芳官和蕊官，暗示了她们之间可能存在某种秘密或情感联系。",[107,5500,5501],{},[11,5502,5503],{},"This reference is from the classical Chinese novel Dream of the Red Chamber, describing a scene where Jia Baoyu discovers Ou Guan burning paper money by the river during his visit to a temple. Ou Guan is a performer in the Jia household, and her actions might represent mourning or honoring deceased loved ones. Jia Baoyu asks her why she is burning the paper, and Ou Guan reveals that she has private emotional issues, mentioning Fang Guan and Rui Guan, hinting that there might be some kind of secret or emotional connection among them.",[11,5505,5506],{},"But the second part seems to contradict the first part of the response:",[107,5508,5509],{},[11,5510,5511],{},"相比之下，你描述的画作场景与《红楼梦》中的描述不同。画作描绘的是两个人在河边的场景，左边的人跪在地上，似乎在烧东西，右边的人站在旁边，似乎在观察或指导。根据你提供的描述，无法确定画中人物的具体行为，如可能是烧水、煮饭、烧药等。因此，这幅画可能是一幅现实主义作品，描绘了日常生活中的一幕。它可能没有像《红楼梦》中的情节那样深刻的情感或秘密背景，而是单纯地表现了人与人之间的关系和行为。此外，画中的背景、人物的表情和姿态等细节也会对画面的意义产生影响，需要结合具体的画作进行分析。",[107,5513,5514],{},[11,5515,5516],{},"In contrast, the scene you described in the painting differs from the depiction in Dream of the Red Chamber. The painting shows two people by the river: the person on the left is kneeling on the ground, seemingly burning something, while the person on the right stands nearby, possibly observing or guiding. Based on your description, it’s unclear what specific action the figures are engaged in, such as possibly boiling water, cooking, or burning herbs. Therefore, this painting might be a realist work, portraying a moment from everyday life. It may not have the profound emotional or secretive background found in the storyline of Dream of the Red Chamber, instead simply highlighting the relationships and actions between individuals. Additionally, details such as the background, expressions, and postures of the figures in the painting would also influence the scene’s meaning and would require analysis based on the specific artwork.",[168,5518,5520],{"id":5519},"llamaindex-developer-experience","LlamaIndex Developer Experience",[11,5522,5523],{},"Overall, I found the LlamaIndex documentation to be very helpful. Before using LlamaIndex for this project I had used LangChain to build a RAG POC, but I didn’t get very good results. I love how the LlamaIndex documentation has a 5-line starter example for building a RAG system:",[26,5525,5527],{"className":1383,"code":5526,"language":1125,"meta":35,"style":35},"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n",[33,5528,5529,5534,5538,5543,5548,5553,5558],{"__ignoreMap":35},[187,5530,5531],{"class":189,"line":190},[187,5532,5533],{},"from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",[187,5535,5536],{"class":189,"line":249},[187,5537,316],{"emptyLinePlaceholder":315},[187,5539,5540],{"class":189,"line":312},[187,5541,5542],{},"documents = SimpleDirectoryReader(\"data\").load_data()\n",[187,5544,5545],{"class":189,"line":319},[187,5546,5547],{},"index = VectorStoreIndex.from_documents(documents)\n",[187,5549,5550],{"class":189,"line":325},[187,5551,5552],{},"query_engine = index.as_query_engine()\n",[187,5554,5555],{"class":189,"line":686},[187,5556,5557],{},"response = query_engine.query(\"Some question about the data should go here\")\n",[187,5559,5560],{"class":189,"line":697},[187,5561,5562],{},"print(response)\n",[11,5564,4359,5565],{},[15,5566,5567],{"href":5567,"rel":5568},"https://docs.llamaindex.ai/en/stable/#getting-started",[19],[11,5570,5571,5572,5577],{},"I was able to expand this simple example to implement the text and image Q&A bots for RedLM fairly easily. The application I built is somewhat similar to the ",[15,5573,5576],{"href":5574,"rel":5575},"https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_app_guide/",[19],"Full-Stack Web App with LLamaIndex"," included in their documentation.",[11,5579,5580,5581,5583,5584,5589,5590,5593],{},"Most of the early development I did on this project used the ",[33,5582,4369],{},". Later I tried using ",[15,5585,5588],{"href":5586,"rel":5587},"https://docs.llamaindex.ai/en/stable/module_guides/workflow/",[19],"LlamaIndex Workflows"," to better organize the logic in the text and image-based Q&A bots. The same workflow ",[33,5591,5592],{},"RAGWorkflow"," is used to handle requests for both the text and image Q&A bot queries. Workflows also work seamlessly with asynchronous Python frameworks like FastAPI. Here's the API endpoint for the multimodal image-Q&A bot using a LlamaIndex Workflow:",[26,5595,5597],{"className":1383,"code":5596,"language":1125,"meta":35,"style":35},"@app.post(\"/mm-q-and-a\")\nasync def mm_q_and_a_workflow(req_data: MultiModalRequest):\n    \"\"\"\n    This function handles Multimodal Q&A bot requests using a LlamaIndex workflow\n    \"\"\"\n    try:\n        # parse data from request object\n        image_b64 = req_data.image\n        prompt = req_data.prompt\n        chapter = req_data.chapter\n\n        # setup LlamaIndex Workflow and run it with data from request\n        w = RAGWorkflow(timeout=None)\n        result = await w.run(query=prompt, image_data=image_b64, chapter_number=chapter)\n\n        # return response\n        return QAQueryResponse(\n            response=result[\"response\"].message.content,\n            metadata=result[\"metadata\"],\n            image_desc=result[\"image_description\"],\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n",[33,5598,5599,5604,5609,5613,5618,5622,5626,5631,5636,5641,5646,5650,5655,5660,5665,5669,5674,5679,5684,5689,5694,5698,5702],{"__ignoreMap":35},[187,5600,5601],{"class":189,"line":190},[187,5602,5603],{},"@app.post(\"/mm-q-and-a\")\n",[187,5605,5606],{"class":189,"line":249},[187,5607,5608],{},"async def mm_q_and_a_workflow(req_data: MultiModalRequest):\n",[187,5610,5611],{"class":189,"line":312},[187,5612,4793],{},[187,5614,5615],{"class":189,"line":319},[187,5616,5617],{},"    This function handles Multimodal Q&A bot requests using a LlamaIndex workflow\n",[187,5619,5620],{"class":189,"line":325},[187,5621,4793],{},[187,5623,5624],{"class":189,"line":686},[187,5625,5405],{},[187,5627,5628],{"class":189,"line":697},[187,5629,5630],{},"        # parse data from request object\n",[187,5632,5633],{"class":189,"line":1291},[187,5634,5635],{},"        image_b64 = req_data.image\n",[187,5637,5638],{"class":189,"line":1306},[187,5639,5640],{},"        prompt = req_data.prompt\n",[187,5642,5643],{"class":189,"line":1434},[187,5644,5645],{},"        chapter = req_data.chapter\n",[187,5647,5648],{"class":189,"line":2599},[187,5649,316],{"emptyLinePlaceholder":315},[187,5651,5652],{"class":189,"line":2607},[187,5653,5654],{},"        # setup LlamaIndex Workflow and run it with data from request\n",[187,5656,5657],{"class":189,"line":2621},[187,5658,5659],{},"        w = RAGWorkflow(timeout=None)\n",[187,5661,5662],{"class":189,"line":2631},[187,5663,5664],{},"        result = await w.run(query=prompt, image_data=image_b64, chapter_number=chapter)\n",[187,5666,5667],{"class":189,"line":2642},[187,5668,316],{"emptyLinePlaceholder":315},[187,5670,5671],{"class":189,"line":2653},[187,5672,5673],{},"        # return response\n",[187,5675,5676],{"class":189,"line":2665},[187,5677,5678],{},"        return QAQueryResponse(\n",[187,5680,5681],{"class":189,"line":2674},[187,5682,5683],{},"            response=result[\"response\"].message.content,\n",[187,5685,5686],{"class":189,"line":2684},[187,5687,5688],{},"            metadata=result[\"metadata\"],\n",[187,5690,5691],{"class":189,"line":2694},[187,5692,5693],{},"            image_desc=result[\"image_description\"],\n",[187,5695,5696],{"class":189,"line":2706},[187,5697,4531],{},[187,5699,5700],{"class":189,"line":2715},[187,5701,5439],{},[187,5703,5704],{"class":189,"line":2725},[187,5705,5706],{},"        raise HTTPException(status_code=500, detail=str(e))\n",[11,5708,5709,5710,5712],{},"Using LlamaIndex Workflows also helped me add additional logic in a maintainable and standardized way. For example, I expanded the ",[33,5711,5592],{}," logic to include LLM-based re-ranking in order to ensure retrieval of the most relevant documents for my chatbot queries. This technique increases request latency, but this is an acceptable tradeoff for an application like RedLM.",[911,5714,5716],{"id":5715},"llmrerank","LLMRerank",[11,5718,5719,5720,5722],{},"LLM Rerank was an interesting technique to try out, and LlamaIndex provides ",[33,5721,5716],{}," to make the implementation as simple as possible. Here's my understanding of how it works:",[916,5724,5725,5728,5731,5737],{},[919,5726,5727],{},"LLMRerank searches in the vector database for a high number of documents that are relevant to your query. This is done using cosine similarity, which essentially compares the vectors that represent the query and the documents.",[919,5729,5730],{},"Next, LLMRerank goes through a process of assigning a numerical to each document to score relevancy. It does this via a special prompt that requests relevancy score for each document in batches.",[919,5732,5733,5734,5736],{},"For example, I configured ",[33,5735,5716],{}," to initially fetch 4 documents from the vector database based on cosine similarity. Then in batches of 2, relevancy scores are assigned. Finally, the top 2 most relevant documents based on the LLM-give scores are used to make the RAG query.",[919,5738,5739],{},"Adding LLMRerank can require a number of additional queries based on how you configure the batch size and the number of documents you would like to compare. This will increase latency for your application and use more resources to make the extra calls.",[11,5741,5742,5743,5745],{},"Here's an example LLM query that ",[33,5744,5716],{}," uses to do assign scores:",[11,5747,5748],{},[511,5749],{"alt":5750,"src":5751},"LLMRerank Prompt","/static/redlm/llmrerank_prompt.png",[11,5753,5754],{},"Here are logs from my application showing what happens inside the workflow.",[11,5756,5757],{},"Application for text-base Q&A query:",[26,5759,5762],{"className":5760,"code":5761,"language":31},[29],"INFO:     💬Request for Q&A chatbot: query='宝玉和谁打架？'\nINFO:     🔀Routing Workflow to next step\nINFO:     💬Routing to QueryEvent\nINFO:     🧮Query the vector database with: 宝玉和谁打架？\nINFO:     🖥️Using in-memory embedding database\nINFO:     ⏳Loading index from storage directory...\nINFO:     ✅Finished loading index.\nINFO:     📐Retrieved 4 nodes.\nINFO:     🔀Doing LLMRerank\nINFO:     ℹ️ Chat Model Info:\nINFO:     🟩Using NVIDIA Cloud API for inference\nINFO:     🔘Chat Model: baichuan-inc/baichuan2-13b-chat\nINFO:     🔢Reranked nodes to 2\nINFO:     🤖Doing inference step\nINFO:     ⚙️ Getting query engine..\nINFO:     🔎Getting response from custom query engine\nINFO:     💬Text-based Q&A query\nINFO:     🀄Text is Chinese\nINFO:     Using nodes from workflow...\nINFO:     🔏Formatting prompt\nINFO:     Prompt is\n\n这是相关的参考资料：\n---------------------\n宝玉从来没有经历过这样的痛苦。起初，他觉得被打得很痛，乱喊乱叫。后来，他的气变得虚弱，声音变得嘶哑，无法说话。众门客见他被打得很惨，赶上来恳求他停下来。贾政不肯听，说：“你们知道他干了什么坏事，还能饶他吗？平时都是你们这些人把他带坏了，现在到了这步田地，你们还来劝他。明天，如果他杀父弑君，你们才不劝吗？”\n\n宝玉从来没有经历过这样的痛苦。起初，他觉得打得很痛，乱喊乱叫。后来，他的气变得虚弱，声音变得嘶哑，无法说话。众门客见他被打得很惨，赶上来恳求他停下来。贾政不肯听，说：“你们知道他干了什么坏事，还能饶他吗？平时都是你们这些人把他带坏了，现在到了这步田地，你们还来劝他。明天，如果他杀父弑君，你们才不劝吗？”\n---------------------\n根据上述的参考资料，回答下面的问题\n问题：宝玉和谁打架？\n\nResponse...\n宝玉和贾政打架。\n",[33,5763,5761],{"__ignoreMap":35},[11,5765,5766],{},"My question here was basically asking \"Who gets in a fight with Baoyu?\" The reply says that his father, Jiazheng, gets in a fight with Baoyu, and the documents that are used here very similar, differing by only one character. One of the documents is supposed to be and English translation, but in fact there was a failure in the translation for this paragraph and it \"translated\" the Chinese by simply repeating it. A translation of this paragraph using GPT 4o describes a tense scene between protagonist Jia Baoyu and his father Jia Zheng:",[107,5768,5769],{},[11,5770,5771],{},"Baoyu had never endured such agony before. At first, he felt the pain intensely and cried out loudly. Later, his breath grew weak, his voice turned hoarse, and he couldn’t speak. The attendants, seeing how severely he was being beaten, rushed forward to plead for him to stop. Jia Zheng refused to listen, saying, “Do you know the misdeeds he’s committed, and still you want to spare him? Normally, it’s you people who lead him astray, and now that it’s come to this, you still try to persuade him? Tomorrow, if he were to commit patricide or treason, would you still not advise him?”",[11,5773,5774],{},"Another benefit of LlamaIndex workflows is the ability to create visualizations of each step, the branches between them and the overall flow of events and the functions that accept/emit them as arguments/return values. It took a little bit of getting used to the patterns used to create workflows, but the documentation for Workflows provided a good starting point that I could adapt for my application. Here's a visualization of the LlamaIndex Workflow that is used by the image and text-based Q&A bots:",[11,5776,5777],{},[511,5778],{"alt":5779,"src":5780},"RedLM RAG Workflow","/static/redlm/rag_workflow.png",[911,5782,5784],{"id":5783},"observability-and-tracing-with-langfuse","Observability and Tracing with Langfuse",[11,5786,5787,5788,5790],{},"It is never too soon to add observability and tracing to a RAG application! I learned this the hard way after doing some refactoring of prompts and ",[33,5789,4369],{}," logic.",[107,5792,5793],{},[11,5794,5795],{},"Langfuse is an open source LLM engineering platform to help teams collaboratively debug, analyze and iterate on their LLM Applications. With the Langfuse integration, you can seamlessly track and monitor performance, traces, and metrics of your LlamaIndex application. Detailed traces of the LlamaIndex context augmentation and the LLM querying processes are captured and can be inspected directly in the Langfuse UI.",[11,5797,5798,5799,5804],{},"LlamaIndex supports lots of different observability and tracing solutions. I tried using ",[15,5800,5803],{"href":5801,"rel":5802},"https://langfuse.com/",[19],"Langfuse"," (YC W23) which is an open-source option that has a self hosted option.",[11,5806,5807],{},[511,5808],{"alt":5809,"src":5810},"Langfuse tracing for RedLM","/static/redlm/langfuse.png",[11,5812,5813],{},"Langfuse came in handy when debugging the prompts for the image-based Q&A bot. This screenshot shows a trace of a multi-modal Q&A bot query about the fire at the Gourd Temple that occurs in Chapter 1 of the book.",[168,5815,5817],{"id":5816},"nvidia-inference-stack-tensorrt-llm-and-buildnvidiacom","NVIDIA inference stack (TensorRT-LLM and build.nvidia.com)",[11,5819,5820],{},"The LLM API for TensorRT-LLM is a very nice developer experience compared with my earlier attempts with manually building inference engines. The roadmap for TensorRT-LLM looks promising, I’m looking forward to support for an OpenAI Compatible API and more models. NVIDIA NIMs using TensorRT-LLM are an easy way to run models as OpenAI compatible API servers, but the selection of models is still pretty limited. vLLM provides a strong alternative with a wide range of support models. NVIDIA NIMs for LLMs build on vLLM libraries and the TensorRT-LLM library, so it is helpful to have an understanding of both of these libraries to stay on the bleeding edge of performant inference engines.",[11,5822,5823],{},[511,5824],{"alt":5825,"src":5826},"trt-llm-roadmap","/static/redlm/trt-llm-roadmap.png",[11,5828,5829],{},"The NVIDIA API catalog is a great way to test a variety of different models, especially large models that cannot fit into consumer hardware like RTX PCs or high-end MacBooks. I got to try out the new meta/llama-3.2-90b-vision-instruct model in my project by simply changing a value in my .env file, this is a great developer experience!",[11,5831,5832],{},[511,5833],{"alt":4182,"src":5834},"/static/redlm/build.nvidia.com.png",[11,5836,5837],{},"The NVIDIA API catalog doesn’t have every model in every size, however. For example, it has the qwen/qwen2-7b-instruct model, but doesn’t have the qwen/qwen2-7b-instruct model. Also, only some of the models are labeled as “Run Anywhere”; a lot of the models say “Self-Hosted API Coming Soon” meaning that they can’t be downloaded an run locally as a container. To get around this, I ran inferences services locally using both vLLM’s vllm/vllm-openai container and my own container running Qwen and other services.",[168,5839,5841],{"id":5840},"my-local-inference-stack-rtx","My local inference stack (RTX)",[11,5843,5844],{},[511,5845],{"alt":5846,"src":5847},"RTX PCs","/static/redlm/rtxpcs.png",[11,5849,5850,5851,1172,5854,5857,5858,5860],{},"Two of the RTX PCs in my home network: ",[33,5852,5853],{},"a1",[33,5855,5856],{},"a3",". ",[33,5859,5853],{}," was the first PC I built by myself and was the beginning of my GeForce journey. Luckily I built it with an over-provisioned PSU, so it can use a 4090 FE card! The front panel doesn't fit, however.",[11,5862,5863],{},"One limitation of the NVIDIA API catalog is the number of free credits given for a trial account. Using 1 credit per API call, I would use up the 1000 credits very quickly when running scripts like translation or the RAG evaluation. The same would be true with rate limits of the OpenAI API. That’s why running LLMs locally is still an important part of the development cycle for this type of project.",[11,5865,5866,5867,5872],{},"This project primarily uses two models: a large language model and a vision language models. Running the Yi-1.5-9B-Chat model from ",[15,5868,5871],{"href":5869,"rel":5870},"http://01.AI",[19],"01.AI"," takes up just about all of the GPU memory on one of my RTX 4090 PCs, so I had to run the vision model on another PC. In a previous project, I used Kubernetes to manage lots of different inference services: LLMs, ComfyUI, ChatTTS and MusicGen for making AI videos and I found it to a nice way to manage different containerized inference services.",[26,5874,5877],{"className":5875,"code":5876,"language":31},[29],"brian@a3:~$ microk8s kubectl get no -o wide\nNAME   STATUS   ROLES    AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\na1     Ready    \u003Cnone>   4d4h   v1.30.5   192.168.5.182   \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\na2     Ready    \u003Cnone>   11d    v1.30.5   192.168.5.96    \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\na3     Ready    \u003Cnone>   11d    v1.30.5   192.168.5.173   \u003Cnone>        Ubuntu 24.04.1 LTS   6.8.0-45-generic   containerd://1.6.28\n",[33,5878,5876],{"__ignoreMap":35},[11,5880,5881,5882,5885],{},"In the RedLM GitHub repo I included kubernetes manifests that show how to run the LLM and VLM across two different computers. I used Kustomize as a way to replace dynamic values in the YAML files for different resources. The kubernetes set up is experimental; the LLM and VLM can more reliably be run with ",[33,5883,5884],{},"docker run"," commands.",[11,5887,5888],{},[511,5889],{"alt":5890,"src":5891},"k8s dashboard for local inference services","/static/redlm/k8s-dashboard.png",[11,5893,5894,5895,5898],{},"I had a lot of driver issues when trying to get kubernetes to run the vLLM container for the Yi LLM. I struggled with the following error message when trying to run the ",[33,5896,5897],{},"vllm"," LLM service:",[107,5900,5901],{},[11,5902,5903],{},"RuntimeError: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",[11,5905,5906],{},"I tried uninstalling and reinstalling different versions of the NVIDIA drivers and CUDA but kept seeing the same message once the server would try to start up in the vLLM container logs. Rebooting my PC didn't work either. I saw a recommendation to turn off secure boot in BIOS. I didn't turn it on, but having nothing else to try I went into the BIOS settings and found that there were some keys configured in the secure boot section. After I deleted these keys and reboot, everything seemed to work normally. I'm not sure why my PC was in secure boot mode, though!",[168,5908,5910],{"id":5909},"ai-models-used-in-this-project","AI Models used in this project",[11,5912,5913],{},"I selected LLMs that run efficiently on RTX PCs, are available in the NVIDIA API catalog, and offer strong bilingual support in Chinese and English, ensuring compatibility, performance, and linguistic flexibility. Here are the models that I ended up using with RedLM:",[911,5915,5917,1172,5919],{"id":5916},"_01-aiyi-15-9b-chat-and-nvidiayi-large",[33,5918,5026],{},[33,5920,5921],{},"nvidia/yi-large",[11,5923,4959,5924,5926,5927,5932,5933,5936,5937,5940],{},[33,5925,5026],{}," for most of the LLM inference while developing RedLM on my RTX PCs. ",[15,5928,5931],{"href":5929,"rel":5930},"https://github.com/01-ai/Yi",[19],"This model family"," performs well on both Chinese and English benchmarks, and has a variety of model sizes. I was able to try using the ",[33,5934,5935],{},"01-ai/yi-large"," model from the NVIDIA API catalog when using remote cloud inference. I used the ",[33,5938,5939],{},"vllm/vllm-openai:latest"," container to run this locally.",[11,5942,5943,5944,5949],{},"There are also vision models in the Yi series, such as ",[15,5945,5948],{"href":5946,"rel":5947},"https://huggingface.co/01-ai/Yi-VL-34B",[19],"01-ai/Yi-VL-34B",", but I didn't use these models in my project.",[911,5951,5953],{"id":5952},"baichuan-incbaichuan2-13b-chat",[33,5954,5955],{},"baichuan-inc/baichuan2-13b-chat",[11,5957,5958],{},"This model is available in the NVIDIA API catalog, and it was the main model I used when testing remote inference. It performs well in a variety of tasks and scores highly on the the Chinese Massive Multitask Language Understanding (CMMLU) benchmark.",[911,5960,5962],{"id":5961},"qwenqwen2-7b",[33,5963,5964],{},"Qwen/Qwen2-7B",[11,5966,5967],{},"This model was used for summary and translation of the source text. It was supported by the TensorRT-LLM LLM API and I didn't have any issues building the TensorRT-LLM model with it on the EC2 instance used to do the completion inference for translations.",[911,5969,5971],{"id":5970},"qwenqwen2-vl-2b-instruct",[33,5972,5191],{},[11,5974,5975],{},"This was the vision language model (VLM) that I used locally when developing on RTX. I was impressed at how well it could describe images given the small parameter count of the model (2 billion parameters). The small size of this model made it easy to run in my RTX PC cluster.",[11,5977,5978,5979,5984],{},"There is ",[15,5980,5983],{"href":5981,"rel":5982},"https://github.com/NVIDIA/TensorRT-LLM/issues/2183",[19],"an open GitHub issue for TensorRT-LLM support for Qwen2-VL"," at the time of writing.",[11,5986,5987,5988,5991,5992,5995],{},"I wrote a simple FastAPI server using the Hugging Face ",[33,5989,5990],{},"transformers"," library based on example code from this model's documentation (see ",[33,5993,5994],{},"services/qwen2-vl"," in the RedLM GitHub repo for more details). I packaged this service into a container in order to run it in my local kubernetes cluster along with other inference services.",[911,5997,5999],{"id":5998},"metallama-32-90b-vision-instruct",[33,6000,5195],{},[11,6002,6003,6004,6007,6008,6010],{},"This model came out while I was working on the project, and I decided to use it instead of the ",[33,6005,6006],{},"adept/fuyu-8b"," model that was previously one of the only vision language models in the NVIDIA API catalog. The ",[33,6009,5195],{}," model has strong Chinese language skills, so it was a good model to use when doing remote inference for the image Q&A bot.",[911,6012,6014],{"id":6013},"nvidianvlm-d-72b",[15,6015,6018],{"href":6016,"rel":6017},"https://huggingface.co/nvidia/NVLM-D-72B",[19],[33,6019,6020],{},"nvidia/NVLM-D-72B",[11,6022,6023,6024,6027],{},"I didn't use this model in my project, but it came out recently and looks awesome! Hopefully this model will be available on the NVIDIA API catalog soon. It is trained on the ",[33,6025,6026],{},"Qwen2-72B-Instruct"," text-only model, so it likely also has very strong support for Chinese language.",[107,6029,6030],{},[11,6031,6032],{},"Today (September 17th, 2024), we introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.",[168,6034,6036],{"id":6035},"the-success-of-black-myth-wukong","The success of Black Myth: Wukong",[11,6038,6039],{},"I originally got the idea to build this project after seeing the release of Black Myth: Wukong. This game is a blockbuster success from a Chinese developer that tells the story of the Monkey King’s adventure in the Journey West universe. Journey West (西游记) is another one of the “Four Great Works” of Chinese literature. It tells the story of the legendary pilgrimage of the monk Xuanzang (also known as Tang Sanzang) to India, accompanied by his three disciples—Sun Wukong (the Monkey King), Zhu Bajie (Pigsy), and Sha Wujing (Sandy). The group travels from China to India to retrieve sacred Buddhist scriptures, facing numerous challenges, demons, and supernatural beings along the way.",[11,6041,6042],{},"The novel blends elements of adventure, mythology, and spiritual allegory, with Sun Wukong's mischievous nature and extraordinary powers adding humor and excitement. Through their journey, the characters grow and overcome personal flaws, ultimately achieving enlightenment and spiritual success. The video game adaptation has set world records for numbers of concurrent players, and it has rewritten the narrative around what is possible with single-player, offline games in the gaming industry.",[11,6044,6045],{},[511,6046],{"alt":6047,"src":6048},"Black Myth: Wukong","/static/redlm/wukong.png",[11,6050,6051],{},"Three renditions of Journey West: Songokū (The Monkey King) polychrome woodblock (surimono) (1824) by Yashima Gakutei (1786–1868), Black Myth: Wukong video game by Game Science (2024), Journey to the West TV series by CCTV (1982-2000)",[168,6053,6055],{"id":6054},"redlm-video","RedLM video",[11,6057,6058],{},[15,6059,6061],{"href":4108,"rel":6060},[19],"Watch the RedLM video on 𝕏",[6063,6064],"red-lm-video",{},[11,6066,6067],{},"I created the video for this project using Blender.The Blender sequencer editor is a great non-linear video editing tool for simple video projects like this one. I used the following formula to create the project video for RedLM:",[2276,6069,6070,6078,6081,6084],{},[919,6071,6072,6073],{},"Background music: I used the AI music generation service called Suno with the prompt “mystical strange traditional Chinese music from the Qing Dynasty”. Here’s the link to my Suno playlist called “Qing Dynasty Music” where you can find the original song and some other good songs that I generated using this prompt. My ",[15,6074,6077],{"href":6075,"rel":6076},"https://suno.com/playlist/863ea0dd-1921-467c-8b69-16dbd126d966",[19],"Qing Dynasty Music Playlist on Suno",[919,6079,6080],{},"Outline: For this project, the main sections are the introduction, then explaining each part with a short demo: translation, text-based Q&A, evaluation for text-based Q&A, image-based Q&A, and finally a short outro. I wrote an outline and then ChatGPT helped with filling out the content.",[919,6082,6083],{},"Narration: I used ElevenLabs to narrate the main part of the video using a clone of my voice using the ElevenLabs Voice Lab. The Chinese voices were generated on my computer with an open-source text-to-speech model called ChatTTS.",[919,6085,6086],{},"Images and videos: I gathered images and screen captures of different parts of the project including code snippets, paintings of the book, flow diagrams and screen recordings of the application.",[11,6088,6089],{},"The video is composed of different “strips”. The green strips represent the music and voice clips. Red strips are images and yellow strips are videos. Here is what the final cut of the video looks like in Blender’s Sequencer view:",[11,6091,6092],{},[511,6093],{"alt":6094,"src":6095},"Blender Sequence Editor","/static/redlm/blender_sequence_editor.png",[11,6097,6098],{},"ChatTTS is one of the most impressive open-source models I have seen for generating conversational speech with prosodic elements (pausing, laughter, etc.) It is developed by a Chinese company called 2noise. Earlier this year I made a small contribution to this project with an API example using FastAPI to show how to run a standalone API using the model. Another example in this project provides a comprehensive example application built with gradio:",[11,6100,6101],{},[511,6102],{"alt":6103,"src":6104},"ChatTTS UI","/static/redlm/chattts_ui.png",[11,6106,6107],{},"I was planning on streaming the narration audio for Q&A answers using my ChatTTS API service, but I didn’t get around to doing this. Instead, I just used the Gradio application to generate the Chinese narration for Q&A and image Q&A examples included in the video.",[911,6109,6111],{"id":6110},"redlm-deep-dive-video-with-notebooklm","RedLM Deep Dive video with NotebookLM",[11,6113,6114],{},"NotebookLM is a new application from Google that is a truly magical application of retrieval augmented generation.",[107,6116,6117],{},[11,6118,6119],{},"NotebookLM is a research and note-taking online tool developed by Google Labs that uses artificial intelligence, specifically Google Gemini, to assist users in interacting with their documents. It can generate summaries, explanations, and answers based on content uploaded by users.",[11,6121,6122],{},"I used NotebookLM to generate a \"Deep Dive\" podcast episode using only this article. I was pretty impressed with what it was able to produce, and I wanted to share it as part of this project, so I used Blender and some Python scripts to put together a simple and engaging visualization.",[11,6124,6125],{},[511,6126],{"alt":6127,"src":6128},"Deep Dive video in Blender","/static/redlm/deep_dive_blender.png",[11,6130,6131,6132,6135,6136,6143,6144,6149],{},"The ",[33,6133,6134],{},"openai/whisper-base"," model was used to get time stamps for the start and end of each spoken word using Automated Speech Recognition (ASR). A speaker segmentation library called ",[15,6137,6140],{"href":6138,"rel":6139},"https://github.com/pyannote/pyannote-audio",[19],[33,6141,6142],{},"pyannote/audio"," was used to perform speaker diarization. This is an interesting algorithm that can segment any number of distinct speakers in an audio recording using a series of models and a discrete-time stochastic process known as the ",[15,6145,6148],{"href":6146,"rel":6147},"https://en.wikipedia.org/wiki/Chinese_restaurant_process",[19],"Chinese restaurant process",". This gave a list of time intervals with a speaker ID, and I used the intervals to attribute a speaker ID to each word. Then I segmented the audio into two files using this data and used the files to generate audio waveforms using Blender's geometry nodes. Another script was used to animate each word of as it is spoken in one of two positions for each speaker.",[168,6151,6153],{"id":6152},"final-thoughts","Final thoughts",[11,6155,6156],{},"I’m glad to have had the opportunity to join three NVIDIA developer contests this year. I like the idea of a “developer contest” that takes place over several weeks compared to hackathons that take place over just a few days. Having more time allows you to learn about a new tool or framework at a deeper level and think about how to apply it in a creative project.",[11,6158,6159],{},[511,6160],{"alt":6161,"src":6162},"NVIDIA and LlamaIndex Contest","/static/redlm/llama-contest-og.jpg",[11,6164,6165],{},"I also like how this contest is not team based. Working on this project I was able to do a lot of high-level thinking, write out features as detailed prompts, and then delegate the code writing to LLMs as if I was giving tasks to teammates.",[11,6167,6168],{},"NVIDIA’s contests are “global developer contests”, but the contests so far are not open to developers in India and China. This is probably due to local rules and regulations governing how contests, prizes and taxes work. It is too bad; I would love to see what types of applications would come from participants in these countries. Also, there are also a lot of really interesting developments happening in the LLM space in both China and India!",[11,6170,6171,6172,6180],{},"The LLMs I used in this project were developed by leading Chinese AI companies, and they are competitive with LLMs from Western countries on LLM benchmarks despite having access to fewer GPU resources. ",[15,6173,6176,6177],{"href":6174,"rel":6175},"https://qwenlm.github.io/blog/qwen2.5-coder-family/",[19],"Qwen recently released a new model called ",[33,6178,6179],{},"Qwen2.5-Coder-32B"," that has outperfomed leading models at coding tasks.",[11,6182,6183],{},[511,6184],{"alt":6185,"src":6186},"Qwen coder model","/static/redlm/qwen_coder.png",[11,6188,6189,6194],{},[15,6190,6193],{"href":6191,"rel":6192},"https://www.youtube.com/watch?v=UitJxc9LE60",[19],"Kaifu Lee mentioned in a Bloomberg interview"," that the scarcity of GPU resources in China will force Chinese engineers to innovate in new ways to gain an advantage. One example of this we saw recently was when Chinese hardware hackers doubled the usable memory of the RTX 4090D (a variant of the RTX 4090 card with lower processing power to comply with US export regulations for China - the D stands for Dragon, apparently!)",[11,6196,6197],{},[511,6198],{"alt":6199,"src":6200},"RTX 4090D 48GB","/static/redlm/RTX4090D.jpg",[11,6202,6203],{},"NVIDIA recently concluded it's AI Summit in Mumbai. I was intrigued by the fact that Hindi has unique challenges that have have limited the development of Hindi LLMs compared to the development of English and Chinese LLMs. In a conversation with Jensen Huang, Indian industrial titan and CEO of Reliance Industries Mukesh Ambani spoke about his aspirations and ambition for India to overcome these challenges and develop a Hindi LLM. In a viral moment Mukesh Ambani shared that through devotion to attaining knowledge through the Hindu Goddess of knowledge Sarawati, India will be met by the Goddess of prosperity, Lakshmi.",[11,6205,6206],{},[511,6207],{"alt":6208,"src":6209},"Mukesh Ambani","/static/redlm/mukesh_ambani.png",[11,6211,6212,6213,6218],{},"NVIDIA recently released a small language model for Hindi at the AI Summit in Mumbai called  ",[15,6214,6217],{"href":6215,"rel":6216},"https://indiaai.gov.in/article/nvidia-unveils-nemotron-4-mini-hindi-4b-ai-for-india-s-500-million-hindi-speakers",[19],"Nemotron-4-Mini-Hindi-4B",". Hindi LLMs could enable applications to explore important works of literature from India. I don't know that much about India literature, but a comparable work of literature in size and cultural significance might be the Ramayana.",[11,6220,6221],{},[4339,6222,6223],{},"The Ramayana is an ancient Indian epic that tells the story of Prince Rama's heroic quest to rescue his wife, Sita, who has been kidnapped by the demon king Ravana. Set in a world of gods, demons, and celestial beings, the story explores themes of duty, loyalty, and the triumph of good over evil. Guided by wisdom, strength, and the support of devoted allies like Hanuman, the monkey god, and his brother Lakshmana, Rama's journey is a deeply spiritual tale, celebrated for its poetic beauty and moral depth. The Ramayana continues to inspire and captivate audiences across cultures.",[11,6225,6226],{},"The Ramayana story journeyed to Thailand centuries ago, transforming into the Ramakien, a Thai adaptation that retains the essence of the original Indian epic while adding distinctive Thai cultural elements. Introduced through trade, diplomacy, and cultural exchange between India and Southeast Asia, the story became deeply woven into Thailand’s art, literature, and performance traditions. Thai kings, particularly King Rama I, adapted and documented the Ramakien, giving it a prominent place in Thai history. Lavishly detailed murals surrounding the Temple of the Emerald Buddha in Bangkok’s Grand Palace depict the Ramakien in over 178 panels that totaling over 2 kilometers in length. On a recent visit to the Grand Palace, I imagined having an application that could link the detailed murals to elements of the story in Hindi, Thai, English, Chinese or any language.",[11,6228,6229],{},[511,6230],{"alt":6231,"src":6232},"Ramakien murals surrounding Temple of the Emerald Buddha","/static/redlm/ramakien.png",[11,6234,6235],{},"The Dream of the Red Chamber, originally titled The Story of the Stone, is one of China’s greatest literary works and a masterpiece of world literature. The novel begins with a frame story centered on a magical stone, left over from the Chinese creation myth where the goddess Nuwa mends the heavens. Longing to experience the human world, the sentient stone persuades a Buddhist monk and a Taoist priest to reincarnate it as a boy. This boy, Baoyu, is born into a wealthy and influential family—a character partly based on the author, Cao Xueqin, and his own aristocratic upbringing. Through Baoyu's life, friendships, and romantic relationships, the novel delves into his family’s gradual decline, mirroring the instability of China’s own noble families in the late Qing dynasty. The story also portrays the era's customs, social structures, and beliefs, offering readers a richly detailed exploration of life in Qing China.",[11,6237,6238],{},"It was a lot of fun to work on this project with tools from LlamaIndex and NVIDIA. With AI technology, GPUs are now essentially sentient stones, and I was able to share this important touchstone of the human experience with my computers using LlamaIndex and open source language models. In turn, RedLM shared with me delightful insights into world of Dream of the Red Chamber.",[11,6240,6241],{},[511,6242],{"alt":6243,"src":6244},"Story of a Stone","/static/redlm/stone_story.png",[11,6246,6247],{},[511,6248],{"alt":6249,"src":6250},"Story of a Stone Analysis","/static/redlm/stone_story_analysis.png",[107,6252,6253],{},[11,6254,6255],{},"This scene describes a piece of traditional Chinese painting, depicting two elderly figures conversing amidst mountains and rivers. The painting likely visually represents the scene from the book where a monk and a Taoist are chatting at the foot of Qinggeng Peak. The two elderly figures in the painting may represent the monk and Taoist from the book, discussing their discovery of a bright and pristine stone, and planning to take it to a bustling, splendid place for a happy life. The painting’s elements—mountains, peaks, flowing water, trees, and rocks—might echo the book's descriptions, illustrating the natural environment at the base of Qinggeng Peak where the monk and Taoist reside. The painting’s tranquil and harmonious atmosphere may also align with the storyline, expressing the monk and Taoist's care for the stone and their wish for it to live a happy life. In summary, this painted scene might be an artistic portrayal of the story between the monk, the Taoist, and the stone from the book, using visual elements and ambiance to convey the narrative and themes within the story.",[855,6257,6258],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":6260},[6261,6264,6265,6266,6267,6268,6273,6274,6277,6281,6282,6283,6292,6293,6296],{"id":4087,"depth":249,"text":4088,"children":6262},[6263],{"id":4097,"depth":312,"text":4098},{"id":4119,"depth":249,"text":4120},{"id":4146,"depth":249,"text":4147},{"id":4169,"depth":249,"text":4170},{"id":4186,"depth":249,"text":4187},{"id":4324,"depth":249,"text":4325,"children":6269},[6270,6271,6272],{"id":4543,"depth":312,"text":4544},{"id":4735,"depth":312,"text":4736},{"id":4774,"depth":312,"text":4775},{"id":4952,"depth":249,"text":4953},{"id":5114,"depth":249,"text":5115,"children":6275},[6276],{"id":5455,"depth":312,"text":5456},{"id":5519,"depth":249,"text":5520,"children":6278},[6279,6280],{"id":5715,"depth":312,"text":5716},{"id":5783,"depth":312,"text":5784},{"id":5816,"depth":249,"text":5817},{"id":5840,"depth":249,"text":5841},{"id":5909,"depth":249,"text":5910,"children":6284},[6285,6287,6288,6289,6290,6291],{"id":5916,"depth":312,"text":6286},"01-ai/Yi-1.5-9B-Chat and nvidia/yi-large",{"id":5952,"depth":312,"text":5955},{"id":5961,"depth":312,"text":5964},{"id":5970,"depth":312,"text":5191},{"id":5998,"depth":312,"text":5195},{"id":6013,"depth":312,"text":6020},{"id":6035,"depth":249,"text":6036},{"id":6054,"depth":249,"text":6055,"children":6294},[6295],{"id":6110,"depth":312,"text":6111},{"id":6152,"depth":249,"text":6153},"2024-11-09","RedLM is an AI-powered application for the study of China's greatest classical novel: Dream of the Red Chamber",[6300,6301],{"link":4108,"site":2196},{"link":6302,"site":6303},"https://dev.to/briancaffey/redlm-my-submission-for-the-nvidia-and-llamaindex-developer-contest-1c3k","dev",{},"/2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest",{"title":4082,"description":6298},"2024/10/09/redlm-ai-application-for-studying-chinese-literature-redology-nvidia-llama-index-developer-contest",[2203,6309,2204,4077,6310,6311,6312,6313],"llama-index","rag","tensorrt-llm","chinese","redology","OM85_mWiwtPzijlEceEuWUPiQPqehYA90bCReYA0_RM",{"id":6316,"title":6317,"body":6318,"comments":315,"date":8165,"description":8166,"draft":872,"extension":873,"external":874,"image":8167,"meta":8168,"navigation":315,"path":8169,"seo":8170,"stem":8171,"tags":8172,"__hash__":8175},"blog/2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3.md","Upgrading my GitHub Pages blog to Nuxt 3",{"type":8,"value":6319,"toc":8134},[6320,6324,6334,6337,6340,6346,6350,6354,6360,6516,6520,6523,6690,6701,6707,6713,6861,6872,7013,7040,7049,7201,7204,7208,7219,7233,7239,7243,7251,7257,7260,7264,7270,7272,7275,7582,7597,7600,7606,7612,7634,7639,7643,7647,7659,7662,7665,7723,7727,7731,7734,7786,7790,7800,7807,7810,7814,7817,7823,7850,7854,7857,7861,7864,7868,7879,7882,7885,7887,7890,7894,7902,7906,7909,7913,7917,7929,7935,8056,8060,8063,8072,8076,8087,8114,8117,8121,8128,8131],[168,6321,6323],{"id":6322},"blog-history","Blog history",[11,6325,6326,6327,6333],{},"My personal website has always lived on GitHub Pages at ",[15,6328,6331],{"href":6329,"rel":6330},"https://briancaffey.github.io",[19],[33,6332,743],{},". The first version was built with the Jekyll framework. I started learning about Vue, and Nuxt seemed like an interesting alternative to Jekyll that would allow me to practice frontend development. In September 2020 I deployed the first version of the new site using Nuxt 2 and Vue 2.",[11,6335,6336],{},"I recently went through the process of upgrading from Nuxt 2 to Nuxt 3. This upgrade path also included an upgrade from Vue 2 to Vue 3. My previous attempts to upgrade this site from Nuxt 2 to Nuxt 3 failed because of error messages that I couldn't work through. This time, with a big help from AI, I got through the entire upgrade and learned a lot in the process. I'm happy to share my new blog that is powered by Vue 3, Nuxt 3 and Nuxt Content v2!",[11,6338,6339],{},"This article will go over the features of my site, how I'm using Nuxt and Vue and some of the changes I had to make when doing the upgrade. Let's go!",[11,6341,6342],{},[511,6343],{"alt":6344,"src":6345},"New site powered by Nuxt 3, Tailwind and Pinia","/static/nuxt/nuxt3.png",[168,6347,6349],{"id":6348},"features-of-my-blog","Features of my blog",[911,6351,6353],{"id":6352},"modules-and-plugins-overview","Modules and plugins overview",[11,6355,6356,6357,358],{},"Here are the modules and plugins I use on my site defined in ",[33,6358,6359],{},"nuxt.config.js",[26,6361,6365],{"className":6362,"code":6363,"language":6364,"meta":35,"style":35},"language-javascript shiki shiki-themes github-light github-dark","export default defineNuxtConfig({\n  modules: [\n    '@nuxt/content',\n    '@nuxtjs/tailwindcss',\n    '@nuxtjs/i18n',\n    '@nuxtjs/color-mode',\n    '@pinia/nuxt',\n    '@nuxt/eslint',\n    '@nuxtjs/sitemap',\n    '@nuxt/image',\n    'nuxt-gtag',\n    // '@nuxtjs/feed', --> this module is not yet supported in Nuxt 3!\n  ],\n\n  plugins: [\n    '~/plugins/disqus',\n    { src: '~/plugins/apexcharts', mode: 'client' },\n    { src: '~/plugins/drift', mode: 'client' }\n  ]\n})\n","javascript",[33,6366,6367,6381,6386,6393,6400,6407,6414,6421,6428,6435,6442,6449,6454,6459,6463,6468,6475,6492,6506,6511],{"__ignoreMap":35},[187,6368,6369,6372,6375,6378],{"class":189,"line":190},[187,6370,6371],{"class":573},"export",[187,6373,6374],{"class":573}," default",[187,6376,6377],{"class":193}," defineNuxtConfig",[187,6379,6380],{"class":577},"({\n",[187,6382,6383],{"class":189,"line":249},[187,6384,6385],{"class":577},"  modules: [\n",[187,6387,6388,6391],{"class":189,"line":312},[187,6389,6390],{"class":196},"    '@nuxt/content'",[187,6392,1228],{"class":577},[187,6394,6395,6398],{"class":189,"line":319},[187,6396,6397],{"class":196},"    '@nuxtjs/tailwindcss'",[187,6399,1228],{"class":577},[187,6401,6402,6405],{"class":189,"line":325},[187,6403,6404],{"class":196},"    '@nuxtjs/i18n'",[187,6406,1228],{"class":577},[187,6408,6409,6412],{"class":189,"line":686},[187,6410,6411],{"class":196},"    '@nuxtjs/color-mode'",[187,6413,1228],{"class":577},[187,6415,6416,6419],{"class":189,"line":697},[187,6417,6418],{"class":196},"    '@pinia/nuxt'",[187,6420,1228],{"class":577},[187,6422,6423,6426],{"class":189,"line":1291},[187,6424,6425],{"class":196},"    '@nuxt/eslint'",[187,6427,1228],{"class":577},[187,6429,6430,6433],{"class":189,"line":1306},[187,6431,6432],{"class":196},"    '@nuxtjs/sitemap'",[187,6434,1228],{"class":577},[187,6436,6437,6440],{"class":189,"line":1434},[187,6438,6439],{"class":196},"    '@nuxt/image'",[187,6441,1228],{"class":577},[187,6443,6444,6447],{"class":189,"line":2599},[187,6445,6446],{"class":196},"    'nuxt-gtag'",[187,6448,1228],{"class":577},[187,6450,6451],{"class":189,"line":2607},[187,6452,6453],{"class":295},"    // '@nuxtjs/feed', --> this module is not yet supported in Nuxt 3!\n",[187,6455,6456],{"class":189,"line":2621},[187,6457,6458],{"class":577},"  ],\n",[187,6460,6461],{"class":189,"line":2631},[187,6462,316],{"emptyLinePlaceholder":315},[187,6464,6465],{"class":189,"line":2642},[187,6466,6467],{"class":577},"  plugins: [\n",[187,6469,6470,6473],{"class":189,"line":2653},[187,6471,6472],{"class":196},"    '~/plugins/disqus'",[187,6474,1228],{"class":577},[187,6476,6477,6480,6483,6486,6489],{"class":189,"line":2665},[187,6478,6479],{"class":577},"    { src: ",[187,6481,6482],{"class":196},"'~/plugins/apexcharts'",[187,6484,6485],{"class":577},", mode: ",[187,6487,6488],{"class":196},"'client'",[187,6490,6491],{"class":577}," },\n",[187,6493,6494,6496,6499,6501,6503],{"class":189,"line":2674},[187,6495,6479],{"class":577},[187,6497,6498],{"class":196},"'~/plugins/drift'",[187,6500,6485],{"class":577},[187,6502,6488],{"class":196},[187,6504,6505],{"class":577}," }\n",[187,6507,6508],{"class":189,"line":2684},[187,6509,6510],{"class":577},"  ]\n",[187,6512,6513],{"class":189,"line":2694},[187,6514,6515],{"class":577},"})\n",[911,6517,6519],{"id":6518},"nuxt-content","Nuxt Content",[11,6521,6522],{},"The Nuxt Content module is a powerful git-based CMS. Articles on my site are written in Markdown files that contains frontmatter like the following:",[26,6524,6526],{"className":2507,"code":6525,"language":2509,"meta":35,"style":35},"---\ntitle: \"Upgrading my GitHub Pages blog to Nuxt 3\" # used on the page and in the \u003Chead> metadata\ndate: '2024-08-11'\ndescription: \"An overview of my newly upgraded GitHub Pages blog powered by Nuxt 3\"\nimage: /static/nuxt/nuxt3.png # cover image and og:image + twitter:image\ntags: # tags are used to categorize and navigate content\n  - vue\n  - nuxt\n  - github\n  - pinia\n\ndraft: true # drafts are publicly available but not displayed in the list of blog articles and not indexed\n\nexternal: # a list of external links where the article has been shared or republished\n  - link: https://x.com/briancaffey/status/abc123\n    site: x\n\ncomments: true # shows disqus comments\n---\n",[33,6527,6528,6533,6546,6556,6566,6578,6588,6596,6603,6610,6617,6621,6634,6638,6648,6660,6670,6674,6686],{"__ignoreMap":35},[187,6529,6530],{"class":189,"line":190},[187,6531,6532],{"class":193},"---\n",[187,6534,6535,6538,6540,6543],{"class":189,"line":249},[187,6536,6537],{"class":2516},"title",[187,6539,585],{"class":577},[187,6541,6542],{"class":196},"\"Upgrading my GitHub Pages blog to Nuxt 3\"",[187,6544,6545],{"class":295}," # used on the page and in the \u003Chead> metadata\n",[187,6547,6548,6551,6553],{"class":189,"line":312},[187,6549,6550],{"class":2516},"date",[187,6552,585],{"class":577},[187,6554,6555],{"class":196},"'2024-08-11'\n",[187,6557,6558,6561,6563],{"class":189,"line":319},[187,6559,6560],{"class":2516},"description",[187,6562,585],{"class":577},[187,6564,6565],{"class":196},"\"An overview of my newly upgraded GitHub Pages blog powered by Nuxt 3\"\n",[187,6567,6568,6571,6573,6575],{"class":189,"line":325},[187,6569,6570],{"class":2516},"image",[187,6572,585],{"class":577},[187,6574,6345],{"class":196},[187,6576,6577],{"class":295}," # cover image and og:image + twitter:image\n",[187,6579,6580,6583,6585],{"class":189,"line":686},[187,6581,6582],{"class":2516},"tags",[187,6584,585],{"class":577},[187,6586,6587],{"class":295},"# tags are used to categorize and navigate content\n",[187,6589,6590,6593],{"class":189,"line":697},[187,6591,6592],{"class":577},"  - ",[187,6594,6595],{"class":196},"vue\n",[187,6597,6598,6600],{"class":189,"line":1291},[187,6599,6592],{"class":577},[187,6601,6602],{"class":196},"nuxt\n",[187,6604,6605,6607],{"class":189,"line":1306},[187,6606,6592],{"class":577},[187,6608,6609],{"class":196},"github\n",[187,6611,6612,6614],{"class":189,"line":1434},[187,6613,6592],{"class":577},[187,6615,6616],{"class":196},"pinia\n",[187,6618,6619],{"class":189,"line":2599},[187,6620,316],{"emptyLinePlaceholder":315},[187,6622,6623,6626,6628,6631],{"class":189,"line":2607},[187,6624,6625],{"class":2516},"draft",[187,6627,585],{"class":577},[187,6629,6630],{"class":588},"true",[187,6632,6633],{"class":295}," # drafts are publicly available but not displayed in the list of blog articles and not indexed\n",[187,6635,6636],{"class":189,"line":2621},[187,6637,316],{"emptyLinePlaceholder":315},[187,6639,6640,6643,6645],{"class":189,"line":2631},[187,6641,6642],{"class":2516},"external",[187,6644,585],{"class":577},[187,6646,6647],{"class":295},"# a list of external links where the article has been shared or republished\n",[187,6649,6650,6652,6655,6657],{"class":189,"line":2642},[187,6651,6592],{"class":577},[187,6653,6654],{"class":2516},"link",[187,6656,585],{"class":577},[187,6658,6659],{"class":196},"https://x.com/briancaffey/status/abc123\n",[187,6661,6662,6665,6667],{"class":189,"line":2653},[187,6663,6664],{"class":2516},"    site",[187,6666,585],{"class":577},[187,6668,6669],{"class":196},"x\n",[187,6671,6672],{"class":189,"line":2665},[187,6673,316],{"emptyLinePlaceholder":315},[187,6675,6676,6679,6681,6683],{"class":189,"line":2674},[187,6677,6678],{"class":2516},"comments",[187,6680,585],{"class":577},[187,6682,6630],{"class":588},[187,6684,6685],{"class":295}," # shows disqus comments\n",[187,6687,6688],{"class":189,"line":2684},[187,6689,6532],{"class":193},[11,6691,6692,6693,6696,6697,6700],{},"Files for articles are stored in ",[33,6694,6695],{},"/content/[year]/[month]/[day]/[slug].md",", and the URLs for the articles are ",[33,6698,6699],{},"/${year}/${month}/${day}/${slug}",". This URL scheme was used in the Jekyll blog on my GitHub Pages site and kept this URL structure when I switched to Nuxt.",[11,6702,6703,6706],{},[33,6704,6705],{},"contentQuery"," is used for getting content from Nuxt Content. Here's a comparison of the old and new way of fetching data from Nuxt content.",[11,6708,6709,6710,358],{},"Old way using ",[33,6711,6712],{},"asyncData",[26,6714,6718],{"className":6715,"code":6716,"language":6717,"meta":35,"style":35},"language-html shiki shiki-themes github-light github-dark","\u003Cscript>\nexport default {\n  async asyncData ({ $content, params }) {\n    const file = `${params.year}/${params.month}/${params.day}/${params.slug}`\n    const article = await $content(file).fetch()\n    return { article }\n  }\n}\n\u003C/script>\n","html",[33,6719,6720,6731,6740,6762,6813,6835,6843,6848,6852],{"__ignoreMap":35},[187,6721,6722,6725,6728],{"class":189,"line":190},[187,6723,6724],{"class":577},"\u003C",[187,6726,6727],{"class":2516},"script",[187,6729,6730],{"class":577},">\n",[187,6732,6733,6735,6737],{"class":189,"line":249},[187,6734,6371],{"class":573},[187,6736,6374],{"class":573},[187,6738,6739],{"class":577}," {\n",[187,6741,6742,6745,6748,6751,6754,6756,6759],{"class":189,"line":312},[187,6743,6744],{"class":573},"  async",[187,6746,6747],{"class":193}," asyncData",[187,6749,6750],{"class":577}," ({ ",[187,6752,6753],{"class":581},"$content",[187,6755,637],{"class":577},[187,6757,6758],{"class":581},"params",[187,6760,6761],{"class":577}," }) {\n",[187,6763,6764,6767,6769,6772,6775,6777,6779,6782,6785,6787,6789,6792,6794,6796,6798,6801,6803,6805,6807,6810],{"class":189,"line":319},[187,6765,6766],{"class":573},"    const",[187,6768,1907],{"class":588},[187,6770,6771],{"class":573}," =",[187,6773,6774],{"class":196}," `${",[187,6776,6758],{"class":577},[187,6778,752],{"class":196},[187,6780,6781],{"class":577},"year",[187,6783,6784],{"class":196},"}/${",[187,6786,6758],{"class":577},[187,6788,752],{"class":196},[187,6790,6791],{"class":577},"month",[187,6793,6784],{"class":196},[187,6795,6758],{"class":577},[187,6797,752],{"class":196},[187,6799,6800],{"class":577},"day",[187,6802,6784],{"class":196},[187,6804,6758],{"class":577},[187,6806,752],{"class":196},[187,6808,6809],{"class":577},"slug",[187,6811,6812],{"class":196},"}`\n",[187,6814,6815,6817,6820,6822,6824,6827,6830,6833],{"class":189,"line":325},[187,6816,6766],{"class":573},[187,6818,6819],{"class":588}," article",[187,6821,6771],{"class":573},[187,6823,598],{"class":573},[187,6825,6826],{"class":193}," $content",[187,6828,6829],{"class":577},"(file).",[187,6831,6832],{"class":193},"fetch",[187,6834,694],{"class":577},[187,6836,6837,6840],{"class":189,"line":686},[187,6838,6839],{"class":573},"    return",[187,6841,6842],{"class":577}," { article }\n",[187,6844,6845],{"class":189,"line":697},[187,6846,6847],{"class":577},"  }\n",[187,6849,6850],{"class":189,"line":1291},[187,6851,1309],{"class":577},[187,6853,6854,6857,6859],{"class":189,"line":1306},[187,6855,6856],{"class":577},"\u003C/",[187,6858,6727],{"class":2516},[187,6860,6730],{"class":577},[11,6862,6863,6864,6867,6868,6871],{},"Here's the new way of fetching content using ",[33,6865,6866],{},"useAsyncData"," with ",[33,6869,6870],{},"\u003Cscript setup>"," syntax:",[26,6873,6875],{"className":6715,"code":6874,"language":6717,"meta":35,"style":35},"\u003Cscript setup>\nconst route = useRoute();\nconst { year, month, day, slug } = route.params;\nconst page = `/${year}/${month}/${day}/${slug}`;\nconst { data: article } = await useAsyncData(route.params.slug, () =>\n  queryCollection(page).findOne()\n);\n\u003C/script>\n",[33,6876,6877,6888,6903,6930,6962,6988,7000,7005],{"__ignoreMap":35},[187,6878,6879,6881,6883,6886],{"class":189,"line":190},[187,6880,6724],{"class":577},[187,6882,6727],{"class":2516},[187,6884,6885],{"class":193}," setup",[187,6887,6730],{"class":577},[187,6889,6890,6892,6895,6897,6900],{"class":189,"line":249},[187,6891,574],{"class":573},[187,6893,6894],{"class":588}," route",[187,6896,6771],{"class":573},[187,6898,6899],{"class":193}," useRoute",[187,6901,6902],{"class":577},"();\n",[187,6904,6905,6907,6909,6911,6913,6915,6917,6919,6921,6923,6925,6927],{"class":189,"line":312},[187,6906,574],{"class":573},[187,6908,578],{"class":577},[187,6910,6781],{"class":588},[187,6912,637],{"class":577},[187,6914,6791],{"class":588},[187,6916,637],{"class":577},[187,6918,6800],{"class":588},[187,6920,637],{"class":577},[187,6922,6809],{"class":588},[187,6924,592],{"class":577},[187,6926,595],{"class":573},[187,6928,6929],{"class":577}," route.params;\n",[187,6931,6932,6934,6937,6939,6942,6944,6946,6948,6950,6952,6954,6956,6959],{"class":189,"line":319},[187,6933,574],{"class":573},[187,6935,6936],{"class":588}," page",[187,6938,6771],{"class":573},[187,6940,6941],{"class":196}," `/${",[187,6943,6781],{"class":577},[187,6945,6784],{"class":196},[187,6947,6791],{"class":577},[187,6949,6784],{"class":196},[187,6951,6800],{"class":577},[187,6953,6784],{"class":196},[187,6955,6809],{"class":577},[187,6957,6958],{"class":196},"}`",[187,6960,6961],{"class":577},";\n",[187,6963,6964,6966,6968,6970,6972,6975,6977,6979,6981,6983,6986],{"class":189,"line":325},[187,6965,574],{"class":573},[187,6967,578],{"class":577},[187,6969,582],{"class":581},[187,6971,585],{"class":577},[187,6973,6974],{"class":588},"article",[187,6976,592],{"class":577},[187,6978,595],{"class":573},[187,6980,598],{"class":573},[187,6982,601],{"class":193},[187,6984,6985],{"class":577},"(route.params.slug, () ",[187,6987,607],{"class":573},[187,6989,6990,6992,6995,6998],{"class":189,"line":686},[187,6991,612],{"class":193},[187,6993,6994],{"class":577},"(page).",[187,6996,6997],{"class":193},"findOne",[187,6999,694],{"class":577},[187,7001,7002],{"class":189,"line":697},[187,7003,7004],{"class":577},");\n",[187,7006,7007,7009,7011],{"class":189,"line":1291},[187,7008,6856],{"class":577},[187,7010,6727],{"class":2516},[187,7012,6730],{"class":577},[11,7014,7015,7016,7019,7020,7022,7023,7025,7026,7028,7029,7031,7032,7035,7036,7039],{},"Note: using ",[33,7017,7018],{},"route.param.slug"," as the first argument for ",[33,7021,6866],{}," is important when using ",[33,7024,6866],{},". I initially misunderstood the statement about this needing to be a unique value and I gave it a value of ",[33,7027,6974],{},", but this caused a strange cache issue when I viewed different articles. The problem didn't show when I was developing with ",[33,7030,400],{},", it only showed up when I built the site and served in locally with ",[33,7033,7034],{},"yarn generate && yarn serve",". Since it is used in a dynamic page ",[33,7037,7038],{},"[slug].vue",", it needs to have a unique value for every individual page for the dynamic route.",[11,7041,7042,7043,7048],{},"I learned about this from ",[15,7044,7047],{"href":7045,"rel":7046},"https://nuxt.com/docs/getting-started/upgrade#shared-prerender-data",[19],"an example"," on the Nuxt 3 site about migration to Nuxt 4:",[26,7050,7052],{"className":6362,"code":7051,"language":6364,"meta":35,"style":35},"// This would be unsafe in a dynamic page (e.g. `[slug].vue`) because the route slug makes a difference\n// to the data fetched, but Nuxt can't know that because it's not reflected in the key.\nconst route = useRoute()\nconst { data } = await useAsyncData(async () => {\n  return await $fetch(`/api/my-page/${route.params.slug}`)\n})\n// Instead, you should use a key that uniquely identifies the data fetched.\nconst { data } = await useAsyncData(route.params.slug, async () => {\n  return await $fetch(`/api/my-page/${route.params.slug}`)\n})\n",[33,7053,7054,7059,7064,7076,7105,7135,7139,7144,7171,7197],{"__ignoreMap":35},[187,7055,7056],{"class":189,"line":190},[187,7057,7058],{"class":295},"// This would be unsafe in a dynamic page (e.g. `[slug].vue`) because the route slug makes a difference\n",[187,7060,7061],{"class":189,"line":249},[187,7062,7063],{"class":295},"// to the data fetched, but Nuxt can't know that because it's not reflected in the key.\n",[187,7065,7066,7068,7070,7072,7074],{"class":189,"line":312},[187,7067,574],{"class":573},[187,7069,6894],{"class":588},[187,7071,6771],{"class":573},[187,7073,6899],{"class":193},[187,7075,694],{"class":577},[187,7077,7078,7080,7082,7084,7086,7088,7090,7092,7094,7097,7100,7103],{"class":189,"line":319},[187,7079,574],{"class":573},[187,7081,578],{"class":577},[187,7083,582],{"class":588},[187,7085,592],{"class":577},[187,7087,595],{"class":573},[187,7089,598],{"class":573},[187,7091,601],{"class":193},[187,7093,615],{"class":577},[187,7095,7096],{"class":573},"async",[187,7098,7099],{"class":577}," () ",[187,7101,7102],{"class":573},"=>",[187,7104,6739],{"class":577},[187,7106,7107,7110,7112,7115,7117,7120,7123,7125,7127,7129,7131,7133],{"class":189,"line":325},[187,7108,7109],{"class":573},"  return",[187,7111,598],{"class":573},[187,7113,7114],{"class":193}," $fetch",[187,7116,615],{"class":577},[187,7118,7119],{"class":196},"`/api/my-page/${",[187,7121,7122],{"class":577},"route",[187,7124,752],{"class":196},[187,7126,6758],{"class":577},[187,7128,752],{"class":196},[187,7130,6809],{"class":577},[187,7132,6958],{"class":196},[187,7134,621],{"class":577},[187,7136,7137],{"class":189,"line":686},[187,7138,6515],{"class":577},[187,7140,7141],{"class":189,"line":697},[187,7142,7143],{"class":295},"// Instead, you should use a key that uniquely identifies the data fetched.\n",[187,7145,7146,7148,7150,7152,7154,7156,7158,7160,7163,7165,7167,7169],{"class":189,"line":1291},[187,7147,574],{"class":573},[187,7149,578],{"class":577},[187,7151,582],{"class":588},[187,7153,592],{"class":577},[187,7155,595],{"class":573},[187,7157,598],{"class":573},[187,7159,601],{"class":193},[187,7161,7162],{"class":577},"(route.params.slug, ",[187,7164,7096],{"class":573},[187,7166,7099],{"class":577},[187,7168,7102],{"class":573},[187,7170,6739],{"class":577},[187,7172,7173,7175,7177,7179,7181,7183,7185,7187,7189,7191,7193,7195],{"class":189,"line":1306},[187,7174,7109],{"class":573},[187,7176,598],{"class":573},[187,7178,7114],{"class":193},[187,7180,615],{"class":577},[187,7182,7119],{"class":196},[187,7184,7122],{"class":577},[187,7186,752],{"class":196},[187,7188,6758],{"class":577},[187,7190,752],{"class":196},[187,7192,6809],{"class":577},[187,7194,6958],{"class":196},[187,7196,621],{"class":577},[187,7198,7199],{"class":189,"line":1434},[187,7200,6515],{"class":577},[11,7202,7203],{},"Wow, there is already a Nuxt 4 in the works! Hopefully the upgrade process from Nuxt 3 to 4 is easier than Nuxt 2 to 3.",[911,7205,7207],{"id":7206},"development-process","Development process",[916,7209,7210,7214],{},[919,7211,7212],{},[33,7213,400],{},[919,7215,7216],{},[33,7217,7218],{},"yarn clean && yarn generate && yarn serve",[11,7220,7221,7222,7224,7225,7228,7229,7232],{},"When I want to run the site locally I run ",[33,7223,400],{}," which is an alias for ",[33,7226,7227],{},"nuxt dev --host",". Adding the ",[33,7230,7231],{},"--host"," option is important for when you want to view the site on a mobile device. It provides a QR code that links directly to the local IP:",[26,7234,7237],{"className":7235,"code":7236,"language":31},[29],"~/git/github/briancaffey.github.io$ yarn dev\nyarn run v1.22.21\n$ nuxt dev --host\nNuxt 3.12.4 with Nitro 2.9.7                                                   8:04:47 AM\n                                                                               8:04:47 AM\n\n              █▀▀▀▀▀▀▀█▀██▀▀███▀█▀▀▀▀▀▀▀█\n              █ █▀▀▀█ ██▄ █▀  ███ █▀▀▀█ █\n              █ █   █ ██▀█▀▄ ▄▄▄█ █   █ █\n              █ ▀▀▀▀▀ █▀█ █ █▀█▀█ ▀▀▀▀▀ █\n              █▀▀█▀▀█▀█▄▀  █▄▄▀▄█▀█████▀█\n              █▀ ▄▀ ▄▀▄▄▄▀▀▄ ▀ ▄ ▀▀  ▄▄▀█\n              █▄▄  ▀▀▀██ ▄  █▀▄▀ ▀█▄▄▄▄ █\n              █ ▀▀▄▀█▀▄█ █▀▀▄█ ▀▀█▄▄▀▀ ▀█\n              █ █▀▄▀▄▀ █▀▀▄▀▀▀█ ▀▀▀ █ ▀▄█\n              █▀▀▀▀▀▀▀█▄ █ ▄▀ ▄ █▀█ ▀█▄▀█\n              █ █▀▀▀█ █▀ ▀ ▀▄▄▀ ▀▀▀ ▀█▄▄█\n              █ █   █ █▄▄█  █▀███▄▄▀▄▀  █\n              █ ▀▀▀▀▀ █ ██ ▄█▀ ▀▄ ▄▄▀▄▄ █\n              ▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\n\n  ➜ Local:    http://localhost:3000/\n  ➜ Network:  http://192.168.5.98:3000/ [QR code]\n\nℹ Using Tailwind CSS from ~/assets/css/tailwind.css          nuxt:tailwindcss 8:04:49 AM\n  ➜ DevTools: press Shift + Option + D in the browser (v1.3.9)                 8:04:50 AM\n\nℹ Tailwind Viewer: http://localhost:3000/_tailwind/          nuxt:tailwindcss 8:04:50 AM\n✔ Vite client built in 32ms                                                   8:04:51 AM\n✔ Vite server built in 1286ms                                                 8:04:52 AM\n✔ Nuxt Nitro server built in 1895 ms                                                            nitro 8:05:05 AM\nℹ Vite client warmed up in 0ms                                                                        8:05:05 AM\nℹ Vite server warmed up in 2186ms                                                                     8:05:07 AM\n",[33,7238,7236],{"__ignoreMap":35},[911,7240,7242],{"id":7241},"nuxt-dev-tools","Nuxt Dev Tools",[11,7244,7245,7250],{},[15,7246,7249],{"href":7247,"rel":7248},"https://devtools.nuxt.com/guide/getting-started",[19],"Nuxt DevTools"," is amazing! This is one of the best benefits of upgrading to Nuxt 3 for me.",[11,7252,7253],{},[511,7254],{"alt":7255,"src":7256},"png","/static/nuxt/nuxt_dev_tools.png",[11,7258,7259],{},"It is available in Nuxt 3.9.0 or higher.",[911,7261,7263],{"id":7262},"vue-3-and-script-setup","Vue 3 and Script setup",[11,7265,7266,7267,7269],{},"Vue 3 allows for a much more streamlined single file component syntax with ",[33,7268,6870],{},". For most of the component I did some minimal cleanup and then asked ChatGPT to convert the component script tag to use the new script setup syntax, and that worked very well!",[911,7271,737],{"id":736},[11,7273,7274],{},"My blog is update using the following GitHub Action:",[26,7276,7278],{"className":2507,"code":7277,"language":2509,"meta":35,"style":35},"name: github pages\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"20\"\n\n      - name: Cache dependencies\n        uses: actions/cache@v4\n        with:\n          path: ~/.npm\n          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n          restore-keys: |\n            ${{ runner.os }}-node-\n\n      - run: yarn\n      - run: yarn build\n      - run: yarn lint\n      - run: yarn generate\n\n      - name: deploy\n        uses: peaceiris/actions-gh-pages@v4\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: dist\n",[33,7279,7280,7290,7294,7301,7308,7315,7322,7329,7333,7340,7347,7357,7364,7376,7380,7391,7401,7408,7418,7422,7433,7442,7448,7458,7468,7478,7483,7487,7499,7510,7521,7532,7536,7547,7556,7562,7572],{"__ignoreMap":35},[187,7281,7282,7285,7287],{"class":189,"line":190},[187,7283,7284],{"class":2516},"name",[187,7286,585],{"class":577},[187,7288,7289],{"class":196},"github pages\n",[187,7291,7292],{"class":189,"line":249},[187,7293,316],{"emptyLinePlaceholder":315},[187,7295,7296,7299],{"class":189,"line":312},[187,7297,7298],{"class":588},"on",[187,7300,2520],{"class":577},[187,7302,7303,7306],{"class":189,"line":319},[187,7304,7305],{"class":2516},"  push",[187,7307,2520],{"class":577},[187,7309,7310,7313],{"class":189,"line":325},[187,7311,7312],{"class":2516},"    branches",[187,7314,2520],{"class":577},[187,7316,7317,7319],{"class":189,"line":686},[187,7318,2610],{"class":577},[187,7320,7321],{"class":196},"master\n",[187,7323,7324,7327],{"class":189,"line":697},[187,7325,7326],{"class":2516},"  pull_request",[187,7328,2520],{"class":577},[187,7330,7331],{"class":189,"line":1291},[187,7332,316],{"emptyLinePlaceholder":315},[187,7334,7335,7338],{"class":189,"line":1306},[187,7336,7337],{"class":2516},"jobs",[187,7339,2520],{"class":577},[187,7341,7342,7345],{"class":189,"line":1434},[187,7343,7344],{"class":2516},"  deploy",[187,7346,2520],{"class":577},[187,7348,7349,7352,7354],{"class":189,"line":2599},[187,7350,7351],{"class":2516},"    runs-on",[187,7353,585],{"class":577},[187,7355,7356],{"class":196},"ubuntu-latest\n",[187,7358,7359,7362],{"class":189,"line":2607},[187,7360,7361],{"class":2516},"    steps",[187,7363,2520],{"class":577},[187,7365,7366,7368,7371,7373],{"class":189,"line":2621},[187,7367,2610],{"class":577},[187,7369,7370],{"class":2516},"uses",[187,7372,585],{"class":577},[187,7374,7375],{"class":196},"actions/checkout@v4\n",[187,7377,7378],{"class":189,"line":2631},[187,7379,316],{"emptyLinePlaceholder":315},[187,7381,7382,7384,7386,7388],{"class":189,"line":2642},[187,7383,2610],{"class":577},[187,7385,7284],{"class":2516},[187,7387,585],{"class":577},[187,7389,7390],{"class":196},"Setup Node\n",[187,7392,7393,7396,7398],{"class":189,"line":2653},[187,7394,7395],{"class":2516},"        uses",[187,7397,585],{"class":577},[187,7399,7400],{"class":196},"actions/setup-node@v4\n",[187,7402,7403,7406],{"class":189,"line":2665},[187,7404,7405],{"class":2516},"        with",[187,7407,2520],{"class":577},[187,7409,7410,7413,7415],{"class":189,"line":2674},[187,7411,7412],{"class":2516},"          node-version",[187,7414,585],{"class":577},[187,7416,7417],{"class":196},"\"20\"\n",[187,7419,7420],{"class":189,"line":2684},[187,7421,316],{"emptyLinePlaceholder":315},[187,7423,7424,7426,7428,7430],{"class":189,"line":2694},[187,7425,2610],{"class":577},[187,7427,7284],{"class":2516},[187,7429,585],{"class":577},[187,7431,7432],{"class":196},"Cache dependencies\n",[187,7434,7435,7437,7439],{"class":189,"line":2706},[187,7436,7395],{"class":2516},[187,7438,585],{"class":577},[187,7440,7441],{"class":196},"actions/cache@v4\n",[187,7443,7444,7446],{"class":189,"line":2715},[187,7445,7405],{"class":2516},[187,7447,2520],{"class":577},[187,7449,7450,7453,7455],{"class":189,"line":2725},[187,7451,7452],{"class":2516},"          path",[187,7454,585],{"class":577},[187,7456,7457],{"class":196},"~/.npm\n",[187,7459,7460,7463,7465],{"class":189,"line":2735},[187,7461,7462],{"class":2516},"          key",[187,7464,585],{"class":577},[187,7466,7467],{"class":196},"${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n",[187,7469,7470,7473,7475],{"class":189,"line":2743},[187,7471,7472],{"class":2516},"          restore-keys",[187,7474,585],{"class":577},[187,7476,7477],{"class":573},"|\n",[187,7479,7480],{"class":189,"line":2754},[187,7481,7482],{"class":196},"            ${{ runner.os }}-node-\n",[187,7484,7485],{"class":189,"line":2762},[187,7486,316],{"emptyLinePlaceholder":315},[187,7488,7489,7491,7494,7496],{"class":189,"line":2770},[187,7490,2610],{"class":577},[187,7492,7493],{"class":2516},"run",[187,7495,585],{"class":577},[187,7497,7498],{"class":196},"yarn\n",[187,7500,7501,7503,7505,7507],{"class":189,"line":2781},[187,7502,2610],{"class":577},[187,7504,7493],{"class":2516},[187,7506,585],{"class":577},[187,7508,7509],{"class":196},"yarn build\n",[187,7511,7512,7514,7516,7518],{"class":189,"line":2792},[187,7513,2610],{"class":577},[187,7515,7493],{"class":2516},[187,7517,585],{"class":577},[187,7519,7520],{"class":196},"yarn lint\n",[187,7522,7523,7525,7527,7529],{"class":189,"line":2803},[187,7524,2610],{"class":577},[187,7526,7493],{"class":2516},[187,7528,585],{"class":577},[187,7530,7531],{"class":196},"yarn generate\n",[187,7533,7534],{"class":189,"line":2808},[187,7535,316],{"emptyLinePlaceholder":315},[187,7537,7538,7540,7542,7544],{"class":189,"line":2816},[187,7539,2610],{"class":577},[187,7541,7284],{"class":2516},[187,7543,585],{"class":577},[187,7545,7546],{"class":196},"deploy\n",[187,7548,7549,7551,7553],{"class":189,"line":2824},[187,7550,7395],{"class":2516},[187,7552,585],{"class":577},[187,7554,7555],{"class":196},"peaceiris/actions-gh-pages@v4\n",[187,7557,7558,7560],{"class":189,"line":2834},[187,7559,7405],{"class":2516},[187,7561,2520],{"class":577},[187,7563,7564,7567,7569],{"class":189,"line":2845},[187,7565,7566],{"class":2516},"          github_token",[187,7568,585],{"class":577},[187,7570,7571],{"class":196},"${{ secrets.GITHUB_TOKEN }}\n",[187,7573,7574,7577,7579],{"class":189,"line":2856},[187,7575,7576],{"class":2516},"          publish_dir",[187,7578,585],{"class":577},[187,7580,7581],{"class":196},"dist\n",[11,7583,7584,7585,784,7588,7591,7592,784,7594,1737],{},"When I upgraded to Nuxt 3, I needed to add the ",[33,7586,7587],{},"yarn build",[33,7589,7590],{},"nuxt build",") step in order to run ",[33,7593,794],{},[33,7595,7596],{},"eslint .",[11,7598,7599],{},"It takes about 3 minutes to build the site, most of that time is for building the HTML files for each route:",[11,7601,7602],{},[511,7603],{"alt":7604,"src":7605},"GitHub Action for deploying briancaffey.github.io","/static/nuxt/gha.png",[26,7607,7610],{"className":7608,"code":7609,"language":31},[29],"[log] [nitro]   ├─ /sitemap.xml (26ms)\n[info] [nitro] Prerendered 525 routes in 67.595 seconds\n[success] [nitro] Generated public .output/public\n[success] [nitro] You can preview this build using `npx serve .output/public`\n[success] You can now deploy `.output/public` to any static hosting!\nDone in 81.09s.\n",[33,7611,7609],{"__ignoreMap":35},[11,7613,7614,7615,7618,7619,7622,7623,7626,7627,7630,7631,7633],{},"Shout out to GitHub user ",[33,7616,7617],{},"peaceiris"," for maintaining the ",[33,7620,7621],{},"peaceiris/actions-gh-pages@v4"," GitHub Action. ",[33,7624,7625],{},"dist"," is a symbolic link that links to ",[33,7628,7629],{},".output/public"," where the static build files from ",[33,7632,705],{}," are stored.",[11,7635,7636],{},[511,7637],{"alt":7604,"src":7638},"/static/nuxt/gha_deploy.png",[168,7640,7642],{"id":7641},"data-heavy-articles","Data-heavy articles",[911,7644,7646],{"id":7645},"migrating-from-vuex-to-pinia","Migrating from Vuex to Pinia",[11,7648,7649,7650,7652,7653,7658],{},"Most of my blog articles only include text and images. In some articles I include dynamic content through iframes to other projects on my GitHub that are deployed on subdomains of ",[33,7651,743],{},". Another way to add dynamic content it to write Vue components and then embed those directly in the Nuxt Content Markdown files. I wrote ",[15,7654,7657],{"href":7655,"rel":7656},"https://briancaffey.github.io/2021/01/16/i-scraped-analyzed-and-generated-yc-companies-founders-and-work-at-a-startup-job-postings",[19],"an article about data from YC's Work at a Startup jobs page"," and made components that get data from a data store and then render that data using Apex Charts.",[11,7660,7661],{},"Previoulsy I had used Vuex to do this, but I switched to using Pinia which is Vue's new module for managing state. I use LLMs to convert the store module from Vuex to Pinia and also used LLMs to update the components that use the store, and it worked!",[11,7663,7664],{},"Setting up the plugin for Apex Charts looks like this:",[26,7666,7668],{"className":6362,"code":7667,"language":6364,"meta":35,"style":35},"import VueApexCharts from 'vue3-apexcharts'\n\nexport default defineNuxtPlugin(nuxtApp => {\n    nuxtApp.vueApp.use(VueApexCharts)\n});\n",[33,7669,7670,7684,7688,7707,7718],{"__ignoreMap":35},[187,7671,7672,7675,7678,7681],{"class":189,"line":190},[187,7673,7674],{"class":573},"import",[187,7676,7677],{"class":577}," VueApexCharts ",[187,7679,7680],{"class":573},"from",[187,7682,7683],{"class":196}," 'vue3-apexcharts'\n",[187,7685,7686],{"class":189,"line":249},[187,7687,316],{"emptyLinePlaceholder":315},[187,7689,7690,7692,7694,7697,7699,7702,7705],{"class":189,"line":312},[187,7691,6371],{"class":573},[187,7693,6374],{"class":573},[187,7695,7696],{"class":193}," defineNuxtPlugin",[187,7698,615],{"class":577},[187,7700,7701],{"class":581},"nuxtApp",[187,7703,7704],{"class":573}," =>",[187,7706,6739],{"class":577},[187,7708,7709,7712,7715],{"class":189,"line":319},[187,7710,7711],{"class":577},"    nuxtApp.vueApp.",[187,7713,7714],{"class":193},"use",[187,7716,7717],{"class":577},"(VueApexCharts)\n",[187,7719,7720],{"class":189,"line":325},[187,7721,7722],{"class":577},"});\n",[168,7724,7726],{"id":7725},"lessons-learned","Lessons learned",[911,7728,7730],{"id":7729},"embedding-tweets-in-nuxt-content","Embedding Tweets in Nuxt Content",[11,7732,7733],{},"The upgrade from Nuxt 2 to Nuxt 3 broke some twitter embeds that were working in Nuxt 2 (directly copy and pasting the embed code into a Nuxt Content Markdown file). Here's how I got it working for now:",[916,7735,7736,7747,7765,7775,7778],{},[919,7737,7738,7739,784,7742,343],{},"create a new global component in ",[33,7740,7741],{},"components/content",[15,7743,7746],{"href":7744,"rel":7745},"https://github.com/briancaffey/briancaffey.github.io/blob/master/components/content/aoi/AgentsOfInferenceTweet.vue",[19],"example",[919,7748,7749,7750,7753,7754,7757,7758,7761,7762,343],{},"convert the ",[33,7751,7752],{},"\u003Cscript>"," tag in the embed code to a ",[33,7755,7756],{},"\u003Ccomponent>"," tag and include the ",[33,7759,7760],{},":is=\"'script'\""," (ES Lint will throw an error if you do not have the ",[33,7763,7764],{},"v-bind:is",[919,7766,7767,7768,784,7771,7774],{},"include the component in your Markdown like this: ",[33,7769,7770],{},"\u003CMyComponent>\u003C/MyComponent>",[33,7772,7773],{},"\u003CMyComponent />"," will not work, for me it would not render and also cut off the rest of the content from the page)",[919,7776,7777],{},"The same process works for video embeds from 𝕏",[919,7779,7780,7785],{},[15,7781,7784],{"href":7782,"rel":7783},"https://briancaffey.github.io/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",[19],"This post on my GitHub Pages blog"," uses an embedded tweet.",[911,7787,7789],{"id":7788},"internationalization-i18n","Internationalization (i18n)",[11,7791,7792,7793,765,7796,7799],{},"I added i18n to my site mostly to learn how it works. Nuxt i18n has different strategies for how different locales are displayed. Previously I used a URL prefix for all locales other than the default locale (English). Switching to other locales would switch from ",[33,7794,7795],{},"/contact-me",[33,7797,7798],{},"/zh/contact-me"," for example.",[11,7801,7802,7803,7806],{},"In this upgrade I switched to the ",[33,7804,7805],{},"no_prefix"," option which instead stores the locale in a cookie. This makes generating my site easier because it does not require generating a locale for each blog tag or blog article.",[11,7808,7809],{},"I currently do not have i18n for the articles on my blog, but I'm hoping to add this in a future update once there is better support for it in Nuxt Content.",[168,7811,7813],{"id":7812},"lighthouse","Lighthouse",[11,7815,7816],{},"I made a number of improvements to the site to get an almost-perfect Lighthouse score for the home page of my site:",[11,7818,7819],{},[511,7820],{"alt":7821,"src":7822},"Lighthouse results for briancaffey.github.io","/static/nuxt/lighthouse.png",[916,7824,7825,7835,7843],{},[919,7826,7827,7828,7831,7832,343],{},"using ",[33,7829,7830],{},"@nuxt/image"," for optimized image formats (",[33,7833,7834],{},"webp",[919,7836,7837,7838,343],{},"adjust colors for improved contrast (measured using ",[15,7839,7842],{"href":7840,"rel":7841},"https://webaim.org/resources/contrastchecker/",[19],"webaim.org",[919,7844,7845,7846,7849],{},"fixes for ",[33,7847,7848],{},"head"," metadata",[168,7851,7853],{"id":7852},"interactivity","Interactivity",[11,7855,7856],{},"I use a few plugins for interactity on my site. These plugins needed some slight modifications and upgrades",[911,7858,7860],{"id":7859},"drift","Drift",[11,7862,7863],{},"Drift is a chat box that lets users send me message. When someone sends me a message I can see what page of my website they are on and I can also see their location (based on their IP address). I get messages in the Drift app on my phone. In total I have had 320 conversations since I initially added the plugin a few years ago.",[911,7865,7867],{"id":7866},"mailchimp-email-list","MailChimp email list",[11,7869,7870,7871,7874,7875,7878],{},"I have an email list of 55 people that I manage through MailChimp. Occasionally I send out emails about new articles on my blog and other updates. It is a fun way to practice email marketing! It uses a global component in the ",[33,7872,7873],{},"content/components"," directory so I can use the ",[33,7876,7877],{},"\u003CSubscribe>"," component here in the Markdown file where I am writing this article:",[7880,7881],"br",{},[7883,7884],"subscribe",{},[7880,7886],{},[11,7888,7889],{},"I also use this component in the footer of the site. Feel free to sign up to get updates about what I'm doing on this site!",[911,7891,7893],{"id":7892},"formsubmit","FormSubmit",[11,7895,7896,7897,7901],{},"FormSubmit is a free service that lets people send me a message through a form on my site's ",[15,7898,7900],{"href":7899},"/contact","Contact"," page.",[911,7903,7905],{"id":7904},"disqus","Disqus",[11,7907,7908],{},"Disqus is a comments plugin that I use on my blog articles. I don't get a lot of comments, but comments are always welcome!",[168,7910,7912],{"id":7911},"todo","TODO",[911,7914,7916],{"id":7915},"feedxml","feed.xml",[11,7918,7919,7920,7922,7923,7928],{},"I need to find a way to automate ",[33,7921,7916],{}," generation. ",[15,7924,7927],{"href":7925,"rel":7926},"https://nuxt.com/modules/feed",[19],"The feed module"," is not yet compatible with Nuxt 3. I do use the RSS feed with DEV.to which allows me to set up canonical links back to my GitHub Pages site.",[11,7930,7931,7932,7934],{},"For now I am going to copy the ",[33,7933,7916],{}," to a file in the public directory and update it manually. Here's the entry I'll make for this article:",[26,7936,7938],{"className":6715,"code":7937,"language":6717,"meta":35,"style":35},"        \u003Citem>\n            \u003Ctitle>\n                \u003C![CDATA[ Upgrading my GitHub Pages blog to Nuxt 3 ]]>\n            \u003C/title>\n            \u003Clink>\n                https://briancaffey.github.io/2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3\n            \u003C/link>\n            \u003Cguid>\n                https://briancaffey.github.io/2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3\n            \u003C/guid>\n            \u003Cdescription>\n                \u003C![CDATA[ An overview of my newly upgraded GitHub Pages blog powered by Nuxt 3 ]]>\n            \u003C/description>\n        \u003C/item>\n",[33,7939,7940,7951,7960,7971,7980,7988,7993,8001,8010,8014,8022,8030,8039,8047],{"__ignoreMap":35},[187,7941,7942,7945,7949],{"class":189,"line":190},[187,7943,7944],{"class":577},"        \u003C",[187,7946,7948],{"class":7947},"s7hpK","item",[187,7950,6730],{"class":577},[187,7952,7953,7956,7958],{"class":189,"line":249},[187,7954,7955],{"class":577},"            \u003C",[187,7957,6537],{"class":2516},[187,7959,6730],{"class":577},[187,7961,7962,7965,7968],{"class":189,"line":312},[187,7963,7964],{"class":577},"                \u003C![CDATA[",[187,7966,7967],{"class":196}," Upgrading my GitHub Pages blog to Nuxt 3 ",[187,7969,7970],{"class":577},"]]>\n",[187,7972,7973,7976,7978],{"class":189,"line":319},[187,7974,7975],{"class":577},"            \u003C/",[187,7977,6537],{"class":2516},[187,7979,6730],{"class":577},[187,7981,7982,7984,7986],{"class":189,"line":325},[187,7983,7955],{"class":577},[187,7985,6654],{"class":2516},[187,7987,6730],{"class":577},[187,7989,7990],{"class":189,"line":686},[187,7991,7992],{"class":577},"                https://briancaffey.github.io/2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3\n",[187,7994,7995,7997,7999],{"class":189,"line":697},[187,7996,7975],{"class":577},[187,7998,6654],{"class":7947},[187,8000,6730],{"class":577},[187,8002,8003,8005,8008],{"class":189,"line":1291},[187,8004,7955],{"class":577},[187,8006,8007],{"class":7947},"guid",[187,8009,6730],{"class":577},[187,8011,8012],{"class":189,"line":1306},[187,8013,7992],{"class":577},[187,8015,8016,8018,8020],{"class":189,"line":1434},[187,8017,7975],{"class":577},[187,8019,8007],{"class":7947},[187,8021,6730],{"class":577},[187,8023,8024,8026,8028],{"class":189,"line":2599},[187,8025,7955],{"class":577},[187,8027,6560],{"class":7947},[187,8029,6730],{"class":577},[187,8031,8032,8034,8037],{"class":189,"line":2607},[187,8033,7964],{"class":577},[187,8035,8036],{"class":196}," An overview of my newly upgraded GitHub Pages blog powered by Nuxt 3 ",[187,8038,7970],{"class":577},[187,8040,8041,8043,8045],{"class":189,"line":2621},[187,8042,7975],{"class":577},[187,8044,6560],{"class":7947},[187,8046,6730],{"class":577},[187,8048,8049,8052,8054],{"class":189,"line":2631},[187,8050,8051],{"class":577},"        \u003C/",[187,8053,7948],{"class":7947},[187,8055,6730],{"class":577},[911,8057,8059],{"id":8058},"console-errors","Console errors",[11,8061,8062],{},"I have tried to clean up as many of the errors as I could, but there are stil some that I see in the dev console. Here is one of the issues that puzzles me:",[11,8064,8065,8068,8069,8071],{},[33,8066,8067],{},"Hydration completed but contains mismatches.",": I only see this error on the production build; I don't see it when running ",[33,8070,400],{},". As I understand, this error message means that the HTML that was built during prerendering is not the same as the HTML on the site once the Javascript has all been loaded.",[911,8073,8075],{"id":8074},"build-errors","Build errors",[11,8077,8078,8079,8082,8083,8086],{},"I recently got some errors in my CI/CD pipeline related to the ",[33,8080,8081],{},"string-width"," package, and I was able to add the following to ",[33,8084,8085],{},"package.json"," to fix the build pipeline:",[26,8088,8090],{"className":6362,"code":8089,"language":6364,"meta":35,"style":35},"  \"resolutions\": {\n      \"string-width\": \"4.2.3\"\n  }\n",[33,8091,8092,8100,8110],{"__ignoreMap":35},[187,8093,8094,8097],{"class":189,"line":190},[187,8095,8096],{"class":196},"  \"resolutions\"",[187,8098,8099],{"class":577},": {\n",[187,8101,8102,8105,8107],{"class":189,"line":249},[187,8103,8104],{"class":196},"      \"string-width\"",[187,8106,585],{"class":577},[187,8108,8109],{"class":196},"\"4.2.3\"\n",[187,8111,8112],{"class":189,"line":312},[187,8113,6847],{"class":577},[11,8115,8116],{},"I'm still not exactly sure what this is about.",[911,8118,8120],{"id":8119},"refactoring","Refactoring",[11,8122,8123,8124,8127],{},"I like using TailwindCSS, and I was able to use it to build a responsive design for my site. After upgrading to Nuxt 3, I feel like most of the technical debt is now in the design. I also don't change the design that often, but I think I could do a lot to refactor the use of Tailwind, such as using ",[33,8125,8126],{},"@apply"," in CSS to make classes more DRY across the different components I use to build this site.",[11,8129,8130],{},"Please let me know if you have any questions, suggestions or tips for using Nuxt and Vue to build static prerendered sites. Thanks for reading!",[855,8132,8133],{},"html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .s4XuR, html code.shiki .s4XuR{--shiki-default:#E36209;--shiki-dark:#FFAB70}html pre.shiki code .s7hpK, html code.shiki .s7hpK{--shiki-default:#B31D28;--shiki-default-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic}",{"title":35,"searchDepth":249,"depth":249,"links":8135},[8136,8137,8145,8148,8152,8153,8159],{"id":6322,"depth":249,"text":6323},{"id":6348,"depth":249,"text":6349,"children":8138},[8139,8140,8141,8142,8143,8144],{"id":6352,"depth":312,"text":6353},{"id":6518,"depth":312,"text":6519},{"id":7206,"depth":312,"text":7207},{"id":7241,"depth":312,"text":7242},{"id":7262,"depth":312,"text":7263},{"id":736,"depth":312,"text":737},{"id":7641,"depth":249,"text":7642,"children":8146},[8147],{"id":7645,"depth":312,"text":7646},{"id":7725,"depth":249,"text":7726,"children":8149},[8150,8151],{"id":7729,"depth":312,"text":7730},{"id":7788,"depth":312,"text":7789},{"id":7812,"depth":249,"text":7813},{"id":7852,"depth":249,"text":7853,"children":8154},[8155,8156,8157,8158],{"id":7859,"depth":312,"text":7860},{"id":7866,"depth":312,"text":7867},{"id":7892,"depth":312,"text":7893},{"id":7904,"depth":312,"text":7905},{"id":7911,"depth":249,"text":7912,"children":8160},[8161,8162,8163,8164],{"id":7915,"depth":312,"text":7916},{"id":8058,"depth":312,"text":8059},{"id":8074,"depth":312,"text":8075},{"id":8119,"depth":312,"text":8120},"2024-08-11","An overview of my newly upgraded GitHub Pages blog powered by Nuxt 3","/static/nuxt/new-site.png",{},"/2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3",{"title":6317,"description":8166},"2024/08/11/upgrading-my-github-pages-blog-to-nuxt-3",[882,881,8173,8174],"github","pinia","Qj1PoHSnHKDRT66bUSuFYwpiB0zDjVRbi5Vx3H-jPpk",{"id":8177,"title":8178,"body":8179,"comments":315,"date":8440,"description":8441,"draft":872,"extension":873,"external":8442,"image":8445,"meta":8446,"navigation":315,"path":8447,"seo":8448,"stem":8449,"tags":8450,"__hash__":8459},"blog/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update.md","Agents of Inference: Speed of Light -- Accelerating my Generative AI Agents project with NVIDIA NIMs, TensorRT and TensorRT-LLM",{"type":8,"value":8180,"toc":8433},[8181,8185,8192,8195,8198,8206,8210,8213,8220,8226,8233,8239,8245,8248,8254,8257,8263,8266,8272,8304,8307,8313,8316,8322,8331,8335,8338,8344,8351,8357,8360,8366,8369,8373,8382,8385,8391,8394,8400,8403,8409,8412,8415,8419,8422,8427,8430],[168,8182,8184],{"id":8183},"tldr","tl;dr",[11,8186,8187,8188,752],{},"\"Agents of Inference: Speed of Light\" is an update to my original entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This update focuses on how I accelerated local text, image and video generation using TensorRT, TensorRT-LLM and NVIDIA NIMs. You can read the original article about \"Agents of Inference\" ",[15,8189,1321],{"href":8190,"rel":8191},"https://briancaffey.github.io/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",[19],[11,8193,8194],{},"Here's my original project submission post on 𝕏 that introduces the idea of generating short 007-style films using agents, LLMs and stable diffusion:",[8196,8197],"agents-of-inference-tweet",{},[11,8199,8200,8201,752],{},"Here's a link to the ",[15,8202,8205],{"href":8203,"rel":8204},"https://github.com/briancaffey/agents-of-inference",[19],"Agents of Inference code repository on GitHub",[168,8207,8209],{"id":8208},"nvidia-nim-inference-microservices","NVIDIA NIM inference microservices",[11,8211,8212],{},"I thought NVIDIA NIMs was one of the most exciting announcements from GTC 2024. I'm a big fan of using docker containers everywhere, and the idea of standardizing NVIDIA tools and dependencies seemed to make a lot of sense. I had previously struggled to get TensorRT-LLM installed on Windows using example repos provided by NVIDIA.",[11,8214,8215,8216,8219],{},"A few weeks ago NVIDIA announced that NVIDIA NIMs can be downloaded and run anywhere. I was able to download this NIM for the ",[33,8217,8218],{},"meta/llama3-8b-instruct"," model:",[11,8221,8222],{},[511,8223],{"alt":8224,"src":8225},"llama3 nim","/static/aoi/meta-llama3-nim.png",[11,8227,8228,8229,8232],{},"Here are the logs for my NVIDIA NIM ",[33,8230,8231],{},"Meta/Llama-3-8B-Instruct"," running in docker container on Windows Subsystem for Linux on my NVIDIA GeForce RTX 4090 GPU-powered PC. Notice that it generates over 50 tokens per second!",[11,8234,8235],{},[511,8236],{"alt":8237,"src":8238},"trt llama3 local","/static/aoi/trt-llama3.png",[11,8240,8241],{},[511,8242],{"alt":8243,"src":8244},"token factory","/static/aoi/token-factory.png",[11,8246,8247],{},"The one main hurdle I faced when running the NIM local was an error about no runnable profiles being available:",[26,8249,8252],{"className":8250,"code":8251,"language":31},[29],"ERROR 06-23 15:41:21.19 utils.py:21] Could not find a profile that is currently runnable with the detected hardware. Please check the system information below and make sure you have enough free GPUs.\nSYSTEM INFO\n- Free GPUs: \u003CNone>\n- Non-free GPUs:\n  -  [2684:10de] (0) NVIDIA GeForce RTX 4090 [current utilization: 7%]\n",[33,8253,8251],{"__ignoreMap":35},[11,8255,8256],{},"This seemed odd, and I found another user with the same issue on the NVIDIA Developer Forum. I was able to get around this by going into the EUFI/BIOS of my PC and switch to integrated graphics:",[11,8258,8259],{},[511,8260],{"alt":8261,"src":8262},"bios","/static/aoi/bios.jpg",[11,8264,8265],{},"It was great to be able to run \"Agents of Inference\" using NVIDIA NIM because it is just as simple as running a docker container:",[26,8267,8270],{"className":8268,"code":8269,"language":31},[29],"export CONTAINER_NAME=llama3-8b-instruct\nexport IMG_NAME=\"nvcr.io/nim/meta/${CONTAINER_NAME}:1.0.0\"\nexport LOCAL_NIM_CACHE=~/.cache/nim\nmkdir -p \"$LOCAL_NIM_CACHE\"\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus all \\\n  --shm-size=16GB \\\n  -e NGC_API_KEY \\\n  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n  -u $(id -u) \\\n  -p 8000:8000 \\\n  $IMG_NAME\n",[33,8271,8269],{"__ignoreMap":35},[11,8273,8274,8275,8278,8279,8284,8285,8288,8289,8296,8297,8299,8300,8303],{},"Before getting this to work, I was able to get a ",[33,8276,8277],{},"/chat/completions"," endpoint working with the Llama3 model on my fork of the ",[15,8280,8283],{"href":8281,"rel":8282},"https://github.com/briancaffey/trt-llm-as-openai-windows/commit/edaa15fd026fe95e645e3d4ae9718dc3ecc3bb65",[19],"trt-llm-as-openai-windows",". I borrowed code for the ",[33,8286,8287],{},"TrtLlmAPI"," from the ",[15,8290,8293],{"href":8291,"rel":8292},"https://github.com/NVIDIA/ChatRTX",[19],[33,8294,8295],{},"NVIDIA/ChatRTX"," repo and a function from ",[33,8298,6309],{}," called ",[33,8301,8302],{},"messages_to_prompt_v3_instruct"," which encodes messages with special tokens for chat. This was an interesting exercise and it taught me a lot about how LLMs do chat. I would like to continue working on this fork and see how to implement streaming endpoints for the Llama 3 model.",[11,8305,8306],{},"Here is how Llama 3 does the instruct prompting:",[26,8308,8311],{"className":8309,"code":8310,"language":31},[29],"\u003C|begin_of_text|>\u003C|start_header_id|>system\u003C|end_header_id|>\n\nYou are a helpful AI assistant for travel tips and recommendations\u003C|eot_id|>\u003C|start_header_id|>user\u003C|end_header_id|>\n\nWhat can you help me with?\u003C|eot_id|>\u003C|start_header_id|>assistant\u003C|end_header_id|>\n",[33,8312,8310],{"__ignoreMap":35},[11,8314,8315],{},"Compare this with how it was done with Llama2 chat:",[26,8317,8320],{"className":8318,"code":8319,"language":31},[29],"\u003Cs>[INST] \u003C\u003CSYS>>\n{{ system_prompt }}\n\u003C\u003C/SYS>>\n\n{{ user_message_1 }} [/INST] {{ model_answer_1 }} \u003C/s>\n\u003Cs>[INST] {{ user_message_2 }} [/INST]\n",[33,8321,8319],{"__ignoreMap":35},[11,8323,8324,8325,8330],{},"You can read more about the difference between Llama 2 and 3 on the ",[15,8326,8329],{"href":8327,"rel":8328},"https://llama.meta.com/docs/model-cards-and-prompt-formats",[19],"Model Card & Prompt formats"," page on Meta's Llama website.",[168,8332,8334],{"id":8333},"langsmith","LangSmith",[11,8336,8337],{},"I recently started using LangSmith. It is an awesome product and it ties in really well to doing prototype work like in my project \"Agents of Inference\". I wish I had started using it earlier in my development cycle! All you need to do is add an API key to your environment and your application automatically starts tracing LLM calls. It also works well with LangGraph and allows you to trace the execution path of your graph. Also it is good to be aware that there are other products similar to LangSmith like LangFuse. I also saw a really neat demo from Datadog at GTC showing an alpha version of their LLM tracing and observability product.",[11,8339,8340],{},[511,8341],{"alt":8342,"src":8343},"langsmith screenshot","/static/aoi/langsmith.png",[11,8345,8346,8347,8350],{},"LangSmith can also be helpful when the wrong JSON shape is parsed. I had a lot of difficulty with this in my project. When I used the Q4_K_M gguf quantized ",[33,8348,8349],{},"Meta-Llama-3 8B-Instruct"," model I had no issues with output parsing. Switching to the TensorRT-LLM model provided by the NIM resulted in some parsing errors. The application would report that JSON could not be parsed because the result contained text like: \"Here is the JSON that you requested\". I was able to get around this by changing the prompt template from:",[26,8352,8355],{"className":8353,"code":8354,"language":31},[29],"Answer the user query.\n",[33,8356,8354],{"__ignoreMap":35},[11,8358,8359],{},"to",[26,8361,8364],{"className":8362,"code":8363,"language":31},[29],"Don't include ANYTHING except for valid JSON in your response. Answer the user query.\n",[33,8365,8363],{"__ignoreMap":35},[11,8367,8368],{},"This was the most frustrating part of development, and I'm still getting occasional errors that I just skip over. I'm also probably have not exhausted all of the tools that LangChain provides to avoid these types of errors. Don't assume that output parsing that works with one model will work with another! This is another good reason to use something like LangSmith when developing LLM-based applications.",[168,8370,8372],{"id":8371},"comfyui-tensorrt","ComfyUI TensorRT",[11,8374,8375,8376,8381],{},"My goal with \"Agents of Inference\" was to be able to test out how small upstream prompt changes can impact the quality and consistency of a series of generated images and videos. Iteration speed is very important! I was able to significantly speed up image and video generation by using the ",[15,8377,8380],{"href":8378,"rel":8379},"https://github.com/comfyanonymous/ComfyUI_TensorRT",[19],"ComfyUI TensorRT custom nodes",". These nodes allow you to build engines with specifications for parameters that can be either static or dynamic. I had better luck with building dynamic engines. I was able to build and use engines for Stable Diffusion SDXL and Stable Video Diffusion XT.",[11,8383,8384],{},"Building a TensorRT engine for ComfyUI can be done using the following workflow:",[11,8386,8387],{},[511,8388],{"alt":8389,"src":8390},"trt comfyUI build process","/static/aoi/comfyui-trt-svd-xt.png",[11,8392,8393],{},"The engines can then be used in custom workflows like the following:",[11,8395,8396],{},[511,8397],{"alt":8398,"src":8399},"trt comfyui workflow","/static/aoi/svd-workflow-trt.png",[11,8401,8402],{},"Once these workflows are configured and are working as expected, you can export them in API format (JSON) and use them to make API calls to the ComfyUI backend. The agents for stable diffusion and stable video diffusion made API calls in this way and it worked pretty well.",[11,8404,8405],{},[511,8406],{"alt":8407,"src":8408},"comfy its","/static/aoi/comfy-its.png",[11,8410,8411],{},"Using 50 iterations, I was able to generate 1024x576 images in 3 seconds or about 19 iterations per second (it/s). Videos",[11,8413,8414],{},"ComfyUI is still early in development and it refers to itself as \"alpha software\" even though it has a large adoption by a very active community already. I'm excited to see what is next from the developers of ComfyUI.",[168,8416,8418],{"id":8417},"speed-of-light","Speed of Light",[11,8420,8421],{},"\"Speed of Light\" is a term that I learned from a stable diffusion talk at GTC.",[107,8423,8424],{},[11,8425,8426],{},"SOL analysis reveals how your code performs, and device utilization compared to relevant maximums.",[11,8428,8429],{},"Adding TensorRT and TensorRT-LLM to inference services on my RTX PC helped increase the throughput of text, image and video generation for my \"Agents of Inference\" project. I'm looking forward to learning more about profiling and optimization techniques for both LLMs and Stable Diffusion workloads.",[11,8431,8432],{},"Thanks again to NVIDIA and LangChain for organizing this contest! It was a lot of fun to learn about builing agents with LangChain and LangGraph and the latest developments from NVIDIA in Generative AI.",{"title":35,"searchDepth":249,"depth":249,"links":8434},[8435,8436,8437,8438,8439],{"id":8183,"depth":249,"text":8184},{"id":8208,"depth":249,"text":8209},{"id":8333,"depth":249,"text":8334},{"id":8371,"depth":249,"text":8372},{"id":8417,"depth":249,"text":8418},"2024-06-24","This article is a brief discusion on recent updates to my project for the Generative AI Agents Developer Contest by NVIDIA and LangChain",[8443],{"link":8444,"site":2196},"https://x.com/briancaffey/status/1802754703207583886","/static/aoi/aoi_title.png",{},"/2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",{"title":8178,"description":8441},"2024/06/24/agents-of-inference-speed-of-light-nvidia-langchain-generative-ai-agents-developer-contest-update",[2203,8451,4076,2205,8452,8453,6311,2204,4077,8454,8455,8456,8457,8458],"langchain","gpu","tensorrt","llama","007","stable-diffusion","stable-video-diffusion","comfyui","IsVd9Qb_3DZvMylagXMt7xM1d0zC9SVRxuvMqVS_PS8",{"id":8461,"title":8462,"body":8463,"comments":315,"date":9183,"description":9184,"draft":872,"extension":873,"external":9185,"image":8445,"meta":9187,"navigation":315,"path":9188,"seo":9189,"stem":9190,"tags":9191,"__hash__":9192},"blog/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest.md","Agents of Inference: My submission for NVIDIA's Generative AI Agents Developer Contest by NVIDIA and LangChain",{"type":8,"value":8464,"toc":9162},[8465,8469,8475,8477,8480,8483,8485,8490,8494,8497,8501,8504,8508,8511,8537,8543,8546,8550,8553,8561,8569,8576,8639,8642,8651,8654,8749,8764,8768,8771,8835,8849,8854,8857,8861,8864,8912,8916,8919,8980,8986,8989,8992,9023,9034,9038,9049,9072,9075,9078,9081,9088,9094,9098,9101,9104,9107,9111,9114,9118,9121,9125,9128,9134,9137,9141,9147,9150,9153,9156,9159],[168,8466,8468],{"id":8467},"update","Update",[11,8470,8471,8472],{},"I recently posted another article about optimizing this project with TensorRT and TensorRT-LLM running on local NVIDIA NIM inference microservices, please have a look here: ",[15,8473,7782],{"href":7782,"rel":8474},[19],[168,8476,8184],{"id":8183},[11,8478,8479],{},"“Agents of Inference” is my entry for the Generative AI Agents Developer Contest by NVIDIA and LangChain. This project aims to integrate techniques for generating text, images and video to create an application capable of producing short thematic films. In this article, I will detail how I developed the project leveraging LangGraph—a library for building stateful, multi-actor applications with LLMs--and hybrid AI workflows using NVIDIA AI-powered tools and technologies running on RTX PCs and in the cloud.",[11,8481,8482],{},"Here's my project submission post on 𝕏:",[8196,8484],{},[11,8486,8200,8487,752],{},[15,8488,8205],{"href":8203,"rel":8489},[19],[168,8491,8493],{"id":8492},"nvidias-generative-ai-agents-developer-contest","NVIDIA's Generative AI Agents Developer Contest",[11,8495,8496],{},"AI agents are having a moment. They are the building blocks for building \"applications that reason\", and LangChain is a company that provides a comprehensive set of tools for developing, deploying and monitoring AI agents. I have struggled to understand how I can build or use agents in my own projects, and with the contest I have been able to just scratch the surface of what is possible with AI agents--but I think it is a promising paradigm for developing AI-driven applications.",[168,8498,8500],{"id":8499},"coming-up-with-an-idea","Coming up with an idea",[11,8502,8503],{},"I love stable diffusion. I closely follow the development of the three leading applications for generating images with stable dissuion models: Stable Diffusion WebUI, InvokeAI and ComfyUI. Write a prompt, instantly see the result, tweak the prompt and generate again. This is the basic process by which I have previously used stable diffusion. It is a satisfying mental exercise that feeds the creative and imaginative part of my brain. My idea for this project came from wanting to automate this process: use large language models to build cohesive scenes and detailed prompts and then feed them into my stable diffusion programs via API. Using LangChain and LangGraph allowed me to rapidly prototype the idea and start generating short feature films in the style of my favorite British Secret Agent: 007.",[168,8505,8507],{"id":8506},"putting-together-the-puzzle-pieces","Putting together the puzzle pieces",[11,8509,8510],{},"Here's how I set up an MVP for my project project to get started. I set up a simple graph (a linked list, really) that included the following nodes. *Important: in this context, a node is an agent, and that agent is a simple Python function. It takes one parameter which is the state, a Python dictionary, that holds the output of LLM calls that the agents make. Not all nodes make LLM calls, some just run basic functions like initializing directories or calling external stable diffusion APIs.",[916,8512,8513,8516,8519,8522,8525,8528,8531,8534],{},[919,8514,8515],{},"Casting Agent → come up with some characters",[919,8517,8518],{},"Location Agent → come up with some locations",[919,8520,8521],{},"Synopsis Agent → write a synopsis based on the characters and locations",[919,8523,8524],{},"Scene Agent → write some number of scenes based on the synopsis based on the synopsis",[919,8526,8527],{},"Shot agent → describe some number of camera shots for each scene based on the scene",[919,8529,8530],{},"Photography agent → take each shot description and generate and image",[919,8532,8533],{},"Videography agent → take each image generated by the photography agent and convert it to a 4 second clip using stable video diffusion",[919,8535,8536],{},"Editor agent → compile the movie clips together",[11,8538,8539],{},[511,8540],{"alt":8541,"src":8542},"simple graph of agents of inference","/static/aoi/graph.png",[11,8544,8545],{},"It may look simple, but there is a lot going on in this graph.",[911,8547,8549],{"id":8548},"casting-and-location","Casting and Location",[11,8551,8552],{},"The first two agents in my graph are tasked with generating characters and locations that would appear in a British secret agent film. The prompts used for these agents are as follows:",[107,8554,8555],{},[11,8556,8557,8560],{},[338,8558,8559],{},"casting",": \"Come up with four to five characters who will appear in an upcoming British spy movie. The list should include the main character who is male, the villain, an attractive female actress who eventually falls in love with the main character, and some other characters as well.\"",[107,8562,8563],{},[11,8564,8565,8568],{},[338,8566,8567],{},"locations",": \"Provide three main locations that can be used in an international British Spy movie. The locations should include a variety of cities, remote environments, iconic landmarks, etc. The locations should make for good background scenes for an action movie with lots of stunts, chases, explosions, fights, etc. and other things you would find in an action movie. Be sure to include the country and a description of the environment where these places are.\"",[11,8570,8571,8572,8575],{},"These agents leverage the LangChain Expression Language (LCEL) to generate ",[338,8573,8574],{},"structured output"," based on Pydantic models. For",[26,8577,8579],{"className":1383,"code":8578,"language":1125,"meta":35,"style":35},"class Character(BaseModel):\n    \"\"\"\n    The type for character that the casting agent casts for a role in the movie\n    \"\"\"\n    full_name: str = Field(description=\"The character's name\")\n    short_name: str = Field(description=\"The character's short name\")\n    background: str = Field(description=\"The character's background\")\n    physical_traits: str = Field(description=\"The physical traits of the character\")\n    ethnicity: str = Field(description=\"The character's ethnicity\")\n    gender: str = Field(description=\"The character's gender, either male of female\")\n    nationality: str = Field(description=\"The character's nationality\")\n    main_character: bool = Field(description=\"If the character is or is not the main character\")\n\n",[33,8580,8581,8586,8590,8595,8599,8604,8609,8614,8619,8624,8629,8634],{"__ignoreMap":35},[187,8582,8583],{"class":189,"line":190},[187,8584,8585],{},"class Character(BaseModel):\n",[187,8587,8588],{"class":189,"line":249},[187,8589,4793],{},[187,8591,8592],{"class":189,"line":312},[187,8593,8594],{},"    The type for character that the casting agent casts for a role in the movie\n",[187,8596,8597],{"class":189,"line":319},[187,8598,4793],{},[187,8600,8601],{"class":189,"line":325},[187,8602,8603],{},"    full_name: str = Field(description=\"The character's name\")\n",[187,8605,8606],{"class":189,"line":686},[187,8607,8608],{},"    short_name: str = Field(description=\"The character's short name\")\n",[187,8610,8611],{"class":189,"line":697},[187,8612,8613],{},"    background: str = Field(description=\"The character's background\")\n",[187,8615,8616],{"class":189,"line":1291},[187,8617,8618],{},"    physical_traits: str = Field(description=\"The physical traits of the character\")\n",[187,8620,8621],{"class":189,"line":1306},[187,8622,8623],{},"    ethnicity: str = Field(description=\"The character's ethnicity\")\n",[187,8625,8626],{"class":189,"line":1434},[187,8627,8628],{},"    gender: str = Field(description=\"The character's gender, either male of female\")\n",[187,8630,8631],{"class":189,"line":2599},[187,8632,8633],{},"    nationality: str = Field(description=\"The character's nationality\")\n",[187,8635,8636],{"class":189,"line":2607},[187,8637,8638],{},"    main_character: bool = Field(description=\"If the character is or is not the main character\")\n",[11,8640,8641],{},"LCEL offers wonderful syntactic sugar, I can use this model in a parse and pip that into the output from the mode:",[26,8643,8645],{"className":1383,"code":8644,"language":1125,"meta":35,"style":35},"chain = prompt | model | parser\n",[33,8646,8647],{"__ignoreMap":35},[187,8648,8649],{"class":189,"line":190},[187,8650,8644],{},[11,8652,8653],{},"This results in our structured data:",[26,8655,8659],{"className":8656,"code":8657,"language":8658,"meta":35,"style":35},"language-yml shiki shiki-themes github-light github-dark","cast:\n- background: Former MI6 agent\n  ethnicity: British\n  full_name: James Alexander\n  gender: Male\n  main_character: true\n  nationality: British\n  physical_traits: Tall, dark hair, blue eyes\n  short_name: Jamie\n","yml",[33,8660,8661,8668,8681,8691,8701,8711,8720,8729,8739],{"__ignoreMap":35},[187,8662,8663,8666],{"class":189,"line":190},[187,8664,8665],{"class":2516},"cast",[187,8667,2520],{"class":577},[187,8669,8670,8673,8676,8678],{"class":189,"line":249},[187,8671,8672],{"class":577},"- ",[187,8674,8675],{"class":2516},"background",[187,8677,585],{"class":577},[187,8679,8680],{"class":196},"Former MI6 agent\n",[187,8682,8683,8686,8688],{"class":189,"line":312},[187,8684,8685],{"class":2516},"  ethnicity",[187,8687,585],{"class":577},[187,8689,8690],{"class":196},"British\n",[187,8692,8693,8696,8698],{"class":189,"line":319},[187,8694,8695],{"class":2516},"  full_name",[187,8697,585],{"class":577},[187,8699,8700],{"class":196},"James Alexander\n",[187,8702,8703,8706,8708],{"class":189,"line":325},[187,8704,8705],{"class":2516},"  gender",[187,8707,585],{"class":577},[187,8709,8710],{"class":196},"Male\n",[187,8712,8713,8716,8718],{"class":189,"line":686},[187,8714,8715],{"class":2516},"  main_character",[187,8717,585],{"class":577},[187,8719,2530],{"class":588},[187,8721,8722,8725,8727],{"class":189,"line":697},[187,8723,8724],{"class":2516},"  nationality",[187,8726,585],{"class":577},[187,8728,8690],{"class":196},[187,8730,8731,8734,8736],{"class":189,"line":1291},[187,8732,8733],{"class":2516},"  physical_traits",[187,8735,585],{"class":577},[187,8737,8738],{"class":196},"Tall, dark hair, blue eyes\n",[187,8740,8741,8744,8746],{"class":189,"line":1306},[187,8742,8743],{"class":2516},"  short_name",[187,8745,585],{"class":577},[187,8747,8748],{"class":196},"Jamie\n",[11,8750,8751,8752,8755,8756,8763],{},"I saved the state for all \"Agents of Inference\" invocations in the ",[33,8753,8754],{},"output"," directory of my ",[15,8757,8760],{"href":8758,"rel":8759},"https://github.com/briancaffey/agents-of-inference/tree/main/output",[19],[33,8761,8762],{},"agents-of-inference"," GitHub repo. I didn't commit the images and videos, but you can follow @AgentInference on X to see more of the results from my development process and future improvements, as well!",[911,8765,8767],{"id":8766},"synopsis-agent","Synopsis Agent",[11,8769,8770],{},"With a cast of characters and locations selected, we need a synopsis to determine what happens. Here's the prompt:",[26,8772,8774],{"className":2507,"code":8773,"language":2509,"meta":35,"style":35},"synopsis: |\n  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n",[33,8775,8776,8785,8790,8795,8800,8805,8810,8815,8820,8825,8830],{"__ignoreMap":35},[187,8777,8778,8781,8783],{"class":189,"line":190},[187,8779,8780],{"class":2516},"synopsis",[187,8782,585],{"class":577},[187,8784,7477],{"class":573},[187,8786,8787],{"class":189,"line":249},[187,8788,8789],{"class":196},"  Generate a synopsis for a British spy agent movie in the style of the James Bond series. The synopsis should include the following elements:\n",[187,8791,8792],{"class":189,"line":312},[187,8793,8794],{"class":196},"  Protagonist: A charismatic and skilled British secret agent with a code name (e.g., \"Agent X\") who works for a top-secret government agency (e.g., MI6).\n",[187,8796,8797],{"class":189,"line":319},[187,8798,8799],{"class":196},"  Antagonist: A formidable villain with a grand, sinister plan that threatens global security. The antagonist should have a unique, memorable persona and a well-defined motivation.\n",[187,8801,8802],{"class":189,"line":325},[187,8803,8804],{"class":196},"  Mission: Outline the high-stakes mission that the protagonist must undertake to thwart the antagonist’s plan.\n",[187,8806,8807],{"class":189,"line":686},[187,8808,8809],{"class":196},"  Gadgets and Vehicles: Mention the cutting-edge gadgets and vehicles that the protagonist uses throughout the mission. These should be inventive and integral to the plot.\n",[187,8811,8812],{"class":189,"line":697},[187,8813,8814],{"class":196},"  Action Sequences: Include a brief description of some thrilling action sequences, such as car, boat, plane chases, hand-to-hand combat, and daring escapes, and dangerous situations.\n",[187,8816,8817],{"class":189,"line":1291},[187,8818,8819],{"class":196},"  Big Reveal: There is a big reveal toward the end of the storyline that is surprising and the reveal helps to move the story along.\n",[187,8821,8822],{"class":189,"line":1306},[187,8823,8824],{"class":196},"  Climactic Showdown: Describe the final confrontation between the protagonist and the antagonist. This should be intense and action-packed, leading to a satisfying resolution. Should include details about the main character is victorious.\n",[187,8826,8827],{"class":189,"line":1434},[187,8828,8829],{"class":196},"  Setting: Ensure that the settings are diverse and visually striking, adding to the overall excitement and suspense of the story. This should involve multiple locations in exotic environments, the wilderness, in dangerous situations, on board planes, trains, boats and fancy cars, etc.\n",[187,8831,8832],{"class":189,"line":2599},[187,8833,8834],{"class":196},"  Tone and Style: Maintain the sophisticated, suave, and adventurous tone that is characteristic of the James Bond series. Include elements of intrigue, romance, and humor.\n",[11,8836,8837,8838,8841,8842,8845,8846,8848],{},"The synopsis to any good film is key, so I decided to use a feature of LangGraph that would allow a ",[33,8839,8840],{},"synopsis_review_agent"," to provide multiple rounds of feedback to the ",[33,8843,8844],{},"synopsis_agent"," to make it even better. Here's what the new graph look like after implementing the ",[33,8847,8840],{}," using conditional graph edges:",[11,8850,8851],{},[511,8852],{"alt":8840,"src":8853},"/static/aoi/graph_with_cycle.png",[11,8855,8856],{},"Conditional edges are a very powerful feature and I just used it in one part of my graph. Other parts of the graph could benefit from this as well, and they can allow for \"human-in-the-loop\" interactions which are becoming very popular in AI-powered applications.",[911,8858,8860],{"id":8859},"scene-and-shot-agents","Scene and shot agents",[11,8862,8863],{},"With our perfected synopsis, we are ready to put more agents to work. The scene agent builds out the basic structure of the storyline. It provides a structured list of the main sections of the movie. The shot agent then loops over the scenes and creates a number of different shots for the given scene. This was an effective way to have consistent thematic content for shots within a scene. Here are the prompts I used for the scene and shot agents:",[26,8865,8867],{"className":2507,"code":8866,"language":2509,"meta":35,"style":35},"scenes: |\n  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\nshot: |\n  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",[33,8868,8869,8878,8883,8888,8893,8898,8907],{"__ignoreMap":35},[187,8870,8871,8874,8876],{"class":189,"line":190},[187,8872,8873],{"class":2516},"scenes",[187,8875,585],{"class":577},[187,8877,7477],{"class":573},[187,8879,8880],{"class":189,"line":249},[187,8881,8882],{"class":196},"  Create a list of detailed scenes for an exciting and entertaining British spy film. The scenes should be comprehensive and include all scenes necessary for a complete film. Each scene should include the following elements:\n",[187,8884,8885],{"class":189,"line":312},[187,8886,8887],{"class":196},"  Location: Describe the location and setting of the scene, including any notable landmarks, time of day, and general atmosphere.\n",[187,8889,8890],{"class":189,"line":319},[187,8891,8892],{"class":196},"  Characters Involved: List the main characters present in the scene, with a brief description of their roles and appearances.\n",[187,8894,8895],{"class":189,"line":325},[187,8896,8897],{"class":196},"  Description of What Happens: Provide a detailed account of the action, and key events that take place in the scene.\n",[187,8899,8900,8903,8905],{"class":189,"line":686},[187,8901,8902],{"class":2516},"shot",[187,8904,585],{"class":577},[187,8906,7477],{"class":573},[187,8908,8909],{"class":189,"line":697},[187,8910,8911],{"class":196},"  You are a film director working on a new British spy film and your writers have provided you with a scene. Your task is to come up with four to five shots that will be filmed during the scene. The shot descriptions needs to be specific and should include a varietry of closeup shots on characters, environment shots that consider the scene location and shots of specific items or other things that are featured in the scene. Each shot should also have a title. The description should be a brief densely worded block of text that captures the important elements of the scene. Consider the style of camera angle, lighting, character expressions, clothing, and other important visual elements for each shot. Be very descriptive. The description will be used to generate an image of the shot. Also, there should be at most one actor for each shot that contains people. Don't use the name of the character, instead use a physical description of the character based on their physical traits described below if needed. Also consider what the actor is wearing in the description.\n",[911,8913,8915],{"id":8914},"stable-diffusion-and-stable-video-diffusion-agents","Stable Diffusion and Stable Video Diffusion agents",[11,8917,8918],{},"The stable diffusion agent makes an API call to a local instance of the Stable Diffusion WebUI API, saves the generated image and saves a reference to that image in the state:",[26,8920,8922],{"className":2507,"code":8921,"language":2509,"meta":35,"style":35},"- description: A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n  image: 000.png\n  title: Ethan's Concerned Expression\n  video: 000.mp4\n",[33,8923,8924,8935,8940,8945,8950,8960,8970],{"__ignoreMap":35},[187,8925,8926,8928,8930,8932],{"class":189,"line":190},[187,8927,8672],{"class":577},[187,8929,6560],{"class":2516},[187,8931,585],{"class":577},[187,8933,8934],{"class":196},"A medium close-up shot of Ethan Jameson's face, with a concerned expression,\n",[187,8936,8937],{"class":189,"line":249},[187,8938,8939],{"class":196},"    as he reads the message from Natalie Jackson. The lighting is dim, with only a\n",[187,8941,8942],{"class":189,"line":312},[187,8943,8944],{"class":196},"    single lamp on his desk casting a warm glow. His eyes are narrowed, and his brow\n",[187,8946,8947],{"class":189,"line":319},[187,8948,8949],{"class":196},"    is furrowed in concentration. He is wearing a dark blue suit and a white shirt.\n",[187,8951,8952,8955,8957],{"class":189,"line":325},[187,8953,8954],{"class":2516},"  image",[187,8956,585],{"class":577},[187,8958,8959],{"class":196},"000.png\n",[187,8961,8962,8965,8967],{"class":189,"line":686},[187,8963,8964],{"class":2516},"  title",[187,8966,585],{"class":577},[187,8968,8969],{"class":196},"Ethan's Concerned Expression\n",[187,8971,8972,8975,8977],{"class":189,"line":697},[187,8973,8974],{"class":2516},"  video",[187,8976,585],{"class":577},[187,8978,8979],{"class":196},"000.mp4\n",[11,8981,8982],{},[511,8983],{"alt":8984,"src":8985},"A medium close-up shot of Ethan Jameson's face","/static/aoi/ethan.png",[11,8987,8988],{},"With the perfectly prompted image in hand, we can use Stable Video Diffusion to bring it to life. I prompted phind to come up with a FastAPI service that would accept an image in a post request and return a short video created with stable video diffusion using the diffusers library.",[11,8990,8991],{},"Stable video diffusion can generate about 4 seconds of text at 7 frames per second. This isn't great, but I was able to use ffmpeg to do frame interpolation bringing the frame rate to a much smoother 14 fps using motion compensated interpolation (MCI):",[26,8993,8995],{"className":181,"code":8994,"language":183,"meta":35,"style":35},"ffmpeg -i output/1718453390/final.mp4 -crf 10 -vf \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\" output/1718453390/final.14fps.mp4\n",[33,8996,8997],{"__ignoreMap":35},[187,8998,8999,9002,9005,9008,9011,9014,9017,9020],{"class":189,"line":190},[187,9000,9001],{"class":193},"ffmpeg",[187,9003,9004],{"class":588}," -i",[187,9006,9007],{"class":196}," output/1718453390/final.mp4",[187,9009,9010],{"class":588}," -crf",[187,9012,9013],{"class":588}," 10",[187,9015,9016],{"class":588}," -vf",[187,9018,9019],{"class":196}," \"minterpolate=fps=14:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\"",[187,9021,9022],{"class":196}," output/1718453390/final.14fps.mp4\n",[11,9024,9025,9026,9029,9030,9033],{},"Finally, the ",[33,9027,9028],{},"editor_agent"," uses ",[33,9031,9032],{},"moviepy"," to join the clips together into a single video.",[168,9035,9037],{"id":9036},"development-environment","Development environment",[11,9039,9040,9041,9044,9045,9048],{},"I struggled to optimize the ",[33,9042,9043],{},"meta-llama/Meta-Llama-3-8B-Instruct"," with TensorRT-LLM, so I ran LLM inference on a combination of older Llama2 TensorRT-LLM models, and ",[33,9046,9047],{},"Meta-Llama-3-8B-Instruct"," on LM Studio (which I found to be painfully slow compared to TensorRT-LLM).",[11,9050,9051,9052,9054,9055,9058,9059,9062,9063,9068,9069,9071],{},"If you provide an ",[33,9053,1571],{}," in the ",[33,9056,9057],{},".env"," file, LLM calls will be made using the ",[33,9060,9061],{},"meta/llam3-70b-instruct"," model on ",[15,9064,9067],{"href":9065,"rel":9066},"https://build.nvidia.com/meta/llama3-70b",[19],"build.nvidia.com/meta/llama3-70b",". In fact, ",[33,9070,4182],{}," also provides stable diffusion and stable video diffusion inference via API. This would be very convenient in the event that my RTX PCs become compromised.",[11,9073,9074],{},"My RTX 4090 GPU with 24 GB of memory was able to run lots of different inference servers concurrently (LLM, Stable Diffusion WebUI, ComfyUI, InvokeAI, Stable Video Diffusion FastAPI service), but I generally stuck to doing one type of inference at a time, otherwise things would grind to a hault or crash. I also experimented with ChatTTS, a new text-to-speech model.",[11,9076,9077],{},"I developed this project on a MacBook Pro, and I used my RTX PC as if it were a remote service providing inference for text, images and video. This is a helpful mindset when working with hybrid AI workflows that leverage inference services both on local machines and in the cloud.",[168,9079,9080],{"id":1723},"How it works",[11,9082,9083,9084,9087],{},"To run the program, you need to install python dependencies and then run an OpenAI compatible LLM and Stable Duffsion WebUI server with the ",[33,9085,9086],{},"--api"," flag. You also need to run the Stable Video Diffusion service. Apologies for any hardcoded local IP address in the source code. Deadlines, you know! With everything configured, you can run the following command:",[26,9089,9092],{"className":9090,"code":9091,"language":31},[29],"~/git/github/agents-of-inference$ poetry run python agents_of_inference/main.py\n## 📀 Using local models 📀 ##\n## 🎭 Generating Cast 🎭 ##\n## 🗺️ Generating Locations 🗺️ ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to synopsis_review_agent ##\n## 📑 Reviewing Synopsis 📑 ##\n## ✍️ Generating Synopsis ✍️ ##\n## going to scene_agent ##\n## 📒 Generating Scenes 📒 ##\n## 🎬 Generating Shots 🎬 ##\n## Generated 5 shots for scene 1/5 ##\n## Generated 5 shots for scene 2/5 ##\n## Generated 5 shots for scene 3/5 ##\n## Generated 5 shots for scene 4/5 ##\n## Generated 5 shots for scene 5/5 ##\n\n000/0025\nA medium shot of a bustling Tokyo street, with neon lights reflecting off wet pavement. Jim Thompson, dressed in a black leather jacket and dark jeans, walks purposefully through the crowd, his piercing blue eyes scanning the area. The sound design features the hum of traffic and chatter of pedestrians.\nGenerated image output/1718426686/images/000.png\n\n001/0025\nA tight close-up shot of Emily Chen's face, her piercing brown eyes intense as she briefs Jim on the situation. Her short black hair is styled neatly, and she wears a crisp white blouse with a silver necklace. The camera lingers on her lips as she speaks, emphasizing the importance of the information.\nGenerated image output/1718426686/images/001.png\n\nGenerated video output/1718426686/videos/000.mp4\n== stable video diffusion generation complete ==\nGenerated video output/1718426686/videos/001.mp4\n== stable video diffusion generation complete ==\n",[33,9093,9091],{"__ignoreMap":35},[168,9095,9097],{"id":9096},"demo-video-for-contest-submission","Demo Video for Contest Submission",[9099,9100],"agents-of-inference-video",{},[11,9102,9103],{},"Making this video was a lot of fun. The \"Agents of Inference\" highlight reel includes some of the most interesting, exciting and fun clips that I found in the dozens of short films it created. It is important to note that a lot of the content is not very good. Misunderstood prompts, color confusion (prompt includes green eyes, but other things in the scene are also conspicuously green), unrealistic or noisy motion from Stable Video Diffusion--these are some of the issues you will find in the films. Generating AI images sometimes feels like panning for gold: you go through a lot of sediment to get a few good flakes.",[11,9105,9106],{},"Also, I added a few short animations that I made with Blender. The final scene shows the NVIDIA Omniverse orange humanoid from the barrel of a pistol. I think we are rapidly approaching a future where agents can generate full-scale theatrical movies by generating OpenUSD code, directly or indirectly. Maybe for the next NVIDIA Developer contest!",[168,9108,9110],{"id":9109},"shortcomings-of-my-project","Shortcomings of my project",[11,9112,9113],{},"My goodness, how embarrasing. There are quite a few shortcomings that can be easily identified looking over the output and the source code. Here are a few:",[911,9115,9117],{"id":9116},"character-variety","Character variety",[11,9119,9120],{},"When generating characters I would frequently see one named Dr. Sophia Patel who is apparently a brilliant cryptologist. Other characters would often have different names or backgrounds, but a saw Dr. Sophia Patel more often than not.",[911,9122,9124],{"id":9123},"character-consistency","Character consistency",[11,9126,9127],{},"The characters are not consistent. This is a notoriously difficult problem to solve, but I made a lot of progress on it during this contest. I experimented with calling the ComfyUI API to run a custom workflow built with the ComfyUI graph-based workflow tool for face transfer:",[11,9129,9130],{},[511,9131],{"alt":9132,"src":9133},"Dr. Sophia Patel","/static/aoi/sophia.png",[11,9135,9136],{},"Using ComfyUI would be nice, but it wouldn't be as easy to tap into cloud APIs if my workflow heavily relied on ComfyUI server with custom models.",[911,9138,9140],{"id":9139},"understanding-langchain","Understanding LangChain",[11,9142,9143,9144,9146],{},"I started out with the idea I would store all LLM calls to a local JSON to serve as a cache, allowing me to avoid regenerating responses from early in the workflow. This worked well, until I tried to serialize an Annotated list (required for cycles such as the one used with ",[33,9145,8840],{},"). I ended up wasting a lot of time trying to figure this out, and I came across some built-in LangChain features for storing state in memory and in Sqlite. I'm sure there are other areas where I used the wrong pattern, but I turned over a lot of stones and look forward to continuing development with LangChain.",[168,9148,9149],{"id":4039},"What's next?",[11,9151,9152],{},"Thank you to NVIDIA and LangChain for organizing this contest. It was a great way to explore a powerful toolset for automated content generation using AI agents.",[11,9154,9155],{},"Video models like Dream Machine and Sora have made some big splashes on the internet and the results are remarkable. However, I'm still almost more interested in finding the limitations of quality content using open-source models on consumer hardware like RTX GPUs.",[11,9157,9158],{},"I would also have loved to generate my own music for these films. I am a Suno poweruser and love the songs I have generated on that site. Will the gap between video and music generation on private, payed services and local machines? Or does it just need time to catch up? Hopefully a future installment of \"Agents of Inference\" will integrate music and voice, and can't wait to hear it!",[855,9160,9161],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}",{"title":35,"searchDepth":249,"depth":249,"links":9163},[9164,9165,9166,9167,9168,9174,9175,9176,9177,9182],{"id":8467,"depth":249,"text":8468},{"id":8183,"depth":249,"text":8184},{"id":8492,"depth":249,"text":8493},{"id":8499,"depth":249,"text":8500},{"id":8506,"depth":249,"text":8507,"children":9169},[9170,9171,9172,9173],{"id":8548,"depth":312,"text":8549},{"id":8766,"depth":312,"text":8767},{"id":8859,"depth":312,"text":8860},{"id":8914,"depth":312,"text":8915},{"id":9036,"depth":249,"text":9037},{"id":1723,"depth":249,"text":9080},{"id":9096,"depth":249,"text":9097},{"id":9109,"depth":249,"text":9110,"children":9178},[9179,9180,9181],{"id":9116,"depth":312,"text":9117},{"id":9123,"depth":312,"text":9124},{"id":9139,"depth":312,"text":9140},{"id":4039,"depth":249,"text":9149},"2024-06-17","This article discusses my entry for NVIDIA's Generative AI Agents Developer Contest entry: Agents of Inference",[9186],{"link":8444,"site":2196},{},"/2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",{"title":8462,"description":9184},"2024/06/17/agents-of-inference-nvidia-and-langchain-generative-ai-agent-developer-contest",[2203,8451,4076,2205,8452,8453,6311,2204,4077,8454,8455,8456,8457,8458],"3MJVrx_ebYBnymQVv7jgBj8qWIopRmwyb7J1a3M1mIA",{"id":9194,"title":9195,"body":9196,"comments":315,"date":10044,"description":10045,"draft":872,"extension":873,"external":10046,"image":10054,"meta":10055,"navigation":315,"path":10056,"seo":10057,"stem":10058,"tags":10059,"__hash__":10063},"blog/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest.md","Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA's Generative AI on RTX PCs Developer Contest",{"type":8,"value":9197,"toc":10021},[9198,9200,9203,9205,9208,9215,9219,9222,9227,9230,9233,9236,9239,9245,9248,9251,9256,9261,9264,9270,9273,9276,9279,9282,9288,9290,9293,9362,9373,9380,9399,9402,9410,9425,9428,9434,9437,9508,9520,9525,9536,9547,9550,9559,9561,9564,9570,9606,9610,9616,9627,9631,9634,9643,9650,9653,9659,9665,9668,9671,9677,9679,9685,9689,9696,9731,9735,9742,9800,9804,9807,9816,9822,9831,9838,9842,9845,9851,9855,9858,9893,9904,9908,9911,9914,9917,9919,9922,9925,9931,9963,9969,9974,9980,9986,9988,9994,9997,10000,10007,10009,10012,10015,10018],[168,9199,8184],{"id":8183},[11,9201,9202],{},"This article is about my submission to NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA's TensorRT-LLM library.",[11,9204,8482],{},[9206,9207],"rocket-league-bot-chat-tweet",{},[11,9209,8200,9210,752],{},[15,9211,9214],{"href":9212,"rel":9213},"https://github.com/briancaffey/RocketLeagueBotChat",[19],"Rocket League BotChat GitHub repository",[168,9216,9218],{"id":9217},"nvidias-gen-ai-developer-contest","NVIDIA's Gen AI Developer Contest",[11,9220,9221],{},"The following email caught my attention last month:",[107,9223,9224],{},[11,9225,9226],{},"Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems",[11,9228,9229],{},"The part about “on Windows PC” made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had also never used any of the Windows-specific development frameworks like .NET or Visual Studio.",[11,9231,9232],{},"My experience with Windows goes back to 2016 when I built my fist PC with an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives in order to dual boot into both Windows and Ubuntu operating systems. Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer.",[11,9234,9235],{},"Last year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my PC's Ubuntu operating system, and frequently had to switch between using Ubuntu for working with stable diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems pushed me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy.",[11,9237,9238],{},"I learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently compared ot Linux-based operating systems. And just like that, I became a Windows developer! This experience got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest.",[11,9240,9241],{},[511,9242],{"alt":9243,"src":9244},"Windows winfetch screenshot","/static/rlbc/winfetch.png",[168,9246,9247],{"id":8499},"Coming up with an Idea",[11,9249,9250],{},"The contest description and some related NVIDIA articles about the contest helped me with brainstorming:",[107,9252,9253],{},[11,9254,9255],{},"Whether it’s a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless.",[107,9257,9258],{},[11,9259,9260],{},"Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.",[11,9262,9263],{},"This contest is focused on NVIDIA's consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software, creative tools and new AI techniques including image generation and LLM (Large Language Model) inference.",[11,9265,9266],{},[511,9267],{"alt":9268,"src":9269},"A stacked bar chart showing the composition of Nvidia's revenue each quarter going back to fiscal 2019.","https://g.foolcdn.com/image/?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&op=resize&w=700",[11,9271,9272],{},"Gaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don't play these types of games and it didn't seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know.",[11,9274,9275],{},"Rocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and all got hooked. We also came to learn that Rocket League's in-game is varies from entertaining, annoying, toxic and in some cases, sportsmanlike.",[11,9277,9278],{},"One other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas \"bakkes\" Bakke, a Norwegian software engineer). Rocket League's in-game chat inspired the idea for my submission to NVIDIA's Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)",[11,9280,9281],{},"Bots are built into the Rocket League game and you can play with or against them in offline matches. However, the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow and PyTorch. These bots are very good, but they are not infallible. My contest project idea was now clear: develop a plugin for Rocket League capable of sending messages from bot players. This idea seemed to check the boxes for the large language model category of NVIDIA's developer contest: develop a project in a Windows environment for a Windows-specific program, and use an LLM powered by TensorRT-LLM.",[11,9283,9284],{},[511,9285],{"alt":9286,"src":9287},"RLBot Ascii Art","/static/rlbc/bot.png",[168,9289,8507],{"id":8506},[11,9291,9292],{},"With this idea in mind, I looked into the project's feasibility. I really had no idea if this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept.",[916,9294,9295,9324,9336,9343],{},[919,9296,9297,9298,9302],{},"The Bakkesmod Plugin Wiki ",[15,9299,9300],{"href":9300,"rel":9301},"https://wiki.bakkesplugins.com/",[19],[916,9303,9304,9314],{},[919,9305,9306,9313],{},[15,9307,9310],{"href":9308,"rel":9309},"https://wiki.bakkesplugins.com/code_snippets/using_http_wrapper/",[19],[33,9311,9312],{},"HttpWrapper"," for sending HTTP requests from Bakkesmod",[919,9315,9316,9323],{},[15,9317,9320],{"href":9318,"rel":9319},"https://wiki.bakkesplugins.com/functions/stat_events/",[19],[33,9321,9322],{},"StatEvents"," that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save).",[919,9325,9326,9327,9331],{},"The Bakkesmod plugin template: ",[15,9328,9329],{"href":9329,"rel":9330},"https://github.com/Martinii89/BakkesmodPluginTemplate",[19],[916,9332,9333],{},[919,9334,9335],{},"This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly",[919,9337,9338,9339],{},"Plugin Tutorial: ",[15,9340,9341],{"href":9341,"rel":9342},"https://wiki.bakkesplugins.com/plugin_tutorial/getting_started/",[19],[919,9344,9345,9346],{},"Open-source chat-related Bakkesmod plugins on GitHub\n",[916,9347,9348,9355],{},[919,9349,9350,9351],{},"BetterChat: ",[15,9352,9353],{"href":9353,"rel":9354},"https://github.com/JulienML/BetterChat",[19],[919,9356,9357,9358],{},"Translate: ",[15,9359,9360],{"href":9360,"rel":9361},"https://github.com/0xleft/trnslt",[19],[11,9363,9364,9365,9368,9369,9372],{},"Starting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to ",[33,9366,9367],{},"localhost:8000/hello",". I set up a Hello World Flask app running on ",[33,9370,9371],{},"localhost:8000"," and I was able to get a response from my Hello World server! There didn't seem to be any network or permission errors that would prevent my game code from communicating with other applications on my PC.",[11,9374,9375,9376,9379],{},"Next I started looking into how to build and run optimized LLMs with NVIDIA's TensorRT-LLM library, the software that this contest is promoting. The contest announcement included an interesting building block that I thought could be very useful: an example repo showing how to run ",[33,9377,9378],{},"CodeLlama-13b-instruct-hf"," optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev).",[916,9381,9382,9387,9393,9396],{},[919,9383,9384,9386],{},[33,9385,9378],{}," is an open source model from Meta that is trained on code and can help with code generation tasks",[919,9388,9389,9390,9392],{},"TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like ",[33,9391,9378],{}," and generates an engine that can be used for doing inference",[919,9394,9395],{},"VSCode is an open source code editor developed by Microsoft with an large number of community plugins",[919,9397,9398],{},"Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT",[11,9400,9401],{},"To get the coding assistant project working I needed to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:",[916,9403,9404,9407],{},[919,9405,9406],{},"using a \"bare-metal\" virtual environment on Windows (with PowerShell)",[919,9408,9409],{},"using WSL",[11,9411,9412,9413,9416,9417,9420,9421,9424],{},"At the time of writing, building a TensorRT-LLM engine on Windows can only be done with version ",[33,9414,9415],{},"v0.6.1"," of the TensorRT-LLM repo and version ",[33,9418,9419],{},"v0.7.1"," of the ",[33,9422,9423],{},"tensorrt_llm"," Python package.",[11,9426,9427],{},"With WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system.",[11,9429,9430,9431,9433],{},"Once the engines are built you can use them to run the example from the ",[33,9432,8283],{}," repo.",[11,9435,9436],{},"The example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:",[26,9438,9440],{"className":1200,"code":9439,"language":1202,"meta":35,"style":35},"{\n  \"title\": \"CodeLlama-13b-instruct-hf\",\n  \"apiBase\": \"http://192.168.5.96:5000/\",\n  \"provider\": \"openai\",\n  \"apiKey\": \"None\",\n  \"model\": \"gpt-4\"\n}\n",[33,9441,9442,9446,9458,9470,9482,9494,9504],{"__ignoreMap":35},[187,9443,9444],{"class":189,"line":190},[187,9445,1209],{"class":577},[187,9447,9448,9451,9453,9456],{"class":189,"line":249},[187,9449,9450],{"class":588},"  \"title\"",[187,9452,585],{"class":577},[187,9454,9455],{"class":196},"\"CodeLlama-13b-instruct-hf\"",[187,9457,1228],{"class":577},[187,9459,9460,9463,9465,9468],{"class":189,"line":312},[187,9461,9462],{"class":588},"  \"apiBase\"",[187,9464,585],{"class":577},[187,9466,9467],{"class":196},"\"http://192.168.5.96:5000/\"",[187,9469,1228],{"class":577},[187,9471,9472,9475,9477,9480],{"class":189,"line":319},[187,9473,9474],{"class":588},"  \"provider\"",[187,9476,585],{"class":577},[187,9478,9479],{"class":196},"\"openai\"",[187,9481,1228],{"class":577},[187,9483,9484,9487,9489,9492],{"class":189,"line":325},[187,9485,9486],{"class":588},"  \"apiKey\"",[187,9488,585],{"class":577},[187,9490,9491],{"class":196},"\"None\"",[187,9493,1228],{"class":577},[187,9495,9496,9499,9501],{"class":189,"line":686},[187,9497,9498],{"class":588},"  \"model\"",[187,9500,585],{"class":577},[187,9502,9503],{"class":196},"\"gpt-4\"\n",[187,9505,9506],{"class":189,"line":697},[187,9507,1309],{"class":577},[11,9509,9510,9511,9513,9514,9519],{},"The Continue.dev extension using ",[33,9512,9378],{}," accelerated and optimized by TensorRT-LLM is very fast. According to ",[15,9515,9518],{"href":9516,"rel":9517},"https://blog.continue.dev/programming-languages/",[19],"this post on Continue.dev's blog",", C++ is a \"first tier\" language:",[107,9521,9522],{},[11,9523,9524],{},"C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode / TP3, MBXP / Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don’t believe that LLMs are as helpful for C++ as some of the other languages in this tier.",[11,9526,9527,9528,9531,9532,9535],{},"Most of the time I'm working with either Python and TypeScript. I've read about C++ but haven't used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with the Continue.dev autopilot extension was helpful for tackling smaller problems in a REPL-like environment. For example, I used Continue.dev in VSCode to figure out how to handle JSON. Coming from Python and JavaScript languages, I found the ",[33,9529,9530],{},"nlohmann/json"," JSON library syntax to be somewhat different. For example, here is how to add a message to ",[33,9533,9534],{},"messages"," in the body of an OpenAI API request:",[26,9537,9541],{"className":9538,"code":9539,"language":9540,"meta":35,"style":35},"language-cpp shiki shiki-themes github-light github-dark","messages.push_back({ {\"role\", role}, {\"content\", content } });\n","cpp",[33,9542,9543],{"__ignoreMap":35},[187,9544,9545],{"class":189,"line":190},[187,9546,9539],{},[11,9548,9549],{},"In Python the code for appending a message to a list of messages would be written differently:",[26,9551,9553],{"className":1383,"code":9552,"language":1125,"meta":35,"style":35},"messages.append({\"role\": role, \"content\": content})\n",[33,9554,9555],{"__ignoreMap":35},[187,9556,9557],{"class":189,"line":190},[187,9558,9552],{},[168,9560,9037],{"id":9036},[11,9562,9563],{},"While working on different projects using web technologies and frameworks in the Python and JavaScript ecosystems, I developed an appreciation for well-structured development environments that are easy to use. Development environment refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running on a local environment. The local environment (the developer's computer) should be a close proxy for the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality, offering a tight feedback loop. I really like the development environment for this project. Here's a screenshot that shows the different parts of the development environment I used for working on Rocket League BotChat:",[11,9565,9566],{},[511,9567],{"alt":9568,"src":9569},"Screenshot of Rocket League BotChat development environment","/static/rlbc/devenv2.png",[916,9571,9572,9579,9593,9600],{},[919,9573,9574,9575,9578],{},"Rocket League (running with the ",[33,9576,9577],{},"-dev"," flag turned on). The console is helpful for viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine",[919,9580,9581,9582,9585,9586,9585,9589,9592],{},"Visual Studio for working on the plugin code. ",[33,9583,9584],{},"Control","+",[33,9587,9588],{},"Shift",[33,9590,9591],{},"B"," rebuilds the code and automatically reloads the plugin in the game",[919,9594,9595,9596,9599],{},"OpenAI-compatible LLM server powered by TensorRT-LLM (using ",[33,9597,9598],{},"Llama-2-13b-chat-hf"," with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL",[919,9601,9602,9603,9605],{},"VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using ",[33,9604,9378],{}," with AWQ INT4 quantization) running in a virtual environment on Windows",[911,9607,9609],{"id":9608},"building-the-tensorrt-llm-engines","Building the TensorRT-LLM engines",[11,9611,9612,9613,9615],{},"I was able to build and run the TensorRT LLM engines for my game plugin's inference and the Continue.dev extension's inference both in Python virtual environments on Windows and on Ubuntu in WSL. For building the ",[33,9614,9598],{}," model with INT4 AWQ quantization on Windows 11 I used this command:",[26,9617,9621],{"className":9618,"code":9619,"language":9620,"meta":35,"style":35},"language-powershell shiki shiki-themes github-light github-dark","(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT-LLM\\examples\\llama> python build.py --model_dir D:\\llama\\Llama-2-13b-chat-hf\\ --quant_ckpt_path D:\\llama\\Llama-2-13b-chat-hf\\llama_tp1_rank0.npz --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16 --use_weight_only --weight_only_precision int4_awq --per_group --enable_context_fmha --max_batch_size 1 --max_input_len 3500 --max_output_len 1024 --output_dir D:\\llama\\Llama-2-13b-chat-hf\\single-gpu\\ --vocab_size 32064\n","powershell",[33,9622,9623],{"__ignoreMap":35},[187,9624,9625],{"class":189,"line":190},[187,9626,9619],{},[911,9628,9630],{"id":9629},"running-the-tensorrt-llm-engines","Running the TensorRT-LLM engines",[11,9632,9633],{},"Using Windows PowerShell to start the CodeLlama server for Continue.dev:",[26,9635,9637],{"className":9618,"code":9636,"language":9620,"meta":35,"style":35},"(.venv) PS C:\\Users\\My PC\\GitHub\\trt-llm-as-openai-windows> python .\\app.py --trt_engine_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\" --trt_engine_name llama_float16_tp1_rank0.engine --tokenizer_dir_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\\" --port 5000 --host 0.0.0.0\n",[33,9638,9639],{"__ignoreMap":35},[187,9640,9641],{"class":189,"line":190},[187,9642,9636],{},[11,9644,9645,9646,9649],{},"Tip: Adding ",[33,9647,9648],{},"--host 0.0.0.0"," isn't required here, but it allows me to use the CodeLlama/TensorRT-LLM server with VSCode any computer on my local network using my PC's local IP address in the Continue.dev configuration.",[11,9651,9652],{},"Using docker in WSL to start the Llama-2-13b-chat-hf LLM server:",[26,9654,9657],{"className":9655,"code":9656,"language":31},[29],"root@0a5b5b75f079:/code/git/TensorRT-LLM/examples/server/flask# python3 app.py --trt_engine_path /llama/Llama-2-13b-chat-hf/trt_engines/1-gpu/ --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path /llama/Llama-2-13b-chat-hf/ --port 5001 --host 0.0.0.0\n",[33,9658,9656],{"__ignoreMap":35},[11,9660,9661,9662,9664],{},"Note: Here I also add ",[33,9663,9648],{},", but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows.",[11,9666,9667],{},"BakkesMod includes a console window that came in handy for debugging errors during development.",[11,9669,9670],{},"At the beginning of this developer contest on January 9, NVIDIA announced Chat with RTX. This is a demo program for Windows that automates a lots of the processes needed to set up a TensorRT-LLM-powered LLM running on your PC. Keep an eye on this project as it may become the best way to install and manage large language models on Windows PCs.",[11,9672,9673],{},[511,9674],{"alt":9675,"src":9676},"Chat with RTX image","/static/rlbc/chat_with_rtx.jpeg",[168,9678,9080],{"id":1723},[11,9680,9681,9682,1737],{},"Here's a quick look at key parts of the plugin source code (",[15,9683,9212],{"href":9212,"rel":9684},[19],[911,9686,9688],{"id":9687},"hooking-events","Hooking events",[11,9690,9691,9692,9695],{},"Hooking events is the core of how this plugin works. ",[33,9693,9694],{},"StatTickerMessage"," events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car.",[26,9697,9699],{"className":9538,"code":9698,"language":9540,"meta":35,"style":35},"    // Hooks different types of events that are handled in onStatTickerMessage\n    // See https://wiki.bakkesplugins.com/functions/stat_events/\n    gameWrapper->HookEventWithCallerPost\u003CServerWrapper>(\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\",\n        [this](ServerWrapper caller, void* params, std::string eventname) {\n            onStatTickerMessage(params);\n        });\n",[33,9700,9701,9706,9711,9716,9721,9726],{"__ignoreMap":35},[187,9702,9703],{"class":189,"line":190},[187,9704,9705],{},"    // Hooks different types of events that are handled in onStatTickerMessage\n",[187,9707,9708],{"class":189,"line":249},[187,9709,9710],{},"    // See https://wiki.bakkesplugins.com/functions/stat_events/\n",[187,9712,9713],{"class":189,"line":312},[187,9714,9715],{},"    gameWrapper->HookEventWithCallerPost\u003CServerWrapper>(\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\",\n",[187,9717,9718],{"class":189,"line":319},[187,9719,9720],{},"        [this](ServerWrapper caller, void* params, std::string eventname) {\n",[187,9722,9723],{"class":189,"line":325},[187,9724,9725],{},"            onStatTickerMessage(params);\n",[187,9727,9728],{"class":189,"line":686},[187,9729,9730],{},"        });\n",[911,9732,9734],{"id":9733},"handling-events-and-building-the-prompt","Handling events and building the prompt",[11,9736,9737,9738,9741],{},"We can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the ",[33,9739,9740],{},"appendToPrompt"," method.",[26,9743,9745],{"className":9538,"code":9744,"language":9540,"meta":35,"style":35},"    // handle different events like scoring a goal or making a save\n    if (statEvent.GetEventName() == \"Goal\") {\n\n        // was the goal scored by the human player or the bot?\n        if (playerPRI.memory_address == receiver.memory_address) {\n            appendToPrompt(\"Your human opponent just scored a goal against you! \" + score_sentence, \"user\");\n        }\n        else {\n            appendToPrompt(\"You just scored a goal against the human player! \" + score_sentence, \"user\");\n        }\n    }\n",[33,9746,9747,9752,9757,9761,9766,9771,9776,9781,9786,9791,9795],{"__ignoreMap":35},[187,9748,9749],{"class":189,"line":190},[187,9750,9751],{},"    // handle different events like scoring a goal or making a save\n",[187,9753,9754],{"class":189,"line":249},[187,9755,9756],{},"    if (statEvent.GetEventName() == \"Goal\") {\n",[187,9758,9759],{"class":189,"line":312},[187,9760,316],{"emptyLinePlaceholder":315},[187,9762,9763],{"class":189,"line":319},[187,9764,9765],{},"        // was the goal scored by the human player or the bot?\n",[187,9767,9768],{"class":189,"line":325},[187,9769,9770],{},"        if (playerPRI.memory_address == receiver.memory_address) {\n",[187,9772,9773],{"class":189,"line":686},[187,9774,9775],{},"            appendToPrompt(\"Your human opponent just scored a goal against you! \" + score_sentence, \"user\");\n",[187,9777,9778],{"class":189,"line":697},[187,9779,9780],{},"        }\n",[187,9782,9783],{"class":189,"line":1291},[187,9784,9785],{},"        else {\n",[187,9787,9788],{"class":189,"line":1306},[187,9789,9790],{},"            appendToPrompt(\"You just scored a goal against the human player! \" + score_sentence, \"user\");\n",[187,9792,9793],{"class":189,"line":1434},[187,9794,9780],{},[187,9796,9797],{"class":189,"line":2599},[187,9798,9799],{},"    }\n",[911,9801,9803],{"id":9802},"making-requests-and-handling-responses","Making requests and handling responses",[11,9805,9806],{},"The last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI's API.",[26,9808,9810],{"className":9538,"code":9809,"language":9540,"meta":35,"style":35},"std::string message = response_json[\"choices\"][0][\"message\"][\"content\"];\n",[33,9811,9812],{"__ignoreMap":35},[187,9813,9814],{"class":189,"line":190},[187,9815,9809],{},[11,9817,6131,9818,9821],{},[33,9819,9820],{},"LogToChatbox"," method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time.",[26,9823,9825],{"className":9538,"code":9824,"language":9540,"meta":35,"style":35},"gameWrapper->LogToChatbox(messages[i], this->bot_name);\n",[33,9826,9827],{"__ignoreMap":35},[187,9828,9829],{"class":189,"line":190},[187,9830,9824],{},[11,9832,9833,9834,9837],{},"That's it! The code isn't that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages (",[33,9835,9836],{},"\u003C/s>","). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt.",[168,9839,9841],{"id":9840},"rocket-league-botchat-ui","Rocket League BotChat UI",[11,9843,9844],{},"Most BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here's what the UI for Rocket League BotChat looks like:",[11,9846,9847],{},[511,9848],{"alt":9849,"src":9850},"Rocket League BotChat Plugin UI","/static/rlbc/rlbcui.png",[911,9852,9854],{"id":9853},"system-prompt","System prompt",[11,9856,9857],{},"The system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here's the default prompt that I used:",[26,9859,9861],{"className":9538,"code":9860,"language":9540,"meta":35,"style":35},"    std::string ai_player = \"You are an elite AI player in the car soccer game Rocket League. \";\n    std::string one_v_one = \"You are playing a 1v1 match against a human player. \";\n    std::string instructions = \"You will send short chat messages to your human opponent in response to what happens in the game. \";\n    std::string details = \"Respond to the human player with brief messages no more than 12 words long.\";\n    // initial system prompt\n    std::string initial_system_prompt = ai_player + one_v_one + instructions + details;\n",[33,9862,9863,9868,9873,9878,9883,9888],{"__ignoreMap":35},[187,9864,9865],{"class":189,"line":190},[187,9866,9867],{},"    std::string ai_player = \"You are an elite AI player in the car soccer game Rocket League. \";\n",[187,9869,9870],{"class":189,"line":249},[187,9871,9872],{},"    std::string one_v_one = \"You are playing a 1v1 match against a human player. \";\n",[187,9874,9875],{"class":189,"line":312},[187,9876,9877],{},"    std::string instructions = \"You will send short chat messages to your human opponent in response to what happens in the game. \";\n",[187,9879,9880],{"class":189,"line":319},[187,9881,9882],{},"    std::string details = \"Respond to the human player with brief messages no more than 12 words long.\";\n",[187,9884,9885],{"class":189,"line":325},[187,9886,9887],{},"    // initial system prompt\n",[187,9889,9890],{"class":189,"line":686},[187,9891,9892],{},"    std::string initial_system_prompt = ai_player + one_v_one + instructions + details;\n",[11,9894,9895,9896,9899,9900,9903],{},"The last part about ",[33,9897,9898],{},"no more than 12 words long"," was the most effective way of controlling the length responses from the LLM. I tried changing the ",[33,9901,9902],{},"max_output_len"," when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user. Changing the system prompt was a lot of fun to expirment with!",[911,9905,9907],{"id":9906},"temperature-and-seed","Temperature and Seed",[11,9909,9910],{},"These values are included in the body of the request to the LLM, but I didn't have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn't really work.",[911,9912,9913],{"id":9534},"Messages",[11,9915,9916],{},"This section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the \"user\" (which are messages about game events) and the \"assistant\" (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt.",[168,9918,9097],{"id":9096},[9920,9921],"rocket-league-bot-chat-video",{},[11,9923,9924],{},"I used Blender's sequence editor to create a demo video for my contest submission. I don't edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here's how I approached creating the demo video for my project.",[11,9926,9927],{},[511,9928],{"alt":9929,"src":9930},"Blender video sequence editor UI used to create my project video","/static/rlbc/blender.png",[916,9932,9933,9936,9944,9957,9960],{},[919,9934,9935],{},"Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action",[919,9937,9938,9939],{},"Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used ",[15,9940,9943],{"href":9941,"rel":9942},"https://open.spotify.com/track/68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51",[19],"\"Dads in Space\" by Steven Walking",[919,9945,9946,9947,9950,9951,9956],{},"Get stock Rocket League footage from YouTube with ",[33,9948,9949],{},"youtube-dl"," (this is an amazing tool!). I mostly used footage from the ",[15,9952,9955],{"href":9953,"rel":9954},"https://www.youtube.com/watch?v=e1tqWldCYOI&pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D",[19],"RLCS 2023 Winter Major Trailer",". This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps.",[919,9958,9959],{},"Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!",[919,9961,9962],{},"Use ComfyUI to animate some of the images used in the contest description and use these in my video",[11,9964,9965],{},[511,9966],{"alt":9967,"src":9968},"ComfyUI workflow for animating images using img2vid model","/static/rlbc/comfyui.png",[916,9970,9971],{},[919,9972,9973],{},"Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice.",[11,9975,9976],{},[511,9977],{"alt":9978,"src":9979},"Eleven Labs Voice Generation Web UI","/static/rlbc/elevenlabs.png",[11,9981,9982],{},[15,9983,9985],{"href":9984},"#","Embed twitter video here",[168,9987,9110],{"id":9109},[11,9989,9990,9991,9993],{},"This plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in \"user\" and \"assistant\" prompts getting out of order which breaks assertions on the ",[33,9992,8283],{}," repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM.",[11,9995,9996],{},"There are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like \"shot on goal\" to avoid prompt messages getting out of order. This could also be addressed with more code logic.",[11,9998,9999],{},"Prompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example.",[11,10001,10002,10003,10006],{},"I faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like ",[33,10004,10005],{},"SendMessage Good shot, bot!",", for example.",[168,10008,9149],{"id":4039},[11,10010,10011],{},"Participating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a \"Rocket League Coach\" plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc.",[11,10013,10014],{},"I think the gaming industry's adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I'm excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM.",[11,10016,10017],{},"Thanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!",[855,10019,10020],{},"html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":10022},[10023,10024,10025,10026,10027,10031,10036,10041,10042,10043],{"id":8183,"depth":249,"text":8184},{"id":9217,"depth":249,"text":9218},{"id":8499,"depth":249,"text":9247},{"id":8506,"depth":249,"text":8507},{"id":9036,"depth":249,"text":9037,"children":10028},[10029,10030],{"id":9608,"depth":312,"text":9609},{"id":9629,"depth":312,"text":9630},{"id":1723,"depth":249,"text":9080,"children":10032},[10033,10034,10035],{"id":9687,"depth":312,"text":9688},{"id":9733,"depth":312,"text":9734},{"id":9802,"depth":312,"text":9803},{"id":9840,"depth":249,"text":9841,"children":10037},[10038,10039,10040],{"id":9853,"depth":312,"text":9854},{"id":9906,"depth":312,"text":9907},{"id":9534,"depth":312,"text":9913},{"id":9096,"depth":249,"text":9097},{"id":9109,"depth":249,"text":9110},{"id":4039,"depth":249,"text":9149},"2024-02-17","This article discusses my entry for NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat",[10047,10049,10052],{"link":10048,"site":2196},"https://twitter.com/briancaffey/status/1760529251072118901",{"link":10050,"site":10051},"https://www.reddit.com/r/RocketLeague/comments/1au0po3/rocket_league_botchat_an_llmpowered_bakkesmod/","reddit",{"link":10053,"site":6303},"https://dev.to/briancaffey/rocket-league-botchat-powered-by-tensorrt-llm-my-submission-for-nvidias-generative-ai-on-rtx-pcs-developer-contest-2oao","/static/rlbc/cover.png",{},"/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",{"title":9195,"description":10045},"2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",[2203,2205,8452,6311,2204,4077,8454,10060,10061,10062],"rocket-league","gaming","windows","HaAMsCMaaJ3BhIyiZLcYmqoQM-LTHwIY6W9As87NzOs",{"id":10065,"title":10066,"body":10067,"comments":315,"date":11215,"description":11216,"draft":872,"extension":873,"external":11217,"image":11225,"meta":11226,"navigation":315,"path":11227,"seo":11228,"stem":11229,"tags":11230,"__hash__":11234},"blog/2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem.md","Python, Vue, Chinese-LLaMA-2 and The Three-Body Problem",{"type":8,"value":10068,"toc":11198},[10069,10071,10074,10100,10103,10106,10110,10113,10119,10128,10131,10137,10140,10146,10149,10155,10158,10167,10170,10174,10177,10192,10195,10215,10219,10228,10239,10243,10252,10255,10272,10278,10282,10285,10288,10291,10294,10300,10303,10307,10310,10324,10328,10331,10402,10408,10416,10422,10427,10466,10473,10478,10481,10504,10508,10511,10516,10524,10529,10535,10538,10542,10545,10549,10552,10832,10835,10850,10853,10925,10937,10940,10974,10981,10988,11027,11030,11039,11045,11048,11068,11075,11078,11082,11085,11091,11095,11098,11102,11106,11109,11115,11118,11124,11127,11130,11133,11137,11145,11148,11181,11190,11196],[2215,10070,8184],{"id":8183},[11,10072,10073],{},"This articles brings together several of my interest, both old and new:",[916,10075,10076,10079,10082,10085,10088,10091,10094,10097],{},[919,10077,10078],{},"The Sci-Fi book series 'Three-Body Problem' by Liu Cixun",[919,10080,10081],{},"Chinese language",[919,10083,10084],{},"NLP techniques",[919,10086,10087],{},"Large Language Models (LLMs)",[919,10089,10090],{},"Stable Diffusion",[919,10092,10093],{},"Data visualization and 3D graphics",[919,10095,10096],{},"Mathematics",[919,10098,10099],{},"NVIDIA / CUDA",[11,10101,10102],{},"This is a linguistic, artistic and computational experiment with the two big AI algorithms of 2023: large language models (LLMs) and Stable Diffusion. I used the leading open-source LLMs from China’s tech sector to translate and summarize the text of Chinese author Liu Cixin’s award-winning science fiction novel: The Three-Body Problem. The book's storyline is based on a simple yet elusive problem from classical physics: predicting the movement of three gravitationally-attracted objects in space. I generated code for simulations and visualizations of this physics problem to present my own solutions to the three-body problem based on parallel computation. I also used Stable Diffusion to portray the imaginitive solutions to the three-body physics problem from one of the book’s main settings: an immersive virtual-reality game that spans centries of world history.",[11,10104,10105],{},"I also share some of my experiences in China as an exchange student and research manager in the renewable energy technology sector. I wrote this article in English and translated it into Chinese using the same large language models I used to translate the Chinese text of the sci-fi novel into English. Warning: this article contains spoilers for the first book in the trilogy!",[168,10107,10109],{"id":10108},"back-story","Back story",[11,10111,10112],{},"A few months ago my company announced that another round of layoffs was to come the following week. I'm on an engineering team that had already been impacted by a few rounds of layoffs in the past year, and I was expecting to be let go. On an impulse I bought a book at the top of my reading list from Amazon: \"Three-Body Problem\". It is an award-winning Sci-Fi trilogy written by Liu Cixin, a Chinese computer engineer who started writing the book as a series of essays that were published in China's \"World of Sci-Fi\" magazine.",[11,10114,10115],{},[511,10116],{"alt":10117,"src":10118},"Images of Three Body Problem Book Series","/static/three-body-problem/books.png",[11,10120,10121,10122,10127],{},"I started learning Chinese in college, adding a major in Chinese Language to the mathematics major I decided on in my freshman year after taking vector calculus and linear algebra. In my sophmore year I did a semester abroad at Fudan University's ",[15,10123,10126],{"href":10124,"rel":10125},"https://ices.fudan.edu.cn/6628/list.htm",[19],"International Cultural Exchange School",". In 2007, living and studying Chinese in Shanghai as a 19 year old American was a really fun time. I was placed in an advanced-level course with a diverse group of students where English was not the lowest common linguistic denominator. We had a demanding cirriculum that emphasized reading, listening and speaking Chinese, but most of the language learning came through extracirricular activities: exploring Shanghai's food scene, bartering with vendors at the fabric markets, late night clubbing, walking around the Bund and the French Concession and chatting with my taxi cab drivers. It is hard to imagine how I did this without an iPhone, but I was able to get pretty far with an old Nokia 3310.",[11,10129,10130],{},"At the end of one night of particularly heavy drinking, some of my classmates and I dropped in on an wangba (internet cafe) before heading back to the international dorm. Chinese internet cafes in 2007 were an expansive underground dens of computers, monitors, MMORPGs, FPSs, cigarets, and on-demand instant noodles delivered directly to your seat through an app on the desktop. That night our game of choice was Counter-Strike. In one of the lowest points of my gaming career, my classmates and I were crushed by our Chinese counterterrorist opponent.",[11,10132,10133],{},[511,10134],{"alt":10135,"src":10136},"Chinese internet cafe","/static/three-body-problem/wangba.webp",[11,10138,10139],{},"My favorite memory of that semester at Fudan University was travelling on an epic over-night sleeper train from Shanghai to Guangxi province with a school-sponsored class trip to see Guilin. Multiple games of sam-yuk-gu (3-6-9) ran in parallel across the matrix of 3-by-2 sleeper car bunk beds lining the train car like workloads distributed across multiple GPU cores. The rules of 3-6-9 are simple: a group of people go around in a circle counting up from 1. If your number contains a 3, 6 or 9, you clap once for each occurance of the number instead of saying your number. The first person to break the rules takes a drink. Then repeat indefinitely. The next morning we all boarded a boat cruise in a daze to see the Lijiang river's stunning limestone peaks featured on the 20 yuan note:",[11,10141,10142],{},[511,10143],{"alt":10144,"src":10145},"20 yuan note with Guilin rock formations","/static/three-body-problem/twenty_small.gif",[11,10147,10148],{},"My second job after college took me back to China where I specialized in the technologies, policies and applications of large scale battery projects as a research manager for China's energy storage industry association. The job exposed me to the power industry and cutting-edge battery projects, and also sharpened my technical Chinese as I was frequently reading, translating in a bi-linguagl environment. It was fun  I didn't realize it at the time, but that job was great preperation for reading Chinese Sci-Fi novels.",[11,10150,10151],{},[511,10152],{"alt":10153,"src":10154},"State Grid HQ in Xi Cheng","/static/three-body-problem/invokeai/castles.png",[11,10156,10157],{},"My first introduction to the 'Three-Body Problem' book came from one of my best friends from college. He lived at the inner-most leaf-node of one of Beijing's most labrythnian hutongs next to a family that trained racing pigeons. My friend and I bonded over our study of Chinese language, classical guitar and our experiences in Beijing. I strongly considered his recommendation to check out 三体 (Three Body), the Chinese Sci-Fi novel about alien life in a solar system with three stars as he described it, but I never had the chance to read the book.",[10159,10160],"iframe",{"width":10161,"height":10162,"src":10163,"title":10164,"frameBorder":10165,"allow":10166,"allowFullScreen":315},"100%",315,"https://www.youtube.com/embed/5lj99Uz1d50?si=TwrypbY4vTfeWGRf","YouTube video player","0","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",[11,10168,10169],{},"Almost 10 years later I came across across a preview for the Netflix production of \"3 Body Problem\" scheduled to come out in early 2024. With the possibility of loosing my job weighing heavily on me, I picked up the books on Amazon hoping to have something to if I was going to be layed off. Over the weekend I was able to read a few of the chapters on my Kindle. I had not completely forgotten how to speak Chinese, and I could easily look up words and translate entire paragraphs with Google Translate.",[168,10171,10173],{"id":10172},"chinese-in-numbers","Chinese in numbers",[11,10175,10176],{},"Here's a quick primer on the Chinese language from a mathematical perspective. This will be helpful before jumping into using NLP and LLMs with Chinese text later in this article.",[11,10178,10179,10180,10185,10186,10191],{},"First, how many Chinese characters are there? This question isn't specific enough to have a single answer. A common rule of thumb that I have heard before says that there are over 50,000 characters in total with roughly 10,000 characters in use and about 3,000 characters frequently used in Chinese media and newspapers (",[15,10181,10184],{"href":10182,"rel":10183},"https://en.wikipedia.org/wiki/Chinese_language#Vocabulary",[19],"source","). ",[15,10187,10190],{"href":10188,"rel":10189},"https://stackoverflow.com/a/1366113/6084948",[19],"This answer"," from StackOverflow's legendary #1 ranked user VonC gives a good answer based on the number of Unicode characters in the CJK Unified Ideographs block: 20,992.",[11,10193,10194],{},"Here are some numbers and statistics to be better understand the text of the Three-Body Problem Chinese text:",[916,10196,10197,10200,10203,10206,10209,10212],{},[919,10198,10199],{},"188,380 total charactes in the book",[919,10201,10202],{},"2,859 unique characters in the book",[919,10204,10205],{},"36 chapters in the book",[919,10207,10208],{},"average of 69.78 paragraphs per chapter",[919,10210,10211],{},"total of 2,512 paragraphs in the book",[919,10213,10214],{},"average of 74.99 characters per paragraph",[911,10216,10218],{"id":10217},"character-frequency","Character Frequency",[11,10220,10221,10222,10227],{},"Let's look at how frequently each character in the book is used. We can also combine this with some data on the overall frequency of Chinese characters. The best measurement I found for overall character frequency is from ",[15,10223,10226],{"href":10224,"rel":10225},"https://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO",[19],"Middle Tennessee State University",". Here's a visualization that shows of all of unique characters in the book. The height of a column represents how frequently a character occurs in the book, and the color represents the relatively frequency of the character in Chinese language overall.",[10229,10230,10233],"div",{"className":10231},[10232],"wrap",[10159,10234],{"className":10235,"src":10237,"width":10161,"height":10238},[10236],"p-4","https://briancaffey.github.io/three-body-problem/tjs/load.html",550,[168,10240,10242],{"id":10241},"meta-llms-and-grass-mud-horse","Meta, LLMs, and Grass Mud Horse",[11,10244,10245,10246,10251],{},"In recent months I have been following the development of big open source AI projects. Two projects in particular are InvokeAI, an image generation tool based on Stable Diffusion, and ",[15,10247,10250],{"href":10248,"rel":10249},"https://ai.meta.com/llama/",[19],"LLaMA 2",", the latest generation of Meta's open source LLM. The name LLaMA stands for 'Large Language Model Meta AI', which happens to be the same spelling as the word for the domesticated South American camelid: llama. Before going deeper into LLMs we need a quick Chinese lesson.",[11,10253,10254],{},"草泥马 is a non-technical word that referes to animals like Llama or Alpaca. It can be directly translated as \"Grass Mud Horse\" and it is phonetically similar to the most common Chinese profanity: 操你妈, which literally means \"f*** your mother\". The characters in these two words are nearly synonymous: the sounds of both words are \"cao ni ma\", but the tones are different, which in Chinese changes the meaning completely. The llama is basically a legendary Chinese internet meme subversive in the face of government censorship. 🦙 was approved as part of Unicode 11.0 in 2018. The extended version of this profanity is 草泥马戈壁 (Cǎonímǎ Gēbì: Grass Mud Horse Gobi), refering to the geographical origin of this mythical creature: the Gobi Dessert. This term is more explicit as it synonymous with \"f*** your mother's c***\". Coincidentally, the Gobi Desert is a region of Inner Mongolia which borders the mountainous region of Greater Khingan Range (大兴安岭), the location of Red Coast and Radar Peak in Three-Body Problem where Ye Wenjie makes first contact with the Trisolarians.",[11,10256,10257,10258,10265,10266,10271],{},"I requested access to Meta's LLaMa 2 models as soon as they came out and I was able to get it to run on my NVIDIA RTX 4090 GPU. I also joined a subreddit called ",[15,10259,10262],{"href":10260,"rel":10261},"https://www.reddit.com/r/LocalLLaMA/",[19],[33,10263,10264],{},"r/LocalLLaMa"," with over seventy thousand members discussing how to run large language models on consumer hardware. Another annoucement that caught my attention in July was the release of ",[15,10267,10270],{"href":10268,"rel":10269},"https://github.com/ymcui/Chinese-LLaMA-Alpaca-2",[19],"Chinese LLaMa 2",", an open-source large language model trained on Chinese and English which does very well against Chinese Language LLM Benchmarks such as the CMMCU: Chinese Massive Multitask Language Understanding.",[11,10273,10274],{},[511,10275],{"alt":10276,"src":10277},"image of CMMLU","/static/three-body-problem/cmmlu.jpeg",[168,10279,10281],{"id":10280},"translation-in-and-of-the-three-body-problem","Translation in and of The Three-Body Problem",[11,10283,10284],{},"There are two important plot developments related to language translation in the Three-Body Problem novel, both of which involve book’s main female protagonist Ye Wenjie. First, copying the translation of Rachel Carson's 'Silent Spring' leads to her being relegated to the Red Coast project. At the Red Coast Ye Wenjie communicates with extraterrestrial life through a universal translation technology developed by the top-secret project.",[11,10286,10287],{},"Ken Liu’s translation of the Three-Body Problem book from Chinese to English places the events during the Cultural Revolution at the beginning of the book rather than in the middle of the book. According to Liu, this was done in order to avoid attention of government censors, and his original intention was to tell the story in this way, starting with the events of the late 1960's in China.",[11,10289,10290],{},"I tried translating the Chinese text of the Three-Body Problem book using LLMs. I started with the Chinese-LLaMA-2 model and then tried Qwen-7B-Chat, Baichuan-13B-Chat when these models came out. I found that the Qwen-7B-Chat model worked best for my translation tasks. Qwen is short for Qian Wen (千问, or \"one thousand questions\") and is developed by Alibaba Cloud.",[11,10292,10293],{},"How do you get an LLM to translate text? Ultimately the quality of the translation returned by the LLM depends on the prompt and other parameters used for inference. I experimented with both chat and completion approaches and tried lots of different kinds of prompts. The models I worked with have a 4K context window (the number of tokens the model can take as input when generating responses), so for translation tasks I had the LLM work on one paragraph at a time. Here's the prompt I used with the Qwen-7B-Chat model:",[26,10295,10298],{"className":10296,"code":10297,"language":31},[29],"\"你是一名翻译。请将每条消息从中文翻译成英文。\"\n(You are a translator. Please translate each message from Chinese to English.)\n",[33,10299,10297],{"__ignoreMap":35},[11,10301,10302],{},"I did some basic prompt engineering to get the LLM to translate the books in the Three-Body problem paragraph by paragraph. My computer was able to translate the first book overnight in under 500 minutes. Here are the results of my translation of Three-Body Problem with Qwen-7B-Chat model:",[10159,10304],{"className":10305,"src":10306,"width":10161,"height":10238},[10236],"https://briancaffey.github.io/three-body-problem/reader/?book=three_body&chapterNumber=1",[11,10308,10309],{},"It was interesting to see the failure modes of translation tasks for the different models. Most of the time the LLM was able to provided accurate translations. Some of the failure modes I observed were:",[916,10311,10312,10315,10318,10321],{},[919,10313,10314],{},"a few Chinese characters would show up in the English translations",[919,10316,10317],{},"a complete Chinese sentence would show up in an otherwise complete translation of a paragraph",[919,10319,10320],{},"The LLM refused to translate certain paragraphs that included violent imagery, such as the violent scenes from the Cultural Revolution chapters",[919,10322,10323],{},"If the sentence it was asked to translate was a question, the LLM would respond in Chinese to the question rather than providing a translation of the question itself",[911,10325,10327],{"id":10326},"tokenization","Tokenization",[11,10329,10330],{},"When you feed a prompt to an LLM, it first puts the prompt through a process called tokenization. Tokenization takes a string of text and breaks it down into tokens (defined by the Large Language Model you are using). The process of tokenization is similar to the tokenization done by spaCy mentioned earlier. These tokens produced by LLM tokenization are numbers. Here's an example of tokenization in action using the Chinese-Llama-2 model:",[26,10332,10334],{"className":1383,"code":10333,"language":1125,"meta":35,"style":35},"import json\nimport os\nfrom llama_cpp import Llama, LlamaTokenizer\n\nllm = Llama(\n    model_path=\"/path/to/models/ggml-model-q4_0.bin\",\n    n_ctx=4096,\n    n_gpu_layers=30\n)\n\ntokenizer = LlamaTokenizer(llama=llm)\n\nTEXT=\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"\ntokens = tokenizer.encode(TEXT)\n",[33,10335,10336,10341,10346,10351,10355,10360,10365,10370,10375,10379,10383,10388,10392,10397],{"__ignoreMap":35},[187,10337,10338],{"class":189,"line":190},[187,10339,10340],{},"import json\n",[187,10342,10343],{"class":189,"line":249},[187,10344,10345],{},"import os\n",[187,10347,10348],{"class":189,"line":312},[187,10349,10350],{},"from llama_cpp import Llama, LlamaTokenizer\n",[187,10352,10353],{"class":189,"line":319},[187,10354,316],{"emptyLinePlaceholder":315},[187,10356,10357],{"class":189,"line":325},[187,10358,10359],{},"llm = Llama(\n",[187,10361,10362],{"class":189,"line":686},[187,10363,10364],{},"    model_path=\"/path/to/models/ggml-model-q4_0.bin\",\n",[187,10366,10367],{"class":189,"line":697},[187,10368,10369],{},"    n_ctx=4096,\n",[187,10371,10372],{"class":189,"line":1291},[187,10373,10374],{},"    n_gpu_layers=30\n",[187,10376,10377],{"class":189,"line":1306},[187,10378,621],{},[187,10380,10381],{"class":189,"line":1434},[187,10382,316],{"emptyLinePlaceholder":315},[187,10384,10385],{"class":189,"line":2599},[187,10386,10387],{},"tokenizer = LlamaTokenizer(llama=llm)\n",[187,10389,10390],{"class":189,"line":2607},[187,10391,316],{"emptyLinePlaceholder":315},[187,10393,10394],{"class":189,"line":2621},[187,10395,10396],{},"TEXT=\"在那个已被忘却的日子里，它的世界颠覆了。泥土飞走，出现了一条又深又宽的峡谷，然后泥土又轰隆隆地飞回来，峡谷消失了，在原来峡谷的尽头出现了一座黑色的孤峰。其实，在这片广阔的疆域上，这种事常常发生，泥土飞走又飞回，峡谷出现又消失，然后是孤峰降临，好像是给每次灾变打上一个醒目的标记。褐蚁和几百个同族带着幸存的蚁后向太阳落下的方向走了一段路，建立了新的帝国。\"\n",[187,10398,10399],{"class":189,"line":2631},[187,10400,10401],{},"tokens = tokenizer.encode(TEXT)\n",[26,10403,10406],{"className":10404,"code":10405,"language":31},[29],"print(str(tokens[:4]) + \" ...\")\n",[33,10407,10405],{"__ignoreMap":35},[107,10409,10410],{},[11,10411,10412,10415],{},[187,10413,10414],{},"1, 30505, 32380, 36812"," ...",[26,10417,10420],{"className":10418,"code":10419,"language":31},[29],"for token in tokens:\n    text = tokenizer.decode([token])\n    print(text, end=\" \")\n",[33,10421,10419],{"__ignoreMap":35},[107,10423,10424],{},[11,10425,10426],{},"在 那个 已被 忘 却 的日子 里 ， 它的 世界 颠覆 了 。 泥 土 飞 走 ， 出现了 一条 又 深 又 宽 的 峡谷 ， 然后 泥 土 又 轰 隆 隆 地 飞 回来 ， 峡谷 消失 了 ， 在 原来 峡谷 的 尽头 出现了 一座 黑色 的 孤 峰 。 其实 ， 在这 片 广阔 的 疆 域 上 ， 这种事 常常 发生 ， 泥 土 飞 走 又 飞 回 ， 峡谷 出现 又 消失 ， 然后 是 孤 峰 降临 ， 好像是 给 每次 灾 变 打 上 一个 醒目 的 标记 。 褐 蚁 和 几百 个 同 族 带着 幸 存 的 蚁 后 向 太阳 落 下的 方向 走了 一段 路 ， 建立了 新的 帝国 。",[26,10428,10430],{"className":1383,"code":10429,"language":1125,"meta":35,"style":35},"english_text = \"This is an example of tokenization using a large language model.\"\nenglish_tokens = tokenizer.encode(english_text)\nprint(str(english_tokens[:4]) + \" ...\")\n\nfor token in english_tokens:\n    text = tokenizer.decode([token])\n    print(f\"'{text}'\", end=\" \")\n",[33,10431,10432,10437,10442,10447,10451,10456,10461],{"__ignoreMap":35},[187,10433,10434],{"class":189,"line":190},[187,10435,10436],{},"english_text = \"This is an example of tokenization using a large language model.\"\n",[187,10438,10439],{"class":189,"line":249},[187,10440,10441],{},"english_tokens = tokenizer.encode(english_text)\n",[187,10443,10444],{"class":189,"line":312},[187,10445,10446],{},"print(str(english_tokens[:4]) + \" ...\")\n",[187,10448,10449],{"class":189,"line":319},[187,10450,316],{"emptyLinePlaceholder":315},[187,10452,10453],{"class":189,"line":325},[187,10454,10455],{},"for token in english_tokens:\n",[187,10457,10458],{"class":189,"line":686},[187,10459,10460],{},"    text = tokenizer.decode([token])\n",[187,10462,10463],{"class":189,"line":697},[187,10464,10465],{},"    print(f\"'{text}'\", end=\" \")\n",[107,10467,10468],{},[11,10469,10470,10415],{},[187,10471,10472],{},"1, 4013, 338, 385",[107,10474,10475],{},[11,10476,10477],{},"'' 'This' ' is' ' an' ' example' ' of' ' token' 'ization' ' using' ' a' ' large' ' language' ' model' '.'",[11,10479,10480],{},"Here are some key differences between English and Chinese that have implications for how the language is tokenized by large language models:",[916,10482,10483,10486,10489,10492,10495,10498,10501],{},[919,10484,10485],{},"Chinese does not use spaces between words like English does",[919,10487,10488],{},"Chinese words are typically formed from 2 or more characters",[919,10490,10491],{},"Chinese verbs are not conjugated and do not have different tenses",[919,10493,10494],{},"Chinese words don't have singular and plural variants",[919,10496,10497],{},"Chinese grammar is very simple and is similar to English",[919,10499,10500],{},"Chinese characters do not have capitization like ASCII characters",[919,10502,10503],{},"The token represented by the number 1 encodes a starting token",[168,10505,10507],{"id":10506},"imagining-scenes-from-three-body-problem-with-stable-diffusion","Imagining scenes from Three-Body Problem with Stable Diffusion",[11,10509,10510],{},"Here are some images I generated using Stable Diffusion with InvokeAI that depict scenes from the Three-Body Problem book. These scenes portray solutions to the Three-Body Problem that players in the Three-Body game devised. The first is a Confucian system of etiquette for predicting the movement of the three suns. The second is a human-powered computer that Qin Shi Huang used to try to predict the movement of the three suns.",[107,10512,10513],{},[11,10514,10515],{},"Prompt: Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style",[10517,10518,10519],"client-only",{},[10520,10521],"carousel",{":count":10522,"dir":10523},"8","confucius",[107,10525,10526],{},[11,10527,10528],{},"array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle",[10517,10530,10531],{},[10520,10532],{":count":10533,"dir":10534},"5","computer",[11,10536,10537],{},"Congrats to the InvokeAI team on the 3.0 release. It has been awesome to use and the current docker compose setup is a huge improvement on the 2.x version.",[168,10539,10541],{"id":10540},"n-body-simulations-cuda-and-threejs","n-body simulations, CUDA and Three.js",[11,10543,10544],{},"The nbody problem has no closed-form analytical solution, but it is possible to do a basic simulation of the three-body problem on consumer hardware and open source software, like NVIDIA and CUDA.",[911,10546,10548],{"id":10547},"three-body-cuda-simulation","Three-Body CUDA simulation",[11,10550,10551],{},"I wrote a simple program with the help of ChatGPT for running nbody problem simulations. The program uses CuPy, a Python library that exposes APIs for doing matrix multiplication to predict the position of three bodies using Euclidian Integration. Here's the script:",[26,10553,10557],{"className":10554,"code":10555,"language":10556,"meta":35,"style":35},"language-py shiki shiki-themes github-light github-dark","import numpy as np\nimport cupy as cp\nimport time\nimport json\n\n# Simulation parameters\nNUM_PARTICLES = 3\nDIMENSIONS = 3 # 3D space\nNUM_STEPS = 30\nDT = 0.1\n\n# Generate initial positions and velocities\nnp_positions = np.random.randn(NUM_PARTICLES, DIMENSIONS)\nnp_velocities = np.random.randn(NUM_PARTICLES, DIMENSIONS)\n\ncp_positions = cp.array(np_positions)\ncp_velocities = cp.array(np_velocities)\n\nnp_ticks = np.expand_dims(np_positions, axis=0)\ncp_ticks = cp.array(np_ticks)\n\n# nbody simulation loop\nstart_time = time.time()\nfor step in range(NUM_STEPS):\n\n    # this gets pairwise differences\n    diff = cp_positions[:, None, :] - cp_positions[None, :, :]\n    distances = cp.sqrt(cp.sum(diff**2, axis=2))\n\n    # avoid division by zero\n    epsilon = 1e-5\n    inv_distances = 1.0 / cp.maximum(distances, epsilon)\n\n    # calculate forces\n    cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n\n    # update velocities and positions\n    cp_velocities += DT * cp_forces\n    cp_positions += DT * cp_velocities\n    cp_ticks = cp.append(cp_ticks, cp.expand_dims(cp_positions, 0), 0)\n\nsim_time = time.time() - start_time\nprint(\"Simulation time:\", sim_time)\n\n\nclass NumpyArrayEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\n\nnp_ticks = cp_ticks.get()\n\n\n# this is data we can work with in python and write to a file\nwith open(\"ticks.json\", \"w\") as f:\n    f.write(json.dumps(np_ticks, cls=NumpyArrayEncoder))\n","py",[33,10558,10559,10564,10569,10574,10578,10582,10587,10592,10597,10602,10607,10611,10616,10621,10626,10630,10635,10640,10644,10649,10654,10658,10663,10668,10673,10677,10682,10687,10692,10696,10701,10706,10711,10715,10720,10725,10729,10734,10739,10744,10749,10753,10758,10763,10767,10771,10776,10781,10786,10791,10796,10800,10804,10809,10813,10817,10822,10827],{"__ignoreMap":35},[187,10560,10561],{"class":189,"line":190},[187,10562,10563],{},"import numpy as np\n",[187,10565,10566],{"class":189,"line":249},[187,10567,10568],{},"import cupy as cp\n",[187,10570,10571],{"class":189,"line":312},[187,10572,10573],{},"import time\n",[187,10575,10576],{"class":189,"line":319},[187,10577,10340],{},[187,10579,10580],{"class":189,"line":325},[187,10581,316],{"emptyLinePlaceholder":315},[187,10583,10584],{"class":189,"line":686},[187,10585,10586],{},"# Simulation parameters\n",[187,10588,10589],{"class":189,"line":697},[187,10590,10591],{},"NUM_PARTICLES = 3\n",[187,10593,10594],{"class":189,"line":1291},[187,10595,10596],{},"DIMENSIONS = 3 # 3D space\n",[187,10598,10599],{"class":189,"line":1306},[187,10600,10601],{},"NUM_STEPS = 30\n",[187,10603,10604],{"class":189,"line":1434},[187,10605,10606],{},"DT = 0.1\n",[187,10608,10609],{"class":189,"line":2599},[187,10610,316],{"emptyLinePlaceholder":315},[187,10612,10613],{"class":189,"line":2607},[187,10614,10615],{},"# Generate initial positions and velocities\n",[187,10617,10618],{"class":189,"line":2621},[187,10619,10620],{},"np_positions = np.random.randn(NUM_PARTICLES, DIMENSIONS)\n",[187,10622,10623],{"class":189,"line":2631},[187,10624,10625],{},"np_velocities = np.random.randn(NUM_PARTICLES, DIMENSIONS)\n",[187,10627,10628],{"class":189,"line":2642},[187,10629,316],{"emptyLinePlaceholder":315},[187,10631,10632],{"class":189,"line":2653},[187,10633,10634],{},"cp_positions = cp.array(np_positions)\n",[187,10636,10637],{"class":189,"line":2665},[187,10638,10639],{},"cp_velocities = cp.array(np_velocities)\n",[187,10641,10642],{"class":189,"line":2674},[187,10643,316],{"emptyLinePlaceholder":315},[187,10645,10646],{"class":189,"line":2684},[187,10647,10648],{},"np_ticks = np.expand_dims(np_positions, axis=0)\n",[187,10650,10651],{"class":189,"line":2694},[187,10652,10653],{},"cp_ticks = cp.array(np_ticks)\n",[187,10655,10656],{"class":189,"line":2706},[187,10657,316],{"emptyLinePlaceholder":315},[187,10659,10660],{"class":189,"line":2715},[187,10661,10662],{},"# nbody simulation loop\n",[187,10664,10665],{"class":189,"line":2725},[187,10666,10667],{},"start_time = time.time()\n",[187,10669,10670],{"class":189,"line":2735},[187,10671,10672],{},"for step in range(NUM_STEPS):\n",[187,10674,10675],{"class":189,"line":2743},[187,10676,316],{"emptyLinePlaceholder":315},[187,10678,10679],{"class":189,"line":2754},[187,10680,10681],{},"    # this gets pairwise differences\n",[187,10683,10684],{"class":189,"line":2762},[187,10685,10686],{},"    diff = cp_positions[:, None, :] - cp_positions[None, :, :]\n",[187,10688,10689],{"class":189,"line":2770},[187,10690,10691],{},"    distances = cp.sqrt(cp.sum(diff**2, axis=2))\n",[187,10693,10694],{"class":189,"line":2781},[187,10695,316],{"emptyLinePlaceholder":315},[187,10697,10698],{"class":189,"line":2792},[187,10699,10700],{},"    # avoid division by zero\n",[187,10702,10703],{"class":189,"line":2803},[187,10704,10705],{},"    epsilon = 1e-5\n",[187,10707,10708],{"class":189,"line":2808},[187,10709,10710],{},"    inv_distances = 1.0 / cp.maximum(distances, epsilon)\n",[187,10712,10713],{"class":189,"line":2816},[187,10714,316],{"emptyLinePlaceholder":315},[187,10716,10717],{"class":189,"line":2824},[187,10718,10719],{},"    # calculate forces\n",[187,10721,10722],{"class":189,"line":2834},[187,10723,10724],{},"    cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n",[187,10726,10727],{"class":189,"line":2845},[187,10728,316],{"emptyLinePlaceholder":315},[187,10730,10731],{"class":189,"line":2856},[187,10732,10733],{},"    # update velocities and positions\n",[187,10735,10736],{"class":189,"line":2867},[187,10737,10738],{},"    cp_velocities += DT * cp_forces\n",[187,10740,10741],{"class":189,"line":2878},[187,10742,10743],{},"    cp_positions += DT * cp_velocities\n",[187,10745,10746],{"class":189,"line":2886},[187,10747,10748],{},"    cp_ticks = cp.append(cp_ticks, cp.expand_dims(cp_positions, 0), 0)\n",[187,10750,10751],{"class":189,"line":2900},[187,10752,316],{"emptyLinePlaceholder":315},[187,10754,10755],{"class":189,"line":2905},[187,10756,10757],{},"sim_time = time.time() - start_time\n",[187,10759,10760],{"class":189,"line":2913},[187,10761,10762],{},"print(\"Simulation time:\", sim_time)\n",[187,10764,10765],{"class":189,"line":2921},[187,10766,316],{"emptyLinePlaceholder":315},[187,10768,10769],{"class":189,"line":2931},[187,10770,316],{"emptyLinePlaceholder":315},[187,10772,10773],{"class":189,"line":2942},[187,10774,10775],{},"class NumpyArrayEncoder(json.JSONEncoder):\n",[187,10777,10778],{"class":189,"line":2953},[187,10779,10780],{},"    def default(self, obj):\n",[187,10782,10783],{"class":189,"line":2964},[187,10784,10785],{},"        if isinstance(obj, np.ndarray):\n",[187,10787,10788],{"class":189,"line":2975},[187,10789,10790],{},"            return obj.tolist()\n",[187,10792,10793],{"class":189,"line":2983},[187,10794,10795],{},"        return json.JSONEncoder.default(self, obj)\n",[187,10797,10798],{"class":189,"line":2992},[187,10799,316],{"emptyLinePlaceholder":315},[187,10801,10802],{"class":189,"line":3001},[187,10803,316],{"emptyLinePlaceholder":315},[187,10805,10806],{"class":189,"line":3010},[187,10807,10808],{},"np_ticks = cp_ticks.get()\n",[187,10810,10811],{"class":189,"line":3019},[187,10812,316],{"emptyLinePlaceholder":315},[187,10814,10815],{"class":189,"line":3028},[187,10816,316],{"emptyLinePlaceholder":315},[187,10818,10819],{"class":189,"line":3033},[187,10820,10821],{},"# this is data we can work with in python and write to a file\n",[187,10823,10824],{"class":189,"line":3041},[187,10825,10826],{},"with open(\"ticks.json\", \"w\") as f:\n",[187,10828,10829],{"class":189,"line":3049},[187,10830,10831],{},"    f.write(json.dumps(np_ticks, cls=NumpyArrayEncoder))\n",[11,10833,10834],{},"To better understand the matrix math here I walked through a simple example of what each step does:",[26,10836,10838],{"className":10554,"code":10837,"language":10556,"meta":35,"style":35},"# particle coordinates (x,y,z) in 3D space\npositions = cp.array([[1,2.5,3], [4,5,6], [7,8,9]])\n",[33,10839,10840,10845],{"__ignoreMap":35},[187,10841,10842],{"class":189,"line":190},[187,10843,10844],{},"# particle coordinates (x,y,z) in 3D space\n",[187,10846,10847],{"class":189,"line":249},[187,10848,10849],{},"positions = cp.array([[1,2.5,3], [4,5,6], [7,8,9]])\n",[11,10851,10852],{},"The first operation creates an array for pairwise distances for each dimension:",[26,10854,10856],{"className":1383,"code":10855,"language":1125,"meta":35,"style":35},"diff = positions[None, :, :] - positions[:, None, :]\nprint(diff)\n\narray([[[ 0. ,  0. ,  0. ],\n        [ 3. ,  2.5,  3. ],\n        [ 6. ,  5.5,  6. ]],\n\n       [[-3. , -2.5, -3. ],\n        [ 0. ,  0. ,  0. ],\n        [ 3. ,  3. ,  3. ]],\n\n       [[-6. , -5.5, -6. ],\n        [-3. , -3. , -3. ],\n        [ 0. ,  0. ,  0. ]]])\n",[33,10857,10858,10863,10868,10872,10877,10882,10887,10891,10896,10901,10906,10910,10915,10920],{"__ignoreMap":35},[187,10859,10860],{"class":189,"line":190},[187,10861,10862],{},"diff = positions[None, :, :] - positions[:, None, :]\n",[187,10864,10865],{"class":189,"line":249},[187,10866,10867],{},"print(diff)\n",[187,10869,10870],{"class":189,"line":312},[187,10871,316],{"emptyLinePlaceholder":315},[187,10873,10874],{"class":189,"line":319},[187,10875,10876],{},"array([[[ 0. ,  0. ,  0. ],\n",[187,10878,10879],{"class":189,"line":325},[187,10880,10881],{},"        [ 3. ,  2.5,  3. ],\n",[187,10883,10884],{"class":189,"line":686},[187,10885,10886],{},"        [ 6. ,  5.5,  6. ]],\n",[187,10888,10889],{"class":189,"line":697},[187,10890,316],{"emptyLinePlaceholder":315},[187,10892,10893],{"class":189,"line":1291},[187,10894,10895],{},"       [[-3. , -2.5, -3. ],\n",[187,10897,10898],{"class":189,"line":1306},[187,10899,10900],{},"        [ 0. ,  0. ,  0. ],\n",[187,10902,10903],{"class":189,"line":1434},[187,10904,10905],{},"        [ 3. ,  3. ,  3. ]],\n",[187,10907,10908],{"class":189,"line":2599},[187,10909,316],{"emptyLinePlaceholder":315},[187,10911,10912],{"class":189,"line":2607},[187,10913,10914],{},"       [[-6. , -5.5, -6. ],\n",[187,10916,10917],{"class":189,"line":2621},[187,10918,10919],{},"        [-3. , -3. , -3. ],\n",[187,10921,10922],{"class":189,"line":2631},[187,10923,10924],{},"        [ 0. ,  0. ,  0. ]]])\n",[11,10926,10927,10928,637,10930,1172,10933,10936],{},"The rows of zeros correspond to a particle's ",[33,10929,2196],{},[33,10931,10932],{},"y",[33,10934,10935],{},"z"," distances to itself, which are all zero by axioms of Euclidian vector spaces.",[11,10938,10939],{},"The next operation calculates the distance between each particle:",[26,10941,10943],{"className":1383,"code":10942,"language":1125,"meta":35,"style":35},"distances = cp.sqrt(cp.sum(diff**2, axis=2))\nprint(distances)\n\narray([[ 0.        ,  4.9244289 , 10.11187421],\n       [ 4.9244289 ,  0.        ,  5.19615242],\n       [10.11187421,  5.19615242,  0.        ]])\n",[33,10944,10945,10950,10955,10959,10964,10969],{"__ignoreMap":35},[187,10946,10947],{"class":189,"line":190},[187,10948,10949],{},"distances = cp.sqrt(cp.sum(diff**2, axis=2))\n",[187,10951,10952],{"class":189,"line":249},[187,10953,10954],{},"print(distances)\n",[187,10956,10957],{"class":189,"line":312},[187,10958,316],{"emptyLinePlaceholder":315},[187,10960,10961],{"class":189,"line":319},[187,10962,10963],{},"array([[ 0.        ,  4.9244289 , 10.11187421],\n",[187,10965,10966],{"class":189,"line":325},[187,10967,10968],{},"       [ 4.9244289 ,  0.        ,  5.19615242],\n",[187,10970,10971],{"class":189,"line":686},[187,10972,10973],{},"       [10.11187421,  5.19615242,  0.        ]])\n",[11,10975,10976,10977,10980],{},"The diagonal or zeros represents that fact that a particle ",[33,10978,10979],{},"n"," has a distance of zero to iself.",[11,10982,10983,10984,10987],{},"The next step calculates inverse distances and uses a small ",[33,10985,10986],{},"epsilon"," value to avoid division by 0:",[26,10989,10991],{"className":1383,"code":10990,"language":1125,"meta":35,"style":35},"epsilon = 1e-5\ninv_distances = 1.0 / cp.maximum(distances, epsilon)\nprint(inv_distances)\n\narray([[1.00000000e+05, 2.03069233e-01, 9.88936353e-02],\n       [2.03069233e-01, 1.00000000e+05, 1.92450090e-01],\n       [9.88936353e-02, 1.92450090e-01, 1.00000000e+05]])\n",[33,10992,10993,10998,11003,11008,11012,11017,11022],{"__ignoreMap":35},[187,10994,10995],{"class":189,"line":190},[187,10996,10997],{},"epsilon = 1e-5\n",[187,10999,11000],{"class":189,"line":249},[187,11001,11002],{},"inv_distances = 1.0 / cp.maximum(distances, epsilon)\n",[187,11004,11005],{"class":189,"line":312},[187,11006,11007],{},"print(inv_distances)\n",[187,11009,11010],{"class":189,"line":319},[187,11011,316],{"emptyLinePlaceholder":315},[187,11013,11014],{"class":189,"line":325},[187,11015,11016],{},"array([[1.00000000e+05, 2.03069233e-01, 9.88936353e-02],\n",[187,11018,11019],{"class":189,"line":686},[187,11020,11021],{},"       [2.03069233e-01, 1.00000000e+05, 1.92450090e-01],\n",[187,11023,11024],{"class":189,"line":697},[187,11025,11026],{},"       [9.88936353e-02, 1.92450090e-01, 1.00000000e+05]])\n",[11,11028,11029],{},"The next step is the most elegant part of the simulation and really flexes the GPU's parallel compute capabilities:",[26,11031,11033],{"className":1383,"code":11032,"language":1125,"meta":35,"style":35},"cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n",[33,11034,11035],{"__ignoreMap":35},[187,11036,11037],{"class":189,"line":190},[187,11038,11032],{},[11,11040,6131,11041,11044],{},[33,11042,11043],{},".T"," operation transposes a matrix, multiplies by the cube of inverse distances, then transposes the matrix again before summing along the first axis. Transposing a matrix basically swaps rows and columns.",[11,11046,11047],{},"The next two steps are also pretty elegant:",[26,11049,11051],{"className":1383,"code":11050,"language":1125,"meta":35,"style":35},"# update velocities and positions\ncp_velocities += DT * cp_forces\ncp_positions += DT * cp_velocities\n",[33,11052,11053,11058,11063],{"__ignoreMap":35},[187,11054,11055],{"class":189,"line":190},[187,11056,11057],{},"# update velocities and positions\n",[187,11059,11060],{"class":189,"line":249},[187,11061,11062],{},"cp_velocities += DT * cp_forces\n",[187,11064,11065],{"class":189,"line":312},[187,11066,11067],{},"cp_positions += DT * cp_velocities\n",[11,11069,11070,11071,11074],{},"In the last step I append the updated positions to an array that holds every \"tick\" (the positions of each particle between each time interval, ",[33,11072,11073],{},"DT"," - \"delta time\")",[11,11076,11077],{},"Here's the formula for the mathematical equation used to calculate the force on any given body in an n-body system:",[10159,11079],{"src":11080,"width":10161,"height":11081},"https://briancaffey.github.io/three-body-problem/iframe/formula.html",110,[11,11083,11084],{},"To test that the simulation was working correctly I used ChatGPT again to construct a 3D scene in Blender with a Python script:",[11,11086,11087],{},[511,11088],{"alt":11089,"src":11090},"Blender Animation","/static/three-body-problem/blender.png",[911,11092,11094],{"id":11093},"threejs","Three.js",[11,11096,11097],{},"Imagine that we are working for a Chinese startup called the Qin Dynasty. The founder, Qin Shi Huang, is a brutal tyrant with an obsessive fear of assassination. Let's put on our product hat for a minute and think about how we can impress him with a clean solution to the three-body problem. A recent attempt involved building a 30,000-person analog computer that was destroyed in tri-solar syzygy. Failing to accurately predict the movement of the suns could mean execution by live-burial, a fiery death or worse. Using CUDA and Blender is a good MVP but doesn't make for the best technical demo since it involves so many different steps: running the simulation in CUDA, exporting data to JSON, loading data into a visualuzation and then finally rendering a video of the simulation. With a popular Javascript library called Three.js we can run an interactive three-body problem simulation in real-time right in the browser. Here's the three-body simulation I also co-authored with ChatGPT-4 using Three.js:",[10159,11099],{"src":11100,"width":10161,"height":11101},"https://briancaffey.github.io/three-body-problem/three/",350,[168,11103,11105],{"id":11104},"screen-adaptations-the-gaming-industry-and-the-ccp","Screen adaptations, the gaming industry and the CCP",[11,11107,11108],{},"Dream of the Red Chamber is one of China's Four Great Classical Novels and is often seen as the pinacle of Chinese fiction. It was written in the mid 18th century and first published in 1791. It is a long saga that totals 960,000 characters in length, on similar scale to the length of the Three-Body Problem trilogy. Sun Wen, a Qing dynasty artist, spent 36 years of his life doing a series of 230 paintings depicting scenes from the Dream of the Red Chamber: dream sequences, demons, goddesses, nuns, nobles, beggars, raging fires, landscapes, interiors, wildlife, gardens, temples, funerals, battles, processions, banquets, trials, operas, marriages.",[11,11110,11111],{},[511,11112],{"alt":11113,"src":11114},"Sun Wen paintings sample","/static/three-body-problem/dorc.png",[11,11116,11117],{},"Following in this tradition of celebrating great literature, Tencent Video and China Central Television produced a 30-episode adaptation of the Three-Body Problem that was released in Feburary 2023. It is a surprisingly faithful reproduction of the book that is worth checking out. The portrayal of Shi Qiang (Da Shi) was easily my favorite part of the series. I was also impressed by how the Three-Body VR game scenes were done with computer graphics. It got me thinking about how China is represented in some of the worlds most popular video games.",[11,11119,11120],{},[511,11121],{"alt":11122,"src":11123},"games","/static/three-body-problem/game.png",[11,11125,11126],{},"This is Rocket League, a competitive vehicular soccer game where players, like in the Three-Body game, must master the laws of gravity. The Chinese-themed Forbiden Temple arena shown here is one of many virtual international venues in the game. Epic Games (creator of Fortnite) bought Rocket League in 2019 for an estimated $250 to $300 million.",[11,11128,11129],{},"Like Rocket League, Overwatch is a highly-competitive eSport on a global scale. It features a large roster of 38 players from all over the world. Dr. Mei-Ling Zhou (周美灵) is a Chinese climatologist who uses ice both to attack opponents and to defend herself. Mei became controversial in China due to her adoption as a symbolic figure in the 2019 Hong Kong protests.",[11,11131,11132],{},"Three college friends combined their interest in anime, comics and games (ACG) and literature to publish one of the most successful games created by a Chinese company and arguably one of China's most important cultural exports: Genshin Impact. miHoYo, the Shanghai-based company that develops Genshin Impact, grossed $4 billion of revenue globally in the game's first year setting a new record in the gaming industry. Like other companies of its size, miHoYo has a party committee under the Chinese Communist Party that influences the company's operations.",[168,11134,11136],{"id":11135},"ai-and-layoffs-in-the-tech-industry","AI and layoffs in the tech industry",[11,11138,11139,11140,752],{},"I have a positive attitude toward AI and its ability to supercharge the creative work we do, but I also think that replacing humans and AI-related layoffs should should be a part of the conversation. I wasn't impacted by the recent round of layoffs at my company, and I'm grateful to have the opportunity to work with a talented team on interesting problems in the health tech industry. I do have some solid references for folks in DevOps, product and backend and frontend engineering, and I’m happy to ",[15,11141,11144],{"href":11142,"rel":11143},"https://www.linkedin.com/in/brian-caffey-06b22a18/",[19],"connect and share via LinkedIn",[11,11146,11147],{},"Layoffs in both China and the U.S. have been pummled the tech sector over the last two years. New graduates in China are also facing a difficult job market. There are some popular expressions that paint a picture of the job market in China:",[916,11149,11150,11153,11156,11164,11173],{},[919,11151,11152],{},"躺平 Lying flat: avoiding relentless work",[919,11154,11155],{},"九九六 996 Work culture: describes working from 9AM to 9PM, 6 days a week, a common work schedule for many Chinese employees",[919,11157,11158,11159],{},"全职儿女 Full-time Children: ",[15,11160,11163],{"href":11161,"rel":11162},"https://www.cnn.com/2023/07/26/economy/china-youth-unemployment-intl-hnk/index.html",[19],"Young Chinese are getting paid to be ‘full-time children’ as jobs become harder to find",[919,11165,11166,11167,11172],{},"35岁诅咒 The curse of 35: ",[15,11168,11171],{"href":11169,"rel":11170},"https://www.marketplace.org/shows/marketplace-tech/chinas-tech-workers-ageism-the-curse-of-35/",[19],"Ageism in China’s tech sector has workers fearing the “curse of 35”",". Shout out to my fellow Year of the Dragon 35 year olds! 🐲",[919,11174,11175,11176],{},"吃苦 Eat Bitterness: ",[15,11177,11180],{"href":11178,"rel":11179},"https://www.nytimes.com/2023/05/30/business/china-youth-unemployment.html",[19],"China’s Young People Can’t Find Jobs. Xi Jinping Says to ‘Eat Bitterness.’",[11,11182,11183,11184,11189],{},"It is an exciting time for AI. Elon Musk and Kaifu Lee have both recently released open-source large language models: Grok and Yi. Sam Altman was fired as CEO of OpenAI, then came back. Stable Diffusion just released a text to video model. AGI might already be here. In the U.S., we are going into our first presidential election cycle with AI fully turned on. Here's a ",[15,11185,11188],{"href":11186,"rel":11187},"https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032",[19],"link to The Three-Body Problem book on Amazon",". Thanks for reading and Happy Thanksgiving!",[11,11191,11192],{},[511,11193],{"alt":11194,"src":11195},"Happy Thanksgiving","/static/three-body-problem/thanksgiving.png",[855,11197,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":11199},[11200,11201,11204,11205,11208,11209,11213,11214],{"id":10108,"depth":249,"text":10109},{"id":10172,"depth":249,"text":10173,"children":11202},[11203],{"id":10217,"depth":312,"text":10218},{"id":10241,"depth":249,"text":10242},{"id":10280,"depth":249,"text":10281,"children":11206},[11207],{"id":10326,"depth":312,"text":10327},{"id":10506,"depth":249,"text":10507},{"id":10540,"depth":249,"text":10541,"children":11210},[11211,11212],{"id":10547,"depth":312,"text":10548},{"id":11093,"depth":312,"text":11094},{"id":11104,"depth":249,"text":11105},{"id":11135,"depth":249,"text":11136},"2023-11-23","Translating The Three-Body Problem book to English with Chinese LLMs, making visualizations with stable diffusion and running n-body simulations with CUDA",[11218,11221,11223],{"link":11219,"site":11220},"https://news.ycombinator.com/item?id=38393757","hn",{"link":11222,"site":10051},"https://www.reddit.com/r/threebodyproblem/comments/1823l5j/python_vue_chinesellama2_and_the_threebody_problem/",{"link":11224,"site":2196},"https://twitter.com/briancaffey/status/1727710878349332614","/static/three-body-problem/cover.png",{},"/2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem",{"title":10066,"description":11216},"2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem",[11231,8454,4077,2204,6312,1125,2203,11232,8452,8456,8451,882,11233],"three-body-problem","cuda","three.js","8BBXhYErGw8y8iMUxQLkwR29D5LohviB5LPXpWkxrKU",{"id":11236,"title":11237,"body":11238,"comments":315,"date":15266,"description":15267,"draft":872,"extension":873,"external":15268,"image":15284,"meta":15285,"navigation":315,"path":15286,"seo":15287,"stem":15288,"tags":15289,"__hash__":15299},"blog/2023/01/07/i-deployed-the-same-containerized-serverless-django-app-with-aws-cdk-terraform-and-pulumi.md","My Infrastructure as Code Rosetta Stone - Deploying the same web application on AWS ECS Fargate with CDK, Terraform and Pulumi",{"type":8,"value":11239,"toc":15211},[11240,11242,11245,11295,11298,11301,11304,11315,11319,11324,11332,11337,11348,11353,11364,11369,11388,11397,11409,11414,11425,11430,11438,11443,11461,11465,11469,11472,11491,11495,11498,11505,11508,11519,11538,11549,11552,11581,11585,11588,11617,11621,11639,11648,11657,11661,11686,11708,11717,11723,11727,11730,11789,11795,11896,11902,11905,11909,11934,11938,11941,11957,11970,11975,11981,11986,11992,11997,12003,12008,12014,12018,12024,12030,12036,12042,12048,12054,12060,12064,12067,12090,12093,12096,12102,12105,12109,12116,12136,12139,12165,12168,12188,12192,12198,12211,12218,12222,12225,12249,12252,12255,12264,12296,12299,12307,12311,12314,12340,12344,12351,12357,12361,12364,12372,12375,12379,12385,12402,12413,12416,12419,12523,12527,12530,12544,12547,12550,12554,12560,12563,12566,12672,12679,12685,12692,12714,12717,12720,12736,12739,12761,12765,12768,12800,12813,12817,12820,12823,12829,12833,12848,12857,12861,12867,12908,12911,12953,12959,12965,12969,12972,12983,12987,12990,12993,12997,13017,13020,13036,13039,13043,13051,13060,13064,13081,13089,13095,13126,13129,13307,13310,13394,13397,13400,13403,13524,13527,13712,13715,14230,14234,14237,14270,14274,14286,14295,14298,14322,14326,14329,14337,14340,14367,14377,14382,14451,14454,14537,14540,14569,14572,14665,14676,14746,14749,14753,14759,14762,14768,14774,14780,14784,14789,14802,14811,14822,14830,14833,14844,14847,14853,14857,14871,14875,14925,14932,14937,14940,14946,14950,14961,14973,14976,14980,15012,15016,15019,15128,15132,15135,15193,15196,15199,15208],[168,11241,8184],{"id":8183},[11,11243,11244],{},"I wrote three infrastructure as code libraries for deploying containerized 3-tier web apps on AWS ECS Fargate using CDK, Terraform and Pulumi. This article will provide an overview of my experience working with these three IaC tools and will show how I use my libraries in automated infrastructure deployment pipelines with GitHub Actions.",[916,11246,11247,11257,11267,11277],{},[919,11248,11249,585,11252],{},[338,11250,11251],{},"CDK Construct Library",[15,11253,11256],{"href":11254,"rel":11255},"https://github.com/briancaffey/cdk-django",[19],"github.com/briancaffey/cdk-django",[919,11258,11259,585,11262],{},[338,11260,11261],{},"Terraform Modules",[15,11263,11266],{"href":11264,"rel":11265},"https://github.com/briancaffey/terraform-aws-django",[19],"github.com/briancaffey/terraform-aws-django",[919,11268,11269,585,11272],{},[338,11270,11271],{},"Pulumi Component Library",[15,11273,11276],{"href":11274,"rel":11275},"https://github.com/briancaffey/pulumi-aws-django",[19],"github.com/briancaffey/pulumi-aws-django",[919,11278,11279,11280,11283,11284,11289,11290],{},"Mono repo with a sample Django micro blogging app (μblog) and frontend app (Vue SPA written with Quasar), GitHub Action workflows for infrastructure and (separate) application deployment pipelines, IaC code that ",[4339,11281,11282],{},"consumes"," each of the libraries listed above, ",[15,11285,11288],{"href":11286,"rel":11287},"https://briancaffey.github.io/django-step-by-step/",[19],"VuePress documentation site"," and miscellaneous items (k6 load testing scripts, Cypress tests, docker-compose, etc.): ",[15,11291,11294],{"href":11292,"rel":11293},"https://github.com/briancaffey/django-step-by-step",[19],"github.com/briancaffey/django-step-by-step",[168,11296,11297],{"id":11297},"eli5",[11,11299,11300],{},"Pretend we are at the beach building sandcastles. We can build sandcastles using our hands, but this takes a lot of time, and we might bump into each other and accidentally knock over part of our sandcastle. I made some tools for building sandcastles. We have one tool for building a sand castle base that includes the wall around the outside, the moat, the door, and different sections inside the walls. And I made another tool for deploying smaller sand \\castle houses inside the walls of the sandcastle base. We fill the tool with sand and water and then turn it over inside of our base and we can build an entire city of sandcastles. Also, the tool lets us carefully remove parts of our sandcastle without knocking over any of the other parts. We can share the tool with all of our friends and they can make cool sandcastles too, and the tool is free for them to use.",[11,11302,11303],{},"Instead of sandcastles, I'm working with computer systems that can power internet applications, like YouTube for example. I'm building tools that can allow me or anyone else to build really awesome internet applications using computers.",[11,11305,11306,11307,11310,11311,11314],{},"The tools are not physical tools like the ones for building sandcastles, but instead, these tools are made with code. The code for websites like YouTube allows you to upload videos ",[4339,11308,11309],{},"to YouTube",", but the code I'm writing allows you to upload any type of website (even site YouTube) ",[4339,11312,11313],{},"to the internet",". When we run this code, it creates applications on the internet. Also, sand is very expensive and Jeff Bezos owns the beach.",[168,11316,11318],{"id":11317},"why-i-made-an-infrastructure-as-code-rosetta-stone-with-cdk-terraform-and-pulumi","Why I made an Infrastructure as Code Rosetta Stone with CDK, Terraform and Pulumi",[11,11320,11321],{},[338,11322,11323],{},"To push me to learn more about AWS, IaC, CI/CD, automation, and Platform Engineering",[916,11325,11326,11329],{},[919,11327,11328],{},"Learn the differences between major IaC tools and how to use them to do exactly the same thing (build a web app) on the same Cloud (AWS) in the same way (serverless container technology using ECS Fargate).",[919,11330,11331],{},"Get more experience publishing software packages (npm) and finding the right level of abstraction for IaC libraries that is both dynamic and straightforward",[11,11333,11334],{},[338,11335,11336],{},"To fail as many times as possible",[916,11338,11339,11342,11345],{},[919,11340,11341],{},"Every time I fail when I think I have things right, I learn something new",[919,11343,11344],{},"Failed IaC pipelines can sometimes be scary, and every failure I have on these projects can teach me about potential failure modes for live projects running in production",[919,11346,11347],{},"You can oftentimes be \"stuck\" where you have a set of resources that you can't update or delete. Learning to get unstuck from these scenarios is important",[11,11349,11350],{},[338,11351,11352],{},"To take an application-first approach to DevOps",[916,11354,11355,11358,11361],{},[919,11356,11357],{},"Application developers are increasingly being tasked with operational duties",[919,11359,11360],{},"While learning about IaC, I had a hard time finding in-depth materials covering application development, CI/CD pipelines, automation, and Infrastructure as Code and how these three knowledge domains work together. There are important considerations to make when  between a Hello World docker image",[919,11362,11363],{},"You could probably use another framework with these IaC libraries like Flask or Rails, but for now I'm building these projects with Django first-in-mind",[11,11365,11366],{},[338,11367,11368],{},"To develop a project I can reference when helping myself and others",[916,11370,11371,11374,11381],{},[919,11372,11373],{},"companies and projects that do IaC and CI/CD for the most part have things in private repos for obvious reasons, there isn't any good reason to share this type of code unless you are sharing it with an auditor",[919,11375,11376,11377,11380],{},"Hopefully, the sample application, IaC, and CI/CD pipelines ",[4339,11378,11379],{},"aren't overly complex",". There are more complex examples of open-source companies out there, but their repos have steep learning curves and a lot going on",[919,11382,11383,11384,11387],{},"People often ask about how to split up IaC deployments and application deployments. I want to be able to use this project to ",[338,11385,11386],{},"show"," people how it can be done",[11,11389,11390],{},[338,11391,11392,11393,11396],{},"To encourage others (specifically Developer Advocates / Developer Relations / Solutions Architects in the CDK, Terraform, and Pulumi communities) to share complete and non-trivial examples of IaC software ",[338,11394,11395],{},"in use"," with an actual application.",[916,11398,11399,11406],{},[919,11400,11401,11402,11405],{},"There are many ways one could create an \"IaC Rosetta Stone\" (",[33,11403,11404],{},"public cloud providers x CI/CD providers x IaC tools"," is a big number)",[919,11407,11408],{},"This takes a lot of effort and time",[11,11410,11411],{},[338,11412,11413],{},"I have nothing to sell you",[916,11415,11416,11419,11422],{},[919,11417,11418],{},"So many articles about Cloud/DevOps are trying to sell you a tool. Outside of what I consider to be mainstream vendors like GitHub and AWS, there are no products that I'm promoting here",[919,11420,11421],{},"I'm also not trying to sell anyone on using my IaC packages",[919,11423,11424],{},"Hopefully, my IaC packages can serve as a helpful reference or starting point",[11,11426,11427],{},[338,11428,11429],{},"Walk before running",[916,11431,11432,11435],{},[919,11433,11434],{},"I want to build up confidence with vanilla use cases before getting too fancy",[919,11436,11437],{},"With a solid foundation in these tools, I want to learn about some of the more advanced patterns teams are adopting (Pulumi Automation API, Terragrunt for Terraform, self-mutating CDK Pipelines)",[11,11439,11440],{},[338,11441,11442],{},"12 Factor App, DevOps, and Platform Engineering",[916,11444,11445,11453],{},[919,11446,11447,11452],{},[15,11448,11451],{"href":11449,"rel":11450},"https://12factor.net/",[19],"12 Factor App"," is great and has guided how I approach both Django application development and IaC library development",[919,11454,6131,11455,11460],{},[15,11456,11459],{"href":11457,"rel":11458},"https://platformengineering.org/",[19],"platformengineering.org"," community has some good guiding principles",[168,11462,11464],{"id":11463},"cdkterraformpulumi-terminology","CDK/Terraform/Pulumi terminology",[911,11466,11468],{"id":11467},"constructs-modules-and-components","constructs, modules and components",[11,11470,11471],{},"A CDK construct, Terraform module and Pulumi component generally mean the same thing: an abstract grouping of one or more cloud resources.",[11,11473,11474,11475,11478,11479,11482,11483,11486,11487,11490],{},"In this article I will refer to ",[338,11476,11477],{},"constructs/modules/components"," as ",[338,11480,11481],{},"c/m/c"," for short, and the term ",[338,11484,11485],{},"stack"," can generally be used to refer to either a CloudFormation stack, a Pulumi Stack or a Terraform group of resources that are part of a module that has had ",[33,11488,11489],{},"apply"," ran against it.",[911,11492,11494],{"id":11493},"what-is-a-stack","what is a stack?",[11,11496,11497],{},"AWS has a resource type called CloudFormation Stacks, and Pulumi also has a concept of stacks. Terraform documentation doesn't refer to stacks, and instead in Terraform docs use the words \"Terraform configuration\" to refer to some group of resources that were built using a module.",[11,11499,11500,11501,11504],{},"CDK Constructs and Pulumi Components are somewhat similar, however CDK Constructs map to CloudFormation and the Pulumi components I'm using from the ",[33,11502,11503],{},"@pulumi/aws"," package generally map directly to Terraform resources from the AWS Provider (the Pulumi AWS Provider uses much of the same code that the Terraform AWS Provider uses).",[911,11506,11507],{"id":11507},"verbs",[11,11509,11510,11511,11514,11515,11518],{},"In CDK you ",[33,11512,11513],{},"synth"," CDK code to generate CloudFormation templates. You can also run ",[33,11516,11517],{},"diff"," to see what changes would be applied during a stack update.",[11,11520,11521,11522,11525,11526,11529,11530,11533,11534,11537],{},"In Terraform you ",[33,11523,11524],{},"init"," to download all providers and modules. This is sort of like running ",[33,11527,11528],{},"npm install"," in CDK and Pulumi. You then run ",[33,11531,11532],{},"terraform plan"," to see the changes that would result. ",[33,11535,11536],{},"terraform apply"," does CRUD operations on your cloud resources.",[11,11539,11540,11541,11544,11545,11548],{},"In Pulumi you run ",[33,11542,11543],{},"pulumi preview"," to see what changes would be made to a stack. You can use the ",[33,11546,11547],{},"--diff"," flag to see the specifics of what would change.",[11,11550,11551],{},"To summarize:",[916,11553,11554,11557,11568,11575],{},[919,11555,11556],{},"In CDK you synth CloudFormation and use these templates to deploy stacks made up of constructs. An \"app\" can contain multiple stacks, and you can deploy one or more stacks in an app at a time",[919,11558,11559,11560,11562,11563,343],{},"In Terraform you plan a configuration made up of modules, and then run ",[33,11561,11536],{}," to build the configuration/stack (",[15,11564,11567],{"href":11565,"rel":11566},"https://discuss.hashicorp.com/t/what-is-a-terraform-stack/31985",[19],"discuss.hashicorp.com/t/what-is-a-terraform-stack/31985",[919,11569,11570,11571,11574],{},"Pulumi: You preview a Pulumi stack made up of components, and then run ",[33,11572,11573],{},"pulumi up"," to build the resources",[919,11576,11577,11578],{},"To tear down a stack in all three tools, you run ",[33,11579,11580],{},"destroy",[168,11582,11584],{"id":11583},"infrastructure-as-code-library-repos","Infrastructure as Code library repos",[11,11586,11587],{},"Let's look at the three repos that I wrote for deploying the same type of 3-tier web application to AWS using ECS Fargate.",[916,11589,11590,11599,11608],{},[919,11591,11592,11593],{},"CDK: ",[15,11594,11596],{"href":11254,"rel":11595},[19],[33,11597,11598],{},"cdk-django",[919,11600,11601,11602],{},"Terraform: ",[15,11603,11605],{"href":11264,"rel":11604},[19],[33,11606,11607],{},"terraform-aws-django",[919,11609,11610,11611],{},"Pulumi: ",[15,11612,11614],{"href":11274,"rel":11613},[19],[33,11615,11616],{},"pulumi-aws-django",[911,11618,11620],{"id":11619},"language","Language",[11,11622,11623,1172,11625,11627,11628,11630,11631,11633,11634,752],{},[33,11624,11598],{},[33,11626,11616],{}," are both written in TypeScript. ",[33,11629,11607],{}," is written in HCL, a domain specific language created by HashiCorp. The ",[33,11632,11598],{}," is published to both npm and PyPI, so you can use it in JavaScript, TypeScript and Python projects, other languages are supported as well, but you need to write your library in TypeScript so it can be transpiled to other languages using ",[15,11635,11638],{"href":11636,"rel":11637},"https://github.com/aws/jsii",[19],"jsii",[11,11640,11641,11642,11647],{},"My Pulumi library is written in TypeScript and is published to NPM. For now it can only be used in JavaScript and TypeScript projects. There is a way in Pulumi to write in any language and then publish to any other major language, but I haven't  done this yet. See ",[15,11643,11646],{"href":11644,"rel":11645},"https://github.com/pulumi/pulumi-component-provider-ts-boilerplate",[19],"this GitHub repo"," for more information on this.",[11,11649,11650,11651,11656],{},"The HCL is pretty simple when you get used to it. I find that I don't like adding lots of logic in Terraform code because it takes away from the readability of a module. There is a tool called ",[15,11652,11655],{"href":11653,"rel":11654},"https://developer.hashicorp.com/terraform/cdktf",[19],"CDKTF"," which allows you to write HCL Terraform in TypeScript, but I haven't used it yet.",[911,11658,11660],{"id":11659},"release-management-versioning-and-publishing","Release management, versioning and publishing",[11,11662,11663,1172,11665,11667,11668,11671,11672,11674,11675,11678,11679,11681,11682,11685],{},[33,11664,11616],{},[33,11666,11607],{}," both use ",[33,11669,11670],{},"release-please"," for automatically generating a changelog file and bumping versions. ",[33,11673,11670],{}," is an open source tool from Google that they use to version their Terraform GCP modules. Whenever I push new commits to ",[33,11676,11677],{},"main",", a new PR is created that adds changes to the CHANGELOG.md file, bumps the version of the library in ",[33,11680,8085],{}," and adds a new git tag (e.g. ",[33,11683,11684],{},"v1.2.3",") based on commit messages.",[11,11687,11688,9029,11690,11697,11698,11701,11702,637,11705,11707],{},[33,11689,11598],{},[15,11691,11694],{"href":11692,"rel":11693},"https://github.com/projen/projen",[19],[33,11695,11696],{},"projen"," for maintaining the changelog and bumping versions and publishing to npm. It is popular among developers in the CDK community and is a really awesome tool since it basically uses one file (",[33,11699,11700],{},".projenrc.ts",") to configure your entire repo, including files like ",[33,11703,11704],{},"tsconfig.json",[33,11706,8085],{},", and even GitHub Action workflows. It has a lot of configuration options, but I'm using it in a pretty simple way. It generates a new release and items to the changelog when I manually trigger a GitHub Action.",[11,11709,11710,11711,11716],{},"These tools are both based on ",[15,11712,11715],{"href":11713,"rel":11714},"https://www.conventionalcommits.org/en/v1.0.0/",[19],"conventional commits"," to automatically update the Changelog file.",[11,11718,11719,11720,11722],{},"I'm still manually publishing my ",[33,11721,11616],{}," package from the CLI. I need to add a GitHub Action to do this for me. This and other backlog items are listed at the end of the article!",[911,11724,11726],{"id":11725},"makefile-examples-and-local-development","Makefile, examples and local development",[11,11728,11729],{},"Each repo has a Makefile that includes commands that I frequently use when developing new features or fixing bugs. Each repo has commands for the following:",[916,11731,11732,11738,11743,11748,11757,11764,11770,11775,11784],{},[919,11733,11734,11735,11737],{},"synthesizing CDK to CloudFormation / running ",[33,11736,11532],{}," / previewing pulumi up for both the base and app stacks",[919,11739,11740,11741],{},"creating/updating an ad hoc base stack called ",[33,11742,6303],{},[919,11744,11745,11746],{},"destroying resources in the ad-hoc base stack called ",[33,11747,6303],{},[919,11749,11750,11751,11754,11755],{},"creating an ad hoc app stack called ",[33,11752,11753],{},"alpha"," that uses resources from ",[33,11756,6303],{},[919,11758,11759,11760,11754,11762],{},"destroying an ad hoc app stack called ",[33,11761,11753],{},[33,11763,6303],{},[919,11765,11766,11767],{},"creating/updating a prod base stack called ",[33,11768,11769],{},"stage",[919,11771,11772,11773],{},"destroying resources in the prod base stack called ",[33,11774,11769],{},[919,11776,11777,11778,11780,11781,11783],{},"creating a prod app stack using called ",[33,11779,11769],{}," that uses resources from the ",[33,11782,11769],{}," base stack",[919,11785,11786,11787],{},"destroying resources in the prod app stack called ",[33,11788,11769],{},[11,11790,11791,11792,11794],{},"Here's an example of what these commands look like in ",[33,11793,11616],{}," for prod infrastructure base and app stacks:",[26,11796,11800],{"className":11797,"code":11798,"language":11799,"meta":35,"style":35},"language-make shiki shiki-themes github-light github-dark","prod-base-preview:  build\n    pulumi -C examples/prod/base --stack stage --non-interactive preview\n\nprod-base-up:   build\n    pulumi -C examples/prod/base --stack stage --non-interactive up --yes\n\nprod-base-destroy:  build\n    pulumi -C examples/prod/base --stack stage --non-interactive destroy --yes\n\nprod-app-preview:   build\n    pulumi -C examples/prod/app --stack stage --non-interactive preview\n\nprod-app-preview-diff:  build\n    pulumi -C examples/prod/app --stack stage --non-interactive preview --diff\n\nprod-app-up:    build\n    pulumi -C examples/prod/app --stack stage --non-interactive up --yes\n\nprod-app-destroy:   build\n    pulumi -C examples/prod/app --stack stage --non-interactive destroy --yes\n","make",[33,11801,11802,11807,11812,11816,11821,11826,11830,11835,11840,11844,11849,11854,11858,11863,11868,11872,11877,11882,11886,11891],{"__ignoreMap":35},[187,11803,11804],{"class":189,"line":190},[187,11805,11806],{},"prod-base-preview:  build\n",[187,11808,11809],{"class":189,"line":249},[187,11810,11811],{},"    pulumi -C examples/prod/base --stack stage --non-interactive preview\n",[187,11813,11814],{"class":189,"line":312},[187,11815,316],{"emptyLinePlaceholder":315},[187,11817,11818],{"class":189,"line":319},[187,11819,11820],{},"prod-base-up:   build\n",[187,11822,11823],{"class":189,"line":325},[187,11824,11825],{},"    pulumi -C examples/prod/base --stack stage --non-interactive up --yes\n",[187,11827,11828],{"class":189,"line":686},[187,11829,316],{"emptyLinePlaceholder":315},[187,11831,11832],{"class":189,"line":697},[187,11833,11834],{},"prod-base-destroy:  build\n",[187,11836,11837],{"class":189,"line":1291},[187,11838,11839],{},"    pulumi -C examples/prod/base --stack stage --non-interactive destroy --yes\n",[187,11841,11842],{"class":189,"line":1306},[187,11843,316],{"emptyLinePlaceholder":315},[187,11845,11846],{"class":189,"line":1434},[187,11847,11848],{},"prod-app-preview:   build\n",[187,11850,11851],{"class":189,"line":2599},[187,11852,11853],{},"    pulumi -C examples/prod/app --stack stage --non-interactive preview\n",[187,11855,11856],{"class":189,"line":2607},[187,11857,316],{"emptyLinePlaceholder":315},[187,11859,11860],{"class":189,"line":2621},[187,11861,11862],{},"prod-app-preview-diff:  build\n",[187,11864,11865],{"class":189,"line":2631},[187,11866,11867],{},"    pulumi -C examples/prod/app --stack stage --non-interactive preview --diff\n",[187,11869,11870],{"class":189,"line":2642},[187,11871,316],{"emptyLinePlaceholder":315},[187,11873,11874],{"class":189,"line":2653},[187,11875,11876],{},"prod-app-up:    build\n",[187,11878,11879],{"class":189,"line":2665},[187,11880,11881],{},"    pulumi -C examples/prod/app --stack stage --non-interactive up --yes\n",[187,11883,11884],{"class":189,"line":2674},[187,11885,316],{"emptyLinePlaceholder":315},[187,11887,11888],{"class":189,"line":2684},[187,11889,11890],{},"prod-app-destroy:   build\n",[187,11892,11893],{"class":189,"line":2694},[187,11894,11895],{},"    pulumi -C examples/prod/app --stack stage --non-interactive destroy --yes\n",[11,11897,11898,11899,11901],{},"I currently don't have tests for all of these libraries, but for now the most effective way of testing that things are working correctly is to use the ",[33,11900,11481],{},"s to create environments and smoke check the environments to make sure everything works correctly.",[11,11903,11904],{},"Adding unit tests is another item for the backlog.",[911,11906,11908],{"id":11907},"ad-hoc-vs-prod","ad-hoc vs prod",[916,11910,11911,11919,11922,11925,11928,11931],{},[919,11912,11913,11918],{},[15,11914,11917],{"href":11915,"rel":11916},"https://briancaffey.github.io/2022/03/27/ad-hoc-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions",[19],"the last article I wrote was about ad hoc environments",". Also known as \"on-demand\" environments or \"preview\" environments.",[919,11920,11921],{},"the motivation for using ad-hoc environments is speed and cost (you can stand up an environment in less time and you share the costs of the base environment, including VPC, ALB, RDS)",[919,11923,11924],{},"you can completely ignore \"ad-hoc\" environments and use the \"prod\" infrastructure for any number of environments (such as dev, QA, RC, stage and prod)",[919,11926,11927],{},"prod can be used for a production environment and any number of pre-production environments",[919,11929,11930],{},"multiple environments built with \"prod\" infrastructure can be configured with a \"knobs and dials\" (e.g., how big are app and DB instances, how many tasks to run in a service, etc.)",[919,11932,11933],{},"the \"prod\" infrastructure should be the same for the \"production\" environment and the \"staging\" environment",[911,11935,11937],{"id":11936},"directory-structure","Directory structure",[11,11939,11940],{},"The directory structures for each repo are all similar with some minor differences.",[11,11942,11943,11944,1172,11947,11950,11951,1172,11954,752],{},"There are two types of environments: ",[33,11945,11946],{},"ad-hoc",[33,11948,11949],{},"prod",". Within ad-hoc and production, there are two directories ",[33,11952,11953],{},"base",[33,11955,11956],{},"app",[11,11958,11959,11960,11963,11964,11966,11967,11969],{},"Each repo has a directory called ",[33,11961,11962],{},"internal"," which contain building blocks used by the ",[33,11965,11481],{},"s that are exposed. The contents of the ",[33,11968,11962],{}," directories are not intended to be used by anyone who is using the libraries.",[11,11971,11972],{},[338,11973,11974],{},"CDK construct library repo structure",[26,11976,11979],{"className":11977,"code":11978,"language":31},[29],"~/git/github/cdk-django$ tree -L 4 -d src/\nsrc/\n├── constructs\n│   ├── ad-hoc\n│   │   ├── app\n│   │   └── base\n│   ├── internal\n│   │   ├── alb\n│   │   ├── bastion\n│   │   ├── customResources\n│   │   │   └── highestPriorityRule\n│   │   ├── ecs\n│   │   │   ├── iam\n│   │   │   ├── management-command\n│   │   │   ├── redis\n│   │   │   ├── scheduler\n│   │   │   ├── web\n│   │   │   └── worker\n│   │   ├── rds\n│   │   ├── sg\n│   │   └── vpc\n│   └── prod\n│       ├── app\n│       └── base\n└── examples\n    └── ad-hoc\n        ├── app\n        │   └── config\n        └── base\n            └── config\n",[33,11980,11978],{"__ignoreMap":35},[11,11982,11983],{},[338,11984,11985],{},"Terraform module library repo structure",[26,11987,11990],{"className":11988,"code":11989,"language":31},[29],"~/git/github/terraform-aws-django$ tree -L 4 -d modules\nmodules\n├── ad-hoc\n│   ├── app\n│   └── base\n├── internal\n│   ├── alb\n│   ├── autoscaling\n│   ├── bastion\n│   ├── ecs\n│   │   ├── ad-hoc\n│   │   │   ├── celery_beat\n│   │   │   ├── celery_worker\n│   │   │   ├── cluster\n│   │   │   ├── management_command\n│   │   │   ├── redis\n│   │   │   └── web\n│   │   └── prod\n│   │       ├── celery_beat\n│   │       ├── celery_worker\n│   │       ├── cluster\n│   │       ├── management_command\n│   │       └── web\n│   ├── elasticache\n│   ├── iam\n│   ├── rds\n│   ├── route53\n│   ├── s3\n│   ├── sd\n│   └── sg\n└── prod\n    ├── app\n    └── base\n",[33,11991,11989],{"__ignoreMap":35},[11,11993,11994],{},[338,11995,11996],{},"Pulumi component library repo structure",[26,11998,12001],{"className":11999,"code":12000,"language":31},[29],"~/git/github/pulumi-aws-django$ tree -L 3 src/\nsrc/\n├── components\n│   ├── ad-hoc\n│   │   ├── README.md\n│   │   ├── app\n│   │   └── base\n│   └── internal\n│       ├── README.md\n│       ├── alb\n│       ├── bastion\n│       ├── cw\n│       ├── ecs\n│       ├── iam\n│       ├── rds\n│       └── sg\n└── util\n    ├── index.ts\n    └── taggable.ts\n",[33,12002,12000],{"__ignoreMap":35},[11,12004,12005],{},[338,12006,12007],{},"Pulumi examples directory",[26,12009,12012],{"className":12010,"code":12011,"language":31},[29],"~/git/github/pulumi-aws-django$ tree -L 3 examples/\nexamples/\n└── ad-hoc\n    ├── app\n    │   ├── Pulumi.alpha.yaml\n    │   ├── Pulumi.yaml\n    │   ├── index.ts\n    │   ├── node_modules\n    │   ├── package-lock.json\n    │   ├── package.json\n    │   └── tsconfig.json\n    └── base\n        ├── Pulumi.yaml\n        ├── bin\n        ├── index.ts\n        ├── package-lock.json\n        ├── package.json\n        └── tsconfig.json\n",[33,12013,12011],{"__ignoreMap":35},[911,12015,12017],{"id":12016},"cloc","CLOC",[11,12019,12020,12021,12023],{},"Let's use CLOC (count lines of code) to compare the lines of code used in the ",[33,12022,11481],{}," of CDK/CloudFormation/Terraform/Pulumi.",[11,12025,12026],{},[338,12027,12028],{},[33,12029,11598],{},[26,12031,12034],{"className":12032,"code":12033,"language":31},[29],"~/git/github/cdk-django$ cloc src/constructs/\n      14 text files.\n      14 unique files.\n       0 files ignored.\n\ngithub.com/AlDanial/cloc v 1.94  T=0.04 s (356.1 files/s, 30040.9 lines/s)\n-------------------------------------------------------------------------------\nLanguage                     files          blank        comment           code\n-------------------------------------------------------------------------------\nTypeScript                      13            155             59            908\nPython                           1             18              8             33\n-------------------------------------------------------------------------------\nSUM:                            14            173             67            941\n-------------------------------------------------------------------------------\n",[33,12035,12033],{"__ignoreMap":35},[11,12037,12038],{},[338,12039,12040],{},[33,12041,11607],{},[26,12043,12046],{"className":12044,"code":12045,"language":31},[29],"~/git/github/terraform-aws-django$ cloc modules/\n      68 text files.\n      58 unique files.\n      11 files ignored.\n\ngithub.com/AlDanial/cloc v 1.94  T=0.15 s (385.9 files/s, 20585.1 lines/s)\n-------------------------------------------------------------------------------\nLanguage                     files          blank        comment           code\n-------------------------------------------------------------------------------\nHCL                             55            472            205           2390\nMarkdown                         3              7              0             20\n-------------------------------------------------------------------------------\nSUM:                            58            479            205           2410\n-------------------------------------------------------------------------------\n",[33,12047,12045],{"__ignoreMap":35},[11,12049,12050],{},[338,12051,12052],{},[33,12053,11616],{},[26,12055,12058],{"className":12056,"code":12057,"language":31},[29],"~/git/github/pulumi-aws-django$ cloc src/components/\n      15 text files.\n      15 unique files.\n       0 files ignored.\n\ngithub.com/AlDanial/cloc v 1.94  T=0.11 s (134.5 files/s, 12924.2 lines/s)\n-------------------------------------------------------------------------------\nLanguage                     files          blank        comment           code\n-------------------------------------------------------------------------------\nTypeScript                      13            110            176           1119\nMarkdown                         2              6              0             30\n-------------------------------------------------------------------------------\nSUM:                            15            116            176           1149\n-------------------------------------------------------------------------------\n",[33,12059,12057],{"__ignoreMap":35},[168,12061,12063],{"id":12062},"communities","Communities",[11,12065,12066],{},"The CDK, Terraform and Pulumi communities are all great and a lot of people helped when I got stuck on issues writing these libraries. Thank you!",[916,12068,12069,12076,12083],{},[919,12070,12071],{},[15,12072,12075],{"href":12073,"rel":12074},"https://cdk.dev/",[19],"cdk.dev",[919,12077,12078],{},[15,12079,12082],{"href":12080,"rel":12081},"https://discuss.hashicorp.com/c/terraform-core/27",[19],"Terraform Section of HashiCorp Discuss Forum",[919,12084,12085],{},[15,12086,12089],{"href":12087,"rel":12088},"https://slack.pulumi.com/",[19],"Pulumi Slack",[168,12091,12092],{"id":12092},"μblog",[11,12094,12095],{},"μblog is a micro blogging application that I have written using Django and Vue.js. Here's a screenshot of the homepage:",[11,12097,12098],{},[511,12099],{"alt":12100,"src":12101},"ublog","/static/ublog_screenshot.png",[11,12103,12104],{},"It is a pretty simple app. Users can write posts with text and an optional images. Logged in users can write posts and like posts.",[911,12106,12108],{"id":12107},"mono-repo-structure","Mono-repo structure",[11,12110,12111,12112,12115],{},"It lives in a GitHub mono repo called ",[33,12113,12114],{},"django-step-by-step",". This mono repo contains a few different things:",[916,12117,12118,12121,12124,12133],{},[919,12119,12120],{},"backend Django application",[919,12122,12123],{},"frontend Vue.js application",[919,12125,12126,12127,637,12129,1172,12131],{},"IaC code that uses c/m/c from ",[33,12128,11598],{},[33,12130,11607],{},[33,12132,11616],{},[919,12134,12135],{},"GitHub Actions workflows for both Infrastructure deployments and application deployments",[11,12137,12138],{},"μblog is the reference application that I deploy to infrastructure created with CDK, Terraform and Pulumi. μblog is meant to represent a generic 12 Factor application that uses:",[916,12140,12141,12144,12147,12150,12153,12156,12159,12162],{},[919,12142,12143],{},"gunicorn for a backend API",[919,12145,12146],{},"Vue.js for a client that consumes the backend API",[919,12148,12149],{},"celery for async task processing",[919,12151,12152],{},"celery beat for scheduling tasks",[919,12154,12155],{},"Postgres for relational data",[919,12157,12158],{},"Redis for caching and message brokering",[919,12160,12161],{},"S3 for object storage",[919,12163,12164],{},"Django admin for a simple admin interface",[11,12166,12167],{},"There is a lot more I could add on μblog. For now I'll just mention that it:",[916,12169,12170,12173,12176,12179,12182],{},[919,12171,12172],{},"has a great local development environment (supports both docker-compose and virtual environments)",[919,12174,12175],{},"demonstrates how to use Django in different ways. It implements the same application using Function Based View and Class Based Views, and implements both a REST API (both with FBV and CBV) and GraphQL.",[919,12177,12178],{},"GitHub Actions for running unit tests",[919,12180,12181],{},"k6 for load testing",[919,12183,12184,12185],{},"contains a documentation site deployed to GitHub pages (made with VuePress) can be found here: ",[15,12186,11286],{"href":11286,"rel":12187},[19],[2215,12189,12191],{"id":12190},"infrastructure-deep-dive","Infrastructure Deep Dive",[11,12193,12194,12195,12197],{},"Let's go through each of the ",[33,12196,11481],{},"s used in the three libraries. I'll cover some of the organizational decisions, dependencies and differences between how things are done between CDK, Terraform and Pulumi.",[11,12199,12200,12201,1172,12203,12205,12206,1172,12208,12210],{},"I'll first talk about the two stacks used in ad hoc environments: ",[33,12202,11953],{},[33,12204,11956],{},". Then I'll talk about the prod environments which are also composed of ",[33,12207,11953],{},[33,12209,11956],{}," stacks.",[11,12212,12213,12214,12217],{},"Keep in mind that there aren't that many differences between the ad hoc environment base and app stacks and the prod environment app and base stacks. A future optimization could be to use a single base and app stack, but I think there is a trade-off between readability and DRYness of infrastructure code, ",[338,12215,12216],{},"especially with Terraform",". In general I try to use very little conditionals and logic with Terraform code. It is much easier to have dynamic configuration in CDK and Pulumi, and probably also for other tools like CDKTF (that I have not yet tried).",[168,12219,12221],{"id":12220},"splitting-up-the-stacks","Splitting up the stacks",[11,12223,12224],{},"While it is possible to put all resources in a single stack with both Terraform, CDK and Pulumi, it is not recommended to do so.",[916,12226,12227,12233,12241],{},[919,12228,12229,12230],{},"Terraform enables this with outputs and ",[33,12231,12232],{},"terraform_remote_state",[919,12234,12235,12236],{},"Pulumi encourages the use of ",[15,12237,12240],{"href":12238,"rel":12239},"https://www.pulumi.com/docs/guides/organizing-projects-stacks/",[19],"micro stacks",[919,12242,12243,12244],{},"CDK has an article on how to ",[15,12245,12248],{"href":12246,"rel":12247},"https://docs.aws.amazon.com/cdk/v2/guide/stack_how_to_create_multiple_stacks.html",[19],"create an app with multiple stacks",[11,12250,12251],{},"My design decision was to keep things limited to 2 stacks. Later on it would be interesting to try splitting out another stack.",[11,12253,12254],{},"Also, on-demand environments really lends itself to stacks that are split up.",[11,12256,12257,12258,12263],{},"In the section ",[15,12259,12262],{"href":12260,"rel":12261},"https://docs.aws.amazon.com/cdk/v2/guide/resources.html",[19],"\"Passing unique identifiers\"",", the CDK recommends that we keep the two stacks in the same app. In Terraform and Pulumi, each stack environment is in its own app.",[11,12265,12266,12267,12269,12270,12272,12273,12275,12276,1172,12279,12282,12283,12285,12286,637,12289,1172,12292,12295],{},"There is a balance to be found between single stacks vs micro stacks. Both the base and app ",[33,12268,11481],{},"s could be split out further. For example, the ",[33,12271,11953],{}," ",[33,12274,11481],{},"s could be split into ",[33,12277,12278],{},"networking",[33,12280,12281],{},"rds",". The ",[33,12284,11956],{}," stack could be split into different ECS services so that their infrastructure can be deployed independently, like ",[33,12287,12288],{},"cluster",[33,12290,12291],{},"backend",[33,12293,12294],{},"frontend",". The more resources that a stack has, the longer it takes to deploy and the more risky it gets, but adding lots of stacks can add to mental overhead, and pipeline complexity. Each tool has ways of dealing with these complexities (CDK Pipelines, Terragrunt, Pulumi Automation API), but I won't be getting into any of these options in this article. I would like to try these out and share in a future article.",[11,12297,12298],{},"My rules of thumbs are:",[916,12300,12301,12304],{},[919,12302,12303],{},"single stacks are bad because you don't want to put all your eggs in one basket, however your IaC tool should give you confidence about what is going to change when you try to make a change",[919,12305,12306],{},"Lots of small stacks can cause overhead and make things more complex than they need to be",[168,12308,12310],{"id":12309},"ad-hoc-base-overview","Ad hoc base overview",[11,12312,12313],{},"Here's an overview of the resources used in an ad hoc base environment.",[916,12315,12316,12319,12322,12325,12328,12331,12334,12337],{},[919,12317,12318],{},"(Inputs)",[919,12320,12321],{},"(Optional environment configs)",[919,12323,12324],{},"VPC and Service Discovery",[919,12326,12327],{},"S3",[919,12329,12330],{},"Security Groups",[919,12332,12333],{},"Load Balancer",[919,12335,12336],{},"RDS",[919,12338,12339],{},"Bastion Host",[911,12341,12343],{"id":12342},"visualization","Visualization",[11,12345,12346,12347,12350],{},"Here's a dependency graph showing all of the resources in ad hoc base stack. This can be found on the ",[33,12348,12349],{},"Resources"," tab of the ad hoc base stack in the Pulumi console.",[11,12352,12353],{},[511,12354],{"alt":12355,"src":12356},"Graph view of ad hoc base infrastructure","/static/pulumi_ad_hoc_base_dep_graph.png",[911,12358,12360],{"id":12359},"inputs","Inputs",[11,12362,12363],{},"There are only two required inputs for the ad hoc base stack",[916,12365,12366,12369],{},[919,12367,12368],{},"ACM certificate ARN",[919,12370,12371],{},"Domain Name",[11,12373,12374],{},"I store these values in environment variables for the pipelines in CDK, Terraform and Pulumi. When running pipelines from my local environment, they are exported in my shell before running deploy/apply/up or synth/plan/preview.",[911,12376,12378],{"id":12377},"vpc","VPC",[11,12380,12381,12382,12384],{},"The VPC is the first resource that is created as part of the ",[33,12383,11953],{}," stack. There official, high-level constructs in each IaC tool for building VPCs and all related networking resources.",[916,12386,12387,12393,12399],{},[919,12388,12389,12392],{},[33,12390,12391],{},"awsx"," has a VPC module",[919,12394,12395,12398],{},[33,12396,12397],{},"terraform-aws-vpc"," module",[919,12400,12401],{},"L2 VPC Construct in CDK",[11,12403,12404,12405,12408,12409,12412],{},"The setting in the Terraform VPC module ",[33,12406,12407],{},"one_nat_gateway_per_az = false"," doesn't seem to exist on the ",[33,12410,12411],{},"awsx.ec2.Vpc"," module. This will add to cost savings since it will use 1 NAT Gateway instead of 2 or 3.",[911,12414,12330],{"id":12415},"security-groups",[11,12417,12418],{},"Pulumi and Terraform can be used in a similar way to define security groups. CDK has a much more concise option for defining ingress and egress rules for security groups.",[26,12420,12424],{"className":12421,"code":12422,"language":12423,"meta":35,"style":35},"language-ts shiki shiki-themes github-light github-dark","    const albSecurityGroup = new SecurityGroup(scope, 'AlbSecurityGroup', {\n      vpc: props.vpc,\n    });\n\n    albSecurityGroup.addIngressRule(Peer.anyIpv4(), Port.tcp(443), 'HTTPS');\n    albSecurityGroup.addIngressRule(Peer.anyIpv4(), Port.tcp(80), 'HTTP');\n","ts",[33,12425,12426,12450,12455,12460,12464,12497],{"__ignoreMap":35},[187,12427,12428,12430,12433,12435,12438,12441,12444,12447],{"class":189,"line":190},[187,12429,6766],{"class":573},[187,12431,12432],{"class":588}," albSecurityGroup",[187,12434,6771],{"class":573},[187,12436,12437],{"class":573}," new",[187,12439,12440],{"class":193}," SecurityGroup",[187,12442,12443],{"class":577},"(scope, ",[187,12445,12446],{"class":196},"'AlbSecurityGroup'",[187,12448,12449],{"class":577},", {\n",[187,12451,12452],{"class":189,"line":249},[187,12453,12454],{"class":577},"      vpc: props.vpc,\n",[187,12456,12457],{"class":189,"line":312},[187,12458,12459],{"class":577},"    });\n",[187,12461,12462],{"class":189,"line":319},[187,12463,316],{"emptyLinePlaceholder":315},[187,12465,12466,12469,12472,12475,12478,12481,12484,12486,12489,12492,12495],{"class":189,"line":325},[187,12467,12468],{"class":577},"    albSecurityGroup.",[187,12470,12471],{"class":193},"addIngressRule",[187,12473,12474],{"class":577},"(Peer.",[187,12476,12477],{"class":193},"anyIpv4",[187,12479,12480],{"class":577},"(), Port.",[187,12482,12483],{"class":193},"tcp",[187,12485,615],{"class":577},[187,12487,12488],{"class":588},"443",[187,12490,12491],{"class":577},"), ",[187,12493,12494],{"class":196},"'HTTPS'",[187,12496,7004],{"class":577},[187,12498,12499,12501,12503,12505,12507,12509,12511,12513,12516,12518,12521],{"class":189,"line":686},[187,12500,12468],{"class":577},[187,12502,12471],{"class":193},[187,12504,12474],{"class":577},[187,12506,12477],{"class":193},[187,12508,12480],{"class":577},[187,12510,12483],{"class":193},[187,12512,615],{"class":577},[187,12514,12515],{"class":588},"80",[187,12517,12491],{"class":577},[187,12519,12520],{"class":196},"'HTTP'",[187,12522,7004],{"class":577},[911,12524,12526],{"id":12525},"load-balancer-resources","Load Balancer Resources",[11,12528,12529],{},"There's not much to comment on here. In each library I have resource group that defines the following:",[916,12531,12532,12535,12538,12541],{},[919,12533,12534],{},"Application Load Balancer",[919,12536,12537],{},"A default target group",[919,12539,12540],{},"An HTTP listener that redirects to HTTPS",[919,12542,12543],{},"An HTTPS listener with a default \"fixed-response\" action",[11,12545,12546],{},"Properties from these resources are used in the \"app\" stack to build listener rules for ECS services that are configured with load balancers, such as the backend and frontend web services.",[11,12548,12549],{},"Ad hoc app environments all share a common load balancer from the base stack that they use.",[911,12551,12553],{"id":12552},"rds-resources","RDS Resources",[11,12555,12556,12557,12559],{},"All three libraries have the RDS security group and Subnet Group in the same ",[33,12558,11481],{}," as the RDS instance. The SG and DB Subnet group could alternatively be grouped closer to the other network resources.",[11,12561,12562],{},"Currently the RDS resources are part of the \"base\" stack for each library. A future optimization may be to break the RDS instance out of the \"base\" stack and put it in its own stack. The \"RDS\" stack would be dependent on the \"base\" stack, and then \"app\" stack would then be dependent on both the \"base\" stack and the \"RDS\" stack. More stacks isn't necessarily a bad thing, but for my initial implementation of these libraries I have decided to keep the \"micro stacks\" approach limited to only 2 stacks for an environment.",[11,12564,12565],{},"The way that database secrets are handled is another difference between CDK and Terraform and Pulumi. I am currently \"hardcoding\" the RDS password for Terraform and Pulumi, and in CDK I am using a Secrets Manager Secret for the database credential.",[26,12567,12569],{"className":12421,"code":12568,"language":12423,"meta":35,"style":35},"    const secret = new Secret(scope, 'dbSecret', {\n      secretName: props.dbSecretName,\n      description: 'secret for rds',\n      generateSecretString: {\n        secretStringTemplate: JSON.stringify({ username: 'postgres' }),\n        generateStringKey: 'password',\n        excludePunctuation: true,\n        includeSpace: false,\n      },\n    });\n",[33,12570,12571,12592,12597,12607,12612,12634,12644,12653,12663,12668],{"__ignoreMap":35},[187,12572,12573,12575,12578,12580,12582,12585,12587,12590],{"class":189,"line":190},[187,12574,6766],{"class":573},[187,12576,12577],{"class":588}," secret",[187,12579,6771],{"class":573},[187,12581,12437],{"class":573},[187,12583,12584],{"class":193}," Secret",[187,12586,12443],{"class":577},[187,12588,12589],{"class":196},"'dbSecret'",[187,12591,12449],{"class":577},[187,12593,12594],{"class":189,"line":249},[187,12595,12596],{"class":577},"      secretName: props.dbSecretName,\n",[187,12598,12599,12602,12605],{"class":189,"line":312},[187,12600,12601],{"class":577},"      description: ",[187,12603,12604],{"class":196},"'secret for rds'",[187,12606,1228],{"class":577},[187,12608,12609],{"class":189,"line":319},[187,12610,12611],{"class":577},"      generateSecretString: {\n",[187,12613,12614,12617,12620,12622,12625,12628,12631],{"class":189,"line":325},[187,12615,12616],{"class":577},"        secretStringTemplate: ",[187,12618,12619],{"class":588},"JSON",[187,12621,752],{"class":577},[187,12623,12624],{"class":193},"stringify",[187,12626,12627],{"class":577},"({ username: ",[187,12629,12630],{"class":196},"'postgres'",[187,12632,12633],{"class":577}," }),\n",[187,12635,12636,12639,12642],{"class":189,"line":686},[187,12637,12638],{"class":577},"        generateStringKey: ",[187,12640,12641],{"class":196},"'password'",[187,12643,1228],{"class":577},[187,12645,12646,12649,12651],{"class":189,"line":697},[187,12647,12648],{"class":577},"        excludePunctuation: ",[187,12650,6630],{"class":588},[187,12652,1228],{"class":577},[187,12654,12655,12658,12661],{"class":189,"line":1291},[187,12656,12657],{"class":577},"        includeSpace: ",[187,12659,12660],{"class":588},"false",[187,12662,1228],{"class":577},[187,12664,12665],{"class":189,"line":1306},[187,12666,12667],{"class":577},"      },\n",[187,12669,12670],{"class":189,"line":1434},[187,12671,12459],{"class":577},[11,12673,12674,12675,12678],{},"In the ",[33,12676,12677],{},"DatabaseInstance"," props we can then use this secret like so:",[26,12680,12683],{"className":12681,"code":12682,"language":31},[29],"    credentials: Credentials.fromSecret(secret),\n",[33,12684,12682],{"__ignoreMap":35},[11,12686,12687,12688,12691],{},"In the application deployed with CDK, I use a Django settings module package that uses a package called ",[33,12689,12690],{},"aws_secretsmanager_caching"," to get and cache the secrets manager secret for the database, whereas in the apps deployed with Terraform and Pulumi I read in the password from an environment variable.",[11,12693,12694,12695,12698,12699,1172,12706,12713],{},"The Terraform and Pulumi database instance arguments simply accept a ",[33,12696,12697],{},"password"," field. This will be another item for the backlog for Terraform and Pulumi. The ",[15,12700,12703],{"href":12701,"rel":12702},"https://www.pulumi.com/registry/packages/random/api-docs/randompassword/",[19],[33,12704,12705],{},"randompassword",[15,12707,12710],{"href":12708,"rel":12709},"https://www.pulumi.com/registry/packages/aws/api-docs/secretsmanager/secretversion/",[19],[33,12711,12712],{},"secretversion"," components can be used to do this.",[911,12715,12339],{"id":12716},"bastion-host",[11,12718,12719],{},"There are two main use cases for the bastion host in ad-hoc environments.",[916,12721,12722,12729],{},[919,12723,12724,12725,12728],{},"When creating a new ad hoc app environment, the bastion host is used to create a new database called ",[33,12726,12727],{},"{ad-hoc-env-name}-db"," that the new ad hoc environment will use. (There might be another way of doing this, but using a bastion host is working well for now).",[919,12730,12731,12732,12735],{},"If you using a database management tool on you local machine like DBeaver, the bastion host can help you connect to the RDS instance in a private subnet. The bastion host instance is configured to run a service that forwards traffic on port 5432 to the RDS instance. If you port forward from your local machine to the bastion host on port 5432, you can connect RDS by simple connecting to ",[33,12733,12734],{},"localhost:5432"," on your local machine.",[11,12737,12738],{},"You don't need to manage SSH keys since you connect to the instance in a private subnet using SSM:",[26,12740,12742],{"className":181,"code":12741,"language":183,"meta":35,"style":35},"aws ssm start-session --target $INSTANCE_ID\n",[33,12743,12744],{"__ignoreMap":35},[187,12745,12746,12749,12752,12755,12758],{"class":189,"line":190},[187,12747,12748],{"class":193},"aws",[187,12750,12751],{"class":196}," ssm",[187,12753,12754],{"class":196}," start-session",[187,12756,12757],{"class":588}," --target",[187,12759,12760],{"class":577}," $INSTANCE_ID\n",[911,12762,12764],{"id":12763},"outputs","Outputs",[11,12766,12767],{},"Here are the outputs for the ad hoc base stack used in Terraform and Pulumi:",[916,12769,12770,12773,12776,12779,12782,12785,12788,12791,12794,12797],{},[919,12771,12772],{},"vpc_id",[919,12774,12775],{},"assets_bucket_name",[919,12777,12778],{},"private_subnet_ids",[919,12780,12781],{},"app_sg_id",[919,12783,12784],{},"alb_sg_id",[919,12786,12787],{},"listener_arn",[919,12789,12790],{},"alb_dns_name",[919,12792,12793],{},"task_role_arn",[919,12795,12796],{},"execution_role_arn",[919,12798,12799],{},"rds_address",[11,12801,12802,12803,1172,12806,12809,12810,752],{},"In CDK, the stack references in the app stack don't reference the unique identifiers from the base stack (such as the VPC id or bastion host instance id), but instead they reference the properties of the stack which have types like ",[33,12804,12805],{},"Vpc",[33,12807,12808],{},"RdsInstance",". More on this later in the following section ",[338,12811,12812],{},"Passing data between stacks",[168,12814,12816],{"id":12815},"ad-hoc-app-overview","Ad hoc app overview",[11,12818,12819],{},"The ad hoc app is an group of resources that powers an on-demand environment that is meant to be short lived for testing, QA, validation, demos, etc.",[11,12821,12822],{},"This visualization shows all of the resources in the ad hoc app stack. It also comes from the Pulumi console.",[11,12824,12825],{},[511,12826],{"alt":12827,"src":12828},"Graph view of ad hoc app infrastructure","/static/pulumi_ad_hoc_app_dep_graph.png",[911,12830,12832],{"id":12831},"ecs-cluster","ECS Cluster",[916,12834,12835,12838],{},[919,12836,12837],{},"This is a small component that defines both ECS Cluster and the default capacity providers",[919,12839,12840,12841,12844,12845,12847],{},"It defaults to not using ",[33,12842,12843],{},"FARGATE_SPOT","; ad hoc environments do use ",[33,12846,12843],{}," for cost savings",[107,12849,12850],{},[11,12851,12852,12853,343],{},"NOTE: defaultCapacityProviderStrategy on cluster not currently supported. (",[15,12854,6654],{"href":12855,"rel":12856},"https://docs.aws.amazon.com/cdk/api/v1/docs/@aws-cdk_aws-ecs.CapacityProviderStrategy.html",[19],[911,12858,12860],{"id":12859},"shared-environment-variables","Shared environment variables",[11,12862,12863,12864,12866],{},"The backend containers should all have the same environment variables, so I define them once in the app stack and pass these into the service resource ",[33,12865,11481],{},"s.",[916,12868,12869,12883,12889,12895,12901],{},[919,12870,12871,12872,12875,12876,12879,12880,752],{},"I struggled to get this right in pulumi. A lot of Pulumi examples used ",[33,12873,12874],{},"JSON.stringify"," for containerDefinitions in task definitions. I was able to get help from the Pulumi Slack channel; someone recommended that I use ",[33,12877,12878],{},"pulumi.jsonStringify"," which was added in a relatively recent version of ",[33,12881,12882],{},"pulumi/pulumi",[919,12884,12885,12886],{},"CDK allows you to declare environment variables for a containerDefinition like ",[33,12887,12888],{},"{ FOO: \"bar\" }",[919,12890,12891,12892],{},"Pulumi and Terraform require that values are passed like ",[33,12893,12894],{},"{ name: \"FOO\", value: \"bar\"}",[919,12896,12897,12898,12900],{},"You could transform ",[33,12899,12888],{}," into the name/value format, but I didn't bother to do this",[919,12902,12903,12904,12907],{},"extra env vars in Terraform to allow for dynamically passing extra environment variables, and I used the ",[33,12905,12906],{},"concat"," function to add these to the list of default environment variables.",[11,12909,12910],{},"Here's what the code looks like for joining extra environment variables to the default environment variables:",[26,12912,12914],{"className":12421,"code":12913,"language":12423,"meta":35,"style":35},"    // CDK\n    if (extraEnvVars) {\n      environmentVariables = { ...extraEnvVars, ...environmentVariables };\n    }\n",[33,12915,12916,12921,12929,12949],{"__ignoreMap":35},[187,12917,12918],{"class":189,"line":190},[187,12919,12920],{"class":295},"    // CDK\n",[187,12922,12923,12926],{"class":189,"line":249},[187,12924,12925],{"class":573},"    if",[187,12927,12928],{"class":577}," (extraEnvVars) {\n",[187,12930,12931,12934,12936,12938,12941,12944,12946],{"class":189,"line":312},[187,12932,12933],{"class":577},"      environmentVariables ",[187,12935,595],{"class":573},[187,12937,578],{"class":577},[187,12939,12940],{"class":573},"...",[187,12942,12943],{"class":577},"extraEnvVars, ",[187,12945,12940],{"class":573},[187,12947,12948],{"class":577},"environmentVariables };\n",[187,12950,12951],{"class":189,"line":319},[187,12952,9799],{"class":577},[26,12954,12957],{"className":12955,"code":12956,"language":31},[29],"    # terraform\n    env_vars = concat(local.env_vars, var.extra_env_vars)\n",[33,12958,12956],{"__ignoreMap":35},[26,12960,12963],{"className":12961,"code":12962,"language":31},[29],"    // Pulumi\n    if (extraEnvVars) {\n      envVars = envVars.apply(x => x.concat(extraEnvVars!))\n    }\n",[33,12964,12962],{"__ignoreMap":35},[911,12966,12968],{"id":12967},"route53-record","Route53 Record",[11,12970,12971],{},"This is pretty straightforward in each library. Each ad hoc environment gets a Route 53 record, and listener rules for the web services (Django and Vue.js SPA) match on a combination of the host header and path patterns.",[11,12973,12974,12975,12978,12979,12982],{},"This part is pretty opinionated in that it assumes you want to host the frontend and backend services on the same URL. For example, requests matching ",[33,12976,12977],{},"example.com/api/*"," are routed to the backend API and all other requests matching ",[33,12980,12981],{},"example.com/*"," are routed to the frontend service.",[911,12984,12986],{"id":12985},"redis","Redis",[11,12988,12989],{},"I go into more depth about why I run a Redis instance in an ECS service in my other article. This is only for the ad hoc environments. Production environments are configured with ElastiCache running Redis.",[11,12991,12992],{},"I decided to not make this service use any persistent storage. It may be a good idea to not use FARGATE_SPOT for this service, since restarts to the redis service could cause issues in ad hoc environments. For example, you may get a lot of celery errors in ad hoc environments if redis is not reachable.",[911,12994,12996],{"id":12995},"web-service","Web Service",[11,12998,12999,13000,13002,13003,13006,13007,1172,13010,13013,13014,1737],{},"The web service is what defines the main Django application as well as the frontend website (JavaScript SPA or SSR site). I designed the Web Service resources group to be able to support both traditional Django apps (powered by templates), or for Django apps that service only a limited number of endpoints. This ",[33,13001,11481],{}," has an input parameter called ",[33,13004,13005],{},"pathPatterns"," which determines which paths it serves. For example, the API container may serve traffic for ",[33,13008,13009],{},"/api/*",[33,13011,13012],{},"/admin/*"," only, or it may want to serve all traffic (",[33,13015,13016],{},"/*",[11,13018,13019],{},"The way I use these components in ad hoc and prod environments is heavily opinionated in that:",[916,13021,13022],{},[919,13023,13024,13025,13027,13028,637,13030,637,13032,13035],{},"it assumes that the frontend SPA/SSR site should have a lower priority rule than the backend service and should route request paths matching ",[33,13026,13016],{},", while the backend service routes requests for a specific list of path patterns (",[33,13029,13009],{},[33,13031,13012],{},[33,13033,13034],{},"/graphql/*",", etc.).",[11,13037,13038],{},"You may want Django to handle most of your routes and 404 pages, in which case you would want the SPA to only handle requests matching certain paths. This would require some more consideration and careful refactoring.",[911,13040,13042],{"id":13041},"celery","Celery",[916,13044,13045,13048],{},[919,13046,13047],{},"The reason for having a celery service is to be able to have potentially multiple workers that scale independently",[919,13049,13050],{},"I use the same Pulumi component for both works and schedulers",[11,13052,13053,13054,13057,13058,752],{},"The terminology for this resource group could be better. Celery is one of many options for running async task workers, so it should probably be called something like ",[33,13055,13056],{},"AsyncWorker"," across the board rather than using the term ",[33,13059,13041],{},[911,13061,13063],{"id":13062},"management-command","Management Command",[916,13065,13066,13075],{},[919,13067,13068,13069,1172,13072],{},"Defines a task that can be used to run commands like ",[33,13070,13071],{},"collectstatic",[33,13073,13074],{},"migrate",[919,13076,13077,13078,13080],{},"These tasks are ran both after the initial ",[33,13079,11956],{}," stack deployment and before rolling application upgrades",[11,13082,13083,13084,1172,13086,13088],{},"In my Django app I have a single management command that calls ",[33,13085,13074],{},[33,13087,13071],{}," and runs them in the same process one after another. This management command could also be used for clearing caches during updates, loading fixtures, etc.",[11,13090,13091,13092,13094],{},"One other thing to note about this ",[33,13093,11481],{}," is that it outputs a complete script that can be used in GitHub Actions (or on your CLI when testing locally) that does the following:",[916,13096,13097,13104,13107,13110,13115],{},[919,13098,13099,13100,13103],{},"saves the ",[33,13101,13102],{},"START"," timestamp",[919,13105,13106],{},"runs the task with the required settings",[919,13108,13109],{},"waits for the task to complete",[919,13111,13099,13112,13103],{},[33,13113,13114],{},"END",[919,13116,13117,13118,1172,13120,13122,13123],{},"collects the logs for the task between ",[33,13119,13102],{},[33,13121,13114],{}," and prints them to ",[33,13124,13125],{},"stdout",[11,13127,13128],{},"Here's an example of what the script looks like in Pulumi:",[26,13130,13132],{"className":12421,"code":13131,"language":12423,"meta":35,"style":35},"    const executionScript = pulumi.interpolate`#!/bin/bash\nSTART_TIME=$(date +%s000)\nTASK_ID=$(aws ecs run-task --cluster ${props.ecsClusterId} --task-definition ${taskDefinition.arn} --launch-type FARGATE --network-configuration \"awsvpcConfiguration={subnets=[${props.privateSubnetIds.apply(x => x.join(\",\"))}],securityGroups=[${props.appSgId}],assignPublicIp=ENABLED}\" | jq -r '.tasks[0].taskArn')\naws ecs wait tasks-stopped --tasks $TASK_ID --cluster ${props.ecsClusterId}\nEND_TIME=$(date +%s000)\naws logs get-log-events --log-group-name ${cwLoggingResources.cwLogGroupName} --log-stream-name ${props.name}/${props.name}/\\${TASK_ID##*/} --start-time $START_TIME --end-time $END_TIME | jq -r '.events[].message'\n`;\n    this.executionScript = executionScript;\n",[33,13133,13134,13152,13157,13230,13243,13248,13287,13294],{"__ignoreMap":35},[187,13135,13136,13138,13141,13143,13146,13149],{"class":189,"line":190},[187,13137,6766],{"class":573},[187,13139,13140],{"class":588}," executionScript",[187,13142,6771],{"class":573},[187,13144,13145],{"class":577}," pulumi.",[187,13147,13148],{"class":193},"interpolate",[187,13150,13151],{"class":196},"`#!/bin/bash\n",[187,13153,13154],{"class":189,"line":249},[187,13155,13156],{"class":196},"START_TIME=$(date +%s000)\n",[187,13158,13159,13162,13165,13167,13170,13173,13176,13178,13181,13184,13186,13188,13191,13193,13195,13197,13199,13201,13204,13206,13209,13211,13214,13217,13220,13222,13224,13227],{"class":189,"line":312},[187,13160,13161],{"class":196},"TASK_ID=$(aws ecs run-task --cluster ${",[187,13163,13164],{"class":577},"props",[187,13166,752],{"class":196},[187,13168,13169],{"class":577},"ecsClusterId",[187,13171,13172],{"class":196},"} --task-definition ${",[187,13174,13175],{"class":577},"taskDefinition",[187,13177,752],{"class":196},[187,13179,13180],{"class":577},"arn",[187,13182,13183],{"class":196},"} --launch-type FARGATE --network-configuration \"awsvpcConfiguration={subnets=[${",[187,13185,13164],{"class":577},[187,13187,752],{"class":196},[187,13189,13190],{"class":577},"privateSubnetIds",[187,13192,752],{"class":196},[187,13194,11489],{"class":193},[187,13196,615],{"class":196},[187,13198,2196],{"class":588},[187,13200,7704],{"class":573},[187,13202,13203],{"class":577}," x",[187,13205,752],{"class":196},[187,13207,13208],{"class":193},"join",[187,13210,615],{"class":196},[187,13212,13213],{"class":196},"\",\"",[187,13215,13216],{"class":196},"))",[187,13218,13219],{"class":196},"}],securityGroups=[${",[187,13221,13164],{"class":577},[187,13223,752],{"class":196},[187,13225,13226],{"class":577},"appSgId",[187,13228,13229],{"class":196},"}],assignPublicIp=ENABLED}\" | jq -r '.tasks[0].taskArn')\n",[187,13231,13232,13235,13237,13239,13241],{"class":189,"line":319},[187,13233,13234],{"class":196},"aws ecs wait tasks-stopped --tasks $TASK_ID --cluster ${",[187,13236,13164],{"class":577},[187,13238,752],{"class":196},[187,13240,13169],{"class":577},[187,13242,1309],{"class":196},[187,13244,13245],{"class":189,"line":325},[187,13246,13247],{"class":196},"END_TIME=$(date +%s000)\n",[187,13249,13250,13253,13256,13258,13261,13264,13266,13268,13270,13272,13274,13276,13278,13281,13284],{"class":189,"line":686},[187,13251,13252],{"class":196},"aws logs get-log-events --log-group-name ${",[187,13254,13255],{"class":577},"cwLoggingResources",[187,13257,752],{"class":196},[187,13259,13260],{"class":577},"cwLogGroupName",[187,13262,13263],{"class":196},"} --log-stream-name ${",[187,13265,13164],{"class":577},[187,13267,752],{"class":196},[187,13269,7284],{"class":577},[187,13271,6784],{"class":196},[187,13273,13164],{"class":577},[187,13275,752],{"class":196},[187,13277,7284],{"class":577},[187,13279,13280],{"class":196},"}/",[187,13282,13283],{"class":588},"\\$",[187,13285,13286],{"class":196},"{TASK_ID##*/} --start-time $START_TIME --end-time $END_TIME | jq -r '.events[].message'\n",[187,13288,13289,13292],{"class":189,"line":697},[187,13290,13291],{"class":196},"`",[187,13293,6961],{"class":577},[187,13295,13296,13299,13302,13304],{"class":189,"line":1291},[187,13297,13298],{"class":588},"    this",[187,13300,13301],{"class":577},".executionScript ",[187,13303,595],{"class":573},[187,13305,13306],{"class":577}," executionScript;\n",[11,13308,13309],{},"In GitHub Actions we get this command as a stack output, save it to a file, make it executable and then run it. This is what it looks like with CDK as a CloudFormation stack output:",[26,13311,13313],{"className":2507,"code":13312,"language":2509,"meta":35,"style":35},"      - name: \"Run backend update command\"\n        id: run_backend_update\n        run: |\n          # get the script from the stack output with an output key that contains the string `backendUpdate`\n          BACKEND_UPDATE_SCRIPT=$(aws cloudformation describe-stacks \\\n            --stack-name $AD_HOC_APP_NAME \\\n            | jq -r '.Stacks[0].Outputs[]|select(.OutputKey | contains(\"backendUpdate\")) | .OutputValue' \\\n          )\n\n          echo \"$BACKEND_UPDATE_SCRIPT\" > backend_update_command.sh\n          cat backend_update_command.sh\n          sudo chmod +x backend_update_command.sh\n          ./backend_update_command.sh\n",[33,13314,13315,13326,13336,13345,13350,13355,13360,13365,13370,13374,13379,13384,13389],{"__ignoreMap":35},[187,13316,13317,13319,13321,13323],{"class":189,"line":190},[187,13318,2610],{"class":577},[187,13320,7284],{"class":2516},[187,13322,585],{"class":577},[187,13324,13325],{"class":196},"\"Run backend update command\"\n",[187,13327,13328,13331,13333],{"class":189,"line":249},[187,13329,13330],{"class":2516},"        id",[187,13332,585],{"class":577},[187,13334,13335],{"class":196},"run_backend_update\n",[187,13337,13338,13341,13343],{"class":189,"line":312},[187,13339,13340],{"class":2516},"        run",[187,13342,585],{"class":577},[187,13344,7477],{"class":573},[187,13346,13347],{"class":189,"line":319},[187,13348,13349],{"class":196},"          # get the script from the stack output with an output key that contains the string `backendUpdate`\n",[187,13351,13352],{"class":189,"line":325},[187,13353,13354],{"class":196},"          BACKEND_UPDATE_SCRIPT=$(aws cloudformation describe-stacks \\\n",[187,13356,13357],{"class":189,"line":686},[187,13358,13359],{"class":196},"            --stack-name $AD_HOC_APP_NAME \\\n",[187,13361,13362],{"class":189,"line":697},[187,13363,13364],{"class":196},"            | jq -r '.Stacks[0].Outputs[]|select(.OutputKey | contains(\"backendUpdate\")) | .OutputValue' \\\n",[187,13366,13367],{"class":189,"line":1291},[187,13368,13369],{"class":196},"          )\n",[187,13371,13372],{"class":189,"line":1306},[187,13373,316],{"emptyLinePlaceholder":315},[187,13375,13376],{"class":189,"line":1434},[187,13377,13378],{"class":196},"          echo \"$BACKEND_UPDATE_SCRIPT\" > backend_update_command.sh\n",[187,13380,13381],{"class":189,"line":2599},[187,13382,13383],{"class":196},"          cat backend_update_command.sh\n",[187,13385,13386],{"class":189,"line":2607},[187,13387,13388],{"class":196},"          sudo chmod +x backend_update_command.sh\n",[187,13390,13391],{"class":189,"line":2621},[187,13392,13393],{"class":196},"          ./backend_update_command.sh\n",[911,13395,12812],{"id":13396},"passing-data-between-stacks",[11,13398,13399],{},"Pulumi uses stack references, Terraform uses remote state and CDK uses Stack Outputs or Stack References.",[11,13401,13402],{},"Here's what this looks like in Terraform",[26,13404,13408],{"className":13405,"code":13406,"language":13407,"meta":35,"style":35},"language-terraform shiki shiki-themes github-light github-dark","data \"terraform_remote_state\" \"this\" {\n  backend = \"local\"\n\n  config = {\n    path = \"../base/terraform.tfstate\"\n  }\n}\n\nmodule \"main\" {\n  source = \"../../../modules/ad-hoc/app\"\n\n  vpc_id                         = data.terraform_remote_state.this.outputs.vpc_id\n  assets_bucket_name             = data.terraform_remote_state.this.outputs.assets_bucket_name\n  private_subnet_ids             = data.terraform_remote_state.this.outputs.private_subnet_ids\n  app_sg_id                      = data.terraform_remote_state.this.outputs.app_sg_id\n  alb_sg_id                      = data.terraform_remote_state.this.outputs.alb_sg_id\n  listener_arn                   = data.terraform_remote_state.this.outputs.listener_arn\n  alb_dns_name                   = data.terraform_remote_state.this.outputs.alb_dns_name\n  service_discovery_namespace_id = data.terraform_remote_state.this.outputs.service_discovery_namespace_id\n  rds_address                    = data.terraform_remote_state.this.outputs.rds_address\n  domain_name                    = data.terraform_remote_state.this.outputs.domain_name\n  base_stack_name                = data.terraform_remote_state.this.outputs.base_stack_name\n  region                         = var.region\n}\n","terraform",[33,13409,13410,13415,13420,13424,13429,13434,13438,13442,13446,13451,13456,13460,13465,13470,13475,13480,13485,13490,13495,13500,13505,13510,13515,13520],{"__ignoreMap":35},[187,13411,13412],{"class":189,"line":190},[187,13413,13414],{},"data \"terraform_remote_state\" \"this\" {\n",[187,13416,13417],{"class":189,"line":249},[187,13418,13419],{},"  backend = \"local\"\n",[187,13421,13422],{"class":189,"line":312},[187,13423,316],{"emptyLinePlaceholder":315},[187,13425,13426],{"class":189,"line":319},[187,13427,13428],{},"  config = {\n",[187,13430,13431],{"class":189,"line":325},[187,13432,13433],{},"    path = \"../base/terraform.tfstate\"\n",[187,13435,13436],{"class":189,"line":686},[187,13437,6847],{},[187,13439,13440],{"class":189,"line":697},[187,13441,1309],{},[187,13443,13444],{"class":189,"line":1291},[187,13445,316],{"emptyLinePlaceholder":315},[187,13447,13448],{"class":189,"line":1306},[187,13449,13450],{},"module \"main\" {\n",[187,13452,13453],{"class":189,"line":1434},[187,13454,13455],{},"  source = \"../../../modules/ad-hoc/app\"\n",[187,13457,13458],{"class":189,"line":2599},[187,13459,316],{"emptyLinePlaceholder":315},[187,13461,13462],{"class":189,"line":2607},[187,13463,13464],{},"  vpc_id                         = data.terraform_remote_state.this.outputs.vpc_id\n",[187,13466,13467],{"class":189,"line":2621},[187,13468,13469],{},"  assets_bucket_name             = data.terraform_remote_state.this.outputs.assets_bucket_name\n",[187,13471,13472],{"class":189,"line":2631},[187,13473,13474],{},"  private_subnet_ids             = data.terraform_remote_state.this.outputs.private_subnet_ids\n",[187,13476,13477],{"class":189,"line":2642},[187,13478,13479],{},"  app_sg_id                      = data.terraform_remote_state.this.outputs.app_sg_id\n",[187,13481,13482],{"class":189,"line":2653},[187,13483,13484],{},"  alb_sg_id                      = data.terraform_remote_state.this.outputs.alb_sg_id\n",[187,13486,13487],{"class":189,"line":2665},[187,13488,13489],{},"  listener_arn                   = data.terraform_remote_state.this.outputs.listener_arn\n",[187,13491,13492],{"class":189,"line":2674},[187,13493,13494],{},"  alb_dns_name                   = data.terraform_remote_state.this.outputs.alb_dns_name\n",[187,13496,13497],{"class":189,"line":2684},[187,13498,13499],{},"  service_discovery_namespace_id = data.terraform_remote_state.this.outputs.service_discovery_namespace_id\n",[187,13501,13502],{"class":189,"line":2694},[187,13503,13504],{},"  rds_address                    = data.terraform_remote_state.this.outputs.rds_address\n",[187,13506,13507],{"class":189,"line":2706},[187,13508,13509],{},"  domain_name                    = data.terraform_remote_state.this.outputs.domain_name\n",[187,13511,13512],{"class":189,"line":2715},[187,13513,13514],{},"  base_stack_name                = data.terraform_remote_state.this.outputs.base_stack_name\n",[187,13516,13517],{"class":189,"line":2725},[187,13518,13519],{},"  region                         = var.region\n",[187,13521,13522],{"class":189,"line":2735},[187,13523,1309],{},[11,13525,13526],{},"In CDK:",[26,13528,13530],{"className":12421,"code":13529,"language":12423,"meta":35,"style":35},"const baseStack = new Stack(app, 'ExampleAdHocBaseStack', { env, stackName: adHocBaseEnvName });\nbaseStack.node.setContext('config', adHocBaseEnvConfig);\n\nconst appStack = new Stack(app, 'ExampleAdHocAppStack', { env, stackName: adHocAppEnvName });\nappStack.node.setContext('config', adHocAppEnvConfig);\n\nconst adHocBase = new AdHocBase(baseStack, 'AdHocBase', { certificateArn, domainName });\n\nconst addHocApp = new AdHocApp(appStack, 'AdHocApp', {\n  baseStackName: adHocBaseEnvName,\n  vpc: adHocBase.vpc,\n  alb: adHocBase.alb,\n  appSecurityGroup: adHocBase.appSecurityGroup,\n  serviceDiscoveryNamespace: adHocBase.serviceDiscoveryNamespace,\n  rdsInstance: adHocBase.databaseInstance,\n  assetsBucket: adHocBase.assetsBucket,\n  domainName: adHocBase.domainName,\n  listener: adHocBase.listener,\n});\n",[33,13531,13532,13555,13571,13575,13596,13610,13614,13637,13641,13663,13668,13673,13678,13683,13688,13693,13698,13703,13708],{"__ignoreMap":35},[187,13533,13534,13536,13539,13541,13543,13546,13549,13552],{"class":189,"line":190},[187,13535,574],{"class":573},[187,13537,13538],{"class":588}," baseStack",[187,13540,6771],{"class":573},[187,13542,12437],{"class":573},[187,13544,13545],{"class":193}," Stack",[187,13547,13548],{"class":577},"(app, ",[187,13550,13551],{"class":196},"'ExampleAdHocBaseStack'",[187,13553,13554],{"class":577},", { env, stackName: adHocBaseEnvName });\n",[187,13556,13557,13560,13563,13565,13568],{"class":189,"line":249},[187,13558,13559],{"class":577},"baseStack.node.",[187,13561,13562],{"class":193},"setContext",[187,13564,615],{"class":577},[187,13566,13567],{"class":196},"'config'",[187,13569,13570],{"class":577},", adHocBaseEnvConfig);\n",[187,13572,13573],{"class":189,"line":312},[187,13574,316],{"emptyLinePlaceholder":315},[187,13576,13577,13579,13582,13584,13586,13588,13590,13593],{"class":189,"line":319},[187,13578,574],{"class":573},[187,13580,13581],{"class":588}," appStack",[187,13583,6771],{"class":573},[187,13585,12437],{"class":573},[187,13587,13545],{"class":193},[187,13589,13548],{"class":577},[187,13591,13592],{"class":196},"'ExampleAdHocAppStack'",[187,13594,13595],{"class":577},", { env, stackName: adHocAppEnvName });\n",[187,13597,13598,13601,13603,13605,13607],{"class":189,"line":325},[187,13599,13600],{"class":577},"appStack.node.",[187,13602,13562],{"class":193},[187,13604,615],{"class":577},[187,13606,13567],{"class":196},[187,13608,13609],{"class":577},", adHocAppEnvConfig);\n",[187,13611,13612],{"class":189,"line":686},[187,13613,316],{"emptyLinePlaceholder":315},[187,13615,13616,13618,13621,13623,13625,13628,13631,13634],{"class":189,"line":697},[187,13617,574],{"class":573},[187,13619,13620],{"class":588}," adHocBase",[187,13622,6771],{"class":573},[187,13624,12437],{"class":573},[187,13626,13627],{"class":193}," AdHocBase",[187,13629,13630],{"class":577},"(baseStack, ",[187,13632,13633],{"class":196},"'AdHocBase'",[187,13635,13636],{"class":577},", { certificateArn, domainName });\n",[187,13638,13639],{"class":189,"line":1291},[187,13640,316],{"emptyLinePlaceholder":315},[187,13642,13643,13645,13648,13650,13652,13655,13658,13661],{"class":189,"line":1306},[187,13644,574],{"class":573},[187,13646,13647],{"class":588}," addHocApp",[187,13649,6771],{"class":573},[187,13651,12437],{"class":573},[187,13653,13654],{"class":193}," AdHocApp",[187,13656,13657],{"class":577},"(appStack, ",[187,13659,13660],{"class":196},"'AdHocApp'",[187,13662,12449],{"class":577},[187,13664,13665],{"class":189,"line":1434},[187,13666,13667],{"class":577},"  baseStackName: adHocBaseEnvName,\n",[187,13669,13670],{"class":189,"line":2599},[187,13671,13672],{"class":577},"  vpc: adHocBase.vpc,\n",[187,13674,13675],{"class":189,"line":2607},[187,13676,13677],{"class":577},"  alb: adHocBase.alb,\n",[187,13679,13680],{"class":189,"line":2621},[187,13681,13682],{"class":577},"  appSecurityGroup: adHocBase.appSecurityGroup,\n",[187,13684,13685],{"class":189,"line":2631},[187,13686,13687],{"class":577},"  serviceDiscoveryNamespace: adHocBase.serviceDiscoveryNamespace,\n",[187,13689,13690],{"class":189,"line":2642},[187,13691,13692],{"class":577},"  rdsInstance: adHocBase.databaseInstance,\n",[187,13694,13695],{"class":189,"line":2653},[187,13696,13697],{"class":577},"  assetsBucket: adHocBase.assetsBucket,\n",[187,13699,13700],{"class":189,"line":2665},[187,13701,13702],{"class":577},"  domainName: adHocBase.domainName,\n",[187,13704,13705],{"class":189,"line":2674},[187,13706,13707],{"class":577},"  listener: adHocBase.listener,\n",[187,13709,13710],{"class":189,"line":2684},[187,13711,7722],{"class":577},[11,13713,13714],{},"and in Pulumi:",[26,13716,13718],{"className":12421,"code":13717,"language":12423,"meta":35,"style":35},"const stackReference = new pulumi.StackReference(`${org}/ad-hoc-base/${environment}`)\n\nconst vpcId = stackReference.getOutput(\"vpcId\") as pulumi.Output\u003Cstring>;\nconst assetsBucketName = stackReference.getOutput(\"assetsBucketName\") as pulumi.Output\u003Cstring>;\nconst privateSubnets = stackReference.getOutput(\"privateSubnetIds\") as pulumi.Output\u003Cstring[]>;\nconst appSgId = stackReference.getOutput(\"appSgId\") as pulumi.Output\u003Cstring>;\nconst albSgId = stackReference.getOutput(\"albSgId\") as pulumi.Output\u003Cstring>;\nconst listenerArn = stackReference.getOutput(\"listenerArn\") as pulumi.Output\u003Cstring>;\nconst albDnsName = stackReference.getOutput(\"albDnsName\") as pulumi.Output\u003Cstring>;\nconst serviceDiscoveryNamespaceId = stackReference.getOutput(\"serviceDiscoveryNamespaceId\") as pulumi.Output\u003Cstring>;\nconst rdsAddress = stackReference.getOutput(\"rdsAddress\") as pulumi.Output\u003Cstring>;\nconst domainName = stackReference.getOutput(\"domainName\") as pulumi.Output\u003Cstring>;\nconst baseStackName = stackReference.getOutput(\"baseStackName\") as pulumi.Output\u003Cstring>;\n\n// ad hoc app env\nconst adHocAppComponent = new AdHocAppComponent(\"AdHocAppComponent\", {\n  vpcId,\n  assetsBucketName,\n  privateSubnets,\n  appSgId,\n  albSgId,\n  listenerArn,\n  albDnsName,\n  serviceDiscoveryNamespaceId,\n  rdsAddress,\n  domainName,\n  baseStackName\n});\n",[33,13719,13720,13754,13758,13800,13834,13869,13903,13937,13971,14005,14039,14073,14107,14141,14145,14150,14171,14176,14181,14186,14191,14196,14201,14206,14211,14216,14221,14226],{"__ignoreMap":35},[187,13721,13722,13724,13727,13729,13731,13733,13736,13738,13741,13744,13747,13750,13752],{"class":189,"line":190},[187,13723,574],{"class":573},[187,13725,13726],{"class":588}," stackReference",[187,13728,6771],{"class":573},[187,13730,12437],{"class":573},[187,13732,13145],{"class":577},[187,13734,13735],{"class":193},"StackReference",[187,13737,615],{"class":577},[187,13739,13740],{"class":196},"`${",[187,13742,13743],{"class":577},"org",[187,13745,13746],{"class":196},"}/ad-hoc-base/${",[187,13748,13749],{"class":577},"environment",[187,13751,6958],{"class":196},[187,13753,621],{"class":577},[187,13755,13756],{"class":189,"line":249},[187,13757,316],{"emptyLinePlaceholder":315},[187,13759,13760,13762,13765,13767,13770,13773,13775,13778,13781,13784,13787,13789,13792,13794,13797],{"class":189,"line":312},[187,13761,574],{"class":573},[187,13763,13764],{"class":588}," vpcId",[187,13766,6771],{"class":573},[187,13768,13769],{"class":577}," stackReference.",[187,13771,13772],{"class":193},"getOutput",[187,13774,615],{"class":577},[187,13776,13777],{"class":196},"\"vpcId\"",[187,13779,13780],{"class":577},") ",[187,13782,13783],{"class":573},"as",[187,13785,13786],{"class":193}," pulumi",[187,13788,752],{"class":577},[187,13790,13791],{"class":193},"Output",[187,13793,6724],{"class":577},[187,13795,13796],{"class":588},"string",[187,13798,13799],{"class":577},">;\n",[187,13801,13802,13804,13807,13809,13811,13813,13815,13818,13820,13822,13824,13826,13828,13830,13832],{"class":189,"line":319},[187,13803,574],{"class":573},[187,13805,13806],{"class":588}," assetsBucketName",[187,13808,6771],{"class":573},[187,13810,13769],{"class":577},[187,13812,13772],{"class":193},[187,13814,615],{"class":577},[187,13816,13817],{"class":196},"\"assetsBucketName\"",[187,13819,13780],{"class":577},[187,13821,13783],{"class":573},[187,13823,13786],{"class":193},[187,13825,752],{"class":577},[187,13827,13791],{"class":193},[187,13829,6724],{"class":577},[187,13831,13796],{"class":588},[187,13833,13799],{"class":577},[187,13835,13836,13838,13841,13843,13845,13847,13849,13852,13854,13856,13858,13860,13862,13864,13866],{"class":189,"line":325},[187,13837,574],{"class":573},[187,13839,13840],{"class":588}," privateSubnets",[187,13842,6771],{"class":573},[187,13844,13769],{"class":577},[187,13846,13772],{"class":193},[187,13848,615],{"class":577},[187,13850,13851],{"class":196},"\"privateSubnetIds\"",[187,13853,13780],{"class":577},[187,13855,13783],{"class":573},[187,13857,13786],{"class":193},[187,13859,752],{"class":577},[187,13861,13791],{"class":193},[187,13863,6724],{"class":577},[187,13865,13796],{"class":588},[187,13867,13868],{"class":577},"[]>;\n",[187,13870,13871,13873,13876,13878,13880,13882,13884,13887,13889,13891,13893,13895,13897,13899,13901],{"class":189,"line":686},[187,13872,574],{"class":573},[187,13874,13875],{"class":588}," appSgId",[187,13877,6771],{"class":573},[187,13879,13769],{"class":577},[187,13881,13772],{"class":193},[187,13883,615],{"class":577},[187,13885,13886],{"class":196},"\"appSgId\"",[187,13888,13780],{"class":577},[187,13890,13783],{"class":573},[187,13892,13786],{"class":193},[187,13894,752],{"class":577},[187,13896,13791],{"class":193},[187,13898,6724],{"class":577},[187,13900,13796],{"class":588},[187,13902,13799],{"class":577},[187,13904,13905,13907,13910,13912,13914,13916,13918,13921,13923,13925,13927,13929,13931,13933,13935],{"class":189,"line":697},[187,13906,574],{"class":573},[187,13908,13909],{"class":588}," albSgId",[187,13911,6771],{"class":573},[187,13913,13769],{"class":577},[187,13915,13772],{"class":193},[187,13917,615],{"class":577},[187,13919,13920],{"class":196},"\"albSgId\"",[187,13922,13780],{"class":577},[187,13924,13783],{"class":573},[187,13926,13786],{"class":193},[187,13928,752],{"class":577},[187,13930,13791],{"class":193},[187,13932,6724],{"class":577},[187,13934,13796],{"class":588},[187,13936,13799],{"class":577},[187,13938,13939,13941,13944,13946,13948,13950,13952,13955,13957,13959,13961,13963,13965,13967,13969],{"class":189,"line":1291},[187,13940,574],{"class":573},[187,13942,13943],{"class":588}," listenerArn",[187,13945,6771],{"class":573},[187,13947,13769],{"class":577},[187,13949,13772],{"class":193},[187,13951,615],{"class":577},[187,13953,13954],{"class":196},"\"listenerArn\"",[187,13956,13780],{"class":577},[187,13958,13783],{"class":573},[187,13960,13786],{"class":193},[187,13962,752],{"class":577},[187,13964,13791],{"class":193},[187,13966,6724],{"class":577},[187,13968,13796],{"class":588},[187,13970,13799],{"class":577},[187,13972,13973,13975,13978,13980,13982,13984,13986,13989,13991,13993,13995,13997,13999,14001,14003],{"class":189,"line":1306},[187,13974,574],{"class":573},[187,13976,13977],{"class":588}," albDnsName",[187,13979,6771],{"class":573},[187,13981,13769],{"class":577},[187,13983,13772],{"class":193},[187,13985,615],{"class":577},[187,13987,13988],{"class":196},"\"albDnsName\"",[187,13990,13780],{"class":577},[187,13992,13783],{"class":573},[187,13994,13786],{"class":193},[187,13996,752],{"class":577},[187,13998,13791],{"class":193},[187,14000,6724],{"class":577},[187,14002,13796],{"class":588},[187,14004,13799],{"class":577},[187,14006,14007,14009,14012,14014,14016,14018,14020,14023,14025,14027,14029,14031,14033,14035,14037],{"class":189,"line":1434},[187,14008,574],{"class":573},[187,14010,14011],{"class":588}," serviceDiscoveryNamespaceId",[187,14013,6771],{"class":573},[187,14015,13769],{"class":577},[187,14017,13772],{"class":193},[187,14019,615],{"class":577},[187,14021,14022],{"class":196},"\"serviceDiscoveryNamespaceId\"",[187,14024,13780],{"class":577},[187,14026,13783],{"class":573},[187,14028,13786],{"class":193},[187,14030,752],{"class":577},[187,14032,13791],{"class":193},[187,14034,6724],{"class":577},[187,14036,13796],{"class":588},[187,14038,13799],{"class":577},[187,14040,14041,14043,14046,14048,14050,14052,14054,14057,14059,14061,14063,14065,14067,14069,14071],{"class":189,"line":2599},[187,14042,574],{"class":573},[187,14044,14045],{"class":588}," rdsAddress",[187,14047,6771],{"class":573},[187,14049,13769],{"class":577},[187,14051,13772],{"class":193},[187,14053,615],{"class":577},[187,14055,14056],{"class":196},"\"rdsAddress\"",[187,14058,13780],{"class":577},[187,14060,13783],{"class":573},[187,14062,13786],{"class":193},[187,14064,752],{"class":577},[187,14066,13791],{"class":193},[187,14068,6724],{"class":577},[187,14070,13796],{"class":588},[187,14072,13799],{"class":577},[187,14074,14075,14077,14080,14082,14084,14086,14088,14091,14093,14095,14097,14099,14101,14103,14105],{"class":189,"line":2607},[187,14076,574],{"class":573},[187,14078,14079],{"class":588}," domainName",[187,14081,6771],{"class":573},[187,14083,13769],{"class":577},[187,14085,13772],{"class":193},[187,14087,615],{"class":577},[187,14089,14090],{"class":196},"\"domainName\"",[187,14092,13780],{"class":577},[187,14094,13783],{"class":573},[187,14096,13786],{"class":193},[187,14098,752],{"class":577},[187,14100,13791],{"class":193},[187,14102,6724],{"class":577},[187,14104,13796],{"class":588},[187,14106,13799],{"class":577},[187,14108,14109,14111,14114,14116,14118,14120,14122,14125,14127,14129,14131,14133,14135,14137,14139],{"class":189,"line":2621},[187,14110,574],{"class":573},[187,14112,14113],{"class":588}," baseStackName",[187,14115,6771],{"class":573},[187,14117,13769],{"class":577},[187,14119,13772],{"class":193},[187,14121,615],{"class":577},[187,14123,14124],{"class":196},"\"baseStackName\"",[187,14126,13780],{"class":577},[187,14128,13783],{"class":573},[187,14130,13786],{"class":193},[187,14132,752],{"class":577},[187,14134,13791],{"class":193},[187,14136,6724],{"class":577},[187,14138,13796],{"class":588},[187,14140,13799],{"class":577},[187,14142,14143],{"class":189,"line":2631},[187,14144,316],{"emptyLinePlaceholder":315},[187,14146,14147],{"class":189,"line":2642},[187,14148,14149],{"class":295},"// ad hoc app env\n",[187,14151,14152,14154,14157,14159,14161,14164,14166,14169],{"class":189,"line":2653},[187,14153,574],{"class":573},[187,14155,14156],{"class":588}," adHocAppComponent",[187,14158,6771],{"class":573},[187,14160,12437],{"class":573},[187,14162,14163],{"class":193}," AdHocAppComponent",[187,14165,615],{"class":577},[187,14167,14168],{"class":196},"\"AdHocAppComponent\"",[187,14170,12449],{"class":577},[187,14172,14173],{"class":189,"line":2665},[187,14174,14175],{"class":577},"  vpcId,\n",[187,14177,14178],{"class":189,"line":2674},[187,14179,14180],{"class":577},"  assetsBucketName,\n",[187,14182,14183],{"class":189,"line":2684},[187,14184,14185],{"class":577},"  privateSubnets,\n",[187,14187,14188],{"class":189,"line":2694},[187,14189,14190],{"class":577},"  appSgId,\n",[187,14192,14193],{"class":189,"line":2706},[187,14194,14195],{"class":577},"  albSgId,\n",[187,14197,14198],{"class":189,"line":2715},[187,14199,14200],{"class":577},"  listenerArn,\n",[187,14202,14203],{"class":189,"line":2725},[187,14204,14205],{"class":577},"  albDnsName,\n",[187,14207,14208],{"class":189,"line":2735},[187,14209,14210],{"class":577},"  serviceDiscoveryNamespaceId,\n",[187,14212,14213],{"class":189,"line":2743},[187,14214,14215],{"class":577},"  rdsAddress,\n",[187,14217,14218],{"class":189,"line":2754},[187,14219,14220],{"class":577},"  domainName,\n",[187,14222,14223],{"class":189,"line":2762},[187,14224,14225],{"class":577},"  baseStackName\n",[187,14227,14228],{"class":189,"line":2770},[187,14229,7722],{"class":577},[168,14231,14233],{"id":14232},"cli-scaffolding","CLI scaffolding",[11,14235,14236],{},"CDK and Pulumi have some good options for how to scaffold a project.",[916,14238,14239,14253,14259,14267],{},[919,14240,14241,14242,14245,14246,14249,14250,14252],{},"Pulumi has ",[33,14243,14244],{},"pulumi new aws-typescript"," among lots of other options (run ",[33,14247,14248],{},"pulumi new -l"," to see over 200 project types). I used this to create the library itself, the examples and the pulumi projects that I use in ",[33,14251,12114],{}," that consume the library.",[919,14254,14255,14256,14258],{},"CDK has ",[33,14257,11696],{}," CLI commands which can help set up either library code or project code",[919,14260,14261,14262,1172,14264,14266],{},"The major benefits of these tools is setting up ",[33,14263,11704],{},[33,14265,8085],{}," correctly",[919,14268,14269],{},"Terraform is so simple that it doesn't really need tooling for scaffolding",[168,14271,14273],{"id":14272},"best-practices","Best practices",[11,14275,14276,14277,14279,14280,14285],{},"For ",[33,14278,11607],{},", I tried to follow the recommendations from ",[15,14281,14284],{"href":14282,"rel":14283},"https://www.terraform-best-practices.com/",[19],"terraform-best-practices.com"," which helped me a lot with things like consistent naming patterns and directory structures. For example:",[916,14287,14288],{},[919,14289,14290,14291,14294],{},"use the name ",[33,14292,14293],{},"this"," for resources in a module where that resource is the only resource of its type",[11,14296,14297],{},"CDK and Pulumi lend themselves to more nesting and abstractions because they can be written in more familiar programming languages with better abstractions, functions, loops, classes, etc., so there are some differences in directory structure of my libraries when comparing Terraform to both CDK and Pulumi.",[11,14299,14300,14301,637,14303,637,14305,637,14308,637,14311,14314,14315,1172,14318,14321],{},"For Pulumi and CDK, I mostly tried to follow along with recommendations from their documentation and example projects. While working with Pulumi I struggled a bit with the concepts of ",[33,14302,12360],{},[33,14304,12764],{},[33,14306,14307],{},"pulumi.interpolate",[33,14309,14310],{},"apply()",[33,14312,14313],{},"all()"," and the differences between ",[33,14316,14317],{},"getX",[33,14319,14320],{},"getXOutput",". There is a little bit of a learning curve here, but the documentation and examples go a long way in showing how to do things the right way.",[168,14323,14325],{"id":14324},"environment-configuration","Environment configuration",[11,14327,14328],{},"Environment configuration allows for either a base or app stack to be configured with non-default values. For example:",[916,14330,14331,14334],{},[919,14332,14333],{},"you may decide to start a new base environment but you want to provision a powerful database instance class and size. You would change this using environment configuration",[919,14335,14336],{},"You might want to create an ad hoc app environment but you need it to include some special environment variables, you could set these in environment config.",[11,14338,14339],{},"In the examples above, our IaC can optionally take environment configuration values that overwrite default values, or extend default values.",[916,14341,14342,14353,14360],{},[919,14343,14344,14345,784,14348,343],{},"Pulumi defines environment-specific config in files called ",[33,14346,14347],{},"Pulumi.{env}.yaml",[15,14349,14352],{"href":14350,"rel":14351},"https://www.pulumi.com/docs/intro/concepts/config/",[19],"Pulumi article on configuration",[919,14354,14355,14356,14359],{},"Terraform uses ",[33,14357,14358],{},"{env}.tfvars"," for this type of configuration",[919,14361,14362,14363,14366],{},"CDK has several options for this type of configuration (",[33,14364,14365],{},"cdk.context.json",", extending stack props, etc.)",[11,14368,14369,14370,14372,14373,14376],{},"For CDK I have been using ",[33,14371,13562],{}," and the ",[33,14374,14375],{},"tryGetContext"," method:",[11,14378,14379,14381],{},[33,14380,13562],{}," needs to be set on the node before any child nodes are added:",[26,14383,14385],{"className":12421,"code":14384,"language":12423,"meta":35,"style":35},"const baseStack = new Stack(app, 'ExampleAdHocBaseStack', { env, stackName: adHocBaseEnvName });\nbaseStack.node.setContext('config', adHocBaseEnvConfig);\n\nconst appStack = new Stack(app, 'ExampleAdHocAppStack', { env, stackName: adHocAppEnvName });\nappStack.node.setContext('config', adHocAppEnvConfig);\n",[33,14386,14387,14405,14417,14421,14439],{"__ignoreMap":35},[187,14388,14389,14391,14393,14395,14397,14399,14401,14403],{"class":189,"line":190},[187,14390,574],{"class":573},[187,14392,13538],{"class":588},[187,14394,6771],{"class":573},[187,14396,12437],{"class":573},[187,14398,13545],{"class":193},[187,14400,13548],{"class":577},[187,14402,13551],{"class":196},[187,14404,13554],{"class":577},[187,14406,14407,14409,14411,14413,14415],{"class":189,"line":249},[187,14408,13559],{"class":577},[187,14410,13562],{"class":193},[187,14412,615],{"class":577},[187,14414,13567],{"class":196},[187,14416,13570],{"class":577},[187,14418,14419],{"class":189,"line":312},[187,14420,316],{"emptyLinePlaceholder":315},[187,14422,14423,14425,14427,14429,14431,14433,14435,14437],{"class":189,"line":319},[187,14424,574],{"class":573},[187,14426,13581],{"class":588},[187,14428,6771],{"class":573},[187,14430,12437],{"class":573},[187,14432,13545],{"class":193},[187,14434,13548],{"class":577},[187,14436,13592],{"class":196},[187,14438,13595],{"class":577},[187,14440,14441,14443,14445,14447,14449],{"class":189,"line":325},[187,14442,13600],{"class":577},[187,14444,13562],{"class":193},[187,14446,615],{"class":577},[187,14448,13567],{"class":196},[187,14450,13609],{"class":577},[11,14452,14453],{},"And the config objects are read from JSON files like this:",[26,14455,14457],{"className":12421,"code":14456,"language":12423,"meta":35,"style":35},"var adHocBaseEnvConfig = JSON.parse(fs.readFileSync(`src/examples/ad-hoc/base/config/${adHocBaseEnvName}.json`, 'utf8'));\nvar adHocAppEnvConfig = JSON.parse(fs.readFileSync(`src/examples/ad-hoc/app/config/${adHocAppEnvName}.json`, 'utf8'));\n",[33,14458,14459,14502],{"__ignoreMap":35},[187,14460,14461,14464,14467,14469,14472,14474,14477,14480,14483,14485,14488,14491,14494,14496,14499],{"class":189,"line":190},[187,14462,14463],{"class":573},"var",[187,14465,14466],{"class":577}," adHocBaseEnvConfig ",[187,14468,595],{"class":573},[187,14470,14471],{"class":588}," JSON",[187,14473,752],{"class":577},[187,14475,14476],{"class":193},"parse",[187,14478,14479],{"class":577},"(fs.",[187,14481,14482],{"class":193},"readFileSync",[187,14484,615],{"class":577},[187,14486,14487],{"class":196},"`src/examples/ad-hoc/base/config/${",[187,14489,14490],{"class":577},"adHocBaseEnvName",[187,14492,14493],{"class":196},"}.json`",[187,14495,637],{"class":577},[187,14497,14498],{"class":196},"'utf8'",[187,14500,14501],{"class":577},"));\n",[187,14503,14504,14506,14509,14511,14513,14515,14517,14519,14521,14523,14526,14529,14531,14533,14535],{"class":189,"line":249},[187,14505,14463],{"class":573},[187,14507,14508],{"class":577}," adHocAppEnvConfig ",[187,14510,595],{"class":573},[187,14512,14471],{"class":588},[187,14514,752],{"class":577},[187,14516,14476],{"class":193},[187,14518,14479],{"class":577},[187,14520,14482],{"class":193},[187,14522,615],{"class":577},[187,14524,14525],{"class":196},"`src/examples/ad-hoc/app/config/${",[187,14527,14528],{"class":577},"adHocAppEnvName",[187,14530,14493],{"class":196},[187,14532,637],{"class":577},[187,14534,14498],{"class":196},[187,14536,14501],{"class":577},[11,14538,14539],{},"The context can be used in constructs like this:",[26,14541,14543],{"className":12421,"code":14542,"language":12423,"meta":35,"style":35},"    const extraEnvVars = this.node.tryGetContext('config').extraEnvVars;\n",[33,14544,14545],{"__ignoreMap":35},[187,14546,14547,14549,14552,14554,14557,14560,14562,14564,14566],{"class":189,"line":190},[187,14548,6766],{"class":573},[187,14550,14551],{"class":588}," extraEnvVars",[187,14553,6771],{"class":573},[187,14555,14556],{"class":588}," this",[187,14558,14559],{"class":577},".node.",[187,14561,14375],{"class":193},[187,14563,615],{"class":577},[187,14565,13567],{"class":196},[187,14567,14568],{"class":577},").extraEnvVars;\n",[11,14570,14571],{},"Pulumi has similar functions for getting context values, here's an example of how I get extra environment variables for app environments using Pulumi's config:",[26,14573,14575],{"className":12421,"code":14574,"language":12423,"meta":35,"style":35},"    interface EnvVar {\n      name: string;\n      value: string;\n    }\n\n    let config = new pulumi.Config();\n    let extraEnvVars = config.getObject\u003CEnvVar[]>(\"extraEnvVars\");\n",[33,14576,14577,14587,14599,14610,14614,14618,14637],{"__ignoreMap":35},[187,14578,14579,14582,14585],{"class":189,"line":190},[187,14580,14581],{"class":573},"    interface",[187,14583,14584],{"class":193}," EnvVar",[187,14586,6739],{"class":577},[187,14588,14589,14592,14594,14597],{"class":189,"line":249},[187,14590,14591],{"class":581},"      name",[187,14593,358],{"class":573},[187,14595,14596],{"class":588}," string",[187,14598,6961],{"class":577},[187,14600,14601,14604,14606,14608],{"class":189,"line":312},[187,14602,14603],{"class":581},"      value",[187,14605,358],{"class":573},[187,14607,14596],{"class":588},[187,14609,6961],{"class":577},[187,14611,14612],{"class":189,"line":319},[187,14613,9799],{"class":577},[187,14615,14616],{"class":189,"line":325},[187,14617,316],{"emptyLinePlaceholder":315},[187,14619,14620,14623,14626,14628,14630,14632,14635],{"class":189,"line":686},[187,14621,14622],{"class":573},"    let",[187,14624,14625],{"class":577}," config ",[187,14627,595],{"class":573},[187,14629,12437],{"class":573},[187,14631,13145],{"class":577},[187,14633,14634],{"class":193},"Config",[187,14636,6902],{"class":577},[187,14638,14639,14641,14644,14646,14649,14652,14654,14657,14660,14663],{"class":189,"line":697},[187,14640,14622],{"class":573},[187,14642,14643],{"class":577}," extraEnvVars ",[187,14645,595],{"class":573},[187,14647,14648],{"class":577}," config.",[187,14650,14651],{"class":193},"getObject",[187,14653,6724],{"class":577},[187,14655,14656],{"class":193},"EnvVar",[187,14658,14659],{"class":577},"[]>(",[187,14661,14662],{"class":196},"\"extraEnvVars\"",[187,14664,7004],{"class":577},[11,14666,14667,14668,14671,14672,14675],{},"In my ",[33,14669,14670],{},"Pulumi.alpha.yaml"," file I have the ",[33,14673,14674],{},"extraEnvVars"," set like this:",[26,14677,14679],{"className":2507,"code":14678,"language":2509,"meta":35,"style":35},"config:\n  aws:region: us-east-1\n  extraEnvVars:\n    - name: FOO\n      value: BAR\n    - name: BIZ\n      value: BUZ\n",[33,14680,14681,14688,14698,14705,14717,14726,14737],{"__ignoreMap":35},[187,14682,14683,14686],{"class":189,"line":190},[187,14684,14685],{"class":2516},"config",[187,14687,2520],{"class":577},[187,14689,14690,14693,14695],{"class":189,"line":249},[187,14691,14692],{"class":2516},"  aws:region",[187,14694,585],{"class":577},[187,14696,14697],{"class":196},"us-east-1\n",[187,14699,14700,14703],{"class":189,"line":312},[187,14701,14702],{"class":2516},"  extraEnvVars",[187,14704,2520],{"class":577},[187,14706,14707,14710,14712,14714],{"class":189,"line":319},[187,14708,14709],{"class":577},"    - ",[187,14711,7284],{"class":2516},[187,14713,585],{"class":577},[187,14715,14716],{"class":196},"FOO\n",[187,14718,14719,14721,14723],{"class":189,"line":325},[187,14720,14603],{"class":2516},[187,14722,585],{"class":577},[187,14724,14725],{"class":196},"BAR\n",[187,14727,14728,14730,14732,14734],{"class":189,"line":686},[187,14729,14709],{"class":577},[187,14731,7284],{"class":2516},[187,14733,585],{"class":577},[187,14735,14736],{"class":196},"BIZ\n",[187,14738,14739,14741,14743],{"class":189,"line":697},[187,14740,14603],{"class":2516},[187,14742,585],{"class":577},[187,14744,14745],{"class":196},"BUZ\n",[11,14747,14748],{},"I haven't done too much with configuration, but it seems like the right place to build out all of the dials and switches for optional settings in stack resources that you want people to be able to change in their ad hoc environments, or that you want to set per \"production\" environment (QA, stage, prod, etc.)",[168,14750,14752],{"id":14751},"local-development","Local development",[11,14754,14755,14756,14758],{},"Using the Makefile targets in each library repo, my process for developing ",[33,14757,11481],{},"s involves making code changes followed by Makefile targets that preview/plan/diff against my AWS account, then running deploy/apply/up and waiting for things to finish deploying. Once I can validate that things are looking correct in my account, I run the destroy command and make sure that all of the resources are removed successfully. RDS instances can take up to 10 minutes to create, which means that the base stack takes some time to test. The app environment is able to be spun up quickly, but it can sometimes get stuck and take some time to delete services.",[11,14760,14761],{},"Here are some sample times for deploying ad hoc stacks with CDK.",[26,14763,14766],{"className":14764,"code":14765,"language":31},[29],"# CDK ad hoc base deployment time\n\n ✅  ExampleAdHocBaseStack (dev)\n\n✨  Deployment time: 629.64s\n\n# CDK ad hoc app deployment time\n\n ✅  ExampleAdHocAppStack (alpha)\n\n✨  Deployment time: 126.62s\n",[33,14767,14765],{"__ignoreMap":35},[11,14769,14770,14771,14773],{},"Here is an example of what the ",[33,14772,11543],{}," commands shows for the ad-hoc base stack:",[26,14775,14778],{"className":14776,"code":14777,"language":31},[29],"# Pulumi preview\n~/git/github/pulumi-aws-django$ pulumi -C examples/ad-hoc/base --stack dev preview\nPreviewing update (dev)\n\nView Live: https://app.pulumi.com/briancaffey/ad-hoc-base/dev/previews/718625b2-48f5-4ef4-8ed4-9b2694fda64a\n\n     Type                                                    Name                        Plan\n +   pulumi:pulumi:Stack                                     ad-hoc-base-dev             create\n +   └─ pulumi-contrib:components:AdHocBaseEnv               myAdHocEnv                  create\n +      ├─ pulumi-contrib:components:AlbResources            AlbResources                create\n +      │  ├─ aws:alb:TargetGroup                            DefaultTg                   create\n +      │  ├─ aws:alb:LoadBalancer                           LoadBalancer                create\n +      │  ├─ aws:alb:Listener                               HttpListener                create\n +      │  └─ aws:alb:Listener                               HttpsListener               create\n +      ├─ pulumi-contrib:components:BastionHostResources    BastionHostResources        create\n +      │  ├─ aws:iam:Role                                   BastionHostRole             create\n +      │  ├─ aws:iam:RolePolicy                             BastionHostPolicy           create\n +      │  ├─ aws:iam:InstanceProfile                        BastionHostInstanceProfile  create\n +      │  └─ aws:ec2:Instance                               BastionHostInstance         create\n +      ├─ pulumi-contrib:components:RdsResources            RdsResources                create\n +      │  ├─ aws:rds:SubnetGroup                            DbSubnetGroup               create\n +      │  ├─ aws:ec2:SecurityGroup                          RdsSecurityGroup            create\n +      │  └─ aws:rds:Instance                               DbInstance                  create\n +      ├─ pulumi-contrib:components:SecurityGroupResources  SecurityGroupResources      create\n +      │  ├─ aws:ec2:SecurityGroup                          AlbSecurityGroup            create\n +      │  └─ aws:ec2:SecurityGroup                          AppSecurityGroup            create\n +      ├─ aws:s3:Bucket                                     assetsBucket                create\n +      ├─ awsx:ec2:Vpc                                      dev                         create\n +      │  └─ aws:ec2:Vpc                                    dev                         create\n +      │     ├─ aws:ec2:InternetGateway                     dev                         create\n +      │     ├─ aws:ec2:Subnet                              dev-private-1               create\n +      │     │  └─ aws:ec2:RouteTable                       dev-private-1               create\n +      │     │     ├─ aws:ec2:RouteTableAssociation         dev-private-1               create\n +      │     │     └─ aws:ec2:Route                         dev-private-1               create\n +      │     ├─ aws:ec2:Subnet                              dev-private-2               create\n +      │     │  └─ aws:ec2:RouteTable                       dev-private-2               create\n +      │     │     ├─ aws:ec2:RouteTableAssociation         dev-private-2               create\n +      │     │     └─ aws:ec2:Route                         dev-private-2               create\n +      │     ├─ aws:ec2:Subnet                              dev-public-1                create\n +      │     │  ├─ aws:ec2:RouteTable                       dev-public-1                create\n +      │     │  │  ├─ aws:ec2:RouteTableAssociation         dev-public-1                create\n +      │     │  │  └─ aws:ec2:Route                         dev-public-1                create\n +      │     │  ├─ aws:ec2:Eip                              dev-1                       create\n +      │     │  └─ aws:ec2:NatGateway                       dev-1                       create\n +      │     └─ aws:ec2:Subnet                              dev-public-2                create\n +      │        ├─ aws:ec2:RouteTable                       dev-public-2                create\n +      │        │  ├─ aws:ec2:RouteTableAssociation         dev-public-2                create\n +      │        │  └─ aws:ec2:Route                         dev-public-2                create\n +      │        ├─ aws:ec2:Eip                              dev-2                       create\n +      │        └─ aws:ec2:NatGateway                       dev-2                       create\n +      └─ aws:servicediscovery:PrivateDnsNamespace          PrivateDnsNamespace         create\n\n\nOutputs:\n    albDnsName                 : output\u003Cstring>\n    albSgId                    : output\u003Cstring>\n    appSgId                    : output\u003Cstring>\n    assetsBucketName           : output\u003Cstring>\n    baseStackName              : \"dev\"\n    bastionHostInstanceId      : output\u003Cstring>\n    domainName                 : \"example.com\"\n    listenerArn                : output\u003Cstring>\n    privateSubnetIds           : output\u003Cstring>\n    rdsAddress                 : output\u003Cstring>\n    serviceDiscoveryNamespaceId: output\u003Cstring>\n    vpcId                      : output\u003Cstring>\n\nResources:\n    + 44 to create\n",[33,14779,14777],{"__ignoreMap":35},[168,14781,14783],{"id":14782},"running-infrastructure-pipelines-in-github-actions","Running infrastructure pipelines in GitHub Actions",[11,14785,14786],{},[4339,14787,14788],{},"I don't currently have GitHub Actions working for all tools in all environments, this part is still a WIP but is working at a basic level. Another item for the backlog!",[11,14790,12674,14791,14794,14795,14797,14798,14801],{},[33,14792,14793],{},".github/workflows"," directory of the ",[33,14796,12114],{}," repo, I will have the following ",[33,14799,14800],{},"2 * 2 * 2 * 3 = 24"," pipelines for running infrastructure as code pipelines:",[26,14803,14805],{"className":181,"code":14804,"language":183,"meta":35,"style":35},"{ad_hoc,prod}_{base,app}_{create_update,destroy}_{cdk,terraform,pulumi}.yml\n",[33,14806,14807],{"__ignoreMap":35},[187,14808,14809],{"class":189,"line":190},[187,14810,14804],{"class":577},[916,14812,14813,14816,14819],{},[919,14814,14815],{},"For CDK I'm using CDK CLI commands",[919,14817,14818],{},"For Terraform I'm also using terraform CLI commands",[919,14820,14821],{},"For Pulumi I'm using the official Pulumi GitHub Action",[11,14823,14241,14824,14829],{},[15,14825,14828],{"href":14826,"rel":14827},"https://www.pulumi.com/docs/guides/continuous-delivery/github-actions/",[19],"a great article"," about how to use their official GitHub Action. This action calls the Pulumi CLI under the hood with all of the correct flags.",[11,14831,14832],{},"The general pattern that all of these pipelines use is:",[916,14834,14835,14838,14841],{},[919,14836,14837],{},"Do a synth/plan/preview, and upload the synth/plan/preview file to an artifact",[919,14839,14840],{},"Pause and wait on manual review of the planned changes",[919,14842,14843],{},"download the artifact and run deploy/apply/up against it, or optionally cancel the operation if the changes you see in the GitHub Actions pipeline logs are not what you expected.",[11,14845,14846],{},"I do this by having two jobs in each GitHub Action: one for synth/plan/preview and one for deploy/apply/up.",[11,14848,14849,14850,14852],{},"The job for deploy/apply/up includes an ",[33,14851,13749],{}," that is configured in GitHub to be a protected environment that requires approvals. Even if you are the only approver (which I am on this project), it is the easiest and safest way preview infrastructure changes before they happen. If you see something in the plan and it isn't what you wanted to change, you cancel the job.",[168,14854,14856],{"id":14855},"application-deployments","Application deployments",[916,14858,14859,14862,14868],{},[919,14860,14861],{},"There are two GitHub Actions pipelines for deploying the frontend and the backend. Both of these pipelines run bash scripts that call AWS CLI commands to perform rolling updates on all of the services used in the application (frontend, API, workers, scheduler)",[919,14863,14864,14865,14867],{},"The backend deployment script runs database migrations, the ",[33,14866,13071],{}," command and any other commands needed to run before the rolling update starts (clearing the cache, loading fixtures, etc.)",[919,14869,14870],{},"What is important to note here is that application deployments are not dependent on the IaC tool we use. Since we are tagging things consistently across CDK, Terraform and Pulumi, we can look up resources by tag rather than getting \"outputs\" of the app stacks.",[168,14872,14874],{"id":14873},"interacting-with-aws-via-iac","Interacting with AWS via IaC",[916,14876,14877,14886,14901],{},[919,14878,14879,14880,14885],{},"CDK interacts directly with CloudFormation (and custom resources which allow for running arbitrary SDK calls and lambda functions) and provides ",[15,14881,14884],{"href":14882,"rel":14883},"https://docs.aws.amazon.com/cdk/v2/guide/constructs.html",[19],"L1, L2 and L3 constructs"," which offer different levels of abstraction over CloudFormation.",[919,14887,14888,14889,14372,14894,752],{},"Terraform has the ",[15,14890,14893],{"href":14891,"rel":14892},"https://registry.terraform.io/providers/hashicorp/aws/latest/docs",[19],"AWS Provider",[15,14895,14898],{"href":14896,"rel":14897},"https://registry.terraform.io/namespaces/terraform-aws-modules",[19],[33,14899,14900],{},"terraform-aws-modules",[919,14902,14903,14904,13780,14906,1172,14909,784,14912,13780,14917,1172,14920,12272,14923,752],{},"Pulumi has AWS Classic (",[33,14905,11503],{},[338,14907,14908],{},"provider",[33,14910,14911],{},"AWSx",[15,14913,14916],{"href":14914,"rel":14915},"https://www.pulumi.com/registry/packages/awsx/",[19],"Crosswalk for Pulumi",[338,14918,14919],{},"library",[33,14921,14922],{},"aws_native",[338,14924,14908],{},[107,14926,14927],{},[11,14928,14929,14931],{},[33,14930,14922],{}," \"manages and provisions resources using the AWS Cloud Control API, which typically supports new AWS features on the day of launch.\"",[11,14933,14934,14936],{},[33,14935,14922],{}," looks like a really interesting option, but it is currently in public preview so I have not decided to use it. I am using the AWSx library only for my VPC and associated resources, everything else uses the AWS Classic provider.",[11,14938,14939],{},"For CDK I use mostly L2 constructs and some L1 constructs.",[11,14941,14942,14943,14945],{},"Fot Terraform I use the VPC from the ",[33,14944,14900],{},", and everything else uses the AWS Terraform Provider.",[168,14947,14949],{"id":14948},"what-i-did-not-put-in-iac","What I did not put in IaC",[916,14951,14952,14955,14958],{},[919,14953,14954],{},"ECR (Elastic Container Registry)",[919,14956,14957],{},"ACM (Amazon Certificate Manager)",[919,14959,14960],{},"(Roles used for deployments)",[11,14962,14963,14964,1172,14966,14968,14969,14972],{},"I created the Elastic Container Registry ",[33,14965,12291],{},[33,14967,12294],{}," repos manually in the AWS Console. I also manually requested an ACM certificate for ",[33,14970,14971],{},"*.mydomain.com"," for the domain that I use for testing that I purchased through Route53 domains.",[11,14974,14975],{},"I currently am using another less-than best practice of using Administrative Credentials stored in GitHub secrets. The better approach here is to make roles for different pipelines and use OIDC to authenticate instead of storing credentials. This is another good item for the backlog.",[168,14977,14979],{"id":14978},"tagging","Tagging",[916,14981,14982,14985,14988,14994,15000,15003],{},[919,14983,14984],{},"Terraform and CDK both make it easy to automatically tag all resources in a stack",[919,14986,14987],{},"It is possible to do this in Pulumi, but you need to write a little bit of code.",[919,14989,14990],{},[15,14991,14992],{"href":14992,"rel":14993},"https://www.pulumi.com/blog/automatically-enforcing-aws-resource-tagging-policies/",[19],[919,14995,14996],{},[15,14997,14998],{"href":14998,"rel":14999},"https://github.com/joeduffy/aws-tags-example/tree/master/autotag-ts",[19],[919,15001,15002],{},"Tagging is important since I look up resources by tag in GitHub Actions pipelines (for example, the Bastion Host is looked up by tag)",[919,15004,15005,15006,15011],{},"Automatically tagging resources works through ",[15,15007,15010],{"href":15008,"rel":15009},"https://www.pulumi.com/docs/intro/vs/terraform/",[19],"stack transformations"," are unique to Pulumi",[168,15013,15015],{"id":15014},"smoke-checking-application-environments","Smoke checking application environments",[11,15017,15018],{},"Here's the list of things I check when standing up an application environment:",[916,15020,15023,15032,15041,15050,15059,15068,15077,15086,15092,15098,15104,15110,15116,15122],{"className":15021},[15022],"contains-task-list",[919,15024,15027,15031],{"className":15025},[15026],"task-list-item",[15028,15029],"input",{"checked":315,"disabled":315,"type":15030},"checkbox"," Run the init/tsc, synth/plan/preview and deploy/apply/up commands successfully",[919,15033,15035,15037,15038,343],{"className":15034},[15026],[15028,15036],{"checked":315,"disabled":315,"type":15030}," Access the bastion host (",[33,15039,15040],{},"make aws-ssm-start-session",[919,15042,15044,15046,15047,343],{"className":15043},[15026],[15028,15045],{"checked":315,"disabled":315,"type":15030}," Run ECSExec to access a shell in a backend container (",[33,15048,15049],{},"make aws-ecs-exec",[919,15051,15053,15055,15056,343],{"className":15052},[15026],[15028,15054],{"checked":315,"disabled":315,"type":15030}," Test database connectivity (",[33,15057,15058],{},"python manage.py showmigrations",[919,15060,15062,15064,15065,343],{"className":15061},[15026],[15028,15063],{"checked":315,"disabled":315,"type":15030}," Run the migrations (",[33,15066,15067],{},"python manage.py migrate",[919,15069,15071,15073,15074,343],{"className":15070},[15026],[15028,15072],{"checked":315,"disabled":315,"type":15030}," Run collectstatic (",[33,15075,15076],{},"python manage.py collectstatic",[919,15078,15080,15082,15083,343],{"className":15079},[15026],[15028,15081],{"checked":315,"disabled":315,"type":15030}," Visit the site (",[33,15084,15085],{},"alpha.example.com",[919,15087,15089,15091],{"className":15088},[15026],[15028,15090],{"checked":315,"disabled":315,"type":15030}," Publish a blog post",[919,15093,15095,15097],{"className":15094},[15026],[15028,15096],{"checked":315,"disabled":315,"type":15030}," Publish a blog post with an image",[919,15099,15101,15103],{"className":15100},[15026],[15028,15102],{"checked":315,"disabled":315,"type":15030}," Check celery worker logs for successfully complete scheduled tasks",[919,15105,15107,15109],{"className":15106},[15026],[15028,15108],{"checked":315,"disabled":315,"type":15030}," Trigger an autoscaling event by running k6 load tests against an environment",[919,15111,15113,15115],{"className":15112},[15026],[15028,15114],{"checked":315,"disabled":315,"type":15030}," Optionally deploy another backend or frontend image tag using the GitHub Actions pipelines for backend and frontend updates",[919,15117,15119,15121],{"className":15118},[15026],[15028,15120],{"checked":315,"disabled":315,"type":15030}," Destroy the app stack",[919,15123,15125,15127],{"className":15124},[15026],[15028,15126],{"checked":315,"disabled":315,"type":15030}," Destroy the base stack",[168,15129,15131],{"id":15130},"backlog-and-next-steps","Backlog and next steps",[11,15133,15134],{},"Here are some of the next things I'll be working on in these project, roughly in order of importance:",[916,15136,15138,15144,15147,15150,15156,15159,15166,15169,15172,15175,15178,15181,15184,15187,15190],{"className":15137},[15022],[919,15139,15141,15143],{"className":15140},[15026],[15028,15142],{"checked":315,"disabled":315,"type":15030}," Introduce manual approvals in GitHub Actions for all deployments and allow for the previewing or \"planning\" before proceeding with an live operations in infrastructure pipelines",[919,15145,15146],{},"Switch to using OIDC for AWS authentication from GitHub Actions and remove AWS secrets from GitHub",[919,15148,15149],{},"Show how to do account isolation (different accounts for prod vs pre-prod environments)",[919,15151,15152,15153,15155],{},"GitHub Actions deployment pipeline for publishing ",[33,15154,11616],{}," package",[919,15157,15158],{},"Complete all GitHub Action deployment pipelines for base and app stacks (both ad hoc and prod)",[919,15160,15161,15162,15165],{},"For Pulumi and Terraform, use a Secrets Manager secret for the database instead of hardcoding it. Use the ",[33,15163,15164],{},"random"," functions to do this",[919,15167,15168],{},"Refactor GitHub Actions and make them reusable across different projects",[919,15170,15171],{},"Writing tests for Pulumi and CDK. Figure out how to write tests for Terraform modules",[919,15173,15174],{},"Use graviton instances and have the option to select between different architectures",[919,15176,15177],{},"Standardize all resources names across CDK, Terraform and Pulumi",[919,15179,15180],{},"The Pulumi components that define the resources associated with each ECS service are not very dry",[919,15182,15183],{},"Interfaces could be constructed with inheritance (base set of properties that is extended for different types of services)",[919,15185,15186],{},"Fix the CDK issue with priority rule on ALB listeners. I need to used a custom resource for this which is currently a WIP. Terraform and Pulumi look up the next highest listener rule priority under the hood, so you are not required to provide it, but CDK requires it, which means that you can't do ad hoc environments in CDK without a custom resource that looks up what the next available priority number is.",[919,15188,15189],{},"Make all three of the libraries less opinionated. For example, the celery worker and scheduler should be optional and the frontend component should also be optional",[919,15191,15192],{},"experiment with using a frontend with SSR. This is supported by Quasar, the framework I'm currently using to build my frontend SPA site",[11,15194,15195],{},"If you want to get involved or help with any of the above, please let me know!",[168,15197,2413],{"id":15198},"conclusion",[11,15200,15201,15202,15207],{},"I first started out with IaC following this project ",[15,15203,15206],{"href":15204,"rel":15205},"https://github.com/aws-samples/ecs-refarch-cloudformation",[19],"aws-samples/ecs-refarch-cloudformation"," (which is pretty old at this point) and wrote a lot of CloudFormation by hand. The pain of doing that lead me to explore the CDK with Python. I learned TypeScript by rewriting the Python CDK code I wrote in TypeScript. I later worked with a team that was more experienced in Terraform and learned how to use that. I feel like Pulumi takes the best of the two tools and has a really great developer experience. There is a little bit of a learning curve with Pulumi, and you give up some of the simplicity of Terraform.",[855,15209,15210],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .s4XuR, html code.shiki .s4XuR{--shiki-default:#E36209;--shiki-dark:#FFAB70}",{"title":35,"searchDepth":249,"depth":249,"links":15212},[15213,15214,15215,15216,15221,15229,15230,15233,15234,15244,15254,15255,15256,15257,15258,15259,15260,15261,15262,15263,15264,15265],{"id":8183,"depth":249,"text":8184},{"id":11297,"depth":249,"text":11297},{"id":11317,"depth":249,"text":11318},{"id":11463,"depth":249,"text":11464,"children":15217},[15218,15219,15220],{"id":11467,"depth":312,"text":11468},{"id":11493,"depth":312,"text":11494},{"id":11507,"depth":312,"text":11507},{"id":11583,"depth":249,"text":11584,"children":15222},[15223,15224,15225,15226,15227,15228],{"id":11619,"depth":312,"text":11620},{"id":11659,"depth":312,"text":11660},{"id":11725,"depth":312,"text":11726},{"id":11907,"depth":312,"text":11908},{"id":11936,"depth":312,"text":11937},{"id":12016,"depth":312,"text":12017},{"id":12062,"depth":249,"text":12063},{"id":12092,"depth":249,"text":12092,"children":15231},[15232],{"id":12107,"depth":312,"text":12108},{"id":12220,"depth":249,"text":12221},{"id":12309,"depth":249,"text":12310,"children":15235},[15236,15237,15238,15239,15240,15241,15242,15243],{"id":12342,"depth":312,"text":12343},{"id":12359,"depth":312,"text":12360},{"id":12377,"depth":312,"text":12378},{"id":12415,"depth":312,"text":12330},{"id":12525,"depth":312,"text":12526},{"id":12552,"depth":312,"text":12553},{"id":12716,"depth":312,"text":12339},{"id":12763,"depth":312,"text":12764},{"id":12815,"depth":249,"text":12816,"children":15245},[15246,15247,15248,15249,15250,15251,15252,15253],{"id":12831,"depth":312,"text":12832},{"id":12859,"depth":312,"text":12860},{"id":12967,"depth":312,"text":12968},{"id":12985,"depth":312,"text":12986},{"id":12995,"depth":312,"text":12996},{"id":13041,"depth":312,"text":13042},{"id":13062,"depth":312,"text":13063},{"id":13396,"depth":312,"text":12812},{"id":14232,"depth":249,"text":14233},{"id":14272,"depth":249,"text":14273},{"id":14324,"depth":249,"text":14325},{"id":14751,"depth":249,"text":14752},{"id":14782,"depth":249,"text":14783},{"id":14855,"depth":249,"text":14856},{"id":14873,"depth":249,"text":14874},{"id":14948,"depth":249,"text":14949},{"id":14978,"depth":249,"text":14979},{"id":15014,"depth":249,"text":15015},{"id":15130,"depth":249,"text":15131},{"id":15198,"depth":249,"text":2413},"2023-01-07","Three reusable infrastructure as code libraries for abstracting containerized web app architecture on AWS ECS",[15269,15271,15273,15275,15278,15281],{"link":15270,"site":11220},"https://news.ycombinator.com/item?id=34291336",{"link":15272,"site":10051},"https://www.reddit.com/r/aws/comments/105vo53/my_infrastructure_as_code_rosetta_stone_deploying/",{"link":15274,"site":6303},"https://dev.to/briancaffey/my-infrastructure-as-code-rosetta-stone-deploying-the-same-web-application-on-aws-ecs-fargate-with-cdk-terraform-and-pulumi-oe4",{"link":15276,"site":15277},"https://medium.com/@briancaffey/my-infrastructure-as-code-rosetta-stone-with-cdk-terraform-and-pulumi-44fcb8233e6a","medium",{"link":15279,"site":15280},"https://briancaffey.hashnode.dev/setting-up-ad-hoc-development-environments-for-django-applications-with-aws-ecs-terraform-and-github-actions","hashnode",{"link":15282,"site":15283},"https://briancaffey.substack.com/p/my-infrastructure-as-code-rosetta","substack","/static/iac_rosetta_stone_og_image.png",{},"/2023/01/07/i-deployed-the-same-containerized-serverless-django-app-with-aws-cdk-terraform-and-pulumi",{"title":11237,"description":15267},"2023/01/07/i-deployed-the-same-containerized-serverless-django-app-with-aws-cdk-terraform-and-pulumi",[15290,15291,13407,15292,15293,15294,12748,15295,15296,15297,15298],"django","cdk","pulumi","cloudformation","github-actions","ecs","fargate","containers","docker","IXkPFsDU8Rmv33le_b89SOtb0DA2bTjvS0Y0CsvdAFQ",{"id":15301,"title":15302,"body":15303,"comments":315,"date":18757,"description":18758,"draft":872,"extension":873,"external":18759,"image":15911,"meta":18777,"navigation":315,"path":18778,"seo":18779,"stem":18780,"tags":18781,"__hash__":18782},"blog/2022/03/27/ad-hoc-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions.md","Setting up ad hoc development environments for Django applications with AWS ECS, Terraform and GitHub Actions",{"type":8,"value":15304,"toc":18686},[15305,15307,15315,15319,15322,15388,15392,15395,15418,15421,15456,15460,15463,15467,15470,15490,15493,15497,15500,15522,15528,15531,15534,15588,15591,15595,15598,15609,15612,15627,15633,15636,15640,15643,15654,15657,15660,15663,15666,15674,15677,15680,15684,15687,15700,15721,15725,15728,15739,15742,15745,15748,15759,15774,15778,15781,15784,15805,15808,15839,15842,15866,15871,15885,15894,15898,15907,15912,15916,15924,15927,15930,15933,15936,15939,15942,15945,15948,15952,15993,15997,16000,16002,16010,16014,16017,16024,16027,16033,16035,16038,16053,16057,16060,16064,16067,16070,16081,16084,16086,16089,16093,16096,16098,16101,16105,16108,16111,16127,16130,16140,16144,16147,16151,16169,16183,16186,16197,16209,16213,16219,16225,16231,16248,16252,16274,16281,16287,16305,16315,16319,16325,16336,16343,16347,16350,16393,16396,16401,16404,16407,18060,18070,18078,18103,18133,18137,18143,18169,18173,18176,18180,18183,18190,18197,18201,18214,18218,18221,18225,18228,18232,18235,18239,18242,18313,18316,18320,18338,18342,18363,18367,18373,18391,18397,18401,18411,18518,18521,18527,18531,18543,18547,18555,18558,18562,18566,18569,18573,18576,18579,18582,18585,18589,18603,18607,18610,18614,18617,18621,18624,18628,18631,18635,18644,18648,18651,18655,18658,18675,18678,18680,18683],[168,15306,8184],{"id":8183},[11,15308,15309,15310,15314],{},"This article will show how software development teams can build on-demand instances of a web application project for dog-food testing, quality review, internal and external demos and other use cases that require short-lived but feature-complete environments. It will focus on the technical implementation of building ad hoc environments using a specific set of tools (including AWS ECS, Terraform and GitHub Actions). I will also be giving context on high-level implementation decisions based on what I think are best practices guided by the ",[15,15311,15313],{"href":11449,"rel":15312},[19],"12-Factor Application methodology",". If any of this interests you, please have a read and let me know what you think in the comments on the outlets where I'll be sharing this article (links at the end).",[168,15316,15318],{"id":15317},"github-links","GitHub Links",[11,15320,15321],{},"This article references three open-source code repositories on GitHub.",[916,15323,15324,15343,15365],{},[919,15325,15326,15329],{},[15,15327,12114],{"href":11292,"rel":15328},[19],[916,15330,15331,15334,15337,15340],{},[919,15332,15333],{},"this repo contains an example microblogging application called μblog built with Django",[919,15335,15336],{},"the same application is implemented as a traditional Model Template View (MTV) site, a decoupled REST API and Javascript web application and a GraphQL API",[919,15338,15339],{},"it is a monorepo that also includes a frontend Vue.js application, CI/CD pipelines, a VuePress documentation site as well as tooling and instructions for settings up a local development environments (both with and without docker)",[919,15341,15342],{},"it includes a complete set of GitHub Action examples for automating the processes of creating, updating and destroying ad hoc environments that will be an important part of what is covered in this article",[919,15344,15345,15348],{},[15,15346,11607],{"href":11264,"rel":15347},[19],[916,15349,15350,15353,15356],{},[919,15351,15352],{},"a collection of modules for running Django applications on AWS using Terraform",[919,15354,15355],{},"one of the submodules can be used for creating ad hoc environments which will be what we use to create ad hoc environments",[919,15357,15358,15359,14794,15362,15364],{},"this module has been published to Terraform Registry and is used in the ",[33,15360,15361],{},"terraform/live/ad-hoc",[33,15363,12114],{}," repo",[919,15366,15367,15372],{},[15,15368,15371],{"href":15369,"rel":15370},"https://github.com/briancaffey/terraform-aws-ad-hoc-environments",[19],"terraform-aws-ad-hoc-environments",[916,15373,15374,15377,15382],{},[919,15375,15376],{},"a Terraform module that provides shared infrastructure used by ad hoc environments (including VPC, RDS instance, bastion host, security groups and IAM roles, etc.)",[919,15378,15379,15380],{},"this module has also been published to Terraform Registry and is also used in ",[33,15381,12114],{},[919,15383,15384,15385,15387],{},"this module is designed to be used with the ",[33,15386,11607],{}," Terraform module",[168,15389,15391],{"id":15390},"assumptions","Assumptions",[11,15393,15394],{},"There are all sorts of applications, and all sort of engineering teams. For some context on what I'm describing in this article, here are some basic assumptions that I'm making about the type of engineering team and software application product that would be a good fit for this type of development workflow.",[916,15396,15397,15400,15403,15406,15409,15412,15415],{},[919,15398,15399],{},"engineering team is composed of a backend team, a frontend team, a devops team and works closely with a product team",[919,15401,15402],{},"backend team primarily develops a REST API",[919,15404,15405],{},"frontend team develops a JavaScript SPA (frontend website)",[919,15407,15408],{},"SPA consumes backend REST API",[919,15410,15411],{},"product team frequently needs to demo applications to prospective clients",[919,15413,15414],{},"development teams don't have deep expertise in infrastructure, containers, CI/CD or automation",[919,15416,15417],{},"devops team has been tasked with building automation that will allow anyone on the team to quickly spin up a complete environment for testing and demoing purposes within minutes",[11,15419,15420],{},"Here are assumptions about specific tools and technologies used at the company:",[916,15422,15423,15426,15429,15432,15435,15438,15441,15444,15447,15450,15453],{},[919,15424,15425],{},"backend is a REST API developed with Django and a Postgres database",[919,15427,15428],{},"backend is packaged into a docker container",[919,15430,15431],{},"frontend is also packaged into a docker container using multi-stage builds and NGINX",[919,15433,15434],{},"frontend does not require any build-time configuration (all configuration needed by frontend is fetched from backend)",[919,15436,15437],{},"backend application's configuration is driven by plain-text environment variables at run-time",[919,15439,15440],{},"engineering team uses AWS",[919,15442,15443],{},"automation pipeline exists for building, tagging and pushing backend and frontend container images to an ECR repository",[919,15445,15446],{},"devops team uses AWS ECS for running containerized workloads",[919,15448,15449],{},"devops team uses Terraform for provisioning infrastructure",[919,15451,15452],{},"devops team uses GitHub Actions for building automation pipelines",[919,15454,15455],{},"team is somewhat cost-conscious",[168,15457,15459],{"id":15458},"what-are-ad-hoc-environments","What are ad hoc environments?",[11,15461,15462],{},"Ad hoc environments are short-lived environments that are designed to be used for testing a specific set of features or for demoing a specific application configuration in an isolated environment. It is intended to be a functional duplicate of the main production environment. An ad hoc environment is the first cloud environment that the application code will be deployed to after a developer has been working on it in a local development environment.",[168,15464,15466],{"id":15465},"trade-offs-to-make-when-designing-ad-hoc-environment-infrastructure-and-automation","Trade-offs to make when designing ad hoc environment infrastructure and automation",[11,15468,15469],{},"Now that we have a sense of what we are building and the team we are working with, let's think about the high-level trade-offs that we will face as we build a solution for providing on-demand ad hoc environments. When building infrastructure and workflows for ad hoc environments, there are a few things to solve for:",[916,15471,15472,15475,15478,15481,15484,15487],{},[919,15473,15474],{},"simplicity of the end-user interface and process for requesting an ad hoc environment",[919,15476,15477],{},"startup speed",[919,15479,15480],{},"total cost of ownership",[919,15482,15483],{},"degree of similarity to production environments",[919,15485,15486],{},"shared vs isolated resources",[919,15488,15489],{},"automation complexity",[11,15491,15492],{},"Let's look at these items by considering how we can set up the main infrastructure components that will be used to run our ad hoc application environments.",[911,15494,15496],{"id":15495},"relational-databases","Relational Databases",[11,15498,15499],{},"Startup speed can be measured by the time between when an environment is requested and when that environment can be used by whoever requested it. In this period of time, an automation pipeline may do some of the following:",[916,15501,15502,15513,15516,15519],{},[919,15503,15504,15505,637,15508,1172,15510,15512],{},"run ",[33,15506,15507],{},"terraform init",[33,15509,11532],{},[33,15511,11536],{}," to build infrastructure",[919,15514,15515],{},"run scripts to prepare the application such as database migrations",[919,15517,15518],{},"seeding initial sample data with a script or database dump",[919,15520,15521],{},"message the user with information about the environment (URLs, commands for accessing an interactive shell, etc.)",[11,15523,15524,15525,15527],{},"RDS instances can take a long time to create relative to other AWS resources such as S3 buckets and IAM roles. RDS instances are also more costly than other resources. We could use a single, shared RDS instance placed in a private subnet of a shared VPC. Each ad hoc environment could use a different named database in the RDS instance in the form ",[33,15526,12727],{},". Using one RDS instance per ad hoc environment would be slow to startup and tear down and also costly if there are many developers using ad hoc environments simultaneously.",[11,15529,15530],{},"If we choose to isolate the application's relational database at the database level (and not the RDS instance level), then we will need our automation workflow to create a database per ad hoc environment.",[11,15532,15533],{},"Let's spin up a simple example to illustrate how this would would work.",[916,15535,15536,15543,15548,15553,15564,15567,15573,15579],{},[919,15537,15538,15539,15542],{},"A developer is working on ",[33,15540,15541],{},"feature-abc"," that involves a significant refactor of the data model.",[919,15544,15545,15546,752],{},"The developer decides to spin up an ad hoc environment called ",[33,15547,15541],{},[919,15549,15550,15551,752],{},"Our automation will need to create a database in the RDS instance called ",[33,15552,15541],{},[919,15554,15555,15556,15559,15560,15563],{},"We can configure a bastion host with ",[33,15557,15558],{},"psql"," installed that has network and security group access to our RDS instance, and we can give our GitHub Actions an SSH key that can be used to run ",[33,15561,15562],{},"createdb"," over SSH.",[919,15565,15566],{},"The automation also runs database migrations once the application has started, and we can view the logs of the database migration to check for any irregularities or other issues.",[919,15568,15569,15570,15572],{},"This will give the developer and the rest of team confidence that the promoting ",[33,15571,15541],{}," to the next pre-production environments will not have any errors.",[919,15574,15575,15576,15578],{},"The developer may even choose to load a SQL dump of the next pre-production environment into their ",[33,15577,15541],{}," database get even more confidence that there will be no data integrity errors.",[919,15580,15581,15582,15584,15585,15587],{},"When the developer's PR is merged and approved, the ad hoc environment ",[33,15583,15541],{}," can be destroyed, including the ",[33,15586,15541],{}," database in the shared RDS instance.",[11,15589,15590],{},"With this approach we won't incur the costs of multiple RDS instances. Ad hoc environments will start up faster because an RDS instance per environment is not required. We do have slightly less resource isolation, and we need to introduce a bastion host, but I consider this an acceptable trade-off.",[911,15592,15594],{"id":15593},"redis-key-value-database","Redis (key-value database)",[11,15596,15597],{},"Redis is another database used in the application and it plays a few different roles:",[916,15599,15600,15603,15606],{},[919,15601,15602],{},"primarily, it is a caching layer that can cache request responses to reduce load on the database and speed up our application",[919,15604,15605],{},"it is a message broker for our async task workers (celery)",[919,15607,15608],{},"it can be used as a backend for other 3rd party Django apps that our main application may need to use (such as django-constance, cache-ops, django-channels, etc.)",[11,15610,15611],{},"AWS offers a managed Redis service called ElastiCache. Redis running on an ElastiCache instance can do database isolation similar to how Postgres running on RDS can do database isolation as we discussed previously, but there are some key differences:",[916,15613,15614,15617],{},[919,15615,15616],{},"redis databases are numbered, not named",[919,15618,15619,15620,15622,15623,15626],{},"the backend application uses isolated numbered databases for the different 3rd party apps that I just mentioned (for example: celery can use database ",[33,15621,10165],{},",  API caching layer can use database ",[33,15624,15625],{},"1",", etc.)",[11,15628,15629,15630,15632],{},"This makes it difficult to use a single ElastiCache instance for our ad hoc environments since we would need to figure out which numbered database to assign to a specific role for each ad hoc environment (e.g. how do we know which numbered database to use for the API caching for the ",[33,15631,15541],{}," ad hoc environment).",[11,15634,15635],{},"So how can we approach providing isolated redis instances for multiple ad hoc environments? Spoiler: my solution is to run redis as a stateful service in ECS. Before we dig into how to do this, we need to talk about another important part of our application: compute.",[911,15637,15639],{"id":15638},"compute","Compute",[11,15641,15642],{},"Our backend application is composed of a few different services that all share the same code base. In other words, our backend's services uses the same docker image but run different processes for each component:",[916,15644,15645,15648,15651],{},[919,15646,15647],{},"gunicorn for the core API",[919,15649,15650],{},"celery for the task workers",[919,15652,15653],{},"celerybeat for task scheduling",[11,15655,15656],{},"If our application used websockets, we could have another service that runs an asgi server process (like daphne or uvicorn).",[11,15658,15659],{},"Since our backend application is packaged into a container and we are using AWS as our cloud provider, ECS is a great choice for running our backend services. ECS is a container orchestration tool that I usually describe as a nice middle ground between docker swarm and Kubernetes. Simply put, it is a flexible option for running our containerized services that make up our backend application.",[11,15661,15662],{},"With ECS you can choose to run containers directly on EC2 instances that you manage, or you can run containers using Fargate. Fargate is a serverless compute option that takes care of managing both the underlying \"computer\" and operating system that run our application's containers. All of our backend dependencies are defined in our Dockerfile, so we do not to maintain or update the underlying operating system that runs our containers -- AWS handles all of this for us. To use Fargate, we simply tell AWS which containers to run and how much CPU and memory to use in the ECS Task that runs the containers. To scale our app horizontally, the ECS service that managed ECS tasks simply increases the number of tasks that run.",[11,15664,15665],{},"Since we are going to use the Fargate launch type for our ECS Tasks, let's talk about the ergonomics of these serverless compute instances compared to running our services directly on an EC2 instances.",[916,15667,15668,15671],{},[919,15669,15670],{},"We can't SSH into Fargate compute instances. We can instead use AWS Systems Manager and EcsExec to open an interactive shell in a running backend container. This can be useful for developers who might need to run a management command or access an interactive Django shell to verify behavior in their ad hoc environment.",[919,15672,15673],{},"We can't simply change code on the server and restart services. This can sometimes be a useful pattern for debugging something that can only be tested on a cloud environment (e.g. something that can't easily be reproduced on your local machine), so this requires that developers push new images to their backend services for every change they want to see reflected on their ad hoc environment. Later on I'll discuss how we can provide tooling for developers to quickly update the image used in their backend services.",[11,15675,15676],{},"With AWS Fargate, you will pay more than you would for a comparable amount of CPU and memory on EC2 instances. Similar to EC2 spot instances, Fargate offers interruptable instances called Fargate Spot which costs significantly less than regular Fargate instances. Fargate spot is appropriate for our ad hoc environments since ad hoc environments are non-critical workloads. In the event that a Fargate spot instance is interrupted, the ECS service will automatically launch another Fargate task to replace the task that was stopped.",[11,15678,15679],{},"In my opinion, ECS with Fargate is ideal for running the stateless services that make up our backend application. In terms of parity with our production environment, we can keep almost everything the same, except use regular Fargate instances instead of Fargate spot instances.",[911,15681,15683],{"id":15682},"redis-revisited","Redis, revisited",[11,15685,15686],{},"We can run redis as an ECS service instead of using ElastiCache. In order to do this, we will need our backend services (gunicorn, celery and celerybeat) to be able to communicate with a fourth ECS service that will be running redis (using an official redis image from Docker Hub, or a redis image that we define in ECR).",[11,15688,15689,15690,15693,15694,15696,15697,15699],{},"By default, ",[338,15691,15692],{},"there is no way for our backend services to know how to communicate with any other service in our ECS cluster",". If you have used docker-compose, you may know that you can use the service name ",[33,15695,12985],{}," in a backend service to easily communicate with a redis service called ",[33,15698,12985],{},". This networking convenience is not available to use out of the box with ECS. To achieve this in AWS, we need some way to manage a unique ad hoc environment-specific Route 53 DNS record that points to the private IP of the Fargate task that is running redis in an ECS cluster for a given ad hoc environment. Such a service exists in AWS and it is called Cloud Map. Cloud Map offers service discovery so that our backend services can make network calls to a static DNS address that will reliably point to the correct private IP of the ECS task running the redis container.",[11,15701,15702,15703,15705,15706,15709,15710,5857,15713,15716,15717,15720],{},"We can define a service discovery namespace (which will essentially be a top level domain, or TLD) that all of our ad hoc environments can share. Let's assume this namespace is called ",[33,15704,11946],{},". Each ad hoc environment can then define a service discovery service in the shared namespace for redis that is called ",[33,15707,15708],{},"{ad-hoc-env-name}-redis",". This way, we can have a reliable address that we can configure as an environment for our backend that will look like this: ",[33,15711,15712],{},"redis://{ad-hoc-env-name}-redis.ad-hoc:6379/0",[33,15714,15715],{},"{ad-hoc-env-name-redis}.ad-hoc"," will be the hostname of the redis service, and Route 53 will create records that point to ",[33,15718,15719],{},"{ad-hoc-env-name}-redis.ad-hoc"," to the private IP of the redis Fargate task for each ad hoc environment.",[911,15722,15724],{"id":15723},"load-balancing","Load Balancing",[11,15726,15727],{},"We now have our backend services (gunicorn, celery and celerybeat) running on Fargate spot instances, and these services can communicate with the redis service in our ad hoc environment's ECS cluster using service discovery that we configured with Cloud Map. We still need to think about a few things:",[916,15729,15730,15733,15736],{},[919,15731,15732],{},"how will we expose our API service to the public (or private) internet",[919,15734,15735],{},"how will we expose our frontend application to the public (or private) internet",[919,15737,15738],{},"how will we make sure that requests go the correct ECS services",[11,15740,15741],{},"Application load balancers (ALBs) are a great way to expose web app traffic to the internet. We could either have one application load balancer per ad hoc environment, or one application load balancer shared between all ad hoc environments. ALBs are somewhat slow to create and they also incur a significant monthly cost. They are also highly scalable, so using a shared ALB for all ad hoc environments would work.",[11,15743,15744],{},"Individual ad hoc environments can then create target groups and listener rules for a shared ALB for each service that needs to serve requests from the internet (the backend and the frontend). In our case this is the backend API server and the frontend server that serves our static frontend site using NGINX.",[11,15746,15747],{},"ECS services that need to be exposed to the internet can specify the target group, port and container to use for load balancing. A target group is created that defines the health check and other settings, and a load balancer listener rule is created on the shared load balancer that will forward traffic matching certain conditions to the target group for our service.",[11,15749,15750,15751,15754,15755,15758],{},"For a given ad hoc environment, we need to specify that only traffic with certain paths should be sent to the backend service, and all other traffic should be sent to the frontend service. For example, we may only want to send traffic that starts with the path ",[33,15752,15753],{},"/api"," or ",[33,15756,15757],{},"/admin"," to the backend target group, and all other traffic should be sent to the frontend target group. We can do this by setting conditions on the listener rules that forward traffic do the frontend and backend target groups based on the hostname and path.",[11,15760,15761,15762,637,15764,15766,15767,15769,15770,15773],{},"We want our listener rule logic to forward ",[33,15763,15753],{},[33,15765,15757],{}," and any other backend traffic to the backend target group, and forward all other traffic (",[33,15768,13016],{},") to the frontend target group. In order to do this, we need the backend listener rule to have a higher priority than the frontend listener rule for each ad hoc environment. Since we are using the same load balancer for all ad hoc environments, the priority values for each listener rule need to be unique. If we don't set the priority explicitly, then the priority will be set automatically to the next available value in ascending order. In order to make sure that the backend listener rule has a higher priority than the frontend listener rule for each ad hoc environment, we need to tell Terraform that the frontend module ",[33,15771,15772],{},"depends_on"," the backend module. This way the backend listener rule will have a higher priority (e.g. priority of 1) because it will be created first, and the frontend listener rule will have a lower priority (e.g. priority of 2).",[168,15775,15777],{"id":15776},"more-on-shared-resources-vs-per-environment-resources","More on shared resources vs per-environment resources",[11,15779,15780],{},"Up until now we have discussed infrastructure design decisions at a high level, but we have not yet talked about how to organize our infrastructure as code. At a basic level, components of our ad hoc environment either fall into shared infrastructure or infrastructure that is specific to an individual ad hoc environment. Here's a list of the resources that are shared and the resources that are specific to each ad hoc environment.",[11,15782,15783],{},"Shared resources include:",[916,15785,15786,15788,15791,15794,15797,15800,15802],{},[919,15787,12378],{},[919,15789,15790],{},"IAM policies",[919,15792,15793],{},"Security groups",[919,15795,15796],{},"RDS instance",[919,15798,15799],{},"Service Discovery namespace",[919,15801,12534],{},[919,15803,15804],{},"Bastion host",[11,15806,15807],{},"Ad hoc environment resources include:",[916,15809,15810,15812,15815,15818,15821,15824,15827,15833,15836],{},[919,15811,12832],{},[919,15813,15814],{},"ECS Tasks and Services (for backend and frontend applications)",[919,15816,15817],{},"ECS Tasks for running management commands (such as migrate)",[919,15819,15820],{},"CloudWatch logging groups for containers defined in ECS Tasks",[919,15822,15823],{},"ALB Target groups",[919,15825,15826],{},"ALB listener rules",[919,15828,15829,15830,343],{},"Route 53 record that points to the load balancer (e.g. ",[33,15831,15832],{},"ad-hoc-env-name.example.com",[919,15834,15835],{},"S3 bucket for static and media assets",[919,15837,15838],{},"Service Discovery Service for redis service in ECS cluster",[11,15840,15841],{},"Shared resources can be defined in one terraform configuration and deployed once. These resources will be long-lived as long as the application is under active development and the team requires on-demand provisioning of ad hoc environments.",[11,15843,15844,15845,15847,15848,15851,15852,637,15855,637,15858,15861,15862,15865],{},"Ad hoc environment resources can be defined in another terraform configuration that references outputs from the shared resource configuration using ",[33,15846,12232],{},". Each ad hoc environment can be defined by a ",[33,15849,15850],{},"\u003Cname>.tfvars"," file that contains the name of the ad hoc environment (such as ",[33,15853,15854],{},"brian",[33,15856,15857],{},"brian2",[33,15859,15860],{},"demo-feature-abc",", etc.). This ",[33,15863,15864],{},"\u003Cname>"," value will also be the name of the Terraform workspace and will be used to name and tag AWS resources associated with the corresponding ad hoc environment.",[11,15867,6131,15868,15870],{},[33,15869,15850],{}," file will allow developers to use a simple, standard file interface for defining application specific values, such as the version of the backend and frontend. This brings developers into the concepts and practices of \"infrastructure as code\" and \"configuration as code\" and also helps the entire team keep track of how different environments are configured.",[11,15872,15873,15874,15876,15877,15880,15881,752],{},"Ad hoc environment ",[33,15875,15850],{}," files are stored in a directory of a special git repository that also defines the ad hoc environment terraform configuration. Currently, the ",[33,15878,15879],{},"tfvars"," files are stored ",[15,15882,1321],{"href":15883,"rel":15884},"https://github.com/briancaffey/django-step-by-step/tree/main/terraform/live/ad-hoc/envs",[19],[11,15886,15887,15888,1172,15891,752],{},"Now let's look at the two terraform configurations used for defining ",[338,15889,15890],{},"shared resources",[338,15892,15893],{},"ad hoc environment resources",[168,15895,15897],{"id":15896},"ad-hoc-environment-diagram","Ad Hoc Environment Diagram",[11,15899,15900,15901,14372,15904,752],{},"Here's an overview of the resources used for the ad hoc environments. The ",[338,15902,15903],{},"letters represent shared resources",[338,15905,15906],{},"numbers represent per-environment resources",[11,15908,15909],{},[511,15910],{"alt":7255,"src":15911},"/static/adhoc.png",[911,15913,15915],{"id":15914},"shared-architecture","Shared architecture",[11,15917,15918,15919,343],{},"A. VPC (created using the ",[15,15920,15923],{"href":15921,"rel":15922},"https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest",[19],"official AWS VPC Module",[11,15925,15926],{},"B. Public subnets for bastion host, NAT Gateways and Load Balancer",[11,15928,15929],{},"C. Private subnets for application workloads and RDS",[11,15931,15932],{},"D. Application Load Balancer that is shared between all ad hoc environments. A pre-provisioned wildcard ACM certificate is attached to the load balancer that is used to secure traffic for load-balanced ECS services",[11,15934,15935],{},"E. Service discovery namespace that provides a namespace for application workloads to access the redis service running in ECS",[11,15937,15938],{},"F. IAM roles needed for ECS tasks to access AWS services",[11,15940,15941],{},"G. RDS instance using postgres engine that is shared between all ad hoc environments",[11,15943,15944],{},"H. Bastion host used to access RDS from GitHub Actions (needed for creating per-environment databases)",[11,15946,15947],{},"I. NAT Gateway used to give traffic in private subnets a route to the public internet",[911,15949,15951],{"id":15950},"environment-specific-architecture","Environment-specific architecture",[2276,15953,15954,15957,15960,15963,15966,15969,15972,15975,15978,15983,15987,15990],{},[919,15955,15956],{},"ECS Cluster that groups all ECS tasks for a single ad hoc environment",[919,15958,15959],{},"Listener rules and target groups that direct traffic from the load balancer to the ECS services for an ad hoc environment.",[919,15961,15962],{},"Redis service running in ECS that provides caching and serves as a task broker for celery",[919,15964,15965],{},"Route53 records that point to the load balancer",[919,15967,15968],{},"Frontend service that serves the Vue.js application over NGINX",[919,15970,15971],{},"API service that serves the backend with Gunicorn",[919,15973,15974],{},"Celery worker that process jobs in the default queue",[919,15976,15977],{},"Celery beat that schedules celery tasks",[919,15979,15980,15982],{},[33,15981,13071],{}," task",[919,15984,15985,15982],{},[33,15986,13074],{},[919,15988,15989],{},"CloudWatch log groups are created for each ECS task in an ad hoc environment",[919,15991,15992],{},"Each ad hoc environment gets a database in the shared RDS instance",[168,15994,15996],{"id":15995},"shared-resources-terraform-configuration","Shared resources terraform configuration",[11,15998,15999],{},"Let's have a detailed look at the terraform configuration for shared resources that will support ad hoc environments.",[911,16001,12378],{"id":12377},[11,16003,16004,16005,16009],{},"We can use the ",[15,16006,16008],{"href":15921,"rel":16007},[19],"AWS VPC module"," for creating the shared VPC with Terraform. This module provides a high level interface that will provision lots of the components that are needed for a VPC following best practices, and it is less code for the DevOps team to manage compared to defining each component of a VPC (route tables, subnets, internet gateways, etc.).",[911,16011,16013],{"id":16012},"cloud-map-service-discovery-namespace","Cloud Map Service Discovery Namespace",[11,16015,16016],{},"Cloud Map is used in order to allow services in our ECS cluster to communicate with each other. The only reason that Cloud Map is needed is so that the backend services (API, celery workers, beat) can communicate with Redis, which will be an important service for our application, providing caching and also serving as a broker for celery. If we were to use Django Channels for websockets, the Redis service would also function as the backend for Django Channels.",[11,16018,16019,16020,16023],{},"We will only need to specify ",[33,16021,16022],{},"service_registries"," on the redis service in our ECS cluster. What this will do is provide an address that our other services can use to communicate with redis. This address is created in the form of a Route 53 record, and it points to the private IP address of the redis service. If the private IP of the redis service is updated, the Route 53 record record for our redis service will be updated as well.",[11,16025,16026],{},"In order for service discovery to work in the VPC that we created, we need to add the following options to the terraform AWS VPC module:",[26,16028,16031],{"className":16029,"code":16030,"language":31},[29],"# DNS settings\nenable_dns_hostnames = true\nenable_dns_support   = true\n",[33,16032,16030],{"__ignoreMap":35},[911,16034,12330],{"id":12415},[11,16036,16037],{},"There are two important security groups that we will set up as part of the shared infrastructure layer to be used by each ad hoc environment: one security group for the load balancer, and one security group where all of our ECS services will run.",[11,16039,16040,16041,1172,16043,16045,16046,1172,16049,16052],{},"The load balancer security group will allow all traffic on port ",[33,16042,12515],{},[33,16044,12488],{}," for ",[33,16047,16048],{},"HTTP",[33,16050,16051],{},"HTTPS"," traffic. The ECS security group will only allow inbound traffic from the application load balancer security group. It will also allow for traffic from port 6379 for redis traffic.",[911,16054,16056],{"id":16055},"iam-roles","IAM Roles",[11,16058,16059],{},"There are two important IAM roles that we will need for our ECS tasks. We need a task execution role that our ECS tasks will use to interact with other AWS services, such as S3, Secrets Manager, etc.",[911,16061,16063],{"id":16062},"rds-instance","RDS Instance",[11,16065,16066],{},"We will create one RDS instance in one of the private subnets in our VPC. This RDS instance will have one Postgres database per ad hoc environment. This RDS instance has a security group that allows all traffic from our ECS security group.",[911,16068,12333],{"id":16069},"load-balancer",[11,16071,16072,16073,16076,16077,16080],{},"We will use one load balancer for all ad hoc environments. This load balancer will have a wildcard ACM certificate attached to it (",[33,16074,16075],{},"*.dev.example.com",", for example). Each ad hoc environment will create a Route 53 record that will point to this load balancer's public DNS name. For example, ",[33,16078,16079],{},"brian.dev.example.com"," will be the address of my ad hoc environment. Requests to this address will then be routed to either the frontend ECS service or the backend ECS service depending on request header values and request path values that will be set on the listener rules.",[11,16082,16083],{},"By default, a load balancer supports up to 50 listener rules, so we can create plenty of ad hoc environments before we need to increase the default quota. There will be a discussion at the end of this article about AWS service quotas.",[911,16085,12339],{"id":12716},[11,16087,16088],{},"The bastion host will be created in one of the VPC's public subnets. This will primarily be used for connecting to RDS to create new databases for new ad hoc environments, or for manually manipulating data in an ad hoc environment for debugging.",[168,16090,16092],{"id":16091},"ad-hoc-environment-resources","Ad hoc environment resources",[11,16094,16095],{},"Now that we have defined a shared set of infrastructure that our ad hoc environments will use, let's have a look at the resources that will be specific to ad hoc environments that will be added on top of the shared resources.",[911,16097,12832],{"id":12831},[11,16099,16100],{},"The ECS Cluster is a simple grouping of ECS tasks and services.",[911,16102,16104],{"id":16103},"ecs-tasks-and-services","ECS Tasks and Services",[11,16106,16107],{},"Each environment will have a set of ECS tasks and services that will be used to run the application.",[11,16109,16110],{},"There are four important ECS services in our application that are used to run \"long-running\" ECS tasks. Long-running tasks are tasks that start processes that run indefinitely, rather than running until completion. The long-running tasks in our application include:",[916,16112,16113,16116,16119,16122,16125],{},[919,16114,16115],{},"backend web application (gunciron web server)",[919,16117,16118],{},"backend celery worker",[919,16120,16121],{},"backend celery beat",[919,16123,16124],{},"frontend web site (nginx web server)",[919,16126,12985],{},[11,16128,16129],{},"The infrastructure code also defines some tasks that are not long-running but rather short lived tasks that run until completion and do not start again. These tasks include:",[916,16131,16132,16134,16137],{},[919,16133,13071],{},[919,16135,16136],{},"database migrations",[919,16138,16139],{},"any other ad-hoc task that we want to run, usually wrapped in a Django management command",[168,16141,16143],{"id":16142},"how-to-setup-an-ad-hoc-environment","How to setup an ad hoc environment",[11,16145,16146],{},"Now that we have been over the resources that will be created to support our ad hoc environments, let's talk about how we can enable individuals on our team to create and update ad hoc environments.",[911,16148,16150],{"id":16149},"design-decisions","Design decisions",[11,16152,16153,16154,16157,16158,1172,16160,16162,16163,16165,16166,16168],{},"The devops team will decide on the interface that will be used for creating an ad hoc environment. Since we are using Terraform, this interface will be a Terraform configuration. The minimum amount of information that our ad hoc environment configuration needs is image tags for the frontend and backend images to use. Other configurations will be provided by default values set in ",[33,16155,16156],{},"variables.tf",", and these defaults can easily be overridden by passing values to ",[33,16159,11532],{},[33,16161,11536],{},". I'm choosing to use ",[33,16164,15850],{}," as the way to pass configuration values to our ad hoc environments where ",[33,16167,15864],{}," is the name of the ad hoc environment being created. This will give us the following benefits:",[916,16170,16171,16177],{},[919,16172,16173,16174,16176],{},"all ad hoc environments will be visible to the entire team in git since each ad hoc environment will have a ",[33,16175,15850],{}," file associated with it",[919,16178,16179,16180],{},"adding additional customization to an ad hoc environment does not add additional complexity to our automation pipeline since all customization is added through a single file that will be referenced by ",[33,16181,16182],{},"$WORKSPACE.tfvars",[11,16184,16185],{},"The downsides of this approach are:",[916,16187,16188,16191],{},[919,16189,16190],{},"creating ad hoc environments requires knowledge of git, so non-technical product team members might need help from the engineering team when setting up an ad hoc environment",[919,16192,16193,16194,16196],{},"there is an additional \"manual\" step of creating a ",[33,16195,15850],{}," file that must be done before running a pipeline to create an ad hoc environment",[11,16198,16199,16200,16202,16203,16205,16206,16208],{},"Provided that a ",[33,16201,15850],{}," file has been created and pushed to the repo, creating or updating an ad hoc environment will be as simple as running a pipeline in GitHub Actions that specifies the ",[33,16204,15864],{}," of our ad hoc environment. If no such ",[33,16207,15850],{}," file exists, our pipeline will fail.",[911,16210,16212],{"id":16211},"github-action","GitHub Action",[11,16214,16215,16216,358],{},"Creating ad hoc environments will involve manually triggering a GitHub Action that runs on ",[33,16217,16218],{},"workflow_dispatch",[26,16220,16223],{"className":16221,"code":16222,"language":31},[29],"on:\n  workflow_dispatch:\n    inputs:\n      workspace:\n        description: 'Name of terraform workspace to use'\n        required: true\n        default: 'dev'\n        type: string\n",[33,16224,16222],{"__ignoreMap":35},[11,16226,16227,16228,16230],{},"We only have to enter the name of the ad hoc environment we want to create or update. The ad hoc environment name is used as the Terraform workspace name. This name is also the name of the ",[33,16229,15850],{}," file that must be created per environment.",[11,16232,16233,16234,637,16236,1172,16238,16240,16241,16243,16244,16247],{},"This workflow will do ",[33,16235,15507],{},[33,16237,11532],{},[33,16239,11536],{}," using the ",[33,16242,15850],{}," file. When everything has been created, we will use the AWS CLI to prepare the environment so that it can be used. We will use the ",[33,16245,16246],{},"aws ecs run-task"," command to run database migrations needed so that the application code can make database queries.",[168,16249,16251],{"id":16250},"how-to-update-code-in-an-existing-ad-hoc-environment","How to update code in an existing ad hoc environment",[11,16253,16254,16255,16257,16258,16261,16262,16265,16266,16269,16270,16273],{},"Assuming that we have deployed an ad hoc environment called ",[33,16256,15854],{}," with version ",[33,16259,16260],{},"v1.0.0"," of the backend application and ",[33,16263,16264],{},"v2.0.0"," of the frontend application, let's think about the process of updating the application to ",[33,16267,16268],{},"v1.1.0"," of the backend and ",[33,16271,16272],{},"v2.1.0"," of the frontend.",[11,16275,16276,16277,16280],{},"The simplest approach to updating the application would be edit the ",[33,16278,16279],{},"brian.tfvars"," file with the new versions:",[26,16282,16285],{"className":16283,"code":16284,"language":31},[29],"# brian.tfvars\nbe_image_tag = \"v1.1.0\"\nfe_image_tag = \"v2.1.0\"\n",[33,16286,16284],{"__ignoreMap":35},[11,16288,16289,16290,637,16292,1172,16294,16296,16297,16299,16300,1737],{},"If we run the same pipeline that we initially used to deploy ad hoc environment (with ",[33,16291,15507],{},[33,16293,11532],{},[33,16295,11536],{},") against the updated ",[33,16298,16279],{}," file, this will result in a rolling update of the frontend and backend services (",[15,16301,16304],{"href":16302,"rel":16303},"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html",[19],"more on rolling updates here",[11,16306,16307,16308,16310,16311,16314],{},"If there are database migrations included in the new version of the code that is going out, we need to run database migrations after the ",[33,16309,11536],{}," completes. We use a top level output from the ad hov environment terraform configuration that is a ",[33,16312,16313],{},"run-task"," command with all appropriate arguments that will run database migrations when called from GitHub Actions.",[911,16316,16318],{"id":16317},"order-of-operations","Order of Operations",[11,16320,16321,16322,752],{},"For ad hoc environments, it is probably fine to update the services and then run the database migrations. Ad hoc environments may only have a single \"user\" -- the developer, so ",[338,16323,16324],{},"we don't need to worry about any errors that may occur if requests are made against the new version of code before database migrations have been applied",[11,16326,16327,16328,16331,16332,16335],{},"Let's consider a simple example to illustrate what can go wrong here. If we add a ",[33,16329,16330],{},"total_views"," to our blog post model to track the total number of page views a post has, we would add a field to the model, generate migration file with ",[33,16333,16334],{},"makemigrations",", and then update our views and model serializers to make use of this new field. In the time between updating our service and running the database migrations, any requests to endpoints that access the new database field will fail since the table does not yet exist.",[11,16337,16338,16339,16342],{},"If we first run database migrations ",[338,16340,16341],{},"and then"," update application code (ECS services), then we can avoid errors about fields not existing. In our production application, we want to aim for fewer errors, so we should be using this \"order of operations\": first run new database migrations and then update application code.",[911,16344,16346],{"id":16345},"github-action-for-application-updates","GitHub Action for application updates",[11,16348,16349],{},"We need a GitHub Action that can do the following:",[916,16351,16352,16358,16364,16370,16375,16381,16387],{},[919,16353,16354,16355,343],{},"fetch the current container definition JSON files for each backend tasks (",[33,16356,16357],{},"aws ecs describe-task-definition",[919,16359,16360,16361,343],{},"write new container definitions JSON with the new backend image tag (using ",[33,16362,16363],{},"jq",[919,16365,16366,16367,343],{},"register new task definitions with the new container definition JSON files for each task (",[33,16368,16369],{},"aws ecs register-task-definition",[919,16371,16372,16373,343],{},"call run-task with the newly updated migration ECS task (",[33,16374,16246],{},[919,16376,16377,16378,343],{},"wait for the task to exit and display the logs (",[33,16379,16380],{},"aws ecs wait tasks-stopped",[919,16382,16383,16384,343],{},"update the backend services (gunicorn, celery, celery beat) (",[33,16385,16386],{},"aws ecs update-service",[919,16388,16389,16390,343],{},"wait for the new backend services to be stable (",[33,16391,16392],{},"aws ecs wait services-stable",[11,16394,16395],{},"Here's a visual representation of the backend update process:",[11,16397,16398],{},[511,16399],{"alt":7255,"src":16400},"/static/adhoc/ad_hoc.backend_update.drawio.png",[11,16402,16403],{},"In order have the correct arguments for all of the AWS CLI calls used in the above workflow, we can use the AWS CLI to fetch resource names by tag.",[11,16405,16406],{},"Here is what I'm using for the script. There lots of comments, so please refer to those comments for an explanation of what the script is doing.",[26,16408,16410],{"className":181,"code":16409,"language":183,"meta":35,"style":35},"#!/bin/bash\n\n# This script will be called to update an ad hoc environment backend\n# with a new image tag. It will first run pre-update tasks (such as migrations)\n# and then do a rolling update of the backend services.\n\n# It is called from the ad_hock_backend_update.yml GitHub Actions file\n\n# Required environment variables that need to be exported before running this script:\n\n# WORKSPACE - ad hoc environment workspace\n# SHARED_RESOURCES_WORKSPACE - shared resources workspace\n# BACKEND_IMAGE_TAG - backend image tag to update services to (e.g. v1.2.3)\n# AWS_ACCOUNT_ID - AWS account ID is used for the ECR repository URL\n\necho \"Updating backend services...\"\n\n# first define a variable containing the new image URI\nNEW_BACKEND_IMAGE_URI=\"$AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/backend:$BACKEND_IMAGE_TAG\"\n\n\n# register new task definitions\n# https://docs.aws.amazon.com/cli/latest/reference/ecs/describe-task-definition.html#description\nfor TASK in \"migrate\" \"gunicorn\" \"default\" \"beat\"\ndo\n  echo \"Updating $TASK task definition...\"\n\n  # in Terraform we name our tasks based on the ad hoc environment name\n  # (also the Terraform workspace name) and the name of the task\n  # (e.g. migrate, gunicorn, default, beat)\n  TASK_FAMILY=$WORKSPACE-$TASK\n\n  # save the task definition JSON to a variable\n  TASK_DESCRIPTION=$(aws ecs describe-task-definition \\\n    --task-definition $TASK_FAMILY \\\n  )\n\n  # save container definitions to a file for each task\n  echo $TASK_DESCRIPTION | jq -r \\\n    .taskDefinition.containerDefinitions \\\n    > /tmp/$TASK_FAMILY.json\n\n  # write new container definition JSON with updated image\n  echo \"Writing new $TASK_FAMILY container definitions JSON...\"\n\n  # replace old image URI with new image URI in a new container definitions JSON\n  cat /tmp/$TASK_FAMILY.json \\\n    | jq \\\n    --arg IMAGE \"$NEW_BACKEND_IMAGE_URI\" '.[0].image |= $IMAGE' \\\n    > /tmp/$TASK_FAMILY-new.json\n\n  # Get the existing configuration for the task definition (memory, cpu, etc.)\n  # from the variable that we saved the task definition JSON to earlier\n  echo \"Getting existing configuration for $TASK_FAMILY...\"\n\n  MEMORY=$( echo $TASK_DESCRIPTION | jq -r \\\n    .taskDefinition.memory \\\n  )\n\n  CPU=$( echo $TASK_DESCRIPTION | jq -r \\\n    .taskDefinition.cpu \\\n  )\n\n  ECS_EXECUTION_ROLE_ARN=$( echo $TASK_DESCRIPTION | jq -r \\\n    .taskDefinition.executionRoleArn \\\n  )\n\n  ECS_TASK_ROLE_ARN=$( echo $TASK_DESCRIPTION | jq -r \\\n    .taskDefinition.taskRoleArn \\\n  )\n\n  # check the content of the new container definition JSON\n  cat /tmp/$TASK_FAMILY-new.json\n\n  # register new task definition using the new container definitions\n  # and the values that we read off of the existing task definitions\n  echo \"Registering new $TASK_FAMILY task definition...\"\n\n  aws ecs register-task-definition \\\n    --family $TASK_FAMILY \\\n    --container-definitions file:///tmp/$TASK_FAMILY-new.json \\\n    --memory $MEMORY \\\n    --cpu $CPU \\\n    --network-mode awsvpc \\\n    --execution-role-arn $ECS_EXECUTION_ROLE_ARN \\\n    --task-role-arn $ECS_TASK_ROLE_ARN \\\n    --requires-compatibilities \"FARGATE\"\n\ndone\n\n# Now we need to run migrate, collectstatic and any other commands that need to be run\n# before doing a rolling update of the backend services\n\n# We will use the new task definitions we just created to run these commands\n\n# get the ARN of the most recent revision of the migrate task definition\nTASK_DEFINITION=$( \\\n  aws ecs describe-task-definition \\\n    --task-definition $WORKSPACE-migrate \\\n    | jq -r \\\n    .taskDefinition.taskDefinitionArn \\\n)\n\n# get private subnets as space separated string from shared resources VPC\nSUBNETS=$( \\\n  aws ec2 describe-subnets \\\n    --filters \"Name=tag:env,Values=$SHARED_RESOURCES_WORKSPACE\" \"Name=tag:Name,Values=*private*\" \\\n    --query 'Subnets[*].SubnetId' \\\n    --output text \\\n)\n\n# replace spaces with commas using tr\nSUBNET_IDS=$(echo $SUBNETS | tr ' ' ',')\n\n# https://github.com/aws/aws-cli/issues/5348\n# get ecs_sg_id - just a single value\nECS_SG_ID=$( \\\n  aws ec2 describe-security-groups \\\n    --filters \"Name=tag:Name,Values=$SHARED_RESOURCES_WORKSPACE-ecs-sg\" \\\n    --query 'SecurityGroups[*].GroupId' \\\n    --output text \\\n)\n\necho \"Running database migrations...\"\n\n# timestamp used for log retrieval (milliseconds after Jan 1, 1970 00:00:00 UTC)\nSTART_TIME=$(date +%s000)\n\n# run the migration task and capture the taskArn into a variable called TASK_ID\nTASK_ID=$( \\\n  aws ecs run-task \\\n    --cluster $WORKSPACE-cluster \\\n    --task-definition $TASK_DEFINITION \\\n    --network-configuration \"awsvpcConfiguration={subnets=[$SUBNET_IDS],securityGroups=[$ECS_SG_ID],assignPublicIp=ENABLED}\" \\\n    | jq -r '.tasks[0].taskArn' \\\n  )\n\necho \"Task ID is $TASK_ID\"\n\n# wait for the migrate task to exit\n# https://docs.aws.amazon.com/cli/latest/reference/ecs/wait/tasks-stopped.html#description\n# > It will poll every 6 seconds until a successful state has been reached.\n# > This will exit with a return code of 255 after 100 failed checks.\naws ecs wait tasks-stopped \\\n  --tasks $TASK_ID \\\n  --cluster $WORKSPACE-cluster\n\n# timestamp used for log retrieval (milliseconds after Jan 1, 1970 00:00:00 UTC)\nEND_TIME=$(date +%s000)\n\n# print the CloudWatch log events to STDOUT\naws logs get-log-events \\\n  --log-group-name \"/ecs/$WORKSPACE/migrate\" \\\n  --log-stream-name \"migrate/migrate/${TASK_ID##*/}\" \\\n  --start-time $START_TIME \\\n  --end-time $END_TIME \\\n  | jq -r '.events[].message'\n\necho \"Migrations complete. Starting rolling update for backend services...\"\n\n# update backend services\nfor TASK in \"gunicorn\" \"default\" \"beat\"\ndo\n\n  # get taskDefinitionArn for each service to be used in update-service command\n  # this will get the most recent revision of each task (the one that was just created)\n  # https://docs.aws.amazon.com/cli/latest/reference/ecs/describe-task-definition.html#description\n  TASK_DEFINITION=$( \\\n    aws ecs describe-task-definition \\\n      --task-definition $WORKSPACE-$TASK \\\n      | jq -r \\\n      .taskDefinition.taskDefinitionArn \\\n  )\n\n  # update each service with new task definintion\n  aws ecs update-service \\\n    --cluster $WORKSPACE-cluster \\\n    --service $WORKSPACE-$TASK \\\n    --task-definition $TASK_DEFINITION \\\n    --no-cli-pager\n\ndone\n\necho \"Services updated. Waiting for services to become stable...\"\n\n# wait for all service to be stable (runningCount == desiredCount for each service)\naws ecs wait services-stable \\\n  --cluster $WORKSPACE-cluster \\\n  --services $WORKSPACE-gunicorn $WORKSPACE-default $WORKSPACE-beat\n\necho \"Services are now stable. Backend services are now up to date with $BACKEND_IMAGE_TAG.\"\n\necho \"Backend update is now complete!\"\n",[33,16411,16412,16417,16421,16426,16431,16436,16440,16445,16449,16454,16458,16463,16468,16473,16478,16482,16490,16494,16499,16521,16525,16529,16534,16539,16562,16567,16581,16585,16590,16595,16600,16615,16619,16624,16645,16656,16661,16665,16670,16687,16694,16708,16712,16717,16729,16733,16738,16752,16761,16782,16793,16797,16802,16807,16819,16823,16845,16852,16856,16860,16881,16888,16892,16896,16917,16924,16928,16932,16953,16960,16964,16968,16973,16983,16987,16992,16997,17008,17012,17024,17033,17048,17058,17068,17078,17088,17098,17106,17110,17115,17119,17124,17129,17133,17139,17144,17150,17162,17173,17186,17197,17205,17210,17215,17221,17233,17246,17265,17276,17287,17291,17296,17302,17330,17335,17341,17347,17359,17371,17386,17396,17405,17410,17415,17423,17428,17434,17451,17456,17462,17474,17486,17499,17509,17532,17546,17551,17556,17569,17574,17580,17586,17592,17598,17613,17624,17635,17640,17645,17661,17666,17672,17685,17701,17720,17731,17742,17755,17760,17768,17773,17779,17794,17799,17804,17810,17816,17822,17834,17846,17861,17873,17881,17886,17891,17897,17909,17920,17934,17943,17949,17954,17959,17964,17972,17977,17983,17997,18008,18029,18034,18047,18052],{"__ignoreMap":35},[187,16413,16414],{"class":189,"line":190},[187,16415,16416],{"class":295},"#!/bin/bash\n",[187,16418,16419],{"class":189,"line":249},[187,16420,316],{"emptyLinePlaceholder":315},[187,16422,16423],{"class":189,"line":312},[187,16424,16425],{"class":295},"# This script will be called to update an ad hoc environment backend\n",[187,16427,16428],{"class":189,"line":319},[187,16429,16430],{"class":295},"# with a new image tag. It will first run pre-update tasks (such as migrations)\n",[187,16432,16433],{"class":189,"line":325},[187,16434,16435],{"class":295},"# and then do a rolling update of the backend services.\n",[187,16437,16438],{"class":189,"line":686},[187,16439,316],{"emptyLinePlaceholder":315},[187,16441,16442],{"class":189,"line":697},[187,16443,16444],{"class":295},"# It is called from the ad_hock_backend_update.yml GitHub Actions file\n",[187,16446,16447],{"class":189,"line":1291},[187,16448,316],{"emptyLinePlaceholder":315},[187,16450,16451],{"class":189,"line":1306},[187,16452,16453],{"class":295},"# Required environment variables that need to be exported before running this script:\n",[187,16455,16456],{"class":189,"line":1434},[187,16457,316],{"emptyLinePlaceholder":315},[187,16459,16460],{"class":189,"line":2599},[187,16461,16462],{"class":295},"# WORKSPACE - ad hoc environment workspace\n",[187,16464,16465],{"class":189,"line":2607},[187,16466,16467],{"class":295},"# SHARED_RESOURCES_WORKSPACE - shared resources workspace\n",[187,16469,16470],{"class":189,"line":2621},[187,16471,16472],{"class":295},"# BACKEND_IMAGE_TAG - backend image tag to update services to (e.g. v1.2.3)\n",[187,16474,16475],{"class":189,"line":2631},[187,16476,16477],{"class":295},"# AWS_ACCOUNT_ID - AWS account ID is used for the ECR repository URL\n",[187,16479,16480],{"class":189,"line":2642},[187,16481,316],{"emptyLinePlaceholder":315},[187,16483,16484,16487],{"class":189,"line":2653},[187,16485,16486],{"class":588},"echo",[187,16488,16489],{"class":196}," \"Updating backend services...\"\n",[187,16491,16492],{"class":189,"line":2665},[187,16493,316],{"emptyLinePlaceholder":315},[187,16495,16496],{"class":189,"line":2674},[187,16497,16498],{"class":295},"# first define a variable containing the new image URI\n",[187,16500,16501,16504,16506,16509,16512,16515,16518],{"class":189,"line":2684},[187,16502,16503],{"class":577},"NEW_BACKEND_IMAGE_URI",[187,16505,595],{"class":573},[187,16507,16508],{"class":196},"\"",[187,16510,16511],{"class":577},"$AWS_ACCOUNT_ID",[187,16513,16514],{"class":196},".dkr.ecr.us-east-1.amazonaws.com/backend:",[187,16516,16517],{"class":577},"$BACKEND_IMAGE_TAG",[187,16519,16520],{"class":196},"\"\n",[187,16522,16523],{"class":189,"line":2694},[187,16524,316],{"emptyLinePlaceholder":315},[187,16526,16527],{"class":189,"line":2706},[187,16528,316],{"emptyLinePlaceholder":315},[187,16530,16531],{"class":189,"line":2715},[187,16532,16533],{"class":295},"# register new task definitions\n",[187,16535,16536],{"class":189,"line":2725},[187,16537,16538],{"class":295},"# https://docs.aws.amazon.com/cli/latest/reference/ecs/describe-task-definition.html#description\n",[187,16540,16541,16544,16547,16550,16553,16556,16559],{"class":189,"line":2735},[187,16542,16543],{"class":573},"for",[187,16545,16546],{"class":577}," TASK ",[187,16548,16549],{"class":573},"in",[187,16551,16552],{"class":196}," \"migrate\"",[187,16554,16555],{"class":196}," \"gunicorn\"",[187,16557,16558],{"class":196}," \"default\"",[187,16560,16561],{"class":196}," \"beat\"\n",[187,16563,16564],{"class":189,"line":2743},[187,16565,16566],{"class":573},"do\n",[187,16568,16569,16572,16575,16578],{"class":189,"line":2754},[187,16570,16571],{"class":588},"  echo",[187,16573,16574],{"class":196}," \"Updating ",[187,16576,16577],{"class":577},"$TASK",[187,16579,16580],{"class":196}," task definition...\"\n",[187,16582,16583],{"class":189,"line":2762},[187,16584,316],{"emptyLinePlaceholder":315},[187,16586,16587],{"class":189,"line":2770},[187,16588,16589],{"class":295},"  # in Terraform we name our tasks based on the ad hoc environment name\n",[187,16591,16592],{"class":189,"line":2781},[187,16593,16594],{"class":295},"  # (also the Terraform workspace name) and the name of the task\n",[187,16596,16597],{"class":189,"line":2792},[187,16598,16599],{"class":295},"  # (e.g. migrate, gunicorn, default, beat)\n",[187,16601,16602,16605,16607,16610,16612],{"class":189,"line":2803},[187,16603,16604],{"class":577},"  TASK_FAMILY",[187,16606,595],{"class":573},[187,16608,16609],{"class":577},"$WORKSPACE",[187,16611,677],{"class":196},[187,16613,16614],{"class":577},"$TASK\n",[187,16616,16617],{"class":189,"line":2808},[187,16618,316],{"emptyLinePlaceholder":315},[187,16620,16621],{"class":189,"line":2816},[187,16622,16623],{"class":295},"  # save the task definition JSON to a variable\n",[187,16625,16626,16629,16631,16634,16636,16639,16642],{"class":189,"line":2824},[187,16627,16628],{"class":577},"  TASK_DESCRIPTION",[187,16630,595],{"class":573},[187,16632,16633],{"class":577},"$(",[187,16635,12748],{"class":193},[187,16637,16638],{"class":196}," ecs",[187,16640,16641],{"class":196}," describe-task-definition",[187,16643,16644],{"class":588}," \\\n",[187,16646,16647,16650,16653],{"class":189,"line":2834},[187,16648,16649],{"class":588},"    --task-definition",[187,16651,16652],{"class":577}," $TASK_FAMILY ",[187,16654,16655],{"class":588},"\\\n",[187,16657,16658],{"class":189,"line":2845},[187,16659,16660],{"class":577},"  )\n",[187,16662,16663],{"class":189,"line":2856},[187,16664,316],{"emptyLinePlaceholder":315},[187,16666,16667],{"class":189,"line":2867},[187,16668,16669],{"class":295},"  # save container definitions to a file for each task\n",[187,16671,16672,16674,16677,16680,16683,16685],{"class":189,"line":2878},[187,16673,16571],{"class":588},[187,16675,16676],{"class":577}," $TASK_DESCRIPTION ",[187,16678,16679],{"class":573},"|",[187,16681,16682],{"class":193}," jq",[187,16684,1154],{"class":588},[187,16686,16644],{"class":588},[187,16688,16689,16692],{"class":189,"line":2886},[187,16690,16691],{"class":196},"    .taskDefinition.containerDefinitions",[187,16693,16644],{"class":588},[187,16695,16696,16699,16702,16705],{"class":189,"line":2900},[187,16697,16698],{"class":573},"    >",[187,16700,16701],{"class":196}," /tmp/",[187,16703,16704],{"class":577},"$TASK_FAMILY",[187,16706,16707],{"class":196},".json\n",[187,16709,16710],{"class":189,"line":2905},[187,16711,316],{"emptyLinePlaceholder":315},[187,16713,16714],{"class":189,"line":2913},[187,16715,16716],{"class":295},"  # write new container definition JSON with updated image\n",[187,16718,16719,16721,16724,16726],{"class":189,"line":2921},[187,16720,16571],{"class":588},[187,16722,16723],{"class":196}," \"Writing new ",[187,16725,16704],{"class":577},[187,16727,16728],{"class":196}," container definitions JSON...\"\n",[187,16730,16731],{"class":189,"line":2931},[187,16732,316],{"emptyLinePlaceholder":315},[187,16734,16735],{"class":189,"line":2942},[187,16736,16737],{"class":295},"  # replace old image URI with new image URI in a new container definitions JSON\n",[187,16739,16740,16743,16745,16747,16750],{"class":189,"line":2953},[187,16741,16742],{"class":193},"  cat",[187,16744,16701],{"class":196},[187,16746,16704],{"class":577},[187,16748,16749],{"class":196},".json",[187,16751,16644],{"class":588},[187,16753,16754,16757,16759],{"class":189,"line":2964},[187,16755,16756],{"class":573},"    |",[187,16758,16682],{"class":193},[187,16760,16644],{"class":588},[187,16762,16763,16766,16769,16772,16775,16777,16780],{"class":189,"line":2975},[187,16764,16765],{"class":588},"    --arg",[187,16767,16768],{"class":196}," IMAGE",[187,16770,16771],{"class":196}," \"",[187,16773,16774],{"class":577},"$NEW_BACKEND_IMAGE_URI",[187,16776,16508],{"class":196},[187,16778,16779],{"class":196}," '.[0].image |= $IMAGE'",[187,16781,16644],{"class":588},[187,16783,16784,16786,16788,16790],{"class":189,"line":2983},[187,16785,16698],{"class":573},[187,16787,16701],{"class":196},[187,16789,16704],{"class":577},[187,16791,16792],{"class":196},"-new.json\n",[187,16794,16795],{"class":189,"line":2992},[187,16796,316],{"emptyLinePlaceholder":315},[187,16798,16799],{"class":189,"line":3001},[187,16800,16801],{"class":295},"  # Get the existing configuration for the task definition (memory, cpu, etc.)\n",[187,16803,16804],{"class":189,"line":3010},[187,16805,16806],{"class":295},"  # from the variable that we saved the task definition JSON to earlier\n",[187,16808,16809,16811,16814,16816],{"class":189,"line":3019},[187,16810,16571],{"class":588},[187,16812,16813],{"class":196}," \"Getting existing configuration for ",[187,16815,16704],{"class":577},[187,16817,16818],{"class":196},"...\"\n",[187,16820,16821],{"class":189,"line":3028},[187,16822,316],{"emptyLinePlaceholder":315},[187,16824,16825,16828,16830,16833,16835,16837,16839,16841,16843],{"class":189,"line":3033},[187,16826,16827],{"class":577},"  MEMORY",[187,16829,595],{"class":573},[187,16831,16832],{"class":577},"$( ",[187,16834,16486],{"class":588},[187,16836,16676],{"class":577},[187,16838,16679],{"class":573},[187,16840,16682],{"class":193},[187,16842,1154],{"class":588},[187,16844,16644],{"class":588},[187,16846,16847,16850],{"class":189,"line":3041},[187,16848,16849],{"class":196},"    .taskDefinition.memory",[187,16851,16644],{"class":588},[187,16853,16854],{"class":189,"line":3049},[187,16855,16660],{"class":577},[187,16857,16858],{"class":189,"line":3059},[187,16859,316],{"emptyLinePlaceholder":315},[187,16861,16862,16865,16867,16869,16871,16873,16875,16877,16879],{"class":189,"line":3070},[187,16863,16864],{"class":577},"  CPU",[187,16866,595],{"class":573},[187,16868,16832],{"class":577},[187,16870,16486],{"class":588},[187,16872,16676],{"class":577},[187,16874,16679],{"class":573},[187,16876,16682],{"class":193},[187,16878,1154],{"class":588},[187,16880,16644],{"class":588},[187,16882,16883,16886],{"class":189,"line":3075},[187,16884,16885],{"class":196},"    .taskDefinition.cpu",[187,16887,16644],{"class":588},[187,16889,16890],{"class":189,"line":3083},[187,16891,16660],{"class":577},[187,16893,16894],{"class":189,"line":3091},[187,16895,316],{"emptyLinePlaceholder":315},[187,16897,16898,16901,16903,16905,16907,16909,16911,16913,16915],{"class":189,"line":3101},[187,16899,16900],{"class":577},"  ECS_EXECUTION_ROLE_ARN",[187,16902,595],{"class":573},[187,16904,16832],{"class":577},[187,16906,16486],{"class":588},[187,16908,16676],{"class":577},[187,16910,16679],{"class":573},[187,16912,16682],{"class":193},[187,16914,1154],{"class":588},[187,16916,16644],{"class":588},[187,16918,16919,16922],{"class":189,"line":3111},[187,16920,16921],{"class":196},"    .taskDefinition.executionRoleArn",[187,16923,16644],{"class":588},[187,16925,16926],{"class":189,"line":3122},[187,16927,16660],{"class":577},[187,16929,16930],{"class":189,"line":3132},[187,16931,316],{"emptyLinePlaceholder":315},[187,16933,16934,16937,16939,16941,16943,16945,16947,16949,16951],{"class":189,"line":3143},[187,16935,16936],{"class":577},"  ECS_TASK_ROLE_ARN",[187,16938,595],{"class":573},[187,16940,16832],{"class":577},[187,16942,16486],{"class":588},[187,16944,16676],{"class":577},[187,16946,16679],{"class":573},[187,16948,16682],{"class":193},[187,16950,1154],{"class":588},[187,16952,16644],{"class":588},[187,16954,16955,16958],{"class":189,"line":3151},[187,16956,16957],{"class":196},"    .taskDefinition.taskRoleArn",[187,16959,16644],{"class":588},[187,16961,16962],{"class":189,"line":3161},[187,16963,16660],{"class":577},[187,16965,16966],{"class":189,"line":3170},[187,16967,316],{"emptyLinePlaceholder":315},[187,16969,16970],{"class":189,"line":3178},[187,16971,16972],{"class":295},"  # check the content of the new container definition JSON\n",[187,16974,16975,16977,16979,16981],{"class":189,"line":3185},[187,16976,16742],{"class":193},[187,16978,16701],{"class":196},[187,16980,16704],{"class":577},[187,16982,16792],{"class":196},[187,16984,16985],{"class":189,"line":3195},[187,16986,316],{"emptyLinePlaceholder":315},[187,16988,16989],{"class":189,"line":3205},[187,16990,16991],{"class":295},"  # register new task definition using the new container definitions\n",[187,16993,16994],{"class":189,"line":3210},[187,16995,16996],{"class":295},"  # and the values that we read off of the existing task definitions\n",[187,16998,16999,17001,17004,17006],{"class":189,"line":3216},[187,17000,16571],{"class":588},[187,17002,17003],{"class":196}," \"Registering new ",[187,17005,16704],{"class":577},[187,17007,16580],{"class":196},[187,17009,17010],{"class":189,"line":3224},[187,17011,316],{"emptyLinePlaceholder":315},[187,17013,17014,17017,17019,17022],{"class":189,"line":3234},[187,17015,17016],{"class":193},"  aws",[187,17018,16638],{"class":196},[187,17020,17021],{"class":196}," register-task-definition",[187,17023,16644],{"class":588},[187,17025,17026,17029,17031],{"class":189,"line":3242},[187,17027,17028],{"class":588},"    --family",[187,17030,16652],{"class":577},[187,17032,16655],{"class":588},[187,17034,17035,17038,17041,17043,17046],{"class":189,"line":3252},[187,17036,17037],{"class":588},"    --container-definitions",[187,17039,17040],{"class":196}," file:///tmp/",[187,17042,16704],{"class":577},[187,17044,17045],{"class":196},"-new.json",[187,17047,16644],{"class":588},[187,17049,17050,17053,17056],{"class":189,"line":3260},[187,17051,17052],{"class":588},"    --memory",[187,17054,17055],{"class":577}," $MEMORY ",[187,17057,16655],{"class":588},[187,17059,17060,17063,17066],{"class":189,"line":3270},[187,17061,17062],{"class":588},"    --cpu",[187,17064,17065],{"class":577}," $CPU ",[187,17067,16655],{"class":588},[187,17069,17070,17073,17076],{"class":189,"line":3275},[187,17071,17072],{"class":588},"    --network-mode",[187,17074,17075],{"class":196}," awsvpc",[187,17077,16644],{"class":588},[187,17079,17080,17083,17086],{"class":189,"line":3283},[187,17081,17082],{"class":588},"    --execution-role-arn",[187,17084,17085],{"class":577}," $ECS_EXECUTION_ROLE_ARN ",[187,17087,16655],{"class":588},[187,17089,17090,17093,17096],{"class":189,"line":3291},[187,17091,17092],{"class":588},"    --task-role-arn",[187,17094,17095],{"class":577}," $ECS_TASK_ROLE_ARN ",[187,17097,16655],{"class":588},[187,17099,17100,17103],{"class":189,"line":3300},[187,17101,17102],{"class":588},"    --requires-compatibilities",[187,17104,17105],{"class":196}," \"FARGATE\"\n",[187,17107,17108],{"class":189,"line":3310},[187,17109,316],{"emptyLinePlaceholder":315},[187,17111,17112],{"class":189,"line":3320},[187,17113,17114],{"class":573},"done\n",[187,17116,17117],{"class":189,"line":3325},[187,17118,316],{"emptyLinePlaceholder":315},[187,17120,17121],{"class":189,"line":3333},[187,17122,17123],{"class":295},"# Now we need to run migrate, collectstatic and any other commands that need to be run\n",[187,17125,17126],{"class":189,"line":3343},[187,17127,17128],{"class":295},"# before doing a rolling update of the backend services\n",[187,17130,17131],{"class":189,"line":3354},[187,17132,316],{"emptyLinePlaceholder":315},[187,17134,17136],{"class":189,"line":17135},94,[187,17137,17138],{"class":295},"# We will use the new task definitions we just created to run these commands\n",[187,17140,17142],{"class":189,"line":17141},95,[187,17143,316],{"emptyLinePlaceholder":315},[187,17145,17147],{"class":189,"line":17146},96,[187,17148,17149],{"class":295},"# get the ARN of the most recent revision of the migrate task definition\n",[187,17151,17153,17156,17158,17160],{"class":189,"line":17152},97,[187,17154,17155],{"class":577},"TASK_DEFINITION",[187,17157,595],{"class":573},[187,17159,16832],{"class":577},[187,17161,16655],{"class":588},[187,17163,17165,17167,17169,17171],{"class":189,"line":17164},98,[187,17166,17016],{"class":193},[187,17168,16638],{"class":196},[187,17170,16641],{"class":196},[187,17172,16644],{"class":588},[187,17174,17176,17178,17181,17184],{"class":189,"line":17175},99,[187,17177,16649],{"class":588},[187,17179,17180],{"class":577}," $WORKSPACE",[187,17182,17183],{"class":196},"-migrate",[187,17185,16644],{"class":588},[187,17187,17189,17191,17193,17195],{"class":189,"line":17188},100,[187,17190,16756],{"class":573},[187,17192,16682],{"class":193},[187,17194,1154],{"class":588},[187,17196,16644],{"class":588},[187,17198,17200,17203],{"class":189,"line":17199},101,[187,17201,17202],{"class":196},"    .taskDefinition.taskDefinitionArn",[187,17204,16644],{"class":588},[187,17206,17208],{"class":189,"line":17207},102,[187,17209,621],{"class":577},[187,17211,17213],{"class":189,"line":17212},103,[187,17214,316],{"emptyLinePlaceholder":315},[187,17216,17218],{"class":189,"line":17217},104,[187,17219,17220],{"class":295},"# get private subnets as space separated string from shared resources VPC\n",[187,17222,17224,17227,17229,17231],{"class":189,"line":17223},105,[187,17225,17226],{"class":577},"SUBNETS",[187,17228,595],{"class":573},[187,17230,16832],{"class":577},[187,17232,16655],{"class":588},[187,17234,17236,17238,17241,17244],{"class":189,"line":17235},106,[187,17237,17016],{"class":193},[187,17239,17240],{"class":196}," ec2",[187,17242,17243],{"class":196}," describe-subnets",[187,17245,16644],{"class":588},[187,17247,17249,17252,17255,17258,17260,17263],{"class":189,"line":17248},107,[187,17250,17251],{"class":588},"    --filters",[187,17253,17254],{"class":196}," \"Name=tag:env,Values=",[187,17256,17257],{"class":577},"$SHARED_RESOURCES_WORKSPACE",[187,17259,16508],{"class":196},[187,17261,17262],{"class":196}," \"Name=tag:Name,Values=*private*\"",[187,17264,16644],{"class":588},[187,17266,17268,17271,17274],{"class":189,"line":17267},108,[187,17269,17270],{"class":588},"    --query",[187,17272,17273],{"class":196}," 'Subnets[*].SubnetId'",[187,17275,16644],{"class":588},[187,17277,17279,17282,17285],{"class":189,"line":17278},109,[187,17280,17281],{"class":588},"    --output",[187,17283,17284],{"class":196}," text",[187,17286,16644],{"class":588},[187,17288,17289],{"class":189,"line":11081},[187,17290,621],{"class":577},[187,17292,17294],{"class":189,"line":17293},111,[187,17295,316],{"emptyLinePlaceholder":315},[187,17297,17299],{"class":189,"line":17298},112,[187,17300,17301],{"class":295},"# replace spaces with commas using tr\n",[187,17303,17305,17308,17310,17312,17314,17317,17319,17322,17325,17328],{"class":189,"line":17304},113,[187,17306,17307],{"class":577},"SUBNET_IDS",[187,17309,595],{"class":573},[187,17311,16633],{"class":577},[187,17313,16486],{"class":588},[187,17315,17316],{"class":577}," $SUBNETS ",[187,17318,16679],{"class":573},[187,17320,17321],{"class":193}," tr",[187,17323,17324],{"class":196}," ' '",[187,17326,17327],{"class":196}," ','",[187,17329,621],{"class":577},[187,17331,17333],{"class":189,"line":17332},114,[187,17334,316],{"emptyLinePlaceholder":315},[187,17336,17338],{"class":189,"line":17337},115,[187,17339,17340],{"class":295},"# https://github.com/aws/aws-cli/issues/5348\n",[187,17342,17344],{"class":189,"line":17343},116,[187,17345,17346],{"class":295},"# get ecs_sg_id - just a single value\n",[187,17348,17350,17353,17355,17357],{"class":189,"line":17349},117,[187,17351,17352],{"class":577},"ECS_SG_ID",[187,17354,595],{"class":573},[187,17356,16832],{"class":577},[187,17358,16655],{"class":588},[187,17360,17362,17364,17366,17369],{"class":189,"line":17361},118,[187,17363,17016],{"class":193},[187,17365,17240],{"class":196},[187,17367,17368],{"class":196}," describe-security-groups",[187,17370,16644],{"class":588},[187,17372,17374,17376,17379,17381,17384],{"class":189,"line":17373},119,[187,17375,17251],{"class":588},[187,17377,17378],{"class":196}," \"Name=tag:Name,Values=",[187,17380,17257],{"class":577},[187,17382,17383],{"class":196},"-ecs-sg\"",[187,17385,16644],{"class":588},[187,17387,17389,17391,17394],{"class":189,"line":17388},120,[187,17390,17270],{"class":588},[187,17392,17393],{"class":196}," 'SecurityGroups[*].GroupId'",[187,17395,16644],{"class":588},[187,17397,17399,17401,17403],{"class":189,"line":17398},121,[187,17400,17281],{"class":588},[187,17402,17284],{"class":196},[187,17404,16644],{"class":588},[187,17406,17408],{"class":189,"line":17407},122,[187,17409,621],{"class":577},[187,17411,17413],{"class":189,"line":17412},123,[187,17414,316],{"emptyLinePlaceholder":315},[187,17416,17418,17420],{"class":189,"line":17417},124,[187,17419,16486],{"class":588},[187,17421,17422],{"class":196}," \"Running database migrations...\"\n",[187,17424,17426],{"class":189,"line":17425},125,[187,17427,316],{"emptyLinePlaceholder":315},[187,17429,17431],{"class":189,"line":17430},126,[187,17432,17433],{"class":295},"# timestamp used for log retrieval (milliseconds after Jan 1, 1970 00:00:00 UTC)\n",[187,17435,17437,17440,17442,17444,17446,17449],{"class":189,"line":17436},127,[187,17438,17439],{"class":577},"START_TIME",[187,17441,595],{"class":573},[187,17443,16633],{"class":577},[187,17445,6550],{"class":193},[187,17447,17448],{"class":196}," +%s000",[187,17450,621],{"class":577},[187,17452,17454],{"class":189,"line":17453},128,[187,17455,316],{"emptyLinePlaceholder":315},[187,17457,17459],{"class":189,"line":17458},129,[187,17460,17461],{"class":295},"# run the migration task and capture the taskArn into a variable called TASK_ID\n",[187,17463,17465,17468,17470,17472],{"class":189,"line":17464},130,[187,17466,17467],{"class":577},"TASK_ID",[187,17469,595],{"class":573},[187,17471,16832],{"class":577},[187,17473,16655],{"class":588},[187,17475,17477,17479,17481,17484],{"class":189,"line":17476},131,[187,17478,17016],{"class":193},[187,17480,16638],{"class":196},[187,17482,17483],{"class":196}," run-task",[187,17485,16644],{"class":588},[187,17487,17489,17492,17494,17497],{"class":189,"line":17488},132,[187,17490,17491],{"class":588},"    --cluster",[187,17493,17180],{"class":577},[187,17495,17496],{"class":196},"-cluster",[187,17498,16644],{"class":588},[187,17500,17502,17504,17507],{"class":189,"line":17501},133,[187,17503,16649],{"class":588},[187,17505,17506],{"class":577}," $TASK_DEFINITION ",[187,17508,16655],{"class":588},[187,17510,17512,17515,17518,17521,17524,17527,17530],{"class":189,"line":17511},134,[187,17513,17514],{"class":588},"    --network-configuration",[187,17516,17517],{"class":196}," \"awsvpcConfiguration={subnets=[",[187,17519,17520],{"class":577},"$SUBNET_IDS",[187,17522,17523],{"class":196},"],securityGroups=[",[187,17525,17526],{"class":577},"$ECS_SG_ID",[187,17528,17529],{"class":196},"],assignPublicIp=ENABLED}\"",[187,17531,16644],{"class":588},[187,17533,17535,17537,17539,17541,17544],{"class":189,"line":17534},135,[187,17536,16756],{"class":573},[187,17538,16682],{"class":193},[187,17540,1154],{"class":588},[187,17542,17543],{"class":196}," '.tasks[0].taskArn'",[187,17545,16644],{"class":588},[187,17547,17549],{"class":189,"line":17548},136,[187,17550,16660],{"class":577},[187,17552,17554],{"class":189,"line":17553},137,[187,17555,316],{"emptyLinePlaceholder":315},[187,17557,17559,17561,17564,17567],{"class":189,"line":17558},138,[187,17560,16486],{"class":588},[187,17562,17563],{"class":196}," \"Task ID is ",[187,17565,17566],{"class":577},"$TASK_ID",[187,17568,16520],{"class":196},[187,17570,17572],{"class":189,"line":17571},139,[187,17573,316],{"emptyLinePlaceholder":315},[187,17575,17577],{"class":189,"line":17576},140,[187,17578,17579],{"class":295},"# wait for the migrate task to exit\n",[187,17581,17583],{"class":189,"line":17582},141,[187,17584,17585],{"class":295},"# https://docs.aws.amazon.com/cli/latest/reference/ecs/wait/tasks-stopped.html#description\n",[187,17587,17589],{"class":189,"line":17588},142,[187,17590,17591],{"class":295},"# > It will poll every 6 seconds until a successful state has been reached.\n",[187,17593,17595],{"class":189,"line":17594},143,[187,17596,17597],{"class":295},"# > This will exit with a return code of 255 after 100 failed checks.\n",[187,17599,17601,17603,17605,17608,17611],{"class":189,"line":17600},144,[187,17602,12748],{"class":193},[187,17604,16638],{"class":196},[187,17606,17607],{"class":196}," wait",[187,17609,17610],{"class":196}," tasks-stopped",[187,17612,16644],{"class":588},[187,17614,17616,17619,17622],{"class":189,"line":17615},145,[187,17617,17618],{"class":588},"  --tasks",[187,17620,17621],{"class":577}," $TASK_ID ",[187,17623,16655],{"class":588},[187,17625,17627,17630,17632],{"class":189,"line":17626},146,[187,17628,17629],{"class":588},"  --cluster",[187,17631,17180],{"class":577},[187,17633,17634],{"class":196},"-cluster\n",[187,17636,17638],{"class":189,"line":17637},147,[187,17639,316],{"emptyLinePlaceholder":315},[187,17641,17643],{"class":189,"line":17642},148,[187,17644,17433],{"class":295},[187,17646,17648,17651,17653,17655,17657,17659],{"class":189,"line":17647},149,[187,17649,17650],{"class":577},"END_TIME",[187,17652,595],{"class":573},[187,17654,16633],{"class":577},[187,17656,6550],{"class":193},[187,17658,17448],{"class":196},[187,17660,621],{"class":577},[187,17662,17664],{"class":189,"line":17663},150,[187,17665,316],{"emptyLinePlaceholder":315},[187,17667,17669],{"class":189,"line":17668},151,[187,17670,17671],{"class":295},"# print the CloudWatch log events to STDOUT\n",[187,17673,17675,17677,17680,17683],{"class":189,"line":17674},152,[187,17676,12748],{"class":193},[187,17678,17679],{"class":196}," logs",[187,17681,17682],{"class":196}," get-log-events",[187,17684,16644],{"class":588},[187,17686,17688,17691,17694,17696,17699],{"class":189,"line":17687},153,[187,17689,17690],{"class":588},"  --log-group-name",[187,17692,17693],{"class":196}," \"/ecs/",[187,17695,16609],{"class":577},[187,17697,17698],{"class":196},"/migrate\"",[187,17700,16644],{"class":588},[187,17702,17704,17707,17710,17712,17715,17718],{"class":189,"line":17703},154,[187,17705,17706],{"class":588},"  --log-stream-name",[187,17708,17709],{"class":196}," \"migrate/migrate/${",[187,17711,17467],{"class":577},[187,17713,17714],{"class":573},"##*/",[187,17716,17717],{"class":196},"}\"",[187,17719,16644],{"class":588},[187,17721,17723,17726,17729],{"class":189,"line":17722},155,[187,17724,17725],{"class":588},"  --start-time",[187,17727,17728],{"class":577}," $START_TIME ",[187,17730,16655],{"class":588},[187,17732,17734,17737,17740],{"class":189,"line":17733},156,[187,17735,17736],{"class":588},"  --end-time",[187,17738,17739],{"class":577}," $END_TIME ",[187,17741,16655],{"class":588},[187,17743,17745,17748,17750,17752],{"class":189,"line":17744},157,[187,17746,17747],{"class":573},"  |",[187,17749,16682],{"class":193},[187,17751,1154],{"class":588},[187,17753,17754],{"class":196}," '.events[].message'\n",[187,17756,17758],{"class":189,"line":17757},158,[187,17759,316],{"emptyLinePlaceholder":315},[187,17761,17763,17765],{"class":189,"line":17762},159,[187,17764,16486],{"class":588},[187,17766,17767],{"class":196}," \"Migrations complete. Starting rolling update for backend services...\"\n",[187,17769,17771],{"class":189,"line":17770},160,[187,17772,316],{"emptyLinePlaceholder":315},[187,17774,17776],{"class":189,"line":17775},161,[187,17777,17778],{"class":295},"# update backend services\n",[187,17780,17782,17784,17786,17788,17790,17792],{"class":189,"line":17781},162,[187,17783,16543],{"class":573},[187,17785,16546],{"class":577},[187,17787,16549],{"class":573},[187,17789,16555],{"class":196},[187,17791,16558],{"class":196},[187,17793,16561],{"class":196},[187,17795,17797],{"class":189,"line":17796},163,[187,17798,16566],{"class":573},[187,17800,17802],{"class":189,"line":17801},164,[187,17803,316],{"emptyLinePlaceholder":315},[187,17805,17807],{"class":189,"line":17806},165,[187,17808,17809],{"class":295},"  # get taskDefinitionArn for each service to be used in update-service command\n",[187,17811,17813],{"class":189,"line":17812},166,[187,17814,17815],{"class":295},"  # this will get the most recent revision of each task (the one that was just created)\n",[187,17817,17819],{"class":189,"line":17818},167,[187,17820,17821],{"class":295},"  # https://docs.aws.amazon.com/cli/latest/reference/ecs/describe-task-definition.html#description\n",[187,17823,17825,17828,17830,17832],{"class":189,"line":17824},168,[187,17826,17827],{"class":577},"  TASK_DEFINITION",[187,17829,595],{"class":573},[187,17831,16832],{"class":577},[187,17833,16655],{"class":588},[187,17835,17837,17840,17842,17844],{"class":189,"line":17836},169,[187,17838,17839],{"class":193},"    aws",[187,17841,16638],{"class":196},[187,17843,16641],{"class":196},[187,17845,16644],{"class":588},[187,17847,17849,17852,17854,17856,17859],{"class":189,"line":17848},170,[187,17850,17851],{"class":588},"      --task-definition",[187,17853,17180],{"class":577},[187,17855,677],{"class":196},[187,17857,17858],{"class":577},"$TASK ",[187,17860,16655],{"class":588},[187,17862,17864,17867,17869,17871],{"class":189,"line":17863},171,[187,17865,17866],{"class":573},"      |",[187,17868,16682],{"class":193},[187,17870,1154],{"class":588},[187,17872,16644],{"class":588},[187,17874,17876,17879],{"class":189,"line":17875},172,[187,17877,17878],{"class":196},"      .taskDefinition.taskDefinitionArn",[187,17880,16644],{"class":588},[187,17882,17884],{"class":189,"line":17883},173,[187,17885,16660],{"class":577},[187,17887,17889],{"class":189,"line":17888},174,[187,17890,316],{"emptyLinePlaceholder":315},[187,17892,17894],{"class":189,"line":17893},175,[187,17895,17896],{"class":295},"  # update each service with new task definintion\n",[187,17898,17900,17902,17904,17907],{"class":189,"line":17899},176,[187,17901,17016],{"class":193},[187,17903,16638],{"class":196},[187,17905,17906],{"class":196}," update-service",[187,17908,16644],{"class":588},[187,17910,17912,17914,17916,17918],{"class":189,"line":17911},177,[187,17913,17491],{"class":588},[187,17915,17180],{"class":577},[187,17917,17496],{"class":196},[187,17919,16644],{"class":588},[187,17921,17923,17926,17928,17930,17932],{"class":189,"line":17922},178,[187,17924,17925],{"class":588},"    --service",[187,17927,17180],{"class":577},[187,17929,677],{"class":196},[187,17931,17858],{"class":577},[187,17933,16655],{"class":588},[187,17935,17937,17939,17941],{"class":189,"line":17936},179,[187,17938,16649],{"class":588},[187,17940,17506],{"class":577},[187,17942,16655],{"class":588},[187,17944,17946],{"class":189,"line":17945},180,[187,17947,17948],{"class":588},"    --no-cli-pager\n",[187,17950,17952],{"class":189,"line":17951},181,[187,17953,316],{"emptyLinePlaceholder":315},[187,17955,17957],{"class":189,"line":17956},182,[187,17958,17114],{"class":573},[187,17960,17962],{"class":189,"line":17961},183,[187,17963,316],{"emptyLinePlaceholder":315},[187,17965,17967,17969],{"class":189,"line":17966},184,[187,17968,16486],{"class":588},[187,17970,17971],{"class":196}," \"Services updated. Waiting for services to become stable...\"\n",[187,17973,17975],{"class":189,"line":17974},185,[187,17976,316],{"emptyLinePlaceholder":315},[187,17978,17980],{"class":189,"line":17979},186,[187,17981,17982],{"class":295},"# wait for all service to be stable (runningCount == desiredCount for each service)\n",[187,17984,17986,17988,17990,17992,17995],{"class":189,"line":17985},187,[187,17987,12748],{"class":193},[187,17989,16638],{"class":196},[187,17991,17607],{"class":196},[187,17993,17994],{"class":196}," services-stable",[187,17996,16644],{"class":588},[187,17998,18000,18002,18004,18006],{"class":189,"line":17999},188,[187,18001,17629],{"class":588},[187,18003,17180],{"class":577},[187,18005,17496],{"class":196},[187,18007,16644],{"class":588},[187,18009,18011,18014,18016,18019,18021,18024,18026],{"class":189,"line":18010},189,[187,18012,18013],{"class":588},"  --services",[187,18015,17180],{"class":577},[187,18017,18018],{"class":196},"-gunicorn",[187,18020,17180],{"class":577},[187,18022,18023],{"class":196},"-default",[187,18025,17180],{"class":577},[187,18027,18028],{"class":196},"-beat\n",[187,18030,18032],{"class":189,"line":18031},190,[187,18033,316],{"emptyLinePlaceholder":315},[187,18035,18037,18039,18042,18044],{"class":189,"line":18036},191,[187,18038,16486],{"class":588},[187,18040,18041],{"class":196}," \"Services are now stable. Backend services are now up to date with ",[187,18043,16517],{"class":577},[187,18045,18046],{"class":196},".\"\n",[187,18048,18050],{"class":189,"line":18049},192,[187,18051,316],{"emptyLinePlaceholder":315},[187,18053,18055,18057],{"class":189,"line":18054},193,[187,18056,16486],{"class":588},[187,18058,18059],{"class":196}," \"Backend update is now complete!\"\n",[11,18061,18062,18063,18066,18067,18069],{},"With this GitHub Actions workflow, a developer can now easily change the version of backend code that is running in their ad hoc environments without needing to involve Terraform. Using the ",[33,18064,18065],{},"ad_hoc_backend_update.yml"," GitHub Actions workflow, a developer only needs to enter the name of the workspace and the version of the backend code they want to use. The workflow will then run the ",[33,18068,13074],{}," task and update the backend services.",[911,18071,18073,18074,18077],{"id":18072},"using-ignore_changes-in-the-definitions","Using ",[33,18075,18076],{},"ignore_changes"," in the definitions",[11,18079,18080,18081,18083,18084,18087,18088,18091,18092,18097,18098,765,18100,18102],{},"There is one more important point to make about Terraform before we conclude this discussion on updating backend code for existing ad hoc environments. Consider the scenario where a developer has launched an ad hoc environment with backend version ",[33,18082,16260],{},". They make a small change to the backend code and push version ",[33,18085,18086],{},"v1.0.1",". Next, the developer remembers that a backend environment variable needs to be changed. This can be done by updating their ",[33,18089,18090],{},"*.tfvars"," file. If they now re-run the ad hoc environment update pipeline ",[338,18093,18094,18095,1907],{},"without also updating the backend version in their ",[33,18096,18090],{},", then their code will be reverted from ",[33,18099,18086],{},[33,18101,16260],{},". We would need to coordinate version changes between updating the backend with the pipelines that use Terraform commands and the pipelines that use the AWS CLI commands.",[11,18104,18105,18106,18109,18110,18116,18117,18120,18121,18123,18124,18126,18127,18129,18130,18132],{},"There is a setting on the ",[33,18107,18108],{},"aws_ecs_service"," resource in Terraform we can can use to prevent this from happening. This setting is called ",[15,18111,18114],{"href":18112,"rel":18113},"https://www.terraform.io/language/meta-arguments/lifecycle",[19],[33,18115,18076],{}," and is defined under the resource's ",[33,18118,18119],{},"lifecycle"," configuration block. With this setting, when we update the ",[33,18122,18090],{}," file with our new environment variable value, we will create another recent task definition with the same ",[33,18125,16260],{}," image, but the ECS service will not update in response to this change (that's what the ",[33,18128,18076],{}," is for). Once we make the ",[33,18131,18090],{}," file update and redeploy using the Terraform pipeline, nothing on our ad hoc changes, but we did get a new task definitions defined in our AWS account for each backend service. When we go to make the backend update with the pipeline that uses AWS CLI commands, the most recent task revision is used to create the new task definition, so it will include the environment variable change that we added earlier.",[911,18134,18136],{"id":18135},"frontend-updates","Frontend updates",[11,18138,18139,18140,18142],{},"The process described above is needed for updating the backend application. Updating the frontend application involves a similar process to the backend update. The main difference is that no task (such as the ",[33,18141,13074],{}," command on the backend) needs to run before the service is updated. Here's an overview of the frontend update process:",[916,18144,18145,18150,18155,18160,18165],{},[919,18146,18147,18148,343],{},"fetch the container definition JSON for the frontend tasks (",[33,18149,16357],{},[919,18151,18152,18153,343],{},"write new container definition JSON with the new frontend image tag (using ",[33,18154,16363],{},[919,18156,18157,18158,343],{},"register new task definitions with the new container definition JSON file for the frontend task (",[33,18159,16369],{},[919,18161,18162,18163,343],{},"update the frontend service (",[33,18164,16386],{},[919,18166,16389,18167,343],{},[33,18168,16392],{},[168,18170,18172],{"id":18171},"setting-up-everything-from-a-new-aws-account-and-github-actions","Setting up everything from a new AWS account and GitHub Actions",[11,18174,18175],{},"Here's a quick overview of initial setup steps that are needed in order to use the automation defined in the GitHub Actions for ad hoc environments.",[911,18177,18179],{"id":18178},"configure-aws-credentials-locally","Configure AWS credentials locally",[11,18181,18182],{},"There is one Terraform command that we will run on our local machine to setup a remote backend for storing our Terraform state. In order to run this, we need to configure AWS credentials locally.",[911,18184,1111,18186,18189],{"id":18185},"run-the-make-tf-bootstrap-command",[33,18187,18188],{},"make tf-bootstrap"," command",[11,18191,18192,18193,18196],{},"This command will setup an S3 bucket and DynamoDB table for storing Terraform state. Running this command will also require that a ",[33,18194,18195],{},"bootstrap.tfvars"," file has been created from the template. This will define the AWS region and name to be used for creating resources.",[911,18198,18200],{"id":18199},"build-and-push-a-backend-and-frontend-image","Build and push a backend and frontend image",[11,18202,6131,18203,18206,18207,1172,18210,18213],{},[33,18204,18205],{},"tf-bootstrap"," command also creates ECR repositories for the backend and frontend images. We can use the ",[33,18208,18209],{},"ecr_backend.yml",[33,18211,18212],{},"ecr_frontend.yml"," GitHub Actions workflows to build and push the backend and frontend images to the ECR repositories. These pipelines accept a single parameter which is a git tag that must exist in the repo. This git tag will then be used as the image tag for the backend and frontend images.",[911,18215,18217],{"id":18216},"purchase-a-domain-name-in-route-53","Purchase a domain name in Route 53",[11,18219,18220],{},"I use Route 53 to manage DNS records, and have purchased a domain name that I use for testing and debugging in this and other projects.",[911,18222,18224],{"id":18223},"create-a-wildcard-acm-certificate","Create a wildcard ACM certificate",[11,18226,18227],{},"I chose to create this certificate outside of Terraform and import it via ARN. We need a wildcard certificate so that multiple ad hoc environments can be hosted on subdomains of the same domain.",[911,18229,18231],{"id":18230},"create-a-new-ec2-key-pair","Create a new EC2 key pair",[11,18233,18234],{},"The key pair should be created manually and it needs to be added to GitHub repository secrets so that it can be used in the ad hoc environment pipelines.",[911,18236,18238],{"id":18237},"add-secrets-to-github","Add secrets to GitHub",[11,18240,18241],{},"The following secrets are needed for GitHub Actions to run. Add these as repository secrets:",[916,18243,18244,18250,18256,18262,18271,18277,18283,18289,18295,18301,18307],{},[919,18245,18246,18249],{},[33,18247,18248],{},"ACM_CERTIFICATE_ARN"," - ARN of the wildcard ACM certificate",[919,18251,18252,18255],{},[33,18253,18254],{},"AWS_ACCESS_KEY_ID"," - AWS access key ID",[919,18257,18258,18261],{},[33,18259,18260],{},"AWS_ACCOUNT_ID"," - AWS account ID",[919,18263,18264,18267,18268,343],{},[33,18265,18266],{},"AWS_DEFAULT_REGION"," - AWS default region (I use ",[33,18269,18270],{},"us-east-1",[919,18272,18273,18276],{},[33,18274,18275],{},"AWS_SECRET_ACCESS_KEY"," - AWS secret access key",[919,18278,18279,18282],{},[33,18280,18281],{},"DOMAIN_NAME"," - domain name for the ad hoc environment (e.g. example.com)",[919,18284,18285,18288],{},[33,18286,18287],{},"KEY_NAME"," - name of the EC2 key pair",[919,18290,18291,18294],{},[33,18292,18293],{},"SSH_PRIVATE_KEY"," - private key for the EC2 key pair",[919,18296,18297,18300],{},[33,18298,18299],{},"TF_BACKEND_BUCKET"," - name of the S3 bucket used for storing Terraform state",[919,18302,18303,18306],{},[33,18304,18305],{},"TF_BACKEND_DYNAMODB_TABLE"," - name of the DynamoDB table used for locking the Terraform state file",[919,18308,18309,18312],{},[33,18310,18311],{},"TF_BACKEND_REGION"," - AWS region for the S3 bucket and DynamoDB table",[11,18314,18315],{},"These secrets are referenced in the GitHub Actions workflows.",[911,18317,18319],{"id":18318},"create-shared-resources","Create shared resources",[11,18321,18322,18323,18326,18327,18329,18330,18333,18334,18337],{},"Now that we have GitHub secrets configured, we can run the ",[33,18324,18325],{},"shared_resources_create_update.yml"," GitHub Actions workflow. This will create  shared resources environment in which we can build our ad hoc environments. This workflow requires a name (e.g. ",[33,18328,6303],{},"). This require that we create a ",[33,18331,18332],{},"dev.tfvars"," file in ",[33,18335,18336],{},"terraform/live/shared-resources"," directory. This usually takes between 5 and 7 minutes to complete since it needs to create an RDS instance which takes a few minutes to provision.",[911,18339,18341],{"id":18340},"create-an-ad-hoc-environment","Create an ad hoc environment",[11,18343,18344,18345,18347,18348,18351,18352,18354,18355,18358,18359,18362],{},"We can now create an ad hoc environment. This requires the name of the shared resources environment (e.g. ",[33,18346,6303],{},") and the name of the ad hoc environment (e.g. ",[33,18349,18350],{},"brian-test","). The only thing we need to do before creating the ",[33,18353,18350],{}," ad hoc environment is to create the ",[33,18356,18357],{},"brian-test.tfvars"," file in the ",[33,18360,18361],{},"terraform/live/ad-hoc/envs"," directory. This will define the versions of the application and any other environment configuration that is needed. This pipeline usually takes about 3 minutes to finish.",[911,18364,18366],{"id":18365},"update-the-backend-version-in-an-ad-hoc-environment","Update the backend version in an ad hoc environment",[11,18368,18369,18370,18372],{},"Now that the backend is up and running and usable, we can update the backend version used in our ad hoc environment. This can be done by running the ",[33,18371,18065],{}," GitHub Actions workflow. To run this workflow you must specify:",[916,18374,18375,18380,18385],{},[919,18376,18377,18378,343],{},"the shared resource workspace (e.g. ",[33,18379,6303],{},[919,18381,18382,18383,343],{},"the ad hoc environment name (e.g. ",[33,18384,18350],{},[919,18386,18387,18388,343],{},"the new version of the backend to be used (e.g. ",[33,18389,18390],{},"v1.0.2",[11,18392,18393,18394,18396],{},"The backend version should already have been built and pushed to the ECR repository using the ",[33,18395,18209],{}," GitHub Actions workflow.",[911,18398,18400],{"id":18399},"use-ecs-exec-to-access-a-django-shell-in-a-running-container","Use ECS Exec to access a Django shell in a running container",[11,18402,18403,18404,18407,18408,18410],{},"Instead of SSHing into the container, we can use the ",[33,18405,18406],{},"ecs-exec"," command to access a shell in the container. This is useful for debugging and testing. One of the outputs of the ad hoc environment terraform configuration contains the script needed to run the ",[33,18409,18406],{}," command:",[26,18412,18414],{"className":181,"code":18413,"language":183,"meta":35,"style":35},"TASK_ARN=$(aws ecs list-tasks \\\n  --cluster alpha-cluster \\\n  --service-name  alpha-gunicorn | jq -r '.taskArns | .[0]' \\\n)\naws ecs execute-command --cluster alpha-cluster \\\n    --task $TASK_ARN \\\n    --container gunicorn \\\n    --interactive \\\n    --command \"/bin/bash\"\n",[33,18415,18416,18434,18443,18463,18467,18483,18493,18503,18510],{"__ignoreMap":35},[187,18417,18418,18421,18423,18425,18427,18429,18432],{"class":189,"line":190},[187,18419,18420],{"class":577},"TASK_ARN",[187,18422,595],{"class":573},[187,18424,16633],{"class":577},[187,18426,12748],{"class":193},[187,18428,16638],{"class":196},[187,18430,18431],{"class":196}," list-tasks",[187,18433,16644],{"class":588},[187,18435,18436,18438,18441],{"class":189,"line":249},[187,18437,17629],{"class":588},[187,18439,18440],{"class":196}," alpha-cluster",[187,18442,16644],{"class":588},[187,18444,18445,18448,18451,18454,18456,18458,18461],{"class":189,"line":312},[187,18446,18447],{"class":588},"  --service-name",[187,18449,18450],{"class":196},"  alpha-gunicorn",[187,18452,18453],{"class":573}," |",[187,18455,16682],{"class":193},[187,18457,1154],{"class":588},[187,18459,18460],{"class":196}," '.taskArns | .[0]'",[187,18462,16644],{"class":588},[187,18464,18465],{"class":189,"line":319},[187,18466,621],{"class":577},[187,18468,18469,18471,18473,18476,18479,18481],{"class":189,"line":325},[187,18470,12748],{"class":193},[187,18472,16638],{"class":196},[187,18474,18475],{"class":196}," execute-command",[187,18477,18478],{"class":588}," --cluster",[187,18480,18440],{"class":196},[187,18482,16644],{"class":588},[187,18484,18485,18488,18491],{"class":189,"line":686},[187,18486,18487],{"class":588},"    --task",[187,18489,18490],{"class":577}," $TASK_ARN ",[187,18492,16655],{"class":588},[187,18494,18495,18498,18501],{"class":189,"line":697},[187,18496,18497],{"class":588},"    --container",[187,18499,18500],{"class":196}," gunicorn",[187,18502,16644],{"class":588},[187,18504,18505,18508],{"class":189,"line":1291},[187,18506,18507],{"class":588},"    --interactive",[187,18509,16644],{"class":588},[187,18511,18512,18515],{"class":189,"line":1306},[187,18513,18514],{"class":588},"    --command",[187,18516,18517],{"class":196}," \"/bin/bash\"\n",[11,18519,18520],{},"You will then have a shell in the container:",[26,18522,18525],{"className":18523,"code":18524,"language":31},[29],"The Session Manager plugin was installed successfully. Use the AWS CLI to start a session.\n\n\nStarting session with SessionId: ecs-execute-command-073d1947fa71c058c\nroot@ip-10-0-2-167:/code# python manage.py shell\nPython 3.9.9 (main, Dec 21 2021, 10:03:34)\n[GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n>>>\n",[33,18526,18524],{"__ignoreMap":35},[911,18528,18530],{"id":18529},"destroy-an-ad-hoc-environment","Destroy an ad hoc environment",[11,18532,18533,18534,18537,18538,18540,18541,1737],{},"Use the ",[33,18535,18536],{},"ad_hoc_env_destroy.yml"," GitHub Actions workflow to destroy an ad hoc environment. To run this workflow you need to specify the shared resource workspace (e.g. ",[33,18539,6303],{},") and the ad hoc environment name (e.g. ",[33,18542,18350],{},[911,18544,18546],{"id":18545},"destroy-the-shared-resources-environment","Destroy the shared resources environment",[11,18548,18533,18549,18552,18553,1737],{},[33,18550,18551],{},"shared_resources_destroy.yml"," GitHub Actions workflow to destroy the shared resources environment. To run this workflow you need to specify the shared resource workspace (e.g. ",[33,18554,6303],{},[11,18556,18557],{},"This defines the full lifecycle of creating and destroying ad hoc environments.",[168,18559,18561],{"id":18560},"future-improvements-open-questions-and-next-steps","Future improvements, open questions and next steps",[911,18563,18565],{"id":18564},"enhanced-security","Enhanced Security",[11,18567,18568],{},"The low hanging fruit here is to use least privilege (for roles used in automation) and to define all roles with IaC. Currently I am using Admin roles for the credentials I store in GitHub which is a shortcut to using IaC and is not a best practice.",[911,18570,18572],{"id":18571},"keeping-track-of-ad-hoc-environments","Keeping track of ad hoc environments",[11,18574,18575],{},"We need to think about how we can keep track of our active ad hoc environments. Active environments will incur additional AWS costs, and we do not want developers or the product team to create lots of environments and then leave them running without actively using them.",[11,18577,18578],{},"We may decide to have some long-lived ad hoc environments, but those would be managed primarily by the DevOps team and respective owners (e.g. QA, product team, etc.).",[11,18580,18581],{},"One way to check the active ad hoc environments would be to use the AWS CLI. We could list the ECS clusters in our development account, and this would show the number of ad hoc environments running. We could go farther and list the ad hoc environments by when they were last updated. We could then request developers or team members to remove ad hoc environments that are not in use.",[11,18583,18584],{},"Or we could have a policy that all ad-hoc environments are deleted automatically at the end of each week.",[911,18586,18588],{"id":18587},"terraform-tooling","Terraform Tooling",[916,18590,18591,18594,18597,18600],{},[919,18592,18593],{},"Testing Terraform Code",[919,18595,18596],{},"Testing GitHub Actions",[919,18598,18599],{},"Using advanced Terraform frameworks like Terragrunt",[919,18601,18602],{},"Using Terraform Cloud for more advanced Terraform features",[911,18604,18606],{"id":18605},"more-secure-way-of-defining-rds-username-and-password","More secure way of defining RDS username and password",[11,18608,18609],{},"Currently the postgres database does not have a secure password. It is both hardcoded in the module as a default value and it will also be saved in plaintext in the Terraform state file.",[911,18611,18613],{"id":18612},"backend-application-update-script","Backend application update script",[11,18615,18616],{},"The script used for updating the backend application could be improved or broken up into multiple scripts to better handle errors and failures that happen during the pipeline. The script runs several different commands and could potentially fail at any step, so it would be nice to improve the error messages so that both developers and DevOps teams can more quickly diagnose pipeline failures.",[911,18618,18620],{"id":18619},"limiting-traffic-to-ad-hoc-environments-to-a-vpn","Limiting traffic to ad hoc environments to a VPN",[11,18622,18623],{},"Another good next step would be to show how we can limit traffic to ad hoc environments to a VPN.",[911,18625,18627],{"id":18626},"figure-out-how-many-ad-hoc-environments-we-can-create-with-the-default-quotas","Figure out how many ad hoc environments we can create with the default quotas",[11,18629,18630],{},"AWS accounts limit the number of resources you can create, but for most of this quota limits you can request an increase per account. I need to figure out how many ad hoc environments I can create with the default quotas.",[911,18632,18634],{"id":18633},"code-repo-organization","Code Repo Organization",[11,18636,18637,18638,18640,18641,18643],{},"One minor improvement would be to move the ",[33,18639,13407],{}," directory out of the ",[33,18642,12114],{}," monorepo into a dedicated repo. We may also want to move GitHub Actions for creating, updating and destroying environments to this new repo. For early stage development, using a single repository that stores both application code and Terraform configuration works, but it would be better to keep these separate at the repository level as the project grows.",[911,18645,18647],{"id":18646},"multiple-aws-accounts","Multiple AWS Accounts",[11,18649,18650],{},"Everything shown here uses a single AWS account: ECR images, Terraform remote state storage, all shared resource environments and all ad hoc environments. Using one account keeps things simple for a demonstration of this workflow, but in practice it would be beneficial to use multiple AWS accounts for different purposes. This would also involve more carefully planned IAM roles for cross-account resource access.",[911,18652,18654],{"id":18653},"modules-for-stable-environments-to-be-used-for-long-lived-pre-production-and-production-environments","Modules for stable environments to be used for long-lived pre-production and production environments",[11,18656,18657],{},"This article looked at how to make tradeoffs between costs, speed of deployment and production parity in ad hoc environments. I'm interested in building a new set of modules that can be used to set up environments that:",[916,18659,18660,18663,18666,18669,18672],{},[919,18661,18662],{},"are more stable and more long-lived",[919,18664,18665],{},"have less resource sharing (dedicated RDS and ElastiCache resources)",[919,18667,18668],{},"implement autoscaling for load-testing (or maybe implement autoscaling for ad hoc environments)",[919,18670,18671],{},"can be used to perform load testing",[919,18673,18674],{},"have enhanced observability tooling",[11,18676,18677],{},"These environments might be used as part of a QA process that does a final sign-off on a new set of features scheduled for deployment to production environments, for example.",[168,18679,2413],{"id":15198},[11,18681,18682],{},"This wraps up the tour of my ad hoc environment infrastructure automation. Thank you for having a read through my article. If you have a similar (or different) approach to building ad hoc environments, I would love to hear about it.",[855,18684,18685],{},"html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":18687},[18688,18689,18690,18691,18692,18699,18700,18704,18713,18717,18721,18728,18744,18756],{"id":8183,"depth":249,"text":8184},{"id":15317,"depth":249,"text":15318},{"id":15390,"depth":249,"text":15391},{"id":15458,"depth":249,"text":15459},{"id":15465,"depth":249,"text":15466,"children":18693},[18694,18695,18696,18697,18698],{"id":15495,"depth":312,"text":15496},{"id":15593,"depth":312,"text":15594},{"id":15638,"depth":312,"text":15639},{"id":15682,"depth":312,"text":15683},{"id":15723,"depth":312,"text":15724},{"id":15776,"depth":249,"text":15777},{"id":15896,"depth":249,"text":15897,"children":18701},[18702,18703],{"id":15914,"depth":312,"text":15915},{"id":15950,"depth":312,"text":15951},{"id":15995,"depth":249,"text":15996,"children":18705},[18706,18707,18708,18709,18710,18711,18712],{"id":12377,"depth":312,"text":12378},{"id":16012,"depth":312,"text":16013},{"id":12415,"depth":312,"text":12330},{"id":16055,"depth":312,"text":16056},{"id":16062,"depth":312,"text":16063},{"id":16069,"depth":312,"text":12333},{"id":12716,"depth":312,"text":12339},{"id":16091,"depth":249,"text":16092,"children":18714},[18715,18716],{"id":12831,"depth":312,"text":12832},{"id":16103,"depth":312,"text":16104},{"id":16142,"depth":249,"text":16143,"children":18718},[18719,18720],{"id":16149,"depth":312,"text":16150},{"id":16211,"depth":312,"text":16212},{"id":16250,"depth":249,"text":16251,"children":18722},[18723,18724,18725,18727],{"id":16317,"depth":312,"text":16318},{"id":16345,"depth":312,"text":16346},{"id":18072,"depth":312,"text":18726},"Using ignore_changes in the definitions",{"id":18135,"depth":312,"text":18136},{"id":18171,"depth":249,"text":18172,"children":18729},[18730,18731,18733,18734,18735,18736,18737,18738,18739,18740,18741,18742,18743],{"id":18178,"depth":312,"text":18179},{"id":18185,"depth":312,"text":18732},"Run the make tf-bootstrap command",{"id":18199,"depth":312,"text":18200},{"id":18216,"depth":312,"text":18217},{"id":18223,"depth":312,"text":18224},{"id":18230,"depth":312,"text":18231},{"id":18237,"depth":312,"text":18238},{"id":18318,"depth":312,"text":18319},{"id":18340,"depth":312,"text":18341},{"id":18365,"depth":312,"text":18366},{"id":18399,"depth":312,"text":18400},{"id":18529,"depth":312,"text":18530},{"id":18545,"depth":312,"text":18546},{"id":18560,"depth":249,"text":18561,"children":18745},[18746,18747,18748,18749,18750,18751,18752,18753,18754,18755],{"id":18564,"depth":312,"text":18565},{"id":18571,"depth":312,"text":18572},{"id":18587,"depth":312,"text":18588},{"id":18605,"depth":312,"text":18606},{"id":18612,"depth":312,"text":18613},{"id":18619,"depth":312,"text":18620},{"id":18626,"depth":312,"text":18627},{"id":18633,"depth":312,"text":18634},{"id":18646,"depth":312,"text":18647},{"id":18653,"depth":312,"text":18654},{"id":15198,"depth":249,"text":2413},"2022-06-11","This article will show how software development teams can build on-demand environments for dog-food testing, quality review, internal and external demos and other use cases that require short-lived but feature-complete instances of a web application.",[18760,18762,18764,18766,18768,18769,18772,18774],{"link":18761,"site":11220},"https://news.ycombinator.com/item?id=31704417",{"link":18763,"site":10051},"https://www.reddit.com/r/aws/comments/v9ycwh/my_approach_to_building_ad_hoc_developer/",{"link":18765,"site":6303},"https://dev.to/briancaffey/setting-up-ad-hoc-development-environments-for-django-applications-with-aws-ecs-terraform-and-github-actions-4abh",{"link":18767,"site":15277},"https://medium.com/@briancaffey/setting-up-ad-hoc-development-environments-for-django-applications-with-aws-ecs-terraform-and-84d26e710539",{"link":15279,"site":15280},{"link":18770,"site":18771},"https://twitter.com/briancaffey/status/1535683865330831360","twitter",{"link":18773,"site":15283},"https://briancaffey.substack.com/p/setting-up-ad-hoc-development-environments",{"link":18775,"site":18776},"https://hackernoon.com/ad-hoc-environments-for-django-applications-with-ecs-terraform-and-github-actions","hackernoon",{},"/2022/03/27/ad-hoc-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions",{"title":15302,"description":18758},"2022/03/27/ad-hoc-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions",[15290,13407,15294,12748,15295,15297,15298],"CPMFzd_ekv2vPmjz73ucpzTesrIbpSk58ispqGjwmjs",{"id":18784,"title":18785,"body":18786,"comments":315,"date":20087,"description":20088,"draft":872,"extension":873,"external":20089,"image":20102,"meta":20103,"navigation":315,"path":20104,"seo":20105,"stem":20106,"tags":20107,"__hash__":20110},"blog/2021/10/31/how-and-why-i-added-adsense-and-adblock-detector-to-my-website.md","How and why I added AdSense and an AdBlock detector to my personal website",{"type":8,"value":18787,"toc":20077},[18788,18791,18804,18807,18811,18814,18834,18837,18840,18843,18867,18870,18876,18881,18985,18988,19006,19009,19012,19047,19050,19054,19057,19061,19070,19074,19077,19085,19090,19098,19106,19115,19120,19125,19136,19141,19146,19149,19154,19161,19620,19627,19640,19645,19655,19833,19838,19951,19956,19961,19965,19968,19979,19990,19993,20016,20022,20025,20028,20031,20042,20045,20049,20052,20058,20066,20074],[11,18789,18790],{},"Update: I disabled the ad block blocker on my personal website. It was an interesting experiment and I learned that ad block detection is a game of cat and mouse. It did not work with uBlock Origin, but it did work with AdBlock. I probably won't be revisiting this topic.",[11,18792,18793,18794,18799,18800,18803],{},"If you are reading this article on ",[15,18795,18798],{"href":18796,"rel":18797},"https://briancaffey.github.io/2021/10/31/how-and-why-i-added-adsense-and-adblock-detector-to-my-website",[19],"briancaffey.github.io/2021/10/31/how-and-why-i-added-adsense-and-adblock-detector-to-my-website",", then you will be prompted to pause your ad blocker if you are using one. If you are ",[4339,18801,18802],{},"not"," using an ad blocker, I recommend that you consider installing one. Reading this article on a browser with AdBlock enabled will allow you to see how I detect AdBlock and ask people to pause it when they are on my site.",[11,18805,18806],{},"This article is a deep dive on how I added ads to my site with Google AdSense and how I request that visitors to my site pause AdBlock so that I can make more money from Google AdSense.",[168,18808,18810],{"id":18809},"how-i-added-adsense-to-my-site","How I added AdSense to my site",[11,18812,18813],{},"I have been enjoying using my GitHub Pages website to learn more about static sites, JAMStack and Nuxt.js, an awesome Vue.js framework with support for building statically generated sites. I have been able to learn and implement several different features which I have written about on my blog. Some examples include:",[916,18815,18816,18819,18822,18825,18828,18831],{},[919,18817,18818],{},"Adding a Drift chat window so users can message me directly",[919,18820,18821],{},"Implementing a contact form with formsubmit.io",[919,18823,18824],{},"Using Vue.js components in Markdown files to add interactive elements to my articles (such as graphs)",[919,18826,18827],{},"Adding a custom MailChimp newsletter sign-up form that is included in the footer of each page of my blog",[919,18829,18830],{},"Adding an RSS feed for my blog",[919,18832,18833],{},"Adding a site index and submitting it to Google",[11,18835,18836],{},"I have also been learning the suite of Google tools for monitoring and measuring traffic to my site, including Google Analytics and Google Search Console. Google Search Console is helpful for understanding the search terms that people are using when searching Google that result in organic traffic to my site.",[11,18838,18839],{},"At one point I found out that another website was using the same Google Tracking code that I had previously hard-coded into an old version of my website, and my Google Analytics started measuring traffic to URLs that I didn't recognize as belonging to my site. I was able to fix this by adding a Hostname filter rule in Google Analytics.",[11,18841,18842],{},"One area that I have not had any experience with until recently is Google AdSense. Google AdSense allows you to place ads on your website. Here's an overview of what I did to get started:",[916,18844,18845,18848,18851,18854,18861,18864],{},[919,18846,18847],{},"Add a site in Google AdSense",[919,18849,18850],{},"Submit my site for approval (this takes a few days)",[919,18852,18853],{},"Install and configure the Google AdSense plugin for NuxtJS",[919,18855,18856,18857,18860],{},"Add the ",[33,18858,18859],{},"ads.txt"," file generated by Google AdSense to my site",[919,18862,18863],{},"Confirm my address by entering a code that was mailed to me",[919,18865,18866],{},"Connect a bank account to my Google AdSense account",[11,18868,18869],{},"Here's the address confirmation code that I received from Google:",[11,18871,18872],{},[511,18873],{"alt":18874,"src":18875},"Address confirmation","/static/google_letter.jpeg",[11,18877,18878,18879,358],{},"Here's the config code for AdSense from ",[33,18880,6359],{},[26,18882,18884],{"className":564,"code":18883,"language":566,"meta":35,"style":35},"  /*\n   ** Nuxt.js modules\n   */\n  modules: [\n    // Doc: https://axios.nuxtjs.org/usage\n    '@nuxtjs/axios',\n    // Doc: https://github.com/nuxt/content\n    '@nuxt/content',\n    // Doc: https://www.npmjs.com/package/@nuxtjs/sitemap\n    '@nuxtjs/sitemap',\n    '@nuxtjs/feed',\n    'nuxt-i18n',\n    ['@nuxtjs/google-adsense', {     \u003C-- AdSense config\n      id: 'ca-pub-4924597640144289'\n    }]\n  ],\n",[33,18885,18886,18891,18896,18901,18909,18914,18921,18926,18932,18937,18943,18950,18957,18968,18976,18981],{"__ignoreMap":35},[187,18887,18888],{"class":189,"line":190},[187,18889,18890],{"class":295},"  /*\n",[187,18892,18893],{"class":189,"line":249},[187,18894,18895],{"class":295},"   ** Nuxt.js modules\n",[187,18897,18898],{"class":189,"line":312},[187,18899,18900],{"class":295},"   */\n",[187,18902,18903,18906],{"class":189,"line":319},[187,18904,18905],{"class":193},"  modules",[187,18907,18908],{"class":577},": [\n",[187,18910,18911],{"class":189,"line":325},[187,18912,18913],{"class":295},"    // Doc: https://axios.nuxtjs.org/usage\n",[187,18915,18916,18919],{"class":189,"line":686},[187,18917,18918],{"class":196},"    '@nuxtjs/axios'",[187,18920,1228],{"class":577},[187,18922,18923],{"class":189,"line":697},[187,18924,18925],{"class":295},"    // Doc: https://github.com/nuxt/content\n",[187,18927,18928,18930],{"class":189,"line":1291},[187,18929,6390],{"class":196},[187,18931,1228],{"class":577},[187,18933,18934],{"class":189,"line":1306},[187,18935,18936],{"class":295},"    // Doc: https://www.npmjs.com/package/@nuxtjs/sitemap\n",[187,18938,18939,18941],{"class":189,"line":1434},[187,18940,6432],{"class":196},[187,18942,1228],{"class":577},[187,18944,18945,18948],{"class":189,"line":2599},[187,18946,18947],{"class":196},"    '@nuxtjs/feed'",[187,18949,1228],{"class":577},[187,18951,18952,18955],{"class":189,"line":2607},[187,18953,18954],{"class":196},"    'nuxt-i18n'",[187,18956,1228],{"class":577},[187,18958,18959,18962,18965],{"class":189,"line":2621},[187,18960,18961],{"class":577},"    [",[187,18963,18964],{"class":196},"'@nuxtjs/google-adsense'",[187,18966,18967],{"class":577},", {     \u003C-- AdSense config\n",[187,18969,18970,18973],{"class":189,"line":2631},[187,18971,18972],{"class":577},"      id: ",[187,18974,18975],{"class":196},"'ca-pub-4924597640144289'\n",[187,18977,18978],{"class":189,"line":2642},[187,18979,18980],{"class":577},"    }]\n",[187,18982,18983],{"class":189,"line":2653},[187,18984,6458],{"class":577},[11,18986,18987],{},"The process was pretty simple. Google now automatically places ads on my site in a few different formats:",[916,18989,18990,18993,18996,18999],{},[919,18991,18992],{},"ads displayed on the top and bottom of the page",[919,18994,18995],{},"popup ads displayed between route navigation",[919,18997,18998],{},"ads automatically inserted into the body of the page between paragraphs in my articles",[919,19000,19001,19002,19005],{},"ads that I place on articles explicitly using the ",[33,19003,19004],{},"\u003Cadsbygoogle />"," Vue component",[11,19007,19008],{},"When everything was set up properly I started seeing ads on my site, and I see a non-zero value in my estimated earnings in the AdSense console. Google has a payment threshold of $100, so I need make this amount before I can start receiving money from Google.",[11,19010,19011],{},"My estimated earning report shows that I make between $0 and $4.32 in ad sales per day. I'm interested to see how much I can make with an article that I post across the many different channels that I can publish to. I explored this in a previous article, but the main channels I can use for sharing content are:",[916,19013,19014,19017,19020,19023,19026,19029,19032,19035,19038,19041,19044],{},[919,19015,19016],{},"DEV.to",[919,19018,19019],{},"Facebook",[919,19021,19022],{},"Hashnode",[919,19024,19025],{},"Medium",[919,19027,19028],{},"Reddit",[919,19030,19031],{},"Discord",[919,19033,19034],{},"Hacker Noon",[919,19036,19037],{},"Twitter",[919,19039,19040],{},"My MailChimp mailing list",[919,19042,19043],{},"Substack",[919,19045,19046],{},"Hacker News",[11,19048,19049],{},"This article should be a good place to start exploring how effective the different channels are in driving content to my site, and I'll update this article later with more details and numbers from my AdSense reports.",[168,19051,19053],{"id":19052},"how-i-built-an-adblock-detector-for-my-site","How I built an AdBlock detector for my site",[11,19055,19056],{},"I assume that most people reading my blog have an AdBlock extension installed in their browser like I do, such as AdBlock or ABP (AdBlock Pro). This got me thinking about how I could implement a simple AdBlock detector for my site that would hide the contents of the page if AdBlock is enabled.",[911,19058,19060],{"id":19059},"how-do-you-check-to-see-if-adblock-is-enabled","How do you check to see if AdBlock is enabled?",[11,19062,19063,19064,19069],{},"I started with this question, and I came across ",[15,19065,19068],{"href":19066,"rel":19067},"https://stackoverflow.com/questions/4869154/how-to-detect-adblock-on-my-website",[19],"this StackOverflow question"," which inspired the code that I am now using on this site to detect AdBlock.",[911,19071,19073],{"id":19072},"the-components-of-my-adblock-detector","The components of my AdBlock detector",[11,19075,19076],{},"There are a few different parts of my Nuxt Application that work together to detect if AdBlock is active and request that the user pause AdBlock for the site. The main components are:",[2276,19078,19079],{},[919,19080,19081,19082],{},"A component called ",[33,19083,19084],{},"AdBlockBlocker",[916,19086,19087],{},[919,19088,19089],{},"This component is used in the default layout, so it is included in all pages on briancaffey.github.io",[2276,19091,19092],{"start":249},[919,19093,19094,19095],{},"Vuex store module called ",[33,19096,19097],{},"adblock",[916,19099,19100,19103],{},[919,19101,19102],{},"this module is used to keep track of a boolean value that indicates if AdBlock is enabled",[919,19104,19105],{},"the module also has a simple getter and a mutation for turning adBlock to true or false",[2276,19107,19108],{"start":312},[919,19109,19110,19111,19114],{},"Some logic in the ",[33,19112,19113],{},"default.vue"," layout that is used for almost all of the pages on my site",[916,19116,19117],{},[919,19118,19119],{},"the getter from the Vuex store is used here to either show regular content or a message that the user needs to pause AdBlock",[2276,19121,19122],{"start":319},[919,19123,19124],{},"A component/page to display when AdBlock is enabled",[916,19126,19127,19130],{},[919,19128,19129],{},"this component asks the user to please pause AdBlock",[919,19131,19132,19133],{},"I named this component ",[33,19134,19135],{},"PleaseDisableAdblock.vue",[2276,19137,19138],{"start":325},[919,19139,19140],{},"localStorage",[916,19142,19143],{},[919,19144,19145],{},"this is used to keep track of the presence of an AdBlocker that is blocking ads",[11,19147,19148],{},"Here's an overview of each part:",[11,19150,19151],{},[338,19152,19153],{},"AdBlockBlocker.vue",[11,19155,19156,19157,19160],{},"This is the key part of how the AdBlock detection works. If the client is unable to download the ",[33,19158,19159],{},"adsbygoogle.js"," file, then that indicates that the user is using AdBlock.",[26,19162,19164],{"className":6715,"code":19163,"language":6717,"meta":35,"style":35},"\u003Ctemplate>\n  \u003Cdiv />\n\u003C/template>\n\n\u003Cscript>\nexport default {\n  mounted () {\n    // if adblock is detected through a value that is set local storage, then show the AdBlock message\n    // the `adblockEnabled` value is set in the catch block of the `detectAdBlock` method\n    // (see adblock/setAdblockEnabled mutation)\n    if (JSON.parse(localStorage.getItem('adblockEnabled')) === true) {\n      this.$store.commit('adblock/setAdblockEnabled', true)\n    }\n    // check to see if the URL can be accessed on a 5 second interval\n    setInterval(() => {\n      this.detectAdBlock()\n    }, 5000)\n  },\n  methods: {\n    async  detectAdBlock () {\n      // this is a URL that should be blocked by AdBlock\n      const googleAdUrl = 'https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'\n      // make a request to the above URL\n      await fetch(new Request(googleAdUrl)).then((_) => {\n        // isAdblockEnabled is false by default\n        // Check to see if isAblockEnabled was set to true by a previous request\n        if (this.$store.getters['adblock/isAdblockEnabled'] === true) {\n          // if the request was successful, then the user does not have AdBlock enabled,\n          // so we can set isAdblockEnabled to false using the setAdblockEnabled mutation\n          // this mutation will also set the `adblockEnabled` value in local storage to \"false\"\n          // `adblockEnabled` be `JSON.parse`d since it is saved in localStorage as a string\n          this.$store.commit('adblock/setAdblockEnabled', false)\n          window.localStorage.setItem('adblockEnabled', 'false')\n          // do a full reload of the page\n          window.location.reload()\n        }\n      }).catch((_) => {\n        // if the request was unsuccessful, then the user has AdBlock enabled.\n        // we can set isAdblockEnabled to true using the setAdblockEnabled mutation\n        // this will also set the `adblockEnabled` value in local storage to \"true\"\n        this.$store.commit('adblock/setAdblockEnabled', true)\n        window.localStorage.setItem('adblockEnabled', 'true')\n      })\n    }\n  }\n}\n\u003C/script>\n",[33,19165,19166,19175,19187,19195,19199,19207,19215,19223,19228,19233,19238,19273,19295,19299,19304,19316,19327,19337,19342,19347,19357,19362,19375,19380,19414,19419,19424,19448,19453,19458,19463,19468,19487,19506,19511,19521,19525,19543,19548,19553,19558,19577,19595,19600,19604,19608,19612],{"__ignoreMap":35},[187,19167,19168,19170,19173],{"class":189,"line":190},[187,19169,6724],{"class":577},[187,19171,19172],{"class":2516},"template",[187,19174,6730],{"class":577},[187,19176,19177,19180,19182,19185],{"class":189,"line":249},[187,19178,19179],{"class":577},"  \u003C",[187,19181,10229],{"class":2516},[187,19183,19184],{"class":7947}," /",[187,19186,6730],{"class":577},[187,19188,19189,19191,19193],{"class":189,"line":312},[187,19190,6856],{"class":577},[187,19192,19172],{"class":2516},[187,19194,6730],{"class":577},[187,19196,19197],{"class":189,"line":319},[187,19198,316],{"emptyLinePlaceholder":315},[187,19200,19201,19203,19205],{"class":189,"line":325},[187,19202,6724],{"class":577},[187,19204,6727],{"class":2516},[187,19206,6730],{"class":577},[187,19208,19209,19211,19213],{"class":189,"line":686},[187,19210,6371],{"class":573},[187,19212,6374],{"class":573},[187,19214,6739],{"class":577},[187,19216,19217,19220],{"class":189,"line":697},[187,19218,19219],{"class":193},"  mounted",[187,19221,19222],{"class":577}," () {\n",[187,19224,19225],{"class":189,"line":1291},[187,19226,19227],{"class":295},"    // if adblock is detected through a value that is set local storage, then show the AdBlock message\n",[187,19229,19230],{"class":189,"line":1306},[187,19231,19232],{"class":295},"    // the `adblockEnabled` value is set in the catch block of the `detectAdBlock` method\n",[187,19234,19235],{"class":189,"line":1434},[187,19236,19237],{"class":295},"    // (see adblock/setAdblockEnabled mutation)\n",[187,19239,19240,19242,19244,19246,19248,19250,19253,19256,19258,19261,19264,19267,19270],{"class":189,"line":2599},[187,19241,12925],{"class":573},[187,19243,784],{"class":577},[187,19245,12619],{"class":588},[187,19247,752],{"class":577},[187,19249,14476],{"class":193},[187,19251,19252],{"class":577},"(localStorage.",[187,19254,19255],{"class":193},"getItem",[187,19257,615],{"class":577},[187,19259,19260],{"class":196},"'adblockEnabled'",[187,19262,19263],{"class":577},")) ",[187,19265,19266],{"class":573},"===",[187,19268,19269],{"class":588}," true",[187,19271,19272],{"class":577},") {\n",[187,19274,19275,19278,19281,19284,19286,19289,19291,19293],{"class":189,"line":2607},[187,19276,19277],{"class":588},"      this",[187,19279,19280],{"class":577},".$store.",[187,19282,19283],{"class":193},"commit",[187,19285,615],{"class":577},[187,19287,19288],{"class":196},"'adblock/setAdblockEnabled'",[187,19290,637],{"class":577},[187,19292,6630],{"class":588},[187,19294,621],{"class":577},[187,19296,19297],{"class":189,"line":2621},[187,19298,9799],{"class":577},[187,19300,19301],{"class":189,"line":2631},[187,19302,19303],{"class":295},"    // check to see if the URL can be accessed on a 5 second interval\n",[187,19305,19306,19309,19312,19314],{"class":189,"line":2642},[187,19307,19308],{"class":193},"    setInterval",[187,19310,19311],{"class":577},"(() ",[187,19313,7102],{"class":573},[187,19315,6739],{"class":577},[187,19317,19318,19320,19322,19325],{"class":189,"line":2653},[187,19319,19277],{"class":588},[187,19321,752],{"class":577},[187,19323,19324],{"class":193},"detectAdBlock",[187,19326,694],{"class":577},[187,19328,19329,19332,19335],{"class":189,"line":2665},[187,19330,19331],{"class":577},"    }, ",[187,19333,19334],{"class":588},"5000",[187,19336,621],{"class":577},[187,19338,19339],{"class":189,"line":2674},[187,19340,19341],{"class":577},"  },\n",[187,19343,19344],{"class":189,"line":2684},[187,19345,19346],{"class":577},"  methods: {\n",[187,19348,19349,19352,19355],{"class":189,"line":2694},[187,19350,19351],{"class":573},"    async",[187,19353,19354],{"class":193},"  detectAdBlock",[187,19356,19222],{"class":577},[187,19358,19359],{"class":189,"line":2706},[187,19360,19361],{"class":295},"      // this is a URL that should be blocked by AdBlock\n",[187,19363,19364,19367,19370,19372],{"class":189,"line":2715},[187,19365,19366],{"class":573},"      const",[187,19368,19369],{"class":588}," googleAdUrl",[187,19371,6771],{"class":573},[187,19373,19374],{"class":196}," 'https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'\n",[187,19376,19377],{"class":189,"line":2725},[187,19378,19379],{"class":295},"      // make a request to the above URL\n",[187,19381,19382,19385,19388,19390,19393,19396,19399,19402,19405,19408,19410,19412],{"class":189,"line":2735},[187,19383,19384],{"class":573},"      await",[187,19386,19387],{"class":193}," fetch",[187,19389,615],{"class":577},[187,19391,19392],{"class":573},"new",[187,19394,19395],{"class":193}," Request",[187,19397,19398],{"class":577},"(googleAdUrl)).",[187,19400,19401],{"class":193},"then",[187,19403,19404],{"class":577},"((",[187,19406,19407],{"class":581},"_",[187,19409,13780],{"class":577},[187,19411,7102],{"class":573},[187,19413,6739],{"class":577},[187,19415,19416],{"class":189,"line":2743},[187,19417,19418],{"class":295},"        // isAdblockEnabled is false by default\n",[187,19420,19421],{"class":189,"line":2754},[187,19422,19423],{"class":295},"        // Check to see if isAblockEnabled was set to true by a previous request\n",[187,19425,19426,19429,19431,19433,19436,19439,19442,19444,19446],{"class":189,"line":2762},[187,19427,19428],{"class":573},"        if",[187,19430,784],{"class":577},[187,19432,14293],{"class":588},[187,19434,19435],{"class":577},".$store.getters[",[187,19437,19438],{"class":196},"'adblock/isAdblockEnabled'",[187,19440,19441],{"class":577},"] ",[187,19443,19266],{"class":573},[187,19445,19269],{"class":588},[187,19447,19272],{"class":577},[187,19449,19450],{"class":189,"line":2770},[187,19451,19452],{"class":295},"          // if the request was successful, then the user does not have AdBlock enabled,\n",[187,19454,19455],{"class":189,"line":2781},[187,19456,19457],{"class":295},"          // so we can set isAdblockEnabled to false using the setAdblockEnabled mutation\n",[187,19459,19460],{"class":189,"line":2792},[187,19461,19462],{"class":295},"          // this mutation will also set the `adblockEnabled` value in local storage to \"false\"\n",[187,19464,19465],{"class":189,"line":2803},[187,19466,19467],{"class":295},"          // `adblockEnabled` be `JSON.parse`d since it is saved in localStorage as a string\n",[187,19469,19470,19473,19475,19477,19479,19481,19483,19485],{"class":189,"line":2808},[187,19471,19472],{"class":588},"          this",[187,19474,19280],{"class":577},[187,19476,19283],{"class":193},[187,19478,615],{"class":577},[187,19480,19288],{"class":196},[187,19482,637],{"class":577},[187,19484,12660],{"class":588},[187,19486,621],{"class":577},[187,19488,19489,19492,19495,19497,19499,19501,19504],{"class":189,"line":2816},[187,19490,19491],{"class":577},"          window.localStorage.",[187,19493,19494],{"class":193},"setItem",[187,19496,615],{"class":577},[187,19498,19260],{"class":196},[187,19500,637],{"class":577},[187,19502,19503],{"class":196},"'false'",[187,19505,621],{"class":577},[187,19507,19508],{"class":189,"line":2824},[187,19509,19510],{"class":295},"          // do a full reload of the page\n",[187,19512,19513,19516,19519],{"class":189,"line":2834},[187,19514,19515],{"class":577},"          window.location.",[187,19517,19518],{"class":193},"reload",[187,19520,694],{"class":577},[187,19522,19523],{"class":189,"line":2845},[187,19524,9780],{"class":577},[187,19526,19527,19530,19533,19535,19537,19539,19541],{"class":189,"line":2856},[187,19528,19529],{"class":577},"      }).",[187,19531,19532],{"class":193},"catch",[187,19534,19404],{"class":577},[187,19536,19407],{"class":581},[187,19538,13780],{"class":577},[187,19540,7102],{"class":573},[187,19542,6739],{"class":577},[187,19544,19545],{"class":189,"line":2867},[187,19546,19547],{"class":295},"        // if the request was unsuccessful, then the user has AdBlock enabled.\n",[187,19549,19550],{"class":189,"line":2878},[187,19551,19552],{"class":295},"        // we can set isAdblockEnabled to true using the setAdblockEnabled mutation\n",[187,19554,19555],{"class":189,"line":2886},[187,19556,19557],{"class":295},"        // this will also set the `adblockEnabled` value in local storage to \"true\"\n",[187,19559,19560,19563,19565,19567,19569,19571,19573,19575],{"class":189,"line":2900},[187,19561,19562],{"class":588},"        this",[187,19564,19280],{"class":577},[187,19566,19283],{"class":193},[187,19568,615],{"class":577},[187,19570,19288],{"class":196},[187,19572,637],{"class":577},[187,19574,6630],{"class":588},[187,19576,621],{"class":577},[187,19578,19579,19582,19584,19586,19588,19590,19593],{"class":189,"line":2905},[187,19580,19581],{"class":577},"        window.localStorage.",[187,19583,19494],{"class":193},[187,19585,615],{"class":577},[187,19587,19260],{"class":196},[187,19589,637],{"class":577},[187,19591,19592],{"class":196},"'true'",[187,19594,621],{"class":577},[187,19596,19597],{"class":189,"line":2913},[187,19598,19599],{"class":577},"      })\n",[187,19601,19602],{"class":189,"line":2921},[187,19603,9799],{"class":577},[187,19605,19606],{"class":189,"line":2931},[187,19607,6847],{"class":577},[187,19609,19610],{"class":189,"line":2942},[187,19611,1309],{"class":577},[187,19613,19614,19616,19618],{"class":189,"line":2953},[187,19615,6856],{"class":577},[187,19617,6727],{"class":2516},[187,19619,6730],{"class":577},[11,19621,19622,19623,19626],{},"One other important point about this component is that it runs AdBlock detection using ",[33,19624,19625],{},"setInterval",", meaning that it will check if AdBlock is enabled every few seconds while a user is on my site.",[916,19628,19629,19635],{},[919,19630,19631,19632,19634],{},"If AdBlock is enabled, the request will fail and the Vuex store value will be updated, which will cause the site to display the ",[33,19633,19135],{}," component.",[919,19636,19637,19638,752],{},"Id AdBlock is not enabled, then the file will be read from disk cache and the Vuex store value will remain ",[33,19639,12660],{},[11,19641,19642],{},[338,19643,19644],{},"Vuex Store",[11,19646,19647,19648,19651,19652,19654],{},"This is a very simple Vuex store module. The ",[33,19649,19650],{},"isAdblockEnabled"," getter will be used in the ",[33,19653,19113],{}," layout to component.",[26,19656,19658],{"className":564,"code":19657,"language":566,"meta":35,"style":35},"let initialValue = false\n\nif (process.window) {\n  initialValue = JSON.parse(window.localStorage.getItem('adblockEnabled')) || false\n}\nexport const state = () => ({\n  adblockEnabled: initialValue\n})\n\nexport const getters = {\n  isAdblockEnabled: state => state.adblockEnabled\n}\n\nexport const mutations = {\n  setAdblockEnabled (state, payload) {\n    state.adblockEnabled = payload\n  }\n}\n",[33,19659,19660,19673,19677,19685,19714,19718,19737,19742,19746,19750,19763,19778,19782,19786,19799,19815,19825,19829],{"__ignoreMap":35},[187,19661,19662,19665,19668,19670],{"class":189,"line":190},[187,19663,19664],{"class":573},"let",[187,19666,19667],{"class":577}," initialValue ",[187,19669,595],{"class":573},[187,19671,19672],{"class":588}," false\n",[187,19674,19675],{"class":189,"line":249},[187,19676,316],{"emptyLinePlaceholder":315},[187,19678,19679,19682],{"class":189,"line":312},[187,19680,19681],{"class":573},"if",[187,19683,19684],{"class":577}," (process.window) {\n",[187,19686,19687,19690,19692,19694,19696,19698,19701,19703,19705,19707,19709,19712],{"class":189,"line":319},[187,19688,19689],{"class":577},"  initialValue ",[187,19691,595],{"class":573},[187,19693,14471],{"class":588},[187,19695,752],{"class":577},[187,19697,14476],{"class":193},[187,19699,19700],{"class":577},"(window.localStorage.",[187,19702,19255],{"class":193},[187,19704,615],{"class":577},[187,19706,19260],{"class":196},[187,19708,19263],{"class":577},[187,19710,19711],{"class":573},"||",[187,19713,19672],{"class":588},[187,19715,19716],{"class":189,"line":325},[187,19717,1309],{"class":577},[187,19719,19720,19722,19725,19728,19730,19732,19734],{"class":189,"line":686},[187,19721,6371],{"class":573},[187,19723,19724],{"class":573}," const",[187,19726,19727],{"class":193}," state",[187,19729,6771],{"class":573},[187,19731,7099],{"class":577},[187,19733,7102],{"class":573},[187,19735,19736],{"class":577}," ({\n",[187,19738,19739],{"class":189,"line":697},[187,19740,19741],{"class":577},"  adblockEnabled: initialValue\n",[187,19743,19744],{"class":189,"line":1291},[187,19745,6515],{"class":577},[187,19747,19748],{"class":189,"line":1306},[187,19749,316],{"emptyLinePlaceholder":315},[187,19751,19752,19754,19756,19759,19761],{"class":189,"line":1434},[187,19753,6371],{"class":573},[187,19755,19724],{"class":573},[187,19757,19758],{"class":588}," getters",[187,19760,6771],{"class":573},[187,19762,6739],{"class":577},[187,19764,19765,19768,19770,19773,19775],{"class":189,"line":2599},[187,19766,19767],{"class":193},"  isAdblockEnabled",[187,19769,585],{"class":577},[187,19771,19772],{"class":581},"state",[187,19774,7704],{"class":573},[187,19776,19777],{"class":577}," state.adblockEnabled\n",[187,19779,19780],{"class":189,"line":2607},[187,19781,1309],{"class":577},[187,19783,19784],{"class":189,"line":2621},[187,19785,316],{"emptyLinePlaceholder":315},[187,19787,19788,19790,19792,19795,19797],{"class":189,"line":2631},[187,19789,6371],{"class":573},[187,19791,19724],{"class":573},[187,19793,19794],{"class":588}," mutations",[187,19796,6771],{"class":573},[187,19798,6739],{"class":577},[187,19800,19801,19804,19806,19808,19810,19813],{"class":189,"line":2642},[187,19802,19803],{"class":193},"  setAdblockEnabled",[187,19805,784],{"class":577},[187,19807,19772],{"class":581},[187,19809,637],{"class":577},[187,19811,19812],{"class":581},"payload",[187,19814,19272],{"class":577},[187,19816,19817,19820,19822],{"class":189,"line":2653},[187,19818,19819],{"class":577},"    state.adblockEnabled ",[187,19821,595],{"class":573},[187,19823,19824],{"class":577}," payload\n",[187,19826,19827],{"class":189,"line":2665},[187,19828,6847],{"class":577},[187,19830,19831],{"class":189,"line":2674},[187,19832,1309],{"class":577},[11,19834,19835],{},[338,19836,19837],{},"default.vue layout logic",[26,19839,19843],{"className":19840,"code":19841,"language":19842,"meta":35,"style":35},"language-vue-html shiki shiki-themes github-light github-dark","\u003Ctemplate>\n  \u003Cdiv>\n    \u003CNavigation />\n    \u003CPleaseDisableAdblock v-if=\"$store.getters['adblock/isAdblockEnabled']\" />\n    \u003CNuxt v-else />\n    \u003CAdBlockerBlocker />\n\n    \u003CFooter />\n  \u003C/div>\n\u003C/template>\n","vue-html",[33,19844,19845,19853,19861,19872,19898,19910,19919,19923,19934,19943],{"__ignoreMap":35},[187,19846,19847,19849,19851],{"class":189,"line":190},[187,19848,6724],{"class":577},[187,19850,19172],{"class":2516},[187,19852,6730],{"class":577},[187,19854,19855,19857,19859],{"class":189,"line":249},[187,19856,19179],{"class":577},[187,19858,10229],{"class":2516},[187,19860,6730],{"class":577},[187,19862,19863,19866,19869],{"class":189,"line":312},[187,19864,19865],{"class":577},"    \u003C",[187,19867,19868],{"class":2516},"Navigation",[187,19870,19871],{"class":577}," />\n",[187,19873,19874,19876,19879,19882,19884,19886,19889,19891,19894,19896],{"class":189,"line":319},[187,19875,19865],{"class":577},[187,19877,19878],{"class":2516},"PleaseDisableAdblock",[187,19880,19881],{"class":573}," v-if",[187,19883,595],{"class":577},[187,19885,16508],{"class":196},[187,19887,19888],{"class":577},"$store.getters[",[187,19890,19438],{"class":196},[187,19892,19893],{"class":577},"]",[187,19895,16508],{"class":196},[187,19897,19871],{"class":577},[187,19899,19900,19902,19905,19908],{"class":189,"line":325},[187,19901,19865],{"class":577},[187,19903,19904],{"class":2516},"Nuxt",[187,19906,19907],{"class":573}," v-else",[187,19909,19871],{"class":577},[187,19911,19912,19914,19917],{"class":189,"line":686},[187,19913,19865],{"class":577},[187,19915,19916],{"class":2516},"AdBlockerBlocker",[187,19918,19871],{"class":577},[187,19920,19921],{"class":189,"line":697},[187,19922,316],{"emptyLinePlaceholder":315},[187,19924,19925,19927,19930,19932],{"class":189,"line":1291},[187,19926,19865],{"class":577},[187,19928,19929],{"class":2516},"Footer",[187,19931,19184],{"class":7947},[187,19933,6730],{"class":577},[187,19935,19936,19939,19941],{"class":189,"line":1306},[187,19937,19938],{"class":577},"  \u003C/",[187,19940,10229],{"class":2516},[187,19942,6730],{"class":577},[187,19944,19945,19947,19949],{"class":189,"line":1434},[187,19946,6856],{"class":577},[187,19948,19172],{"class":2516},[187,19950,6730],{"class":577},[11,19952,19953],{},[338,19954,19955],{},"Content to show to request that a user pauses AdBlock",[11,19957,19958,19959,752],{},"When users has AdBlock enabled, I show a simple message that asks them to please disable AdBlock. I also want to invite people who do not wish to disabled AdBlock to read my blog directly on GitHub, or to read it without ads by cloning or forking my repo, building it and running it in development mode with ",[33,19960,400],{},[168,19962,19964],{"id":19963},"why-am-i-adding-ads-and-an-adblock-detector-to-my-site","Why am I adding ads and an AdBlock detector to my site?",[11,19966,19967],{},"Being asked to pause AdBlock is increasingly common. I have noticed that the experience is not the same on each site that requests AdBlock be paused. For example:",[916,19969,19970,19973,19976],{},[919,19971,19972],{},"a site might ask you to pause AdBlock, but you have the option to continue without pausing AdBlock",[919,19974,19975],{},"a site shows you a preview of an article and asks you to unpause AdBlock to see the full article",[919,19977,19978],{},"all site content is hidden if you are using AdBlock, and a pop-up message ask you to \"Continue to site\" once you have paused AdBlock.",[11,19980,19981,19982,19985,19986,19989],{},"I remember being able to delete AdBlock detection modals and backgrounds on some sites as a way of getting around ad block detectors. This is as easy as ",[33,19983,19984],{},"Cmd + Shift + C",", click on the element that is blocking the page content and ",[33,19987,19988],{},"delete"," the selected element.",[11,19991,19992],{},"One of the reasons I wanted to implement AdBlock detection is to see what is possible from the perspective of user experience (UX). Ideally, here's how I want to the \"please disable AdBlock\" experience to work on my site:",[2276,19994,19995,19998,20001,20004,20007,20010,20013],{},[919,19996,19997],{},"A user visits my site with AdBlock enabled",[919,19999,20000],{},"For a few seconds, the user can start reading the content of the article",[919,20002,20003],{},"The page content is replaced with a message that says \"Please disable AdBlock\" and some other links that people may find interesting or helpful.",[919,20005,20006],{},"The user goes into the AdBlock extension and pauses AdBlock on my site",[919,20008,20009],{},"The original page content is then displayed with ads",[919,20011,20012],{},"If the user re-enables AdBlock shortly after pausing it, then the \"Please disable AdBlock\" message should be displayed again",[919,20014,20015],{},"If a u",[11,20017,20018,20019,20021],{},"I don't want to ask the user to press a button or make the user think that they need to refresh the page. This is why I'm using ",[33,20020,19625],{},", I continuously make requests to the Google Ads JavaScript file that will be used to detect if AdBlock enabled.",[11,20023,20024],{},"I'm happy to pause my AdBlock for smaller sites that ask me to, or for newspaper sites that are supported by advertising, and I'm assuming that people visiting my site will also be OK with pausing AdBlock as a way of thanking me for the work that goes into what I share on my blog.",[11,20026,20027],{},"I'm mostly curious to see what happens to my site's traffic, and to see what impact it could have on the earnings I make from AdSense.",[11,20029,20030],{},"Some of my questions are:",[916,20032,20033,20036,20039],{},[919,20034,20035],{},"What will happen to the bounce rate if I request that AdBlock users pause AdBlock for my site?",[919,20037,20038],{},"What is the most effective amount of time to wait before requesting that a new user pause AdBlock for my site",[919,20040,20041],{},"What else can I include on my \"Please Disable AdBlock\" page to encourage new users to pause AdBlock on my site?",[11,20043,20044],{},"I'll have a good follow-up article to share with numbers from my AdSense and Google analytics.",[911,20046,20048],{"id":20047},"one-small-issue","One small issue",[11,20050,20051],{},"While ads are working on my site, I did notice that a console error related to AdSense:",[26,20053,20056],{"className":20054,"code":20055,"language":31},[29],"K {message: \"adsbygoogle.push() error: Only one 'enable_page_level_ads' allowed per page.\", name: 'TagError', pbr: true, stack: \"TagError: adsbygoogle.push() error: Only one 'enab…agead/js/adsbygoogle.js?client=ca-google:77:1130)\"}\nmessage: \"adsbygoogle.push() error: Only one 'enable_page_level_ads' allowed per page.\"\nname: \"TagError\"\npbr: true\nstack: \"TagError: adsbygoogle.push() error: Only one 'enable_page_level_ads' allowed per page.\n    at go (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:219:326)\n    at fo (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:218:788)\n    at mo (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:225:365)\n    at c (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:226:38)\n    at no (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:226:156)\n    at yo (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:235:248)\n    at oo (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:232:89)\n    at http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:227:47\n    at Od.aa.ma (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:64:802)\n    at Jf (http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-google:77:1130)\"\n",[33,20057,20055],{"__ignoreMap":35},[11,20059,20060,20061,20065],{},"Here's a related issue on GitHub: ",[15,20062,20063],{"href":20063,"rel":20064},"https://github.com/nuxt-community/google-adsense-module/issues/141",[19],". I'm still not sure how to fix this issue. If anyone has any ideas, please let me know!",[11,20067,20068,20069,752],{},"If you are interested in following my progress, feel free to subscribe to my MailChimp newsletter by filling out the form in the footer of my website, or by following me on any of the accounts listed on ",[15,20070,20073],{"href":20071,"rel":20072},"https://briancaffey.github.io/contact",[19],"briancaffey.github.io/contact",[855,20075,20076],{},"html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .s7hpK, html code.shiki .s7hpK{--shiki-default:#B31D28;--shiki-default-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .s4XuR, html code.shiki .s4XuR{--shiki-default:#E36209;--shiki-dark:#FFAB70}",{"title":35,"searchDepth":249,"depth":249,"links":20078},[20079,20080,20084],{"id":18809,"depth":249,"text":18810},{"id":19052,"depth":249,"text":19053,"children":20081},[20082,20083],{"id":19059,"depth":312,"text":19060},{"id":19072,"depth":312,"text":19073},{"id":19963,"depth":249,"text":19964,"children":20085},[20086],{"id":20047,"depth":312,"text":20048},"2021-10-31","This article describes how I added AdSense to my personal website and how I request that site visitors pause AdBlock while reading my blog",[20090,20092,20094,20096,20098,20100],{"link":20091,"site":11220},"https://news.ycombinator.com/item?id=29143003",{"link":20093,"site":10051},"https://www.reddit.com/r/Nuxt/comments/qowjbu/how_and_why_i_added_adsense_and_a_custom_adblock/",{"link":20095,"site":6303},"https://dev.to/briancaffey/how-and-why-i-added-adsense-and-an-adblock-detector-to-my-personal-website-5ag3",{"link":20097,"site":15277},"https://medium.com/@briancaffey/how-and-why-i-added-adsense-and-an-adblock-detector-to-my-personal-website-b210d7f45e22",{"link":20099,"site":15280},"https://briancaffey.hashnode.dev/how-and-why-i-added-adsense-and-an-adblock-detector-to-my-personal-website",{"link":20101,"site":15283},"https://briancaffey.substack.com/p/how-and-why-i-added-adsense-and-an","https://briancaffey.github.io/static/ab_cover.png",{},"/2021/10/31/how-and-why-i-added-adsense-and-adblock-detector-to-my-website",{"title":18785,"description":20088},"2021/10/31/how-and-why-i-added-adsense-and-adblock-detector-to-my-website",[881,20108,20109,19097],"ads","adsense","R7cy9wAlFgyBOXwpHkNqVyTuo__tUCeE6Qwq4ayRzIo",{"id":20112,"title":20113,"body":20114,"comments":872,"date":21146,"description":21147,"draft":872,"extension":873,"external":21148,"image":20610,"meta":21156,"navigation":315,"path":21157,"seo":21158,"stem":21159,"tags":21160,"__hash__":21163},"blog/2021/10/02/how-i-write-and-share-technical-software-development-articles-in-2021.md","How I write and share technical software development articles in 2021",{"type":8,"value":20115,"toc":21112},[20116,20119,20122,20137,20144,20153,20157,20175,20178,20181,20444,20462,20466,20469,20473,20481,20492,20496,20499,20505,20508,20514,20517,20534,20540,20546,20556,20560,20563,20569,20579,20583,20586,20589,20596,20602,20605,20611,20615,20618,20626,20706,20709,20723,20727,20737,20741,20748,20752,20755,20759,20762,20765,20768,20778,20782,20789,20792,20794,20800,20804,20815,20817,20822,20828,20841,20843,20846,20850,20869,20873,20876,20882,20885,20888,20894,20901,20904,20910,20921,20923,20929,20932,20938,20940,20946,20953,20959,20963,20970,20973,20979,20982,20985,20991,20993,20999,21001,21011,21017,21020,21027,21031,21034,21037,21040,21051,21062,21087,21097,21103,21106,21109],[11,20117,20118],{},"This article describes how I write and share technical articles on my personal website and other developer websites and technical article aggregation sites.",[11,20120,20121],{},"This article is broken into three sections:",[2276,20123,20124,20131,20134],{},[919,20125,20126,20127],{},"How I use Nuxt.js to build ",[15,20128,20130],{"href":6329,"rel":20129},[19],"my personal website",[919,20132,20133],{},"How I share my articles on other development sites and article aggregators",[919,20135,20136],{},"Bonus content, project plug and conclusion",[168,20138,20140,20141,20143],{"id":20139},"building-briancaffeygithubio-with-nuxtjs","Building ",[33,20142,743],{}," with Nuxt.js",[11,20145,20146,20147,20152],{},"I first started writing my personal website on GitHub pages using a static website builder called ",[15,20148,20151],{"href":20149,"rel":20150},"https://jekyllrb.com/docs/github-pages/",[19],"Jekyll",". Jekyll is a great tool for getting started with building a personal portfolio or technical blog, and it served me well for several years. I eventually changed my static site generation tool from Jekyll to Nuxt since I wanted to learn more about Vue.js. Also, I don't know Ruby very well, and it was difficult for me to use the Jekyll template language.",[911,20154,20156],{"id":20155},"github-pages","GitHub Pages",[11,20158,20159,20160,20163,20164,20167,20168,20171,20172,752],{},"I host my website on GitHub pages at the following domain: ",[15,20161,743],{"href":6329,"rel":20162},[19],". GitHub pages is a great way to host a public site. The subdomain, ",[33,20165,20166],{},"briancaffey"," in my case, is your GitHub username. GitHub pages will serve content from a specified branch and nested folder. My website uses the branch ",[33,20169,20170],{},"gh-pages"," and the root folder ",[33,20173,20174],{},"/",[911,20176,20177],{"id":15294},"GitHub Actions",[11,20179,20180],{},"I use GitHub actions to deploy my website to GitHub pages. Here's the file that sets up the GitHub action that builds and deploys my site:",[26,20182,20184],{"className":8656,"code":20183,"language":8658,"meta":35,"style":35},"name: github pages\n\non:\n  push:\n    branches:\n      - master\n  pull_request:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Setup Node\n        uses: actions/setup-node@v2\n        with:\n          node-version: \"14\"\n\n      - name: Cache dependencies\n        uses: actions/cache@v2\n        with:\n          path: ~/.npm\n          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n          restore-keys: |\n            ${{ runner.os }}-node-\n\n      - run: yarn\n      - run: yarn lint\n      - run: yarn generate\n\n      - name: deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./docs\n",[33,20185,20186,20194,20198,20204,20210,20216,20222,20228,20232,20238,20244,20252,20258,20269,20273,20283,20292,20298,20307,20311,20321,20330,20336,20344,20352,20360,20364,20368,20378,20388,20398,20402,20412,20421,20427,20435],{"__ignoreMap":35},[187,20187,20188,20190,20192],{"class":189,"line":190},[187,20189,7284],{"class":2516},[187,20191,585],{"class":577},[187,20193,7289],{"class":196},[187,20195,20196],{"class":189,"line":249},[187,20197,316],{"emptyLinePlaceholder":315},[187,20199,20200,20202],{"class":189,"line":312},[187,20201,7298],{"class":588},[187,20203,2520],{"class":577},[187,20205,20206,20208],{"class":189,"line":319},[187,20207,7305],{"class":2516},[187,20209,2520],{"class":577},[187,20211,20212,20214],{"class":189,"line":325},[187,20213,7312],{"class":2516},[187,20215,2520],{"class":577},[187,20217,20218,20220],{"class":189,"line":686},[187,20219,2610],{"class":577},[187,20221,7321],{"class":196},[187,20223,20224,20226],{"class":189,"line":697},[187,20225,7326],{"class":2516},[187,20227,2520],{"class":577},[187,20229,20230],{"class":189,"line":1291},[187,20231,316],{"emptyLinePlaceholder":315},[187,20233,20234,20236],{"class":189,"line":1306},[187,20235,7337],{"class":2516},[187,20237,2520],{"class":577},[187,20239,20240,20242],{"class":189,"line":1434},[187,20241,7344],{"class":2516},[187,20243,2520],{"class":577},[187,20245,20246,20248,20250],{"class":189,"line":2599},[187,20247,7351],{"class":2516},[187,20249,585],{"class":577},[187,20251,7356],{"class":196},[187,20253,20254,20256],{"class":189,"line":2607},[187,20255,7361],{"class":2516},[187,20257,2520],{"class":577},[187,20259,20260,20262,20264,20266],{"class":189,"line":2621},[187,20261,2610],{"class":577},[187,20263,7370],{"class":2516},[187,20265,585],{"class":577},[187,20267,20268],{"class":196},"actions/checkout@v2\n",[187,20270,20271],{"class":189,"line":2631},[187,20272,316],{"emptyLinePlaceholder":315},[187,20274,20275,20277,20279,20281],{"class":189,"line":2642},[187,20276,2610],{"class":577},[187,20278,7284],{"class":2516},[187,20280,585],{"class":577},[187,20282,7390],{"class":196},[187,20284,20285,20287,20289],{"class":189,"line":2653},[187,20286,7395],{"class":2516},[187,20288,585],{"class":577},[187,20290,20291],{"class":196},"actions/setup-node@v2\n",[187,20293,20294,20296],{"class":189,"line":2665},[187,20295,7405],{"class":2516},[187,20297,2520],{"class":577},[187,20299,20300,20302,20304],{"class":189,"line":2674},[187,20301,7412],{"class":2516},[187,20303,585],{"class":577},[187,20305,20306],{"class":196},"\"14\"\n",[187,20308,20309],{"class":189,"line":2684},[187,20310,316],{"emptyLinePlaceholder":315},[187,20312,20313,20315,20317,20319],{"class":189,"line":2694},[187,20314,2610],{"class":577},[187,20316,7284],{"class":2516},[187,20318,585],{"class":577},[187,20320,7432],{"class":196},[187,20322,20323,20325,20327],{"class":189,"line":2706},[187,20324,7395],{"class":2516},[187,20326,585],{"class":577},[187,20328,20329],{"class":196},"actions/cache@v2\n",[187,20331,20332,20334],{"class":189,"line":2715},[187,20333,7405],{"class":2516},[187,20335,2520],{"class":577},[187,20337,20338,20340,20342],{"class":189,"line":2725},[187,20339,7452],{"class":2516},[187,20341,585],{"class":577},[187,20343,7457],{"class":196},[187,20345,20346,20348,20350],{"class":189,"line":2735},[187,20347,7462],{"class":2516},[187,20349,585],{"class":577},[187,20351,7467],{"class":196},[187,20353,20354,20356,20358],{"class":189,"line":2743},[187,20355,7472],{"class":2516},[187,20357,585],{"class":577},[187,20359,7477],{"class":573},[187,20361,20362],{"class":189,"line":2754},[187,20363,7482],{"class":196},[187,20365,20366],{"class":189,"line":2762},[187,20367,316],{"emptyLinePlaceholder":315},[187,20369,20370,20372,20374,20376],{"class":189,"line":2770},[187,20371,2610],{"class":577},[187,20373,7493],{"class":2516},[187,20375,585],{"class":577},[187,20377,7498],{"class":196},[187,20379,20380,20382,20384,20386],{"class":189,"line":2781},[187,20381,2610],{"class":577},[187,20383,7493],{"class":2516},[187,20385,585],{"class":577},[187,20387,7520],{"class":196},[187,20389,20390,20392,20394,20396],{"class":189,"line":2792},[187,20391,2610],{"class":577},[187,20393,7493],{"class":2516},[187,20395,585],{"class":577},[187,20397,7531],{"class":196},[187,20399,20400],{"class":189,"line":2803},[187,20401,316],{"emptyLinePlaceholder":315},[187,20403,20404,20406,20408,20410],{"class":189,"line":2808},[187,20405,2610],{"class":577},[187,20407,7284],{"class":2516},[187,20409,585],{"class":577},[187,20411,7546],{"class":196},[187,20413,20414,20416,20418],{"class":189,"line":2816},[187,20415,7395],{"class":2516},[187,20417,585],{"class":577},[187,20419,20420],{"class":196},"peaceiris/actions-gh-pages@v3\n",[187,20422,20423,20425],{"class":189,"line":2824},[187,20424,7405],{"class":2516},[187,20426,2520],{"class":577},[187,20428,20429,20431,20433],{"class":189,"line":2834},[187,20430,7566],{"class":2516},[187,20432,585],{"class":577},[187,20434,7571],{"class":196},[187,20436,20437,20439,20441],{"class":189,"line":2845},[187,20438,7576],{"class":2516},[187,20440,585],{"class":577},[187,20442,20443],{"class":196},"./docs\n",[11,20445,20446,20447,20449,20450,20452,20453,20456,20457,20459,20460,752],{},"When changes are pushed to the ",[33,20448,747],{}," branch, this GitHub Action runs. It lints the code, builds the site with ",[33,20451,705],{}," and then the ",[33,20454,20455],{},"peaceiris/actions-gh-pages@v3"," GitHub Action commits only the build artifacts to the ",[33,20458,20170],{}," branch where the content is served on ",[33,20461,743],{},[911,20463,20465],{"id":20464},"nuxtjs-framework","Nuxt.js Framework",[11,20467,20468],{},"Nuxt.js is a versatile Vue.js framework. It can be used to build several different types of websites, including Server Side Rendered (SSR) websites, single page applications (SPA) and static websites. I use the static website mode, also called Server Side Generation (SSG).",[911,20470,20472],{"id":20471},"content-api","Content API",[11,20474,20475,20476,20480],{},"Nuxt has a module called ",[15,20477,20472],{"href":20478,"rel":20479},"https://nuxtjs.org/api/content-api",[19]," that allows you to do the following:",[916,20482,20483,20486,20489],{},[919,20484,20485],{},"write articles in Markdown",[919,20487,20488],{},"Use Vue components in Markdown",[919,20490,20491],{},"write custom front-matter that can be used to query Markdown articles using an API",[911,20493,20495],{"id":20494},"articles-and-folder-structure","Articles and Folder Structure",[11,20497,20498],{},"Nuxt uses a folder structure that automatically generates routes and helps you organize components, pages and layouts. Here is the folder structure for my personal website's repository:",[26,20500,20503],{"className":20501,"code":20502,"language":31},[29],"$ tree -L 1 .\n.\n├── README.md\n├── assets  \u003C-- compiled assets\n├── components \u003C-- Vue components used in the site\n├── content  \u003C-- contains Markdown files for articles\n├── i18n  \u003C-- contains translations\n├── jsconfig.json\n├── layouts  \u003C-- layouts used for each page\n├── middleware  \u003C-- I'm not using this folder\n├── node_modules\n├── nuxt.config.js  \u003C-- config file\n├── package.json\n├── pages  \u003C-- directory structure defines URLs for site pages\n├── plugins  \u003C-- plugins\n├── static  \u003C-- files in this folder are served as-is\n├── store  \u003C-- Vuex store\n├── tailwind.config.js\n└── yarn.lock\n",[33,20504,20502],{"__ignoreMap":35},[11,20506,20507],{},"The content folder has the following structure:",[26,20509,20512],{"className":20510,"code":20511,"language":31},[29],"$ tree -L 3 content\ncontent/\n├── 2016\n│   └── 04\n│       └── 07\n│           └── my-article.md\n├── 2017\n│   └── 01\n│      └── 01\n│           └── my-other-article.md\n...\n└── projects\n│   └── my-project.md\n",[33,20513,20511],{"__ignoreMap":35},[11,20515,20516],{},"This will produce the following routes:",[916,20518,20519,20524,20529],{},[919,20520,20521],{},[33,20522,20523],{},"/2016/04/07/my-article.html",[919,20525,20526],{},[33,20527,20528],{},"/2017/01/01/my-other-article.html",[919,20530,20531],{},[33,20532,20533],{},"/projects/my-project.html",[11,20535,20536,20537,388],{},"Here's the folder structure for the ",[33,20538,20539],{},"pages",[26,20541,20544],{"className":20542,"code":20543,"language":31},[29],"$ tree -L 4 pages\npages\n├── README.md\n├── _year\n│   └── _month\n│       └── _day\n│           └── _slug.vue\n├── blog\n│   ├── index.vue\n│   └── tags\n│       ├── _tag.vue\n│       └── index.vue\n├── confirm-subscription.vue\n├── contact\n│   └── index.vue\n├── drafts\n│   └── index.vue\n├── index.vue\n├── projects\n│   ├── _slug.vue\n│   └── index.vue\n└── thank-you.vue\n",[33,20545,20543],{"__ignoreMap":35},[11,20547,20548,20549,20551,20552,20555],{},"Directories in the ",[33,20550,20539],{}," directory starting with an underscore (like ",[33,20553,20554],{},"_year",") can be used as URL parameters.",[911,20557,20559],{"id":20558},"markdown-and-vue-components","Markdown and Vue Components",[11,20561,20562],{},"One nice feature of Nuxt and the Nuxt Content module is the ability to use Vue components directly in Markdown files. Here's an example of a Vue component in a Markdown file:",[26,20564,20567],{"className":20565,"code":20566,"language":31},[29],"# Markdown with Vue components\n\nThis is some content\n\n\u003Cmy-component />\n\nThis is more content.\n",[33,20568,20566],{"__ignoreMap":35},[11,20570,20571,20572,20575,20576,752],{},"Vue components used in Markdown files must be included in the ",[33,20573,20574],{},"components/global"," directory. Here's an article on my blog where I used Vue components to show interactive graphs: ",[15,20577,7655],{"href":7655,"rel":20578},[19],[911,20580,20582],{"id":20581},"images","Images",[11,20584,20585],{},"Images are an important part of the articles that I write. Each article has an optional cover image. I try to include a cover image for each of the articles I write. I mostly use Gimp to create the cover images.",[11,20587,20588],{},"The cover image for this article was made with Inkscape. I have also used Gimp and Blender to generate images for my blog.",[11,20590,20591,20592,20595],{},"Including images in the body of an article is pretty simple on my personal site. First I need to add the image to the ",[33,20593,20594],{},"static"," directory, and then I can reference that image using the following syntax:",[26,20597,20600],{"className":20598,"code":20599,"language":31},[29],"![alt text](/static/path/to/my-image.png)\n",[33,20601,20599],{"__ignoreMap":35},[11,20603,20604],{},"Here's the image for this article:",[11,20606,20607],{},[511,20608],{"alt":20609,"src":20610},"Writing and publishing dev articles","/static/dev-sites.png",[911,20612,20614],{"id":20613},"markdown-front-matter","Markdown front-matter",[11,20616,20617],{},"Front-matter is a way to define metadata for a Markdown file. It is used to define the title, description, and other metadata that can be used to query articles.",[11,20619,20620,20621,358],{},"Here's whe front-matter for this article on my personal site ",[15,20622,20625],{"href":20623,"rel":20624},"https://briancaffey.github.io/2021/10/02/sharing-a-dev-article-everywhere",[19],"briancaffey.github.io/2021/10/02/sharing-a-dev-article-everywhere",[26,20627,20629],{"className":8656,"code":20628,"language":8658,"meta":35,"style":35},"---\ntitle: How I write and share technical development articles in 2021\ndate: '2021-10-02'\ndescription: This article describes how to write and share technical articles in 2021\ntags:\n  - nuxt\n  - vue\n  - publishing\n  - writing\ndraft: true\n---\n",[33,20630,20631,20635,20644,20653,20662,20668,20674,20680,20687,20694,20702],{"__ignoreMap":35},[187,20632,20633],{"class":189,"line":190},[187,20634,6532],{"class":193},[187,20636,20637,20639,20641],{"class":189,"line":249},[187,20638,6537],{"class":2516},[187,20640,585],{"class":577},[187,20642,20643],{"class":196},"How I write and share technical development articles in 2021\n",[187,20645,20646,20648,20650],{"class":189,"line":312},[187,20647,6550],{"class":2516},[187,20649,585],{"class":577},[187,20651,20652],{"class":196},"'2021-10-02'\n",[187,20654,20655,20657,20659],{"class":189,"line":319},[187,20656,6560],{"class":2516},[187,20658,585],{"class":577},[187,20660,20661],{"class":196},"This article describes how to write and share technical articles in 2021\n",[187,20663,20664,20666],{"class":189,"line":325},[187,20665,6582],{"class":2516},[187,20667,2520],{"class":577},[187,20669,20670,20672],{"class":189,"line":686},[187,20671,6592],{"class":577},[187,20673,6602],{"class":196},[187,20675,20676,20678],{"class":189,"line":697},[187,20677,6592],{"class":577},[187,20679,6595],{"class":196},[187,20681,20682,20684],{"class":189,"line":1291},[187,20683,6592],{"class":577},[187,20685,20686],{"class":196},"publishing\n",[187,20688,20689,20691],{"class":189,"line":1306},[187,20690,6592],{"class":577},[187,20692,20693],{"class":196},"writing\n",[187,20695,20696,20698,20700],{"class":189,"line":1434},[187,20697,6625],{"class":2516},[187,20699,585],{"class":577},[187,20701,2530],{"class":588},[187,20703,20704],{"class":189,"line":2599},[187,20705,6532],{"class":193},[11,20707,20708],{},"I also use front-matter for the following:",[916,20710,20711,20714,20717,20720],{},[919,20712,20713],{},"OpenGraph meta tags & social sharing",[919,20715,20716],{},"links to other outlets",[919,20718,20719],{},"meta tags",[919,20721,20722],{},"article tags",[911,20724,20726],{"id":20725},"nuxt-sitemap","Nuxt Sitemap",[11,20728,4765,20729,20732,20733,20736],{},[33,20730,20731],{},"@nuxt/sitemap"," module, it is easy to generate a sitemap.xml file. This generates a static file that is available on ",[33,20734,20735],{},"/sitemap.xml",". This file is used in the Google Search Console to tell Google which pages on my site should be indexed. This helps with SEO.",[911,20738,20740],{"id":20739},"rss-feed","RSS Feed",[11,20742,20743,20744,20747],{},"An RSS feed is configured using another official Nuxt module called ",[33,20745,20746],{},"@nuxtjs/feed",". This plugin generates an RSS feed for the site in XML. An RSS feed can be used to automatically publish articles to other sites, I'll show this in the next section of this article.",[911,20749,20751],{"id":20750},"google-analytics","Google Analytics",[11,20753,20754],{},"Google Analytics is used to track site traffic and gives insights into what content is popular, where my users are visiting from, how long they spend browsing my site and other helpful metrics. It is likely that many readers of my site may have disabled Google Analytics in their browsers.",[911,20756,20758],{"id":20757},"google-search-console","Google Search Console",[11,20760,20761],{},"Google Search Console is another tool that is helpful from an SEO perspective.",[11,20763,20764],{},"Here is a report from Google Data Studio showing some some of the metrics that I use to analyze my site's traffic:",[855,20766,20767],{},"\n  .google-data-studio {\nposition: relative;\npadding-bottom: 56.25%;\npadding-top: 30px; height: 0; overflow: hidden;\n}\n\n.google-data-studio iframe,\n.google-data-studio object,\n.google-data-studio embed {\nposition: absolute;\ntop: 0;\nleft: 0;\nwidth: 100%;\nheight: 100%;\n}\n",[10229,20769,20772],{"className":20770},[20771],"google-data-studio",[10159,20773],{"width":20774,"height":20775,"src":20776,"frameBorder":10165,"style":20777,"allowFullScreen":315},600,450,"https://datastudio.google.com/embed/reporting/1da052a4-6d61-4dac-b85d-5e9efed870af/page/6zXD","border:0",[911,20779,20781],{"id":20780},"mailchimp","MailChimp",[11,20783,20784,20785,752],{},"I use MailChimp to build a newsletter audience. I'll be sending out a newsletter to my current audience with an update about this article. I wrote an article about how to set up MailChimp on Nuxt. I wrote an article on my blog about how I set up a form for guests to sign up for a newsletter using a MailChimp form: ",[15,20786,20787],{"href":20787,"rel":20788},"https://briancaffey.github.io/zh/2020/10/10/how-to-add-email-signup-form-to-nuxt-site-with-mailchimp.html",[19],[11,20790,20791],{},"Here's what the form looks like:",[7880,20793],{},[10517,20795,20796,20797],{},"\n  ",[20798,20799],"newsletter",{},[911,20801,20803],{"id":20802},"formsubmitco","formsubmit.co",[11,20805,20806,20807,20811,20812,7901],{},"Site visitors can send me messages through an online form called ",[15,20808,20803],{"href":20809,"rel":20810},"https://formsubmit.co",[19],". I include this form on my site's ",[15,20813,7900],{"href":20071,"rel":20814},[19],[7880,20816],{},[10517,20818,20796,20819],{},[20820,20821],"contact-form",{},[11,20823,20824,20826],{},[7880,20825],{},[7880,20827],{},[11,20829,20830,20831,1172,20834,20837,20838,20840],{},"This two forms are examples of using Vue components in Markdown files that I mentioned earlier. Both the ",[33,20832,20833],{},"Nesletter",[33,20835,20836],{},"ContactForm"," components must be in the ",[33,20839,20574],{}," directory.",[911,20842,7860],{"id":7859},[11,20844,20845],{},"Drift is a freemium service that allows site visitors to send me messages in real time. It's a great way to get in touch with site visitors, and it can be configured so that messages go to the Drift mobile app.",[911,20847,20849],{"id":20848},"drafts","Drafts",[11,20851,20852,20853,20855,20856,20859,20860,20864,20865,20868],{},"One option in the front-matter for my blog articles is ",[33,20854,6625],{},". If an article has ",[33,20857,20858],{},"draft: true"," set in the front-matter, then the article will not be listed on the main list of blog articles on my site, and the page will not be indexed by Google. Here's where you can find the draft articles for my site: ",[15,20861,20862],{"href":20862,"rel":20863},"https://briancaffey.github.io/drafts",[19],". The articles here can be accessed publicly, but I only show them on the ",[33,20866,20867],{},"/drafts"," page which is not listed anywhere else on my site.",[168,20870,20872],{"id":20871},"publishing-on-other-outlets","Publishing on other outlets",[11,20874,20875],{},"When publishing articles from my personal website on other sites, I make sure that custom content is either removed or replaced with a link or static image that I can upload to the other site while editing the article.",[11,20877,20878,20879,20881],{},"For example, this article includes an embedded Google Data Studio report in the version that is published on ",[33,20880,743],{},". This embedded iframe will not work when posted to other platforms, so I can instead link to an anchor tag that corresponds to the location of the custom element on my site.",[11,20883,20884],{},"For this article, I have mostly tried to keep the custom content to a minimum so that it will be easy to cross publish on other sites without having to make lots of edits to the markdown. Most of the tweaking will likely have to do with preview images and other custom front-matter properties that some site (like DEV.to) support.",[911,20886,19016],{"id":20887},"devto",[11,20889,20890],{},[15,20891,20892],{"href":20892,"rel":20893},"https://dev.to/briancaffey/how-i-write-and-share-technical-software-development-articles-in-2021-27n2",[19],[11,20895,20896,20900],{},[15,20897,19016],{"href":20898,"rel":20899},"https://dev.to",[19]," is a popular site for sharing technical articles. They allow you to automatically draft articles to publish on their site by adding your site's RSS feed. This article will be published on DEV.to through the RSS feed connection that my account has with DEV.to.",[11,20902,20903],{},"DEV.to articles support their own custom front-matter properties. Here's what the front-matter for the DEV.to article looks like:",[26,20905,20908],{"className":20906,"code":20907,"language":31},[29],"---\ntitle: How I write and share technical software development articles in 2021\npublished: false\ndate: '2021-10-02'\ntags:  nuxt, vue, publishing, blogging\nimage: 'https://briancaffey.github.io/static/dev-sites.png'\ncanonical_url: https://briancaffey.github.io/2021/10/02/how-i-write-and-share-technical-software-development-articles-in-2021\n---\n",[33,20909,20907],{"__ignoreMap":35},[11,20911,20912,20913,20916,20917,20920],{},"If you don't see your article in your list of article drafts on your DEV.to dashboard, you can go into ",[33,20914,20915],{},"Settings > Extensions > Publishing to DEV Community from RSS"," and click on ",[33,20918,20919],{},"Save Feed Settings",". I think this refreshes your RSS feed in your dashboard.",[911,20922,19025],{"id":15277},[11,20924,20925],{},[15,20926,20927],{"href":20927,"rel":20928},"https://medium.com/@briancaffey/how-i-write-and-share-technical-software-development-articles-in-2021-8168d3871bf9",[19],[11,20930,20931],{},"I haven't published anything on Medium, so one of my goals for this article is to cross publish it on Medium in my first article on that platform.",[11,20933,20934],{},[15,20935,20936],{"href":20936,"rel":20937},"https://medium.com/new-story",[19],[911,20939,19022],{"id":15280},[11,20941,20942],{},[15,20943,20944],{"href":20944,"rel":20945},"https://briancaffey.hashnode.dev/how-i-write-and-share-technical-software-development-articles-in-2021",[19],[11,20947,20948,20949,752],{},"Hashnode seems very similar to DEV.to. Here's a comparison that shows some of the advantages of using Hashnode as a blogging platform over DEV.to: ",[15,20950,20951],{"href":20951,"rel":20952},"https://hashnode.com/vs/devto",[19],[11,20954,20955],{},[15,20956,20957],{"href":20957,"rel":20958},"https://hashnode.com/create/story",[19],[911,20960,20962],{"id":20961},"eggheadio","egghead.io",[11,20964,20965,20966,20969],{},"Egghead is another blogging platform that allows you to helps you ",[33,20967,20968],{},"Own Your Online Presence"," and also lets you create free and paid courses and content.",[911,20971,19034],{"id":20972},"hacker-noon",[11,20974,20975],{},[15,20976,20977],{"href":20977,"rel":20978},"https://hackernoon.com/how-to-get-your-dev-blog-noticed-in-2021",[19],[11,20980,20981],{},"Hacker Noon is another platform that I haven't used before as a writer. In order to publish an article on Hacker Noon, the article must go through an editing process. The Hacker Noon editors change the title and cover image of my article, and also added some minor editor notes.",[911,20983,19046],{"id":20984},"hacker-news",[11,20986,20987],{},[15,20988,20989],{"href":20989,"rel":20990},"https://news.ycombinator.com/item?id=28740415",[19],[911,20992,19043],{"id":15283},[11,20994,20995],{},[15,20996,20997],{"href":20997,"rel":20998},"https://briancaffey.substack.com/p/how-i-write-and-share-technical-software",[19],[911,21000,19028],{"id":10051},[11,21002,21003,21004,1172,21007,21010],{},"I have shared a lot of content on different programming subreddits specific to some of the tools and frameworks I use, such as ",[33,21005,21006],{},"r/aws",[33,21008,21009],{},"r/django",". When sharing on reddit, I like to share links to my personal website with at least on comment that provides a detailed summary of the article. When sharing on",[11,21012,21013],{},[15,21014,21015],{"href":21015,"rel":21016},"https://www.reddit.com/r/webdev/comments/q0qc3l/how_i_write_and_share_technical_software/",[19],[911,21018,19019],{"id":21019},"facebook",[11,21021,21022,21023,21026],{},"Facebook has very large and active developer communities. Sometimes the communities are more fragmented than the communities on reddit. For example, there are several Nuxt communities on Facebook, but there is just one ",[33,21024,21025],{},"r/nuxt",". Similar to sharing content on reddit, I like to share links to my personal websites with detailed comments on the content of my article.",[911,21028,21030],{"id":21029},"discord-servers","Discord Servers",[11,21032,21033],{},"Discord also has some dedicated servers for software frameworks, such as Nuxt.js. Discord seems to be the official place where Vue.js community members chat in real-time. There are dedicated channels on the server for sharing articles.",[168,21035,20136],{"id":21036},"bonus-content-project-plug-and-conclusion",[11,21038,21039],{},"One more great thing about GitHub pages is that you can publish a site on any of your GitHub repositories that will be hosted on a subpath of your GitHub pages blog.",[11,21041,21042,21043,21046,21047,752],{},"I have been working updating and rewriting my Django + Vue.js + AWS reference project. It contains a documentation site that I am making with VuePress. The repo for this project is here: ",[15,21044,11294],{"href":11292,"rel":21045},[19],". This repository has it's own GitHub Pages configuration, as well as a GitHub Action to help automate the deployment of this project documentation site to GitHub Pages. The project site is currently hosted on ",[15,21048,21050],{"href":11286,"rel":21049},[19],"briancaffey.github.io/django-step-by-step/",[11,21052,21053,21054,21061],{},"The project uses a CDK construct library that I have been developing alongside of this reference project called ",[15,21055,21058],{"href":21056,"rel":21057},"https://github.com/briancaffey/django-cdk",[19],[33,21059,21060],{},"django-cdk",". It provides high-level constructs that allow you to deploy a complete stack of resources on AWS that will support your Django + Vue.js application including:",[916,21063,21064,21067,21070,21073,21076,21078,21081,21084],{},[919,21065,21066],{},"VPC networking",[919,21068,21069],{},"CloudFront & S3 for Frontend",[919,21071,21072],{},"RDS & Elasticache",[919,21074,21075],{},"ECS & EKS options",[919,21077,12534],{},[919,21079,21080],{},"Automatic migrations",[919,21082,21083],{},"Asset and Media file storage with S3",[919,21085,21086],{},"Shell access (for debugging)",[11,21088,21089,21090,21092,21093],{},"The documentation for ",[33,21091,21060],{}," can be found here: ",[15,21094,21095],{"href":21095,"rel":21096},"https://briancaffey.github.io/django-step-by-step/deploy/aws/#about-django-cdk",[19],[11,21098,21099,21100,21102],{},"You may want to split a large project's documentation site into its own site, rather than having it live on the nested path of a personal blog. Following this pattern, your GitHub pages blog can become a site that is much larger than one single Nuxt static site. ",[33,21101,743],{}," is now a hybrid Nuxt.js and VuePress site, with a subset of routes (starting with /django-step-by-step/) being served by VuePress.",[11,21104,21105],{},"As I'm writing this article, Nuxt 3 is almost one week away from a public beta. I'm excited to try upgrading this site to Nuxt 3 and trying out some of the new features that it includes.",[11,21107,21108],{},"Thanks for reading this article, wherever you may have found it on the internet!",[855,21110,21111],{},"html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}",{"title":35,"searchDepth":249,"depth":249,"links":21113},[21114,21133,21145],{"id":20139,"depth":249,"text":21115,"children":21116},"Building briancaffey.github.io with Nuxt.js",[21117,21118,21119,21120,21121,21122,21123,21124,21125,21126,21127,21128,21129,21130,21131,21132],{"id":20155,"depth":312,"text":20156},{"id":15294,"depth":312,"text":20177},{"id":20464,"depth":312,"text":20465},{"id":20471,"depth":312,"text":20472},{"id":20494,"depth":312,"text":20495},{"id":20558,"depth":312,"text":20559},{"id":20581,"depth":312,"text":20582},{"id":20613,"depth":312,"text":20614},{"id":20725,"depth":312,"text":20726},{"id":20739,"depth":312,"text":20740},{"id":20750,"depth":312,"text":20751},{"id":20757,"depth":312,"text":20758},{"id":20780,"depth":312,"text":20781},{"id":20802,"depth":312,"text":20803},{"id":7859,"depth":312,"text":7860},{"id":20848,"depth":312,"text":20849},{"id":20871,"depth":249,"text":20872,"children":21134},[21135,21136,21137,21138,21139,21140,21141,21142,21143,21144],{"id":20887,"depth":312,"text":19016},{"id":15277,"depth":312,"text":19025},{"id":15280,"depth":312,"text":19022},{"id":20961,"depth":312,"text":20962},{"id":20972,"depth":312,"text":19034},{"id":20984,"depth":312,"text":19046},{"id":15283,"depth":312,"text":19043},{"id":10051,"depth":312,"text":19028},{"id":21019,"depth":312,"text":19019},{"id":21029,"depth":312,"text":21030},{"id":21036,"depth":249,"text":20136},"2021-10-02","This article describes how to write and share technical articles in 2021",[21149,21150,21151,21152,21153,21154,21155],{"link":20989,"site":11220},{"link":21015,"site":10051},{"link":20892,"site":6303},{"link":20927,"site":15277},{"link":20944,"site":15280},{"link":20997,"site":15283},{"link":20977,"site":18776},{},"/2021/10/02/how-i-write-and-share-technical-software-development-articles-in-2021",{"title":20113,"description":21147},"2021/10/02/how-i-write-and-share-technical-software-development-articles-in-2021",[881,882,21161,21162,883],"publishing","writing","w4p75sF_46iKBx4S62UrjblTvqLE2DWllOA8NSkz2VM",{"id":21165,"title":21166,"body":21167,"comments":872,"date":21230,"description":21231,"draft":872,"extension":873,"external":874,"image":21232,"meta":21233,"navigation":315,"path":21234,"seo":21235,"stem":21236,"tags":21237,"__hash__":21240},"blog/2021/08/07/authenticating-requests-with-jwt-tokens-stored-in-httponly-cookies-in-django.md","Authenticating requests with JWT tokens stored in HTTPOnly cookies in Django",{"type":8,"value":21168,"toc":21223},[21169,21171,21174,21178,21181,21185,21188,21194,21197,21201,21204,21210,21215,21218],[168,21170,8184],{"id":8183},[11,21172,21173],{},"If you want to use JWTs to securely authenticate requests to Django REST Framework applications in a decoupled frontend JavaScript application, you can do the following: store the access token in memory and store the refresh token  in an HttpOnly cookie. The refresh token is used to request new access tokens on an regular interval.",[168,21175,21177],{"id":21176},"some-context","Some context",[11,21179,21180],{},"Django is a web framework based on the Model, Template, View (MTV) paradigm. Django is increasingly used as an API server that is coupled with a Javascript or native frontend application.",[168,21182,21184],{"id":21183},"jwt-auth-with-httponly-cookies","JWT Auth with HttpOnly Cookies",[11,21186,21187],{},"Following this guide:",[11,21189,21190],{},[15,21191,21192],{"href":21192,"rel":21193},"https://hasura.io/blog/best-practices-of-using-jwt-with-graphql/#jwt_security",[19],[11,21195,21196],{},"We can reimplement our JWT authentication setup to be more secure.",[911,21198,21200],{"id":21199},"drf-simple-jwt-modifications","DRF Simple JWT modifications",[11,21202,21203],{},"We need to change the default behavior of the views from DRF simple JWT as described in this issue:",[11,21205,21206],{},[15,21207,21208],{"href":21208,"rel":21209},"https://github.com/jazzband/djangorestframework-simplejwt/issues/71",[19],[11,21211,21212],{},[511,21213],{"alt":7255,"src":21214},"/static/jwt-authentication.png",[11,21216,21217],{},"This diagram shows how authentication data moves between the Django backend and the Vue.js frontend running in the browser.",[11,21219,21220],{},[4339,21221,21222],{},"This article should now be complete complete",{"title":35,"searchDepth":249,"depth":249,"links":21224},[21225,21226,21227],{"id":8183,"depth":249,"text":8184},{"id":21176,"depth":249,"text":21177},{"id":21183,"depth":249,"text":21184,"children":21228},[21229],{"id":21199,"depth":312,"text":21200},"2021-08-01","This article describes how you can use JWT tokens in Django applications with decoupled frontend JavaScript applications running the browser in secure way using HttpOnly cookies.","/static/djjwt/dj_jwt.png",{},"/2021/08/07/authenticating-requests-with-jwt-tokens-stored-in-httponly-cookies-in-django",{"title":21166,"description":21231},"2021/08/07/authenticating-requests-with-jwt-tokens-stored-in-httponly-cookies-in-django",[15290,882,21238,21239],"jwt","authentication","aNmcAKycPah63ijAQpTQ4t2EZSEo2RrVYllChlVd6qk",{"id":21242,"title":21243,"body":21244,"comments":872,"date":21528,"description":21529,"draft":872,"extension":873,"external":874,"image":21530,"meta":21531,"navigation":315,"path":21532,"seo":21533,"stem":21534,"tags":21535,"__hash__":21536},"blog/2021/07/05/github-actions-for-my-nuxt-blog-hosted-on-github-pages.md","GitHub Action for my Nuxt.js blog hosted on GitHub Pages",{"type":8,"value":21245,"toc":21525},[21246,21250,21253,21259,21515,21522],[168,21247,21249],{"id":21248},"github-actions-for-a-nuxtjs-blog-hosted-on-github-pages","GitHub Actions for a Nuxt.js blog hosted on GitHub Pages",[11,21251,21252],{},"To update my blog, I usually build locally and then push changes to GitHub. This makes my git log unreadable. This article will show how to use a GitHub Action to automate the deployment of my blog to GitHub Pages.",[11,21254,21255,21256,358],{},"In this GitHub repo, the following workflow is triggered on pushes to the main branch which will build and deploy changes to ",[15,21257,743],{"href":6329,"rel":21258},[19],[26,21260,21261],{"className":2507,"code":20183,"language":2509,"meta":35,"style":35},[33,21262,21263,21271,21275,21281,21287,21293,21299,21305,21309,21315,21321,21329,21335,21345,21349,21359,21367,21373,21381,21385,21395,21403,21409,21417,21425,21433,21437,21441,21451,21461,21471,21475,21485,21493,21499,21507],{"__ignoreMap":35},[187,21264,21265,21267,21269],{"class":189,"line":190},[187,21266,7284],{"class":2516},[187,21268,585],{"class":577},[187,21270,7289],{"class":196},[187,21272,21273],{"class":189,"line":249},[187,21274,316],{"emptyLinePlaceholder":315},[187,21276,21277,21279],{"class":189,"line":312},[187,21278,7298],{"class":588},[187,21280,2520],{"class":577},[187,21282,21283,21285],{"class":189,"line":319},[187,21284,7305],{"class":2516},[187,21286,2520],{"class":577},[187,21288,21289,21291],{"class":189,"line":325},[187,21290,7312],{"class":2516},[187,21292,2520],{"class":577},[187,21294,21295,21297],{"class":189,"line":686},[187,21296,2610],{"class":577},[187,21298,7321],{"class":196},[187,21300,21301,21303],{"class":189,"line":697},[187,21302,7326],{"class":2516},[187,21304,2520],{"class":577},[187,21306,21307],{"class":189,"line":1291},[187,21308,316],{"emptyLinePlaceholder":315},[187,21310,21311,21313],{"class":189,"line":1306},[187,21312,7337],{"class":2516},[187,21314,2520],{"class":577},[187,21316,21317,21319],{"class":189,"line":1434},[187,21318,7344],{"class":2516},[187,21320,2520],{"class":577},[187,21322,21323,21325,21327],{"class":189,"line":2599},[187,21324,7351],{"class":2516},[187,21326,585],{"class":577},[187,21328,7356],{"class":196},[187,21330,21331,21333],{"class":189,"line":2607},[187,21332,7361],{"class":2516},[187,21334,2520],{"class":577},[187,21336,21337,21339,21341,21343],{"class":189,"line":2621},[187,21338,2610],{"class":577},[187,21340,7370],{"class":2516},[187,21342,585],{"class":577},[187,21344,20268],{"class":196},[187,21346,21347],{"class":189,"line":2631},[187,21348,316],{"emptyLinePlaceholder":315},[187,21350,21351,21353,21355,21357],{"class":189,"line":2642},[187,21352,2610],{"class":577},[187,21354,7284],{"class":2516},[187,21356,585],{"class":577},[187,21358,7390],{"class":196},[187,21360,21361,21363,21365],{"class":189,"line":2653},[187,21362,7395],{"class":2516},[187,21364,585],{"class":577},[187,21366,20291],{"class":196},[187,21368,21369,21371],{"class":189,"line":2665},[187,21370,7405],{"class":2516},[187,21372,2520],{"class":577},[187,21374,21375,21377,21379],{"class":189,"line":2674},[187,21376,7412],{"class":2516},[187,21378,585],{"class":577},[187,21380,20306],{"class":196},[187,21382,21383],{"class":189,"line":2684},[187,21384,316],{"emptyLinePlaceholder":315},[187,21386,21387,21389,21391,21393],{"class":189,"line":2694},[187,21388,2610],{"class":577},[187,21390,7284],{"class":2516},[187,21392,585],{"class":577},[187,21394,7432],{"class":196},[187,21396,21397,21399,21401],{"class":189,"line":2706},[187,21398,7395],{"class":2516},[187,21400,585],{"class":577},[187,21402,20329],{"class":196},[187,21404,21405,21407],{"class":189,"line":2715},[187,21406,7405],{"class":2516},[187,21408,2520],{"class":577},[187,21410,21411,21413,21415],{"class":189,"line":2725},[187,21412,7452],{"class":2516},[187,21414,585],{"class":577},[187,21416,7457],{"class":196},[187,21418,21419,21421,21423],{"class":189,"line":2735},[187,21420,7462],{"class":2516},[187,21422,585],{"class":577},[187,21424,7467],{"class":196},[187,21426,21427,21429,21431],{"class":189,"line":2743},[187,21428,7472],{"class":2516},[187,21430,585],{"class":577},[187,21432,7477],{"class":573},[187,21434,21435],{"class":189,"line":2754},[187,21436,7482],{"class":196},[187,21438,21439],{"class":189,"line":2762},[187,21440,316],{"emptyLinePlaceholder":315},[187,21442,21443,21445,21447,21449],{"class":189,"line":2770},[187,21444,2610],{"class":577},[187,21446,7493],{"class":2516},[187,21448,585],{"class":577},[187,21450,7498],{"class":196},[187,21452,21453,21455,21457,21459],{"class":189,"line":2781},[187,21454,2610],{"class":577},[187,21456,7493],{"class":2516},[187,21458,585],{"class":577},[187,21460,7520],{"class":196},[187,21462,21463,21465,21467,21469],{"class":189,"line":2792},[187,21464,2610],{"class":577},[187,21466,7493],{"class":2516},[187,21468,585],{"class":577},[187,21470,7531],{"class":196},[187,21472,21473],{"class":189,"line":2803},[187,21474,316],{"emptyLinePlaceholder":315},[187,21476,21477,21479,21481,21483],{"class":189,"line":2808},[187,21478,2610],{"class":577},[187,21480,7284],{"class":2516},[187,21482,585],{"class":577},[187,21484,7546],{"class":196},[187,21486,21487,21489,21491],{"class":189,"line":2816},[187,21488,7395],{"class":2516},[187,21490,585],{"class":577},[187,21492,20420],{"class":196},[187,21494,21495,21497],{"class":189,"line":2824},[187,21496,7405],{"class":2516},[187,21498,2520],{"class":577},[187,21500,21501,21503,21505],{"class":189,"line":2834},[187,21502,7566],{"class":2516},[187,21504,585],{"class":577},[187,21506,7571],{"class":196},[187,21508,21509,21511,21513],{"class":189,"line":2845},[187,21510,7576],{"class":2516},[187,21512,585],{"class":577},[187,21514,20443],{"class":196},[11,21516,21517,21518],{},"Reference: ",[15,21519,21520],{"href":21520,"rel":21521},"https://github.com/marketplace/actions/github-pages-action#%EF%B8%8F-vue-and-nuxt",[19],[855,21523,21524],{},"html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":21526},[21527],{"id":21248,"depth":249,"text":21249},"2021-07-05","This article shows how to use GitHub Actions to update my Nuxt.js blog hosted on GitHub Pages.","/static/gha_nuxt/gha_nuxt.png",{},"/2021/07/05/github-actions-for-my-nuxt-blog-hosted-on-github-pages",{"title":21243,"description":21529},"2021/07/05/github-actions-for-my-nuxt-blog-hosted-on-github-pages",[8173,881,15294,20155],"OcwXYRO_dqrKLIlPEf3LR1vdExCdaQS3uSVbvgd6IEI",{"id":21538,"title":21539,"body":21540,"comments":872,"date":24498,"description":24499,"draft":872,"extension":873,"external":874,"image":24500,"meta":24501,"navigation":315,"path":24502,"seo":24503,"stem":24504,"tags":24505,"__hash__":24509},"blog/2021/03/18/on-demand-dedicated-serverless-valheim-server-with-cdk-discrod-interactions.md","On-demand, serverless Valheim server setup with AWS CDK, Discord Interactions and GitLab CI",{"type":8,"value":21541,"toc":24476},[21542,21551,21554,21557,21560,21567,21578,21585,21591,21602,21606,21618,21636,21670,21673,21678,21681,21685,21692,21709,21715,21718,21732,21738,21742,21753,21758,21765,21774,21780,21794,21798,21804,22033,22039,22045,22048,22054,22057,22063,22073,22076,22082,22096,22100,22111,22433,22443,22447,22454,22460,22466,22472,22476,22483,22533,22545,22715,22721,22727,22734,22740,22744,22751,22876,22890,22893,22899,22906,22944,22955,22959,22965,23089,23092,23098,23101,23104,23108,23128,23134,23139,23145,23164,23169,23654,23669,23673,23685,23997,24004,24007,24016,24020,24023,24136,24159,24162,24166,24169,24175,24187,24193,24202,24319,24322,24330,24339,24343,24346,24350,24442,24444,24447,24470,24473],[107,21543,21544],{},[11,21545,21546,21547],{},"Here's a link to the GitLab repo I'll be referencing in this article: ",[15,21548,21549],{"href":21549,"rel":21550},"https://gitlab.com/briancaffey/valheim-cdk-discord-interactions",[19],[11,21552,21553],{},"This is an in-depth technical article about running an on-demand, dedicated server for Valheim using Amazon Web Services controlled with Discord Slash Commands, a new part their Interactions API that is currently in beta. Valheim is an open-world online multiplayer survival game loosely based on Norse mythology that has blown up recently.",[11,21555,21556],{},"My main goal with this project was to find an inexpensive way of running a server given how my friends and I play the game, which is typically a few times a week in the evenings. Some combination of 4 of us will start playing, and we drop in and out between walking dogs, cooking, etc. When playing, we all jump on a dedicated voice channel on our group's Discord.",[11,21558,21559],{},"Before setting up a dedicated server, our game's world state was stored on files that lived on one of our computers, and that computer needed to be running the game server in order for anyone to connect. Sending files around would be possible, but would quickly become tedious. There are lots of services that offer dedicated servers for Valheim, as well as many technical guides and channels on the official Valheim Discord server to support the use of dedicated servers. I wanted to see if I could set up a server myself on AWS using CDK, or Cloud Development Kit. CDK is an Infrastructure as Code (IaC) tool that allows you to define, deploy and update AWS infrastructure with popular programming languages such as Python, Typescript, Java, etc.",[168,21561,21563,21566],{"id":21562},"cdk-valheim-construct-on-github",[33,21564,21565],{},"cdk-valheim"," construct on GitHub",[11,21568,21569,21570,21574,21575,358],{},"The best part of CDK is that it enables the creation high-level, reusable constructs that can be published to software registries like npm and PyPI. Developers can import and use these constructs in their own CDK code. A quick google search for \"cdk valheim\" turned up a few results. ",[15,21571,21565],{"href":21572,"rel":21573},"https://github.com/gotodeploy/cdk-valheim",[19]," seems like the best option for what I was looking for. This project uses ECS, a container orchestration tool from AWS that I have experience using with web applications and EFS for persistent file storage. Although it is written in Typescript, I can still use the construct in my preferred programming language (Python) without any extra effort or configuration. This is thanks to the jsii. From ",[15,21576,11636],{"href":11636,"rel":21577},[19],[107,21579,21580],{},[11,21581,21582,21584],{},[33,21583,11638],{}," allows code in any language to naturally interact with JavaScript classes. It is the technology that enables the AWS Cloud Development Kit to deliver polyglot libraries from a single codebase!",[11,21586,21587,21588,21590],{},"Here's an overview of the ",[33,21589,21565],{}," construct:",[916,21592,21593,21596,21599],{},[919,21594,21595],{},"Scheduled scaling of an ECS service using AWS Fargate (a serverless compute engine for containers)",[919,21597,21598],{},"Elastic File System (EFS) file system mounted into the Fargate Task container of our ECS service",[919,21600,21601],{},"Optional automated backups of the EFS file system using AWS Backup",[168,21603,21605],{"id":21604},"discord-interactions-and-slash-commands","Discord Interactions and Slash Commands",[11,21607,21608,21609,21611,21612,15754,21615,752],{},"Scheduled ECS scaling is nice, but we don't always know when we will be able to play together, so it is not the best way to minimize infrastructure costs. My plan was to set the ECS service to an initial task count of zero and then let any of us set the number of tasks to either zero or one through a Discord Slash Command. A Slash Command allows you to interact with a discord bot by typing ",[33,21610,20174],{}," and then tabbing through to the option we want, such as ",[33,21613,21614],{},"/valheim server start",[33,21616,21617],{},"/valheim server status",[11,21619,21620,21621,21624,21625,15754,21628,21631,21632,21635],{},"Invoking a Slash Command from Discord sends a ",[33,21622,21623],{},"POST"," request from Discord to a webhook URL that we have to provide. To handle the webhook, one simple and inexpensive approach is to use API Gateway and a Lambda function that serves a Flask app. The Flask app can then use boto3 (which is included in the Lambda execution environment) to call ",[33,21626,21627],{},"update_service",[33,21629,21630],{},"describe_services"," to scaled the ECS task's ",[33,21633,21634],{},"desiredCount"," based on the slash command options and sub options.",[11,21637,21638,21639,21641,21642,21645,21646,637,21649,1172,21652,21655,21656,15754,21659,21662,21663,21665,21666,15754,21668,752],{},"The function that handles the webhook ",[33,21640,21623],{}," request when the sub-command is ",[33,21643,21644],{},"status"," queries AWS for the number of ECS tasks in our service that are ",[33,21647,21648],{},"desired",[33,21650,21651],{},"running",[33,21653,21654],{},"pending"," and then sends back a message that will be displayed to the user who sent the command. When the sub-command is ",[33,21657,21658],{},"start",[33,21660,21661],{},"stop",", the ",[33,21664,21634],{}," is either set to ",[33,21667,15625],{},[33,21669,10165],{},[11,21671,21672],{},"Here's an overview of the server setup and how Discord Slash Commands can be used to control the server:",[11,21674,21675],{},[511,21676],{"alt":7255,"src":21677},"/static/valheim/diagram.png",[11,21679,21680],{},"I'll cover each of the steps labelled in this diagram at the end of the article. The rest of the article will provide detailed instructions for how to set everything up. I won't be going over AWS account setup or Discord server setup.",[168,21682,21684],{"id":21683},"how-to-set-up-the-discord-developer-application-and-interaction","How to set up the Discord developer application and Interaction",[11,21686,21687,21688,21691],{},"First, you need to be the admin of a Discord server. Once you create the server, go to ",[33,21689,21690],{},"Server Settings > Widget"," and take note of the Server ID. This is also known as the Guild ID.",[11,21693,21694,21695,21699,21700,21703,21704,8299,21707,358],{},"Then go to ",[15,21696,21697],{"href":21697,"rel":21698},"https://discord.com/developers/applications",[19]," and create an application. Under ",[33,21701,21702],{},"General Information",", make note of the public key (the application ID). Put these values in a file that will be ",[33,21705,21706],{},".gitignored",[33,21708,9057],{},[26,21710,21713],{"className":21711,"code":21712,"language":31},[29],"export GUILD_ID=123456789\nexport APPLICATION_ID=abc123\n",[33,21714,21712],{"__ignoreMap":35},[11,21716,21717],{},"We will use this file later when registering the Interaction.",[11,21719,21720,21721,21724,21725,1172,21728,21731],{},"Next go to the ",[33,21722,21723],{},"OAuth2"," tab and select the ",[33,21726,21727],{},"bot",[33,21729,21730],{},"applications.commands"," permissions. This will generated an OAuth2 authorization link. Copy the link and open it in a browser. We will see an error:",[26,21733,21736],{"className":21734,"code":21735,"language":31},[29],"OAuth2 application does not have a bot\n",[33,21737,21735],{"__ignoreMap":35},[911,21739,21741],{"id":21740},"configure-a-bot-for-our-discord-application","Configure a bot for our Discord application",[11,21743,21744,21745,21748,21749,21752],{},"Next, got to the ",[33,21746,21747],{},"Bot"," tab and click the ",[33,21750,21751],{},"Add Bot"," button.",[107,21754,21755],{},[11,21756,21757],{},"Adding a bot user gives your app visible life in Discord. However, this action is irrevocable! Choose wisely.",[11,21759,21760,21761,21764],{},"Turn off the ",[33,21762,21763],{},"Public Bot"," option and save changes.",[11,21766,21767,21768,21771,21772,358],{},"Get the bot token by clicking on ",[33,21769,21770],{},"Click to Reveal Token",", and add this to ",[33,21773,9057],{},[26,21775,21778],{"className":21776,"code":21777,"language":31},[29],"export BOT_TOKEN=abc.xyz.123\n",[33,21779,21777],{"__ignoreMap":35},[11,21781,21782,21783,21724,21785,1172,21787,21789,21790,21793],{},"Now go back to the ",[33,21784,21723],{},[33,21786,21727],{},[33,21788,21730],{}," permissions again, copy the link and open it. Select the server that you want to add this application to. You should see a captcha, and then a message that says ",[33,21791,21792],{},"Authorized",". You should also see a message from your discord server that the bot has joined the server.",[911,21795,21797],{"id":21796},"create-the-interaction","Create the Interaction",[11,21799,21800,21801,21803],{},"Now we will set up the Interaction. Currently the only way to set up the interaction is through an HTTP ",[33,21802,21623],{}," request. This Python script sets up our Interaction:",[26,21805,21807],{"className":10554,"code":21806,"language":10556,"meta":35,"style":35},"\"\"\"\nhttps://discord.com/developers/docs/interactions/slash-commands#registering-a-command\n\"\"\"\n\nimport os\n\nimport requests\n\nAPPLICATION_ID = os.environ.get(\"APPLICATION_ID\")\nGUILD_ID = os.environ.get(\"GUILD_ID\")\nBOT_TOKEN = os.environ.get(\"BOT_TOKEN\")\n\nurl = f\"https://discord.com/api/v8/applications/{APPLICATION_ID}/guilds/{GUILD_ID}/commands\"\n\njson = {\n    \"name\": \"vh\",\n    \"description\": \"Start, stop or get the status of the Valheim server\",\n    \"options\": [\n        {\n            \"name\": \"valheim_server_controls\",\n            \"description\": \"What do you want to do?\",\n            \"type\": 3,\n            \"required\": True,\n            \"choices\": [\n                {\n                    \"name\": \"status\",\n                    \"value\": \"status\"\n                },\n                {\n                    \"name\": \"start\",\n                    \"value\": \"start\"\n                },\n                {\n                    \"name\": \"stop\",\n                    \"value\": \"stop\"\n                }\n            ]\n        },\n    ]\n}\n\nheaders = {\n    \"Authorization\": f\"Bot {BOT_TOKEN}\"\n}\n\nif __name__ == \"__main__\":\n    r = requests.post(url, headers=headers, json=json)\n    print(r.content)\n",[33,21808,21809,21814,21819,21823,21827,21831,21835,21840,21844,21849,21854,21859,21863,21868,21872,21877,21882,21887,21892,21897,21902,21907,21912,21917,21922,21927,21932,21937,21942,21946,21951,21956,21960,21964,21969,21974,21979,21983,21988,21993,21997,22001,22006,22011,22015,22019,22023,22028],{"__ignoreMap":35},[187,21810,21811],{"class":189,"line":190},[187,21812,21813],{},"\"\"\"\n",[187,21815,21816],{"class":189,"line":249},[187,21817,21818],{},"https://discord.com/developers/docs/interactions/slash-commands#registering-a-command\n",[187,21820,21821],{"class":189,"line":312},[187,21822,21813],{},[187,21824,21825],{"class":189,"line":319},[187,21826,316],{"emptyLinePlaceholder":315},[187,21828,21829],{"class":189,"line":325},[187,21830,10345],{},[187,21832,21833],{"class":189,"line":686},[187,21834,316],{"emptyLinePlaceholder":315},[187,21836,21837],{"class":189,"line":697},[187,21838,21839],{},"import requests\n",[187,21841,21842],{"class":189,"line":1291},[187,21843,316],{"emptyLinePlaceholder":315},[187,21845,21846],{"class":189,"line":1306},[187,21847,21848],{},"APPLICATION_ID = os.environ.get(\"APPLICATION_ID\")\n",[187,21850,21851],{"class":189,"line":1434},[187,21852,21853],{},"GUILD_ID = os.environ.get(\"GUILD_ID\")\n",[187,21855,21856],{"class":189,"line":2599},[187,21857,21858],{},"BOT_TOKEN = os.environ.get(\"BOT_TOKEN\")\n",[187,21860,21861],{"class":189,"line":2607},[187,21862,316],{"emptyLinePlaceholder":315},[187,21864,21865],{"class":189,"line":2621},[187,21866,21867],{},"url = f\"https://discord.com/api/v8/applications/{APPLICATION_ID}/guilds/{GUILD_ID}/commands\"\n",[187,21869,21870],{"class":189,"line":2631},[187,21871,316],{"emptyLinePlaceholder":315},[187,21873,21874],{"class":189,"line":2642},[187,21875,21876],{},"json = {\n",[187,21878,21879],{"class":189,"line":2653},[187,21880,21881],{},"    \"name\": \"vh\",\n",[187,21883,21884],{"class":189,"line":2665},[187,21885,21886],{},"    \"description\": \"Start, stop or get the status of the Valheim server\",\n",[187,21888,21889],{"class":189,"line":2674},[187,21890,21891],{},"    \"options\": [\n",[187,21893,21894],{"class":189,"line":2684},[187,21895,21896],{},"        {\n",[187,21898,21899],{"class":189,"line":2694},[187,21900,21901],{},"            \"name\": \"valheim_server_controls\",\n",[187,21903,21904],{"class":189,"line":2706},[187,21905,21906],{},"            \"description\": \"What do you want to do?\",\n",[187,21908,21909],{"class":189,"line":2715},[187,21910,21911],{},"            \"type\": 3,\n",[187,21913,21914],{"class":189,"line":2725},[187,21915,21916],{},"            \"required\": True,\n",[187,21918,21919],{"class":189,"line":2735},[187,21920,21921],{},"            \"choices\": [\n",[187,21923,21924],{"class":189,"line":2743},[187,21925,21926],{},"                {\n",[187,21928,21929],{"class":189,"line":2754},[187,21930,21931],{},"                    \"name\": \"status\",\n",[187,21933,21934],{"class":189,"line":2762},[187,21935,21936],{},"                    \"value\": \"status\"\n",[187,21938,21939],{"class":189,"line":2770},[187,21940,21941],{},"                },\n",[187,21943,21944],{"class":189,"line":2781},[187,21945,21926],{},[187,21947,21948],{"class":189,"line":2792},[187,21949,21950],{},"                    \"name\": \"start\",\n",[187,21952,21953],{"class":189,"line":2803},[187,21954,21955],{},"                    \"value\": \"start\"\n",[187,21957,21958],{"class":189,"line":2808},[187,21959,21941],{},[187,21961,21962],{"class":189,"line":2816},[187,21963,21926],{},[187,21965,21966],{"class":189,"line":2824},[187,21967,21968],{},"                    \"name\": \"stop\",\n",[187,21970,21971],{"class":189,"line":2834},[187,21972,21973],{},"                    \"value\": \"stop\"\n",[187,21975,21976],{"class":189,"line":2845},[187,21977,21978],{},"                }\n",[187,21980,21981],{"class":189,"line":2856},[187,21982,4526],{},[187,21984,21985],{"class":189,"line":2867},[187,21986,21987],{},"        },\n",[187,21989,21990],{"class":189,"line":2878},[187,21991,21992],{},"    ]\n",[187,21994,21995],{"class":189,"line":2886},[187,21996,1309],{},[187,21998,21999],{"class":189,"line":2900},[187,22000,316],{"emptyLinePlaceholder":315},[187,22002,22003],{"class":189,"line":2905},[187,22004,22005],{},"headers = {\n",[187,22007,22008],{"class":189,"line":2913},[187,22009,22010],{},"    \"Authorization\": f\"Bot {BOT_TOKEN}\"\n",[187,22012,22013],{"class":189,"line":2921},[187,22014,1309],{},[187,22016,22017],{"class":189,"line":2931},[187,22018,316],{"emptyLinePlaceholder":315},[187,22020,22021],{"class":189,"line":2942},[187,22022,4720],{},[187,22024,22025],{"class":189,"line":2953},[187,22026,22027],{},"    r = requests.post(url, headers=headers, json=json)\n",[187,22029,22030],{"class":189,"line":2964},[187,22031,22032],{},"    print(r.content)\n",[11,22034,22035,22036,22038],{},"Before running this command, source the ",[33,22037,9057],{}," file:",[26,22040,22043],{"className":22041,"code":22042,"language":31},[29],"source .env\n",[33,22044,22042],{"__ignoreMap":35},[11,22046,22047],{},"Then run the script:",[26,22049,22052],{"className":22050,"code":22051,"language":31},[29],"python3 register_bot.py\n",[33,22053,22051],{"__ignoreMap":35},[11,22055,22056],{},"You should see this response:",[26,22058,22061],{"className":22059,"code":22060,"language":31},[29],"b'{\"id\": \"XXXXXXXXXXXXXX\", \"application_id\": \"XXXXXXXXXXXXXX\", \"name\": \"vh\", \"description\": \"Start, stop or get the status of the Valheim server\", \"version\": \"XXXXXXXXXXXXXX\", \"default_permission\": true, \"guild_id\": \"XXXXXXXXXXXXXX\", \"options\": [{\"type\": 3, \"name\": \"valheim_server_controls\", \"description\": \"What do you want to do?\", \"required\": true, \"choices\": [{\"name\": \"status\", \"value\": \"status\"}, {\"name\": \"start\", \"value\": \"start\"}, {\"name\": \"stop\", \"value\": \"stop\"}]}]}'\n",[33,22062,22060],{"__ignoreMap":35},[11,22064,22065,22066,22068,22069,22072],{},"Now, when you type ",[33,22067,20174],{}," in any channel on the Discord server that you authenticated the bot, you should see the ",[33,22070,22071],{},"vh"," command at the top of the list of autocomplete options.",[11,22074,22075],{},"If we run any of these commands, we should see a response saying:",[26,22077,22080],{"className":22078,"code":22079,"language":31},[29],"This interaction failed\n",[33,22081,22079],{"__ignoreMap":35},[11,22083,22084,22085,22088,22089,22091,22092,1737],{},"This is because we have not configured an ",[33,22086,22087],{},"Interactions Endpoint URL"," under the ",[33,22090,21702],{}," section of our Discord Application's admin page (",[15,22093,22094],{"href":22094,"rel":22095},"https://discord.com/developers/applications/",[19],[168,22097,22099],{"id":22098},"setting-up-the-interactions-endpoint-url-for-our-slash-command","Setting up the Interactions Endpoint URL for our Slash Command",[11,22101,22102,22103,22105,22106,358],{},"In order for our Slash Command to do anything, we need to set up URL that Discord will ",[33,22104,21623],{}," the Interaction event data to, including information such as who sent the Interaction, what channel it was sent on, what options were used, etc. You can see an example of the event payload ",[15,22107,22110],{"href":22108,"rel":22109},"https://discord.com/developers/docs/interactions/slash-commands#receiving-an-interaction",[19],"here on the Discord developer documentation",[26,22112,22114],{"className":1200,"code":22113,"language":1202,"meta":35,"style":35},"{\n    \"type\": 2,\n    \"token\": \"A_UNIQUE_TOKEN\",\n    \"member\": {\n        \"user\": {\n            \"id\": 53908232506183680,\n            \"username\": \"Mason\",\n            \"avatar\": \"a_d5efa99b3eeaa7dd43acca82f5692432\",\n            \"discriminator\": \"1337\",\n            \"public_flags\": 131141\n        },\n        \"roles\": [\"539082325061836999\"],\n        \"premium_since\": null,\n        \"permissions\": \"2147483647\",\n        \"pending\": false,\n        \"nick\": null,\n        \"mute\": false,\n        \"joined_at\": \"2017-03-13T19:19:14.040000+00:00\",\n        \"is_pending\": false,\n        \"deaf\": false\n    },\n    \"id\": \"786008729715212338\",\n    \"guild_id\": \"290926798626357999\",\n    \"data\": {\n        \"options\": [{\n            \"name\": \"cardname\",\n            \"value\": \"The Gitrog Monster\"\n        }],\n        \"name\": \"cardsearch\",\n        \"id\": \"771825006014889984\"\n    },\n    \"channel_id\": \"645027906669510667\"\n}\n",[33,22115,22116,22120,22132,22144,22151,22158,22170,22182,22194,22206,22216,22220,22233,22245,22257,22268,22279,22290,22302,22313,22322,22327,22339,22351,22358,22366,22378,22388,22393,22405,22415,22419,22429],{"__ignoreMap":35},[187,22117,22118],{"class":189,"line":190},[187,22119,1209],{"class":577},[187,22121,22122,22125,22127,22130],{"class":189,"line":249},[187,22123,22124],{"class":588},"    \"type\"",[187,22126,585],{"class":577},[187,22128,22129],{"class":588},"2",[187,22131,1228],{"class":577},[187,22133,22134,22137,22139,22142],{"class":189,"line":312},[187,22135,22136],{"class":588},"    \"token\"",[187,22138,585],{"class":577},[187,22140,22141],{"class":196},"\"A_UNIQUE_TOKEN\"",[187,22143,1228],{"class":577},[187,22145,22146,22149],{"class":189,"line":319},[187,22147,22148],{"class":588},"    \"member\"",[187,22150,8099],{"class":577},[187,22152,22153,22156],{"class":189,"line":325},[187,22154,22155],{"class":588},"        \"user\"",[187,22157,8099],{"class":577},[187,22159,22160,22163,22165,22168],{"class":189,"line":686},[187,22161,22162],{"class":588},"            \"id\"",[187,22164,585],{"class":577},[187,22166,22167],{"class":588},"53908232506183680",[187,22169,1228],{"class":577},[187,22171,22172,22175,22177,22180],{"class":189,"line":697},[187,22173,22174],{"class":588},"            \"username\"",[187,22176,585],{"class":577},[187,22178,22179],{"class":196},"\"Mason\"",[187,22181,1228],{"class":577},[187,22183,22184,22187,22189,22192],{"class":189,"line":1291},[187,22185,22186],{"class":588},"            \"avatar\"",[187,22188,585],{"class":577},[187,22190,22191],{"class":196},"\"a_d5efa99b3eeaa7dd43acca82f5692432\"",[187,22193,1228],{"class":577},[187,22195,22196,22199,22201,22204],{"class":189,"line":1306},[187,22197,22198],{"class":588},"            \"discriminator\"",[187,22200,585],{"class":577},[187,22202,22203],{"class":196},"\"1337\"",[187,22205,1228],{"class":577},[187,22207,22208,22211,22213],{"class":189,"line":1434},[187,22209,22210],{"class":588},"            \"public_flags\"",[187,22212,585],{"class":577},[187,22214,22215],{"class":588},"131141\n",[187,22217,22218],{"class":189,"line":2599},[187,22219,21987],{"class":577},[187,22221,22222,22225,22227,22230],{"class":189,"line":2607},[187,22223,22224],{"class":588},"        \"roles\"",[187,22226,2562],{"class":577},[187,22228,22229],{"class":196},"\"539082325061836999\"",[187,22231,22232],{"class":577},"],\n",[187,22234,22235,22238,22240,22243],{"class":189,"line":2621},[187,22236,22237],{"class":588},"        \"premium_since\"",[187,22239,585],{"class":577},[187,22241,22242],{"class":588},"null",[187,22244,1228],{"class":577},[187,22246,22247,22250,22252,22255],{"class":189,"line":2631},[187,22248,22249],{"class":588},"        \"permissions\"",[187,22251,585],{"class":577},[187,22253,22254],{"class":196},"\"2147483647\"",[187,22256,1228],{"class":577},[187,22258,22259,22262,22264,22266],{"class":189,"line":2642},[187,22260,22261],{"class":588},"        \"pending\"",[187,22263,585],{"class":577},[187,22265,12660],{"class":588},[187,22267,1228],{"class":577},[187,22269,22270,22273,22275,22277],{"class":189,"line":2653},[187,22271,22272],{"class":588},"        \"nick\"",[187,22274,585],{"class":577},[187,22276,22242],{"class":588},[187,22278,1228],{"class":577},[187,22280,22281,22284,22286,22288],{"class":189,"line":2665},[187,22282,22283],{"class":588},"        \"mute\"",[187,22285,585],{"class":577},[187,22287,12660],{"class":588},[187,22289,1228],{"class":577},[187,22291,22292,22295,22297,22300],{"class":189,"line":2674},[187,22293,22294],{"class":588},"        \"joined_at\"",[187,22296,585],{"class":577},[187,22298,22299],{"class":196},"\"2017-03-13T19:19:14.040000+00:00\"",[187,22301,1228],{"class":577},[187,22303,22304,22307,22309,22311],{"class":189,"line":2684},[187,22305,22306],{"class":588},"        \"is_pending\"",[187,22308,585],{"class":577},[187,22310,12660],{"class":588},[187,22312,1228],{"class":577},[187,22314,22315,22318,22320],{"class":189,"line":2694},[187,22316,22317],{"class":588},"        \"deaf\"",[187,22319,585],{"class":577},[187,22321,2751],{"class":588},[187,22323,22324],{"class":189,"line":2706},[187,22325,22326],{"class":577},"    },\n",[187,22328,22329,22332,22334,22337],{"class":189,"line":2715},[187,22330,22331],{"class":588},"    \"id\"",[187,22333,585],{"class":577},[187,22335,22336],{"class":196},"\"786008729715212338\"",[187,22338,1228],{"class":577},[187,22340,22341,22344,22346,22349],{"class":189,"line":2725},[187,22342,22343],{"class":588},"    \"guild_id\"",[187,22345,585],{"class":577},[187,22347,22348],{"class":196},"\"290926798626357999\"",[187,22350,1228],{"class":577},[187,22352,22353,22356],{"class":189,"line":2735},[187,22354,22355],{"class":588},"    \"data\"",[187,22357,8099],{"class":577},[187,22359,22360,22363],{"class":189,"line":2743},[187,22361,22362],{"class":588},"        \"options\"",[187,22364,22365],{"class":577},": [{\n",[187,22367,22368,22371,22373,22376],{"class":189,"line":2754},[187,22369,22370],{"class":588},"            \"name\"",[187,22372,585],{"class":577},[187,22374,22375],{"class":196},"\"cardname\"",[187,22377,1228],{"class":577},[187,22379,22380,22383,22385],{"class":189,"line":2762},[187,22381,22382],{"class":588},"            \"value\"",[187,22384,585],{"class":577},[187,22386,22387],{"class":196},"\"The Gitrog Monster\"\n",[187,22389,22390],{"class":189,"line":2770},[187,22391,22392],{"class":577},"        }],\n",[187,22394,22395,22398,22400,22403],{"class":189,"line":2781},[187,22396,22397],{"class":588},"        \"name\"",[187,22399,585],{"class":577},[187,22401,22402],{"class":196},"\"cardsearch\"",[187,22404,1228],{"class":577},[187,22406,22407,22410,22412],{"class":189,"line":2792},[187,22408,22409],{"class":588},"        \"id\"",[187,22411,585],{"class":577},[187,22413,22414],{"class":196},"\"771825006014889984\"\n",[187,22416,22417],{"class":189,"line":2803},[187,22418,22326],{"class":577},[187,22420,22421,22424,22426],{"class":189,"line":2808},[187,22422,22423],{"class":588},"    \"channel_id\"",[187,22425,585],{"class":577},[187,22427,22428],{"class":196},"\"645027906669510667\"\n",[187,22430,22431],{"class":189,"line":2816},[187,22432,1309],{"class":577},[11,22434,22435,22436,22438,22439,22442],{},"This ",[33,22437,21623],{}," request also includes some special headers used for security that we will need to do validation with our handling function. This part can be handled with a decorator provided by the ",[33,22440,22441],{},"discord-interactions"," package on PyPI, but we will need to add some additional configuration to our API Gateway endpoint since these headers will not be passed through the lambda by default.",[168,22444,22446],{"id":22445},"setting-up-aws-infrastructure-with-cdk","Setting up AWS infrastructure with CDK",[11,22448,22449,22450,22453],{},"Let's start a CDK project in a blank repository that will define our infrastructure, Lambda functions and CI/CD pipeline with GitLab CI. Make sure that you have the ",[33,22451,22452],{},"aws-cdk"," CLI installed globally:",[26,22455,22458],{"className":22456,"code":22457,"language":31},[29],"npm i -g aws-cdk\n",[33,22459,22457],{"__ignoreMap":35},[11,22461,22462,22463,22465],{},"Then start a CDK project in a subdirectory called ",[33,22464,15291],{}," with:",[26,22467,22470],{"className":22468,"code":22469,"language":31},[29],"mkdir cdk && cd cdk && cdk init app --language=python\n",[33,22471,22469],{"__ignoreMap":35},[911,22473,22475],{"id":22474},"add-cdk-project-dependencies","Add CDK project dependencies",[11,22477,22478,22479,22482],{},"The next step is to add all of the dependencies to our CDK project that we will use in this project. In ",[33,22480,22481],{},"setup.py"," add the following:",[26,22484,22486],{"className":10554,"code":22485,"language":10556,"meta":35,"style":35},"    install_requires=[\n        \"aws-cdk.core==1.92.0\",\n        \"aws-cdk.aws_applicationautoscaling==1.92.0\",\n        \"aws-cdk.aws_datasync==1.92.0\",\n        \"aws-cdk.aws_lambda==1.92.0\",\n        \"aws-cdk.aws_s3==1.92.0\",\n        \"aws-cdk.aws_apigateway==1.92.0\",\n        \"cdk-valheim==0.0.16\",\n    ],\n",[33,22487,22488,22493,22498,22503,22508,22513,22518,22523,22528],{"__ignoreMap":35},[187,22489,22490],{"class":189,"line":190},[187,22491,22492],{},"    install_requires=[\n",[187,22494,22495],{"class":189,"line":249},[187,22496,22497],{},"        \"aws-cdk.core==1.92.0\",\n",[187,22499,22500],{"class":189,"line":312},[187,22501,22502],{},"        \"aws-cdk.aws_applicationautoscaling==1.92.0\",\n",[187,22504,22505],{"class":189,"line":319},[187,22506,22507],{},"        \"aws-cdk.aws_datasync==1.92.0\",\n",[187,22509,22510],{"class":189,"line":325},[187,22511,22512],{},"        \"aws-cdk.aws_lambda==1.92.0\",\n",[187,22514,22515],{"class":189,"line":686},[187,22516,22517],{},"        \"aws-cdk.aws_s3==1.92.0\",\n",[187,22519,22520],{"class":189,"line":697},[187,22521,22522],{},"        \"aws-cdk.aws_apigateway==1.92.0\",\n",[187,22524,22525],{"class":189,"line":1291},[187,22526,22527],{},"        \"cdk-valheim==0.0.16\",\n",[187,22529,22530],{"class":189,"line":1306},[187,22531,22532],{},"    ],\n",[11,22534,22535,22536,9054,22539,22542,22543,358],{},"Next we can add the CDK construct for ",[33,22537,22538],{},"ValheimWorld",[33,22540,22541],{},"cdk_stack.py"," file that was generated in our project as well as the imports for the packages we included in ",[33,22544,22481],{},[26,22546,22548],{"className":10554,"code":22547,"language":10556,"meta":35,"style":35},"from aws_cdk import core as cdk\n\nfrom aws_cdk import (\n    core,\n    aws_datasync as datasync,\n    aws_iam as iam,\n    aws_lambda as _lambda,\n    aws_apigateway as apigw,\n    aws_applicationautoscaling as appScaling,\n    aws_s3 as s3,\n)\nfrom cdk_valheim import ValheimWorld, ValheimWorldScalingSchedule\n\n\nclass CdkStack(cdk.Stack):\n\n    def __init__(self, scope: cdk.Construct, construct_id: str, **kwargs) -> None:\n        super().__init__(scope, construct_id, **kwargs)\n        # The code that defines your stack goes here\n        self.valheim_world = ValheimWorld(\n            self,\n            'ValheimWorld',\n            cpu=2048,\n            memory_limit_mib=4096,\n            schedules=[ValheimWorldScalingSchedule(\n                start=appScaling.CronOptions(hour='12', week_day='1-5'),\n                stop=appScaling.CronOptions(hour='1', week_day='1-5'),\n            )],\n            environment={\n                \"SERVER_NAME\": os.environ.get(\"SERVER_NAME\", \"CDK Valheim\"),\n                \"WORLD_NAME\": os.environ.get(\"WORLD_NAME\", \"Amazon\"),\n                \"SERVER_PASS\": os.environ.get(\"SERVER_PASS\", \"fargate\"),\n                \"BACKUPS\": 'false',\n            })\n",[33,22549,22550,22555,22559,22564,22569,22574,22579,22584,22589,22594,22599,22603,22608,22612,22616,22621,22625,22630,22635,22640,22645,22650,22655,22660,22665,22670,22675,22680,22685,22690,22695,22700,22705,22710],{"__ignoreMap":35},[187,22551,22552],{"class":189,"line":190},[187,22553,22554],{},"from aws_cdk import core as cdk\n",[187,22556,22557],{"class":189,"line":249},[187,22558,316],{"emptyLinePlaceholder":315},[187,22560,22561],{"class":189,"line":312},[187,22562,22563],{},"from aws_cdk import (\n",[187,22565,22566],{"class":189,"line":319},[187,22567,22568],{},"    core,\n",[187,22570,22571],{"class":189,"line":325},[187,22572,22573],{},"    aws_datasync as datasync,\n",[187,22575,22576],{"class":189,"line":686},[187,22577,22578],{},"    aws_iam as iam,\n",[187,22580,22581],{"class":189,"line":697},[187,22582,22583],{},"    aws_lambda as _lambda,\n",[187,22585,22586],{"class":189,"line":1291},[187,22587,22588],{},"    aws_apigateway as apigw,\n",[187,22590,22591],{"class":189,"line":1306},[187,22592,22593],{},"    aws_applicationautoscaling as appScaling,\n",[187,22595,22596],{"class":189,"line":1434},[187,22597,22598],{},"    aws_s3 as s3,\n",[187,22600,22601],{"class":189,"line":2599},[187,22602,621],{},[187,22604,22605],{"class":189,"line":2607},[187,22606,22607],{},"from cdk_valheim import ValheimWorld, ValheimWorldScalingSchedule\n",[187,22609,22610],{"class":189,"line":2621},[187,22611,316],{"emptyLinePlaceholder":315},[187,22613,22614],{"class":189,"line":2631},[187,22615,316],{"emptyLinePlaceholder":315},[187,22617,22618],{"class":189,"line":2642},[187,22619,22620],{},"class CdkStack(cdk.Stack):\n",[187,22622,22623],{"class":189,"line":2653},[187,22624,316],{"emptyLinePlaceholder":315},[187,22626,22627],{"class":189,"line":2665},[187,22628,22629],{},"    def __init__(self, scope: cdk.Construct, construct_id: str, **kwargs) -> None:\n",[187,22631,22632],{"class":189,"line":2674},[187,22633,22634],{},"        super().__init__(scope, construct_id, **kwargs)\n",[187,22636,22637],{"class":189,"line":2684},[187,22638,22639],{},"        # The code that defines your stack goes here\n",[187,22641,22642],{"class":189,"line":2694},[187,22643,22644],{},"        self.valheim_world = ValheimWorld(\n",[187,22646,22647],{"class":189,"line":2706},[187,22648,22649],{},"            self,\n",[187,22651,22652],{"class":189,"line":2715},[187,22653,22654],{},"            'ValheimWorld',\n",[187,22656,22657],{"class":189,"line":2725},[187,22658,22659],{},"            cpu=2048,\n",[187,22661,22662],{"class":189,"line":2735},[187,22663,22664],{},"            memory_limit_mib=4096,\n",[187,22666,22667],{"class":189,"line":2743},[187,22668,22669],{},"            schedules=[ValheimWorldScalingSchedule(\n",[187,22671,22672],{"class":189,"line":2754},[187,22673,22674],{},"                start=appScaling.CronOptions(hour='12', week_day='1-5'),\n",[187,22676,22677],{"class":189,"line":2762},[187,22678,22679],{},"                stop=appScaling.CronOptions(hour='1', week_day='1-5'),\n",[187,22681,22682],{"class":189,"line":2770},[187,22683,22684],{},"            )],\n",[187,22686,22687],{"class":189,"line":2781},[187,22688,22689],{},"            environment={\n",[187,22691,22692],{"class":189,"line":2792},[187,22693,22694],{},"                \"SERVER_NAME\": os.environ.get(\"SERVER_NAME\", \"CDK Valheim\"),\n",[187,22696,22697],{"class":189,"line":2803},[187,22698,22699],{},"                \"WORLD_NAME\": os.environ.get(\"WORLD_NAME\", \"Amazon\"),\n",[187,22701,22702],{"class":189,"line":2808},[187,22703,22704],{},"                \"SERVER_PASS\": os.environ.get(\"SERVER_PASS\", \"fargate\"),\n",[187,22706,22707],{"class":189,"line":2816},[187,22708,22709],{},"                \"BACKUPS\": 'false',\n",[187,22711,22712],{"class":189,"line":2824},[187,22713,22714],{},"            })\n",[11,22716,22717,22718,22720],{},"We are almost ready to deploy a basic version of our Valheim server using the ",[33,22719,21565],{}," construct. If we were to run the following command:",[26,22722,22725],{"className":22723,"code":22724,"language":31},[29],"cdk deploy --app cdk/app.py --require-approval never\n",[33,22726,22724],{"__ignoreMap":35},[11,22728,22729,22730,22733],{},"from the root of our project, it should work. This assumes that we have default credentials configured in ",[33,22731,22732],{},"~/.aws/credentials"," and that we have also bootstrapped our AWS account with the resources it needs for CDK to work:",[26,22735,22738],{"className":22736,"code":22737,"language":31},[29],"cdk bootstrap --app cdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n",[33,22739,22737],{"__ignoreMap":35},[911,22741,22743],{"id":22742},"setup-gitlab-ci-job-for-automated-deployments","Setup GitLab CI job for automated deployments",[11,22745,22746,22747,22750],{},"Instead of deploying from the command line, it would be better to run the deployment from a CI/CD pipeline. Add a ",[33,22748,22749],{},".gitlab-ci.yml"," file to the root of your project and populate it with the following YAML:",[26,22752,22754],{"className":2507,"code":22753,"language":2509,"meta":35,"style":35},"stages:\n  - deploy\n\nimage: python:3.8\n\ncdk_deploy:\n  stage: deploy\n  rules:\n    - if: \"$CI_COMMIT_TAG\"\n      when: always\n  before_script:\n    - apt-get -qq update && apt-get -y install nodejs npm\n    - npm i -g aws-cdk\n    - pip3 install -e cdk\n  script:\n    - cdk bootstrap --app cdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n    - cdk deploy --app cdk/app.py --require-approval never\n",[33,22755,22756,22763,22769,22773,22782,22786,22793,22802,22809,22820,22830,22837,22844,22850,22857,22864,22870],{"__ignoreMap":35},[187,22757,22758,22761],{"class":189,"line":190},[187,22759,22760],{"class":2516},"stages",[187,22762,2520],{"class":577},[187,22764,22765,22767],{"class":189,"line":249},[187,22766,6592],{"class":577},[187,22768,7546],{"class":196},[187,22770,22771],{"class":189,"line":312},[187,22772,316],{"emptyLinePlaceholder":315},[187,22774,22775,22777,22779],{"class":189,"line":319},[187,22776,6570],{"class":2516},[187,22778,585],{"class":577},[187,22780,22781],{"class":196},"python:3.8\n",[187,22783,22784],{"class":189,"line":325},[187,22785,316],{"emptyLinePlaceholder":315},[187,22787,22788,22791],{"class":189,"line":686},[187,22789,22790],{"class":2516},"cdk_deploy",[187,22792,2520],{"class":577},[187,22794,22795,22798,22800],{"class":189,"line":697},[187,22796,22797],{"class":2516},"  stage",[187,22799,585],{"class":577},[187,22801,7546],{"class":196},[187,22803,22804,22807],{"class":189,"line":1291},[187,22805,22806],{"class":2516},"  rules",[187,22808,2520],{"class":577},[187,22810,22811,22813,22815,22817],{"class":189,"line":1306},[187,22812,14709],{"class":577},[187,22814,19681],{"class":2516},[187,22816,585],{"class":577},[187,22818,22819],{"class":196},"\"$CI_COMMIT_TAG\"\n",[187,22821,22822,22825,22827],{"class":189,"line":1434},[187,22823,22824],{"class":2516},"      when",[187,22826,585],{"class":577},[187,22828,22829],{"class":196},"always\n",[187,22831,22832,22835],{"class":189,"line":2599},[187,22833,22834],{"class":2516},"  before_script",[187,22836,2520],{"class":577},[187,22838,22839,22841],{"class":189,"line":2607},[187,22840,14709],{"class":577},[187,22842,22843],{"class":196},"apt-get -qq update && apt-get -y install nodejs npm\n",[187,22845,22846,22848],{"class":189,"line":2621},[187,22847,14709],{"class":577},[187,22849,22457],{"class":196},[187,22851,22852,22854],{"class":189,"line":2631},[187,22853,14709],{"class":577},[187,22855,22856],{"class":196},"pip3 install -e cdk\n",[187,22858,22859,22862],{"class":189,"line":2642},[187,22860,22861],{"class":2516},"  script",[187,22863,2520],{"class":577},[187,22865,22866,22868],{"class":189,"line":2653},[187,22867,14709],{"class":577},[187,22869,22737],{"class":196},[187,22871,22872,22874],{"class":189,"line":2665},[187,22873,14709],{"class":577},[187,22875,22724],{"class":196},[11,22877,22878,22879,22882,22883,22886,22887,752],{},"Before we initialize a git repository in the root directory of our project, remove the ",[33,22880,22881],{},".git"," repo that CDK created when we initialized the project with ",[33,22884,22885],{},"rf -rf cdk/.git",". Now initialized a project in the root directory with ",[33,22888,22889],{},"git init",[11,22891,22892],{},"Next, create a GitLab repository and add the remote to this project with:",[26,22894,22897],{"className":22895,"code":22896,"language":31},[29],"git remote add origin git@gitlab.com:gitlab-username/project-name.git\n",[33,22898,22896],{"__ignoreMap":35},[11,22900,22901,22902,22905],{},"In the GitLab project's ",[33,22903,22904],{},"Settings > CI/CD > Variables"," section, add the following environment variables as protected variables:",[916,22907,22908,22912,22916,22920,22924,22929,22934,22939],{},[919,22909,22910],{},[33,22911,18254],{},[919,22913,22914],{},[33,22915,18275],{},[919,22917,22918],{},[33,22919,18266],{},[919,22921,22922],{},[33,22923,18260],{},[919,22925,22926],{},[33,22927,22928],{},"APPLICATION_PUBLIC_KEY",[919,22930,22931],{},[33,22932,22933],{},"SERVER_PASS",[919,22935,22936],{},[33,22937,22938],{},"SERVER_NAME",[919,22940,22941],{},[33,22942,22943],{},"WORLD_NAME",[11,22945,22946,22947,22950,22951,22954],{},"Now under ",[33,22948,22949],{},"Settings > Repository > Protected Tags",", add a wildcard (",[33,22952,22953],{},"*",") so that all tags are protected and only maintainers can push tags. This allows us to use the protected environment variables only when a trusted maintainer pushes a tag to the repository.",[911,22956,22958],{"id":22957},"edit-the-name-of-the-stack-and-add-region-and-account-info","Edit the name of the stack and add region and account info",[11,22960,22961,22962,358],{},"We are almost ready to create a tag and push to GitLab, but before we do that let's change the name of the CloudFormation stack that CDK will create in ",[33,22963,22964],{},"cdk/app.py",[26,22966,22968],{"className":10554,"code":22967,"language":10556,"meta":35,"style":35},"#!/usr/bin/env python3\n\nimport os\n\nfrom aws_cdk import core as cdk\n\n# For consistency with TypeScript code, `cdk` is the preferred import name for\n# the CDK's core module.  The following line also imports it as `core` for use\n# with examples from the CDK Developer's Guide, which are in the process of\n# being updated to use `cdk`.  You may delete this import if you don't need it.\nfrom aws_cdk import core\n\nfrom cdk.cdk_stack import CdkStack\n\naws_region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\naws_account = os.environ.get(\"AWS_ACCOUNT_ID\", \"\")\n\n\napp = cdk.App()\nCdkStack(\n    app,\n    \"valheim-server-stack\",\n    env={\"region\": aws_region, \"account\": aws_account}\n)\n\napp.synth()\n",[33,22969,22970,22975,22979,22983,22987,22991,22995,23000,23005,23010,23015,23020,23024,23029,23033,23038,23043,23047,23051,23056,23061,23066,23071,23076,23080,23084],{"__ignoreMap":35},[187,22971,22972],{"class":189,"line":190},[187,22973,22974],{},"#!/usr/bin/env python3\n",[187,22976,22977],{"class":189,"line":249},[187,22978,316],{"emptyLinePlaceholder":315},[187,22980,22981],{"class":189,"line":312},[187,22982,10345],{},[187,22984,22985],{"class":189,"line":319},[187,22986,316],{"emptyLinePlaceholder":315},[187,22988,22989],{"class":189,"line":325},[187,22990,22554],{},[187,22992,22993],{"class":189,"line":686},[187,22994,316],{"emptyLinePlaceholder":315},[187,22996,22997],{"class":189,"line":697},[187,22998,22999],{},"# For consistency with TypeScript code, `cdk` is the preferred import name for\n",[187,23001,23002],{"class":189,"line":1291},[187,23003,23004],{},"# the CDK's core module.  The following line also imports it as `core` for use\n",[187,23006,23007],{"class":189,"line":1306},[187,23008,23009],{},"# with examples from the CDK Developer's Guide, which are in the process of\n",[187,23011,23012],{"class":189,"line":1434},[187,23013,23014],{},"# being updated to use `cdk`.  You may delete this import if you don't need it.\n",[187,23016,23017],{"class":189,"line":2599},[187,23018,23019],{},"from aws_cdk import core\n",[187,23021,23022],{"class":189,"line":2607},[187,23023,316],{"emptyLinePlaceholder":315},[187,23025,23026],{"class":189,"line":2621},[187,23027,23028],{},"from cdk.cdk_stack import CdkStack\n",[187,23030,23031],{"class":189,"line":2631},[187,23032,316],{"emptyLinePlaceholder":315},[187,23034,23035],{"class":189,"line":2642},[187,23036,23037],{},"aws_region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",[187,23039,23040],{"class":189,"line":2653},[187,23041,23042],{},"aws_account = os.environ.get(\"AWS_ACCOUNT_ID\", \"\")\n",[187,23044,23045],{"class":189,"line":2665},[187,23046,316],{"emptyLinePlaceholder":315},[187,23048,23049],{"class":189,"line":2674},[187,23050,316],{"emptyLinePlaceholder":315},[187,23052,23053],{"class":189,"line":2684},[187,23054,23055],{},"app = cdk.App()\n",[187,23057,23058],{"class":189,"line":2694},[187,23059,23060],{},"CdkStack(\n",[187,23062,23063],{"class":189,"line":2706},[187,23064,23065],{},"    app,\n",[187,23067,23068],{"class":189,"line":2715},[187,23069,23070],{},"    \"valheim-server-stack\",\n",[187,23072,23073],{"class":189,"line":2725},[187,23074,23075],{},"    env={\"region\": aws_region, \"account\": aws_account}\n",[187,23077,23078],{"class":189,"line":2735},[187,23079,621],{},[187,23081,23082],{"class":189,"line":2743},[187,23083,316],{"emptyLinePlaceholder":315},[187,23085,23086],{"class":189,"line":2754},[187,23087,23088],{},"app.synth()\n",[11,23090,23091],{},"Now commit changes, create a tag and push it to GitLab:",[26,23093,23096],{"className":23094,"code":23095,"language":31},[29],"git add .\ngit commit -m \"initial commit\"\ngit tag v0.0.1\ngit push origin v0.0.1\n",[33,23097,23095],{"__ignoreMap":35},[11,23099,23100],{},"Check the logs of the GitLab CI pipeline that this creates in your GitLab project's CI/CD settings.",[11,23102,23103],{},"If everything runs successfully, you should be able to see your Valheim server listed in the list of community servers once it comes online, and you should be able to connect to it with the password you set in GitLab project variables.",[168,23105,23107],{"id":23106},"add-the-lambda-function-handler-code","Add the Lambda function handler code",[11,23109,23110,23111,23113,23114,4313,23117,23120,23121,4313,23124,23127],{},"We will have a simple Flask application respond the the Discord ",[33,23112,21623],{}," requests that send Interaction events. Let's add ",[33,23115,23116],{},"lambda-handler.py",[33,23118,23119],{},"lambda/functions/interactions/lambda-handler.py",", and ",[33,23122,23123],{},"requirements.txt",[33,23125,23126],{},"lambda/functions/interactions/requirements.txt",". Our project structure should look like this:",[26,23129,23132],{"className":23130,"code":23131,"language":31},[29],"$ tree -L 4\n.\n├── cdk\n│   ├── app.py\n│   ├── cdk\n│   │   ├── cdk_stack.py\n│   │   └── __init__.py\n│   ├── cdk.json\n│   ├── README.md\n│   ├── requirements.txt\n│   ├── setup.py\n│   └── source.bat\n├── lambda\n│   └── functions\n│       └── interactions\n│           ├── lambda-handler.py    \u003C-- here\n│           └── requirements.txt     \u003C-- and here\n├── README.md\n└── register_bot.py\n",[33,23133,23131],{"__ignoreMap":35},[11,23135,6131,23136,23138],{},[33,23137,23123],{}," defines the pip dependencies for our Lambda function. It should include the following:",[26,23140,23143],{"className":23141,"code":23142,"language":31},[29],"aws-wsgi==0.2.7\ndiscord-interactions==0.2.0\nFlask==1.1.2\n",[33,23144,23142],{"__ignoreMap":35},[916,23146,23147,23153,23158],{},[919,23148,23149,23152],{},[33,23150,23151],{},"aws-wsgi"," will transform API Gateway requests into WSGI application requests that Flask can handle",[919,23154,23155,23157],{},[33,23156,22441],{}," will help us with some security-related requirements",[919,23159,23160,23163],{},[33,23161,23162],{},"Flask"," will be our web application framework",[11,23165,23166,23167,358],{},"Here's the code for ",[33,23168,23116],{},[26,23170,23172],{"className":10554,"code":23171,"language":10556,"meta":35,"style":35},"import os\nimport logging\n\nimport awsgi\nimport boto3\nfrom discord_interactions import verify_key_decorator\nfrom flask import (\n    Flask,\n    jsonify,\n    request\n)\n\n\nclient = boto3.client('ecs')\n\n# Your public key can be found on your application in the Developer Portal\nPUBLIC_KEY = os.environ.get('APPLICATION_PUBLIC_KEY')\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\napp = Flask(__name__)\n\n\n@app.route('/discord', methods=['POST'])\n@verify_key_decorator(PUBLIC_KEY)\ndef index():\n    if request.json[\"type\"] == 1:\n        return jsonify({\"type\": 1})\n    else:\n        logger.info(request.json)\n        try:\n            interaction_option = request.json[\"data\"][\"options\"][0][\"value\"]\n        except KeyError:\n            logger.info(\"Could not parse the interaction option\")\n            interaction_option = \"status\"\n\n        logger.info(\"Interaction:\")\n        logger.info(interaction_option)\n\n        content = \"\"\n\n        if interaction_option == \"status\":\n            try:\n\n                resp = client.describe_services(\n                    cluster=os.environ.get(\"ECS_CLUSTER_ARN\", \"\"),\n                    services=[\n                        os.environ.get(\"ECS_SERVICE_NAME\", \"\"),\n                    ]\n                )\n                desired_count = resp[\"services\"][0][\"desiredCount\"]\n                running_count = resp[\"services\"][0][\"runningCount\"]\n                pending_count = resp[\"services\"][0][\"pendingCount\"]\n\n                content = f\"Desired: {desired_count} | Running: {running_count} | Pending: {pending_count}\"\n\n            except Error as e:\n                content = \"Could not get server status\"\n                logger.info(\"Could not get the server status\")\n                logger.info(e)\n\n        elif interaction_option == \"start\":\n            content = \"Starting the server\"\n\n            resp = client.update_service(\n                cluster=os.environ.get(\"ECS_CLUSTER_ARN\", \"\"),\n                service=os.environ.get(\"ECS_SERVICE_NAME\", \"\"),\n                desiredCount=1\n            )\n\n        elif interaction_option == \"stop\":\n            content = \"Stopping the server\"\n\n            resp = client.update_service(\n                cluster=os.environ.get(\"ECS_CLUSTER_ARN\", \"\"),\n                service=os.environ.get(\"ECS_SERVICE_NAME\", \"\"),\n                desiredCount=0\n            )\n\n        else:\n            content = \"Unknown command\"\n\n        logger.info(resp)\n\n        return jsonify({\n            \"type\": 4,\n            \"data\": {\n                \"tts\": False,\n                \"content\": content,\n                \"embeds\": [],\n                \"allowed_mentions\": { \"parse\": [] }\n            }\n        })\n\ndef handler(event, context):\n    return awsgi.response(\n        app,\n        event,\n        context,\n        base64_content_types={\"image/png\"}\n    )\n",[33,23173,23174,23178,23183,23187,23192,23197,23202,23207,23212,23217,23222,23226,23230,23234,23239,23243,23248,23253,23257,23262,23267,23271,23276,23280,23284,23289,23294,23299,23304,23309,23314,23319,23324,23329,23334,23339,23344,23348,23353,23358,23362,23367,23371,23376,23381,23385,23390,23395,23400,23405,23410,23414,23419,23424,23429,23433,23438,23442,23447,23452,23457,23462,23466,23471,23476,23480,23485,23490,23495,23500,23504,23508,23513,23518,23522,23526,23530,23534,23539,23543,23547,23552,23557,23561,23566,23570,23575,23580,23585,23590,23595,23600,23605,23610,23615,23619,23624,23629,23634,23639,23644,23649],{"__ignoreMap":35},[187,23175,23176],{"class":189,"line":190},[187,23177,10345],{},[187,23179,23180],{"class":189,"line":249},[187,23181,23182],{},"import logging\n",[187,23184,23185],{"class":189,"line":312},[187,23186,316],{"emptyLinePlaceholder":315},[187,23188,23189],{"class":189,"line":319},[187,23190,23191],{},"import awsgi\n",[187,23193,23194],{"class":189,"line":325},[187,23195,23196],{},"import boto3\n",[187,23198,23199],{"class":189,"line":686},[187,23200,23201],{},"from discord_interactions import verify_key_decorator\n",[187,23203,23204],{"class":189,"line":697},[187,23205,23206],{},"from flask import (\n",[187,23208,23209],{"class":189,"line":1291},[187,23210,23211],{},"    Flask,\n",[187,23213,23214],{"class":189,"line":1306},[187,23215,23216],{},"    jsonify,\n",[187,23218,23219],{"class":189,"line":1434},[187,23220,23221],{},"    request\n",[187,23223,23224],{"class":189,"line":2599},[187,23225,621],{},[187,23227,23228],{"class":189,"line":2607},[187,23229,316],{"emptyLinePlaceholder":315},[187,23231,23232],{"class":189,"line":2621},[187,23233,316],{"emptyLinePlaceholder":315},[187,23235,23236],{"class":189,"line":2631},[187,23237,23238],{},"client = boto3.client('ecs')\n",[187,23240,23241],{"class":189,"line":2642},[187,23242,316],{"emptyLinePlaceholder":315},[187,23244,23245],{"class":189,"line":2653},[187,23246,23247],{},"# Your public key can be found on your application in the Developer Portal\n",[187,23249,23250],{"class":189,"line":2665},[187,23251,23252],{},"PUBLIC_KEY = os.environ.get('APPLICATION_PUBLIC_KEY')\n",[187,23254,23255],{"class":189,"line":2674},[187,23256,316],{"emptyLinePlaceholder":315},[187,23258,23259],{"class":189,"line":2684},[187,23260,23261],{},"logger = logging.getLogger()\n",[187,23263,23264],{"class":189,"line":2694},[187,23265,23266],{},"logger.setLevel(logging.INFO)\n",[187,23268,23269],{"class":189,"line":2706},[187,23270,316],{"emptyLinePlaceholder":315},[187,23272,23273],{"class":189,"line":2715},[187,23274,23275],{},"app = Flask(__name__)\n",[187,23277,23278],{"class":189,"line":2725},[187,23279,316],{"emptyLinePlaceholder":315},[187,23281,23282],{"class":189,"line":2735},[187,23283,316],{"emptyLinePlaceholder":315},[187,23285,23286],{"class":189,"line":2743},[187,23287,23288],{},"@app.route('/discord', methods=['POST'])\n",[187,23290,23291],{"class":189,"line":2754},[187,23292,23293],{},"@verify_key_decorator(PUBLIC_KEY)\n",[187,23295,23296],{"class":189,"line":2762},[187,23297,23298],{},"def index():\n",[187,23300,23301],{"class":189,"line":2770},[187,23302,23303],{},"    if request.json[\"type\"] == 1:\n",[187,23305,23306],{"class":189,"line":2781},[187,23307,23308],{},"        return jsonify({\"type\": 1})\n",[187,23310,23311],{"class":189,"line":2792},[187,23312,23313],{},"    else:\n",[187,23315,23316],{"class":189,"line":2803},[187,23317,23318],{},"        logger.info(request.json)\n",[187,23320,23321],{"class":189,"line":2808},[187,23322,23323],{},"        try:\n",[187,23325,23326],{"class":189,"line":2816},[187,23327,23328],{},"            interaction_option = request.json[\"data\"][\"options\"][0][\"value\"]\n",[187,23330,23331],{"class":189,"line":2824},[187,23332,23333],{},"        except KeyError:\n",[187,23335,23336],{"class":189,"line":2834},[187,23337,23338],{},"            logger.info(\"Could not parse the interaction option\")\n",[187,23340,23341],{"class":189,"line":2845},[187,23342,23343],{},"            interaction_option = \"status\"\n",[187,23345,23346],{"class":189,"line":2856},[187,23347,316],{"emptyLinePlaceholder":315},[187,23349,23350],{"class":189,"line":2867},[187,23351,23352],{},"        logger.info(\"Interaction:\")\n",[187,23354,23355],{"class":189,"line":2878},[187,23356,23357],{},"        logger.info(interaction_option)\n",[187,23359,23360],{"class":189,"line":2886},[187,23361,316],{"emptyLinePlaceholder":315},[187,23363,23364],{"class":189,"line":2900},[187,23365,23366],{},"        content = \"\"\n",[187,23368,23369],{"class":189,"line":2905},[187,23370,316],{"emptyLinePlaceholder":315},[187,23372,23373],{"class":189,"line":2913},[187,23374,23375],{},"        if interaction_option == \"status\":\n",[187,23377,23378],{"class":189,"line":2921},[187,23379,23380],{},"            try:\n",[187,23382,23383],{"class":189,"line":2931},[187,23384,316],{"emptyLinePlaceholder":315},[187,23386,23387],{"class":189,"line":2942},[187,23388,23389],{},"                resp = client.describe_services(\n",[187,23391,23392],{"class":189,"line":2953},[187,23393,23394],{},"                    cluster=os.environ.get(\"ECS_CLUSTER_ARN\", \"\"),\n",[187,23396,23397],{"class":189,"line":2964},[187,23398,23399],{},"                    services=[\n",[187,23401,23402],{"class":189,"line":2975},[187,23403,23404],{},"                        os.environ.get(\"ECS_SERVICE_NAME\", \"\"),\n",[187,23406,23407],{"class":189,"line":2983},[187,23408,23409],{},"                    ]\n",[187,23411,23412],{"class":189,"line":2992},[187,23413,4521],{},[187,23415,23416],{"class":189,"line":3001},[187,23417,23418],{},"                desired_count = resp[\"services\"][0][\"desiredCount\"]\n",[187,23420,23421],{"class":189,"line":3010},[187,23422,23423],{},"                running_count = resp[\"services\"][0][\"runningCount\"]\n",[187,23425,23426],{"class":189,"line":3019},[187,23427,23428],{},"                pending_count = resp[\"services\"][0][\"pendingCount\"]\n",[187,23430,23431],{"class":189,"line":3028},[187,23432,316],{"emptyLinePlaceholder":315},[187,23434,23435],{"class":189,"line":3033},[187,23436,23437],{},"                content = f\"Desired: {desired_count} | Running: {running_count} | Pending: {pending_count}\"\n",[187,23439,23440],{"class":189,"line":3041},[187,23441,316],{"emptyLinePlaceholder":315},[187,23443,23444],{"class":189,"line":3049},[187,23445,23446],{},"            except Error as e:\n",[187,23448,23449],{"class":189,"line":3059},[187,23450,23451],{},"                content = \"Could not get server status\"\n",[187,23453,23454],{"class":189,"line":3070},[187,23455,23456],{},"                logger.info(\"Could not get the server status\")\n",[187,23458,23459],{"class":189,"line":3075},[187,23460,23461],{},"                logger.info(e)\n",[187,23463,23464],{"class":189,"line":3083},[187,23465,316],{"emptyLinePlaceholder":315},[187,23467,23468],{"class":189,"line":3091},[187,23469,23470],{},"        elif interaction_option == \"start\":\n",[187,23472,23473],{"class":189,"line":3101},[187,23474,23475],{},"            content = \"Starting the server\"\n",[187,23477,23478],{"class":189,"line":3111},[187,23479,316],{"emptyLinePlaceholder":315},[187,23481,23482],{"class":189,"line":3122},[187,23483,23484],{},"            resp = client.update_service(\n",[187,23486,23487],{"class":189,"line":3132},[187,23488,23489],{},"                cluster=os.environ.get(\"ECS_CLUSTER_ARN\", \"\"),\n",[187,23491,23492],{"class":189,"line":3143},[187,23493,23494],{},"                service=os.environ.get(\"ECS_SERVICE_NAME\", \"\"),\n",[187,23496,23497],{"class":189,"line":3151},[187,23498,23499],{},"                desiredCount=1\n",[187,23501,23502],{"class":189,"line":3161},[187,23503,3474],{},[187,23505,23506],{"class":189,"line":3170},[187,23507,316],{"emptyLinePlaceholder":315},[187,23509,23510],{"class":189,"line":3178},[187,23511,23512],{},"        elif interaction_option == \"stop\":\n",[187,23514,23515],{"class":189,"line":3185},[187,23516,23517],{},"            content = \"Stopping the server\"\n",[187,23519,23520],{"class":189,"line":3195},[187,23521,316],{"emptyLinePlaceholder":315},[187,23523,23524],{"class":189,"line":3205},[187,23525,23484],{},[187,23527,23528],{"class":189,"line":3210},[187,23529,23489],{},[187,23531,23532],{"class":189,"line":3216},[187,23533,23494],{},[187,23535,23536],{"class":189,"line":3224},[187,23537,23538],{},"                desiredCount=0\n",[187,23540,23541],{"class":189,"line":3234},[187,23542,3474],{},[187,23544,23545],{"class":189,"line":3242},[187,23546,316],{"emptyLinePlaceholder":315},[187,23548,23549],{"class":189,"line":3252},[187,23550,23551],{},"        else:\n",[187,23553,23554],{"class":189,"line":3260},[187,23555,23556],{},"            content = \"Unknown command\"\n",[187,23558,23559],{"class":189,"line":3270},[187,23560,316],{"emptyLinePlaceholder":315},[187,23562,23563],{"class":189,"line":3275},[187,23564,23565],{},"        logger.info(resp)\n",[187,23567,23568],{"class":189,"line":3283},[187,23569,316],{"emptyLinePlaceholder":315},[187,23571,23572],{"class":189,"line":3291},[187,23573,23574],{},"        return jsonify({\n",[187,23576,23577],{"class":189,"line":3300},[187,23578,23579],{},"            \"type\": 4,\n",[187,23581,23582],{"class":189,"line":3310},[187,23583,23584],{},"            \"data\": {\n",[187,23586,23587],{"class":189,"line":3320},[187,23588,23589],{},"                \"tts\": False,\n",[187,23591,23592],{"class":189,"line":3325},[187,23593,23594],{},"                \"content\": content,\n",[187,23596,23597],{"class":189,"line":3333},[187,23598,23599],{},"                \"embeds\": [],\n",[187,23601,23602],{"class":189,"line":3343},[187,23603,23604],{},"                \"allowed_mentions\": { \"parse\": [] }\n",[187,23606,23607],{"class":189,"line":3354},[187,23608,23609],{},"            }\n",[187,23611,23612],{"class":189,"line":17135},[187,23613,23614],{},"        })\n",[187,23616,23617],{"class":189,"line":17141},[187,23618,316],{"emptyLinePlaceholder":315},[187,23620,23621],{"class":189,"line":17146},[187,23622,23623],{},"def handler(event, context):\n",[187,23625,23626],{"class":189,"line":17152},[187,23627,23628],{},"    return awsgi.response(\n",[187,23630,23631],{"class":189,"line":17164},[187,23632,23633],{},"        app,\n",[187,23635,23636],{"class":189,"line":17175},[187,23637,23638],{},"        event,\n",[187,23640,23641],{"class":189,"line":17188},[187,23642,23643],{},"        context,\n",[187,23645,23646],{"class":189,"line":17199},[187,23647,23648],{},"        base64_content_types={\"image/png\"}\n",[187,23650,23651],{"class":189,"line":17207},[187,23652,23653],{},"    )\n",[11,23655,23656,23657,765,23659,5857,23662,23664,23665,23668],{},"Notice how we pass the Flask ",[33,23658,11956],{},[33,23660,23661],{},"awsgi.response",[33,23663,23151],{}," (or ",[33,23666,23667],{},"awsgi"," as it is imported) is the go-between for API Gateway and WSGI.",[168,23670,23672],{"id":23671},"add-the-cdk-code-for-api-gateway-and-lambda-that-will-serve-our-discord-interaction-endpoint-url","Add the CDK code for API Gateway and Lambda that will serve our Discord Interaction Endpoint URL",[11,23674,23675,23676,23678,23679,23681,23682,358],{},"Now we can add the following code to ",[33,23677,22541],{}," to configure the API Gateway and Lambda function. Add the following to ",[33,23680,22541],{}," after our definition of ",[33,23683,23684],{},"self.valheim_world",[26,23686,23688],{"className":10554,"code":23687,"language":10556,"meta":35,"style":35},"        self.env_vars = {\n            \"APPLICATION_PUBLIC_KEY\": os.environ.get(\"APPLICATION_PUBLIC_KEY\"),\n            \"ECS_SERVICE_NAME\": self.valheim_world.service.service_name,\n            \"ECS_CLUSTER_ARN\": self.valheim_world.service.cluster.cluster_arn\n        }\n\n        self.flask_lambda_layer = _lambda.LayerVersion(\n            self,\n            \"FlaskAppLambdaLayer\",\n            code=_lambda.AssetCode(\"./layers/flask\"),\n            compatible_runtimes=[_lambda.Runtime.PYTHON_3_8,],\n        )\n\n        self.flask_app_lambda = _lambda.Function(\n            self,\n            \"FlaskAppLambda\",\n            runtime=_lambda.Runtime.PYTHON_3_8,\n            code=_lambda.AssetCode('./lambda/functions/interactions'),\n            function_name=\"flask-app-handler\",\n            handler=\"lambda-handler.handler\",\n            layers=[self.flask_lambda_layer],\n            timeout=core.Duration.seconds(60),\n            environment={**self.env_vars},\n        )\n\n        self.flask_app_lambda.role.add_managed_policy(\n            iam.ManagedPolicy.from_managed_policy_arn(\n                self,\n                'ECS_FullAccessPolicy',\n                managed_policy_arn='arn:aws:iam::aws:policy/AmazonECS_FullAccess'\n            )\n        )\n\n        # https://slmkitani.medium.com/passing-custom-headers-through-amazon-api-gateway-to-an-aws-lambda-function-f3a1cfdc0e29\n        self.request_templates = {\n            \"application/json\": '''{\n                \"method\": \"$context.httpMethod\",\n                \"body\" : $input.json(\"$\"),\n                \"headers\": {\n                    #foreach($param in $input.params().header.keySet())\n                    \"$param\": \"$util.escapeJavaScript($input.params().header.get($param))\"\n                    #if($foreach.hasNext),#end\n                    #end\n                }\n            }\n            '''\n        }\n\n        self.apigateway = apigw.RestApi(\n            self,\n            'FlaskAppEndpoint',\n        )\n\n        self.apigateway.root.add_method(\"ANY\")\n\n        self.discord_interaction_webhook = self.apigateway.root.add_resource(\"discord\")\n\n        self.discord_interaction_webhook_integration = apigw.LambdaIntegration(\n            self.flask_app_lambda,\n            request_templates=self.request_templates\n        )\n\n        self.discord_interaction_webhook.add_method(\n            'POST',\n            self.discord_interaction_webhook_integration\n        )\n",[33,23689,23690,23695,23700,23705,23710,23714,23718,23723,23727,23732,23737,23742,23746,23750,23755,23759,23764,23769,23774,23779,23784,23789,23794,23799,23803,23807,23812,23817,23822,23827,23832,23836,23840,23844,23849,23854,23859,23864,23869,23874,23879,23884,23889,23894,23898,23902,23907,23911,23915,23920,23924,23929,23933,23937,23942,23946,23951,23955,23960,23965,23970,23974,23978,23983,23988,23993],{"__ignoreMap":35},[187,23691,23692],{"class":189,"line":190},[187,23693,23694],{},"        self.env_vars = {\n",[187,23696,23697],{"class":189,"line":249},[187,23698,23699],{},"            \"APPLICATION_PUBLIC_KEY\": os.environ.get(\"APPLICATION_PUBLIC_KEY\"),\n",[187,23701,23702],{"class":189,"line":312},[187,23703,23704],{},"            \"ECS_SERVICE_NAME\": self.valheim_world.service.service_name,\n",[187,23706,23707],{"class":189,"line":319},[187,23708,23709],{},"            \"ECS_CLUSTER_ARN\": self.valheim_world.service.cluster.cluster_arn\n",[187,23711,23712],{"class":189,"line":325},[187,23713,9780],{},[187,23715,23716],{"class":189,"line":686},[187,23717,316],{"emptyLinePlaceholder":315},[187,23719,23720],{"class":189,"line":697},[187,23721,23722],{},"        self.flask_lambda_layer = _lambda.LayerVersion(\n",[187,23724,23725],{"class":189,"line":1291},[187,23726,22649],{},[187,23728,23729],{"class":189,"line":1306},[187,23730,23731],{},"            \"FlaskAppLambdaLayer\",\n",[187,23733,23734],{"class":189,"line":1434},[187,23735,23736],{},"            code=_lambda.AssetCode(\"./layers/flask\"),\n",[187,23738,23739],{"class":189,"line":2599},[187,23740,23741],{},"            compatible_runtimes=[_lambda.Runtime.PYTHON_3_8,],\n",[187,23743,23744],{"class":189,"line":2607},[187,23745,4531],{},[187,23747,23748],{"class":189,"line":2621},[187,23749,316],{"emptyLinePlaceholder":315},[187,23751,23752],{"class":189,"line":2631},[187,23753,23754],{},"        self.flask_app_lambda = _lambda.Function(\n",[187,23756,23757],{"class":189,"line":2642},[187,23758,22649],{},[187,23760,23761],{"class":189,"line":2653},[187,23762,23763],{},"            \"FlaskAppLambda\",\n",[187,23765,23766],{"class":189,"line":2665},[187,23767,23768],{},"            runtime=_lambda.Runtime.PYTHON_3_8,\n",[187,23770,23771],{"class":189,"line":2674},[187,23772,23773],{},"            code=_lambda.AssetCode('./lambda/functions/interactions'),\n",[187,23775,23776],{"class":189,"line":2684},[187,23777,23778],{},"            function_name=\"flask-app-handler\",\n",[187,23780,23781],{"class":189,"line":2694},[187,23782,23783],{},"            handler=\"lambda-handler.handler\",\n",[187,23785,23786],{"class":189,"line":2706},[187,23787,23788],{},"            layers=[self.flask_lambda_layer],\n",[187,23790,23791],{"class":189,"line":2715},[187,23792,23793],{},"            timeout=core.Duration.seconds(60),\n",[187,23795,23796],{"class":189,"line":2725},[187,23797,23798],{},"            environment={**self.env_vars},\n",[187,23800,23801],{"class":189,"line":2735},[187,23802,4531],{},[187,23804,23805],{"class":189,"line":2743},[187,23806,316],{"emptyLinePlaceholder":315},[187,23808,23809],{"class":189,"line":2754},[187,23810,23811],{},"        self.flask_app_lambda.role.add_managed_policy(\n",[187,23813,23814],{"class":189,"line":2762},[187,23815,23816],{},"            iam.ManagedPolicy.from_managed_policy_arn(\n",[187,23818,23819],{"class":189,"line":2770},[187,23820,23821],{},"                self,\n",[187,23823,23824],{"class":189,"line":2781},[187,23825,23826],{},"                'ECS_FullAccessPolicy',\n",[187,23828,23829],{"class":189,"line":2792},[187,23830,23831],{},"                managed_policy_arn='arn:aws:iam::aws:policy/AmazonECS_FullAccess'\n",[187,23833,23834],{"class":189,"line":2803},[187,23835,3474],{},[187,23837,23838],{"class":189,"line":2808},[187,23839,4531],{},[187,23841,23842],{"class":189,"line":2816},[187,23843,316],{"emptyLinePlaceholder":315},[187,23845,23846],{"class":189,"line":2824},[187,23847,23848],{},"        # https://slmkitani.medium.com/passing-custom-headers-through-amazon-api-gateway-to-an-aws-lambda-function-f3a1cfdc0e29\n",[187,23850,23851],{"class":189,"line":2834},[187,23852,23853],{},"        self.request_templates = {\n",[187,23855,23856],{"class":189,"line":2845},[187,23857,23858],{},"            \"application/json\": '''{\n",[187,23860,23861],{"class":189,"line":2856},[187,23862,23863],{},"                \"method\": \"$context.httpMethod\",\n",[187,23865,23866],{"class":189,"line":2867},[187,23867,23868],{},"                \"body\" : $input.json(\"$\"),\n",[187,23870,23871],{"class":189,"line":2878},[187,23872,23873],{},"                \"headers\": {\n",[187,23875,23876],{"class":189,"line":2886},[187,23877,23878],{},"                    #foreach($param in $input.params().header.keySet())\n",[187,23880,23881],{"class":189,"line":2900},[187,23882,23883],{},"                    \"$param\": \"$util.escapeJavaScript($input.params().header.get($param))\"\n",[187,23885,23886],{"class":189,"line":2905},[187,23887,23888],{},"                    #if($foreach.hasNext),#end\n",[187,23890,23891],{"class":189,"line":2913},[187,23892,23893],{},"                    #end\n",[187,23895,23896],{"class":189,"line":2921},[187,23897,21978],{},[187,23899,23900],{"class":189,"line":2931},[187,23901,23609],{},[187,23903,23904],{"class":189,"line":2942},[187,23905,23906],{},"            '''\n",[187,23908,23909],{"class":189,"line":2953},[187,23910,9780],{},[187,23912,23913],{"class":189,"line":2964},[187,23914,316],{"emptyLinePlaceholder":315},[187,23916,23917],{"class":189,"line":2975},[187,23918,23919],{},"        self.apigateway = apigw.RestApi(\n",[187,23921,23922],{"class":189,"line":2983},[187,23923,22649],{},[187,23925,23926],{"class":189,"line":2992},[187,23927,23928],{},"            'FlaskAppEndpoint',\n",[187,23930,23931],{"class":189,"line":3001},[187,23932,4531],{},[187,23934,23935],{"class":189,"line":3010},[187,23936,316],{"emptyLinePlaceholder":315},[187,23938,23939],{"class":189,"line":3019},[187,23940,23941],{},"        self.apigateway.root.add_method(\"ANY\")\n",[187,23943,23944],{"class":189,"line":3028},[187,23945,316],{"emptyLinePlaceholder":315},[187,23947,23948],{"class":189,"line":3033},[187,23949,23950],{},"        self.discord_interaction_webhook = self.apigateway.root.add_resource(\"discord\")\n",[187,23952,23953],{"class":189,"line":3041},[187,23954,316],{"emptyLinePlaceholder":315},[187,23956,23957],{"class":189,"line":3049},[187,23958,23959],{},"        self.discord_interaction_webhook_integration = apigw.LambdaIntegration(\n",[187,23961,23962],{"class":189,"line":3059},[187,23963,23964],{},"            self.flask_app_lambda,\n",[187,23966,23967],{"class":189,"line":3070},[187,23968,23969],{},"            request_templates=self.request_templates\n",[187,23971,23972],{"class":189,"line":3075},[187,23973,4531],{},[187,23975,23976],{"class":189,"line":3083},[187,23977,316],{"emptyLinePlaceholder":315},[187,23979,23980],{"class":189,"line":3091},[187,23981,23982],{},"        self.discord_interaction_webhook.add_method(\n",[187,23984,23985],{"class":189,"line":3101},[187,23986,23987],{},"            'POST',\n",[187,23989,23990],{"class":189,"line":3111},[187,23991,23992],{},"            self.discord_interaction_webhook_integration\n",[187,23994,23995],{"class":189,"line":3122},[187,23996,4531],{},[11,23998,23999,24000,24003],{},"First we add some environment variables that will be made available to the Lambda function's execution environment. The ECS cluster and service name as well as our Discord application's ",[33,24001,24002],{},"PUBLIC_KEY"," are needed in the Lambda function for everything to work.",[11,24005,24006],{},"We have to give the lambda function permissions to make changes to ECS since it will be interacting with ECS via boto3.",[11,24008,24009,24012,24013,24015],{},[33,24010,24011],{},"self.request_templates"," is needed in order to pass the special security headers from the Discord ",[33,24014,21623],{}," request that are needed for security. I couldn't find a lot of resources on how to make this work, but I learned that this uses Apache Velocity Template Language.",[911,24017,24019],{"id":24018},"add-a-gitlab-ci-job-for-installing-dependencies-into-lambda-layer","Add a GitLab CI job for installing dependencies into Lambda Layer",[11,24021,24022],{},"There's one more step before we can push our code. We need to add another GitLab CI job that will install the Lambda dependencies so that they can be sent to the Lambda layer that we defined in our Lambda function. A Lambda Layer is where you install dependencies for this type of Lambda setup. Let's add the following stage:",[26,24024,24026],{"className":8656,"code":24025,"language":8658,"meta":35,"style":35},"stages:\n  - build\n  - deploy\n\nimage: python:3.8\n\npip_install:\n  stage: build\n  rules:\n    - if: \"$CI_COMMIT_TAG\"\n      when: always\n  artifacts:\n    paths:\n      - layers/flask/python\n  script:\n    - pip install -r lambda/functions/interactions/requirements.txt -t layers/flask/python\n",[33,24027,24028,24034,24041,24047,24051,24059,24063,24070,24078,24084,24094,24102,24109,24116,24123,24129],{"__ignoreMap":35},[187,24029,24030,24032],{"class":189,"line":190},[187,24031,22760],{"class":2516},[187,24033,2520],{"class":577},[187,24035,24036,24038],{"class":189,"line":249},[187,24037,6592],{"class":577},[187,24039,24040],{"class":196},"build\n",[187,24042,24043,24045],{"class":189,"line":312},[187,24044,6592],{"class":577},[187,24046,7546],{"class":196},[187,24048,24049],{"class":189,"line":319},[187,24050,316],{"emptyLinePlaceholder":315},[187,24052,24053,24055,24057],{"class":189,"line":325},[187,24054,6570],{"class":2516},[187,24056,585],{"class":577},[187,24058,22781],{"class":196},[187,24060,24061],{"class":189,"line":686},[187,24062,316],{"emptyLinePlaceholder":315},[187,24064,24065,24068],{"class":189,"line":697},[187,24066,24067],{"class":2516},"pip_install",[187,24069,2520],{"class":577},[187,24071,24072,24074,24076],{"class":189,"line":1291},[187,24073,22797],{"class":2516},[187,24075,585],{"class":577},[187,24077,24040],{"class":196},[187,24079,24080,24082],{"class":189,"line":1306},[187,24081,22806],{"class":2516},[187,24083,2520],{"class":577},[187,24085,24086,24088,24090,24092],{"class":189,"line":1434},[187,24087,14709],{"class":577},[187,24089,19681],{"class":2516},[187,24091,585],{"class":577},[187,24093,22819],{"class":196},[187,24095,24096,24098,24100],{"class":189,"line":2599},[187,24097,22824],{"class":2516},[187,24099,585],{"class":577},[187,24101,22829],{"class":196},[187,24103,24104,24107],{"class":189,"line":2607},[187,24105,24106],{"class":2516},"  artifacts",[187,24108,2520],{"class":577},[187,24110,24111,24114],{"class":189,"line":2621},[187,24112,24113],{"class":2516},"    paths",[187,24115,2520],{"class":577},[187,24117,24118,24120],{"class":189,"line":2631},[187,24119,2610],{"class":577},[187,24121,24122],{"class":196},"layers/flask/python\n",[187,24124,24125,24127],{"class":189,"line":2642},[187,24126,22861],{"class":2516},[187,24128,2520],{"class":577},[187,24130,24131,24133],{"class":189,"line":2653},[187,24132,14709],{"class":577},[187,24134,24135],{"class":196},"pip install -r lambda/functions/interactions/requirements.txt -t layers/flask/python\n",[11,24137,24138,24139,24142,24143,24145,24146,9054,24149,24152,24153,9054,24156,24158],{},"Now we are installing dependencies into a target location (with the ",[33,24140,24141],{},"-t"," flag) that our Lambda Layer will be able to use in the ",[33,24144,22790],{}," GitLab CI job. This is because we have indicated the path to ",[33,24147,24148],{},"layers/flask/python",[33,24150,24151],{},"paths"," array of ",[33,24154,24155],{},"artifacts",[33,24157,24067],{}," job. There are other ways to add the pip dependencies to the Lambda Layers. We don't absolutely need this to be done in a separate CI job.",[11,24160,24161],{},"Now tag and push the code to GitLab and check to see that the pipeline runs successfully.",[911,24163,24165],{"id":24164},"add-the-api-gateway-url-to-discord-application-settings","Add the API Gateway URL to Discord Application settings",[11,24167,24168],{},"If everything runs smoothly, we should see a URL in the very last lines of the pipeline. This is the URL for our API Gateway endpoint:",[26,24170,24173],{"className":24171,"code":24172,"language":31},[29],"https://abc123xyz.execute-api.us-east-1.amazonaws.com/prod/\n",[33,24174,24172],{"__ignoreMap":35},[11,24176,24177,24178,24181,24182,9054,24184,24186],{},"We need to add ",[33,24179,24180],{},"discord"," to the end of this URL and then add that to our the ",[33,24183,22087],{},[33,24185,21702],{}," section of our Discord application's admin page:",[26,24188,24191],{"className":24189,"code":24190,"language":31},[29],"https://abc123xyz.execute-api.us-east-1.amazonaws.com/prod/discord\n",[33,24192,24190],{"__ignoreMap":35},[11,24194,24195,24196,24201],{},"When we add this URL in the application settings, Discord will make sure that our endpoint is properly verifying the request based on its headers. Check out ",[15,24197,24200],{"href":24198,"rel":24199},"https://github.com/discord/discord-interactions-python/blob/main/discord_interactions/__init__.py#L31",[19],"this function"," to see how it works:",[26,24203,24205],{"className":10554,"code":24204,"language":10556,"meta":35,"style":35},"def verify_key_decorator(client_public_key):\n    from flask import request, jsonify\n\n    # https://stackoverflow.com/questions/51691730/flask-middleware-for-specific-route\n    def _decorator(f):\n        @wraps(f)\n        def __decorator(*args, **kwargs):\n            # Verify request\n            signature = request.headers.get('X-Signature-Ed25519')\n            timestamp = request.headers.get('X-Signature-Timestamp')\n            if signature is None or timestamp is None or not verify_key(request.data, signature, timestamp, client_public_key):\n                return 'Bad request signature', 401\n\n            # Automatically respond to pings\n            if request.json and request.json.get('type') == InteractionType.PING:\n                return jsonify({\n                    'type': InteractionResponseType.PONG\n                })\n\n            # Pass through\n            return f(*args, **kwargs)\n        return __decorator\n    return _decorator\n",[33,24206,24207,24212,24217,24221,24226,24231,24236,24241,24246,24251,24256,24261,24266,24270,24275,24280,24285,24290,24295,24299,24304,24309,24314],{"__ignoreMap":35},[187,24208,24209],{"class":189,"line":190},[187,24210,24211],{},"def verify_key_decorator(client_public_key):\n",[187,24213,24214],{"class":189,"line":249},[187,24215,24216],{},"    from flask import request, jsonify\n",[187,24218,24219],{"class":189,"line":312},[187,24220,316],{"emptyLinePlaceholder":315},[187,24222,24223],{"class":189,"line":319},[187,24224,24225],{},"    # https://stackoverflow.com/questions/51691730/flask-middleware-for-specific-route\n",[187,24227,24228],{"class":189,"line":325},[187,24229,24230],{},"    def _decorator(f):\n",[187,24232,24233],{"class":189,"line":686},[187,24234,24235],{},"        @wraps(f)\n",[187,24237,24238],{"class":189,"line":697},[187,24239,24240],{},"        def __decorator(*args, **kwargs):\n",[187,24242,24243],{"class":189,"line":1291},[187,24244,24245],{},"            # Verify request\n",[187,24247,24248],{"class":189,"line":1306},[187,24249,24250],{},"            signature = request.headers.get('X-Signature-Ed25519')\n",[187,24252,24253],{"class":189,"line":1434},[187,24254,24255],{},"            timestamp = request.headers.get('X-Signature-Timestamp')\n",[187,24257,24258],{"class":189,"line":2599},[187,24259,24260],{},"            if signature is None or timestamp is None or not verify_key(request.data, signature, timestamp, client_public_key):\n",[187,24262,24263],{"class":189,"line":2607},[187,24264,24265],{},"                return 'Bad request signature', 401\n",[187,24267,24268],{"class":189,"line":2621},[187,24269,316],{"emptyLinePlaceholder":315},[187,24271,24272],{"class":189,"line":2631},[187,24273,24274],{},"            # Automatically respond to pings\n",[187,24276,24277],{"class":189,"line":2642},[187,24278,24279],{},"            if request.json and request.json.get('type') == InteractionType.PING:\n",[187,24281,24282],{"class":189,"line":2653},[187,24283,24284],{},"                return jsonify({\n",[187,24286,24287],{"class":189,"line":2665},[187,24288,24289],{},"                    'type': InteractionResponseType.PONG\n",[187,24291,24292],{"class":189,"line":2674},[187,24293,24294],{},"                })\n",[187,24296,24297],{"class":189,"line":2684},[187,24298,316],{"emptyLinePlaceholder":315},[187,24300,24301],{"class":189,"line":2694},[187,24302,24303],{},"            # Pass through\n",[187,24305,24306],{"class":189,"line":2706},[187,24307,24308],{},"            return f(*args, **kwargs)\n",[187,24310,24311],{"class":189,"line":2715},[187,24312,24313],{},"        return __decorator\n",[187,24315,24316],{"class":189,"line":2725},[187,24317,24318],{},"    return _decorator\n",[11,24320,24321],{},"If it fails verification, we will not be able to add the URL and it will not work. You might want to add some additional logging to the Lambda function if you are not able to add the URL successfully.",[11,24323,24324,24325,752],{},"This is all covered in ",[15,24326,24329],{"href":24327,"rel":24328},"https://discord.com/developers/docs/interactions/slash-commands",[19],"the documentation for Discord Interactions",[11,24331,24332,24333,1172,24336,752],{},"Now you should be able to run the Discord slash commands. You can get the status of your ECS cluster and scale it to either 1 or 0 for ",[33,24334,24335],{},"ON",[33,24337,24338],{},"OFF",[168,24340,24342],{"id":24341},"overview","Overview",[11,24344,24345],{},"Here's an overview of what we covered:",[11,24347,24348],{},[511,24349],{"alt":7255,"src":21677},[2276,24351,24353,24356,24359,24362,24365,24368,24376,24379,24385,24388,24397,24400,24403,24406,24408,24414,24420,24426,24435],{"start":24352},0,[919,24354,24355],{},"This is my computer. For development of this project (and most other projects) I used Windows with WSL2.",[919,24357,24358],{},"GitLab CI - This is used to run our automated pipelines whenever we push a tag.",[919,24360,24361],{},"The CDK CLI is used to create, update and delete the infrastructure in our AWS account.",[919,24363,24364],{},"Valheim - The client for the game server that we set up",[919,24366,24367],{},"The public IP address of the ECS Task that can be used to connect to our server on port 2456.",[919,24369,24370,24371,752],{},"The ECS Cluster that runs the actual docker container for the Valheim server. By default, the image used is ",[15,24372,24375],{"href":24373,"rel":24374},"https://hub.docker.com/r/lloesche/valheim-server",[19],"lloesche/valheim-server",[919,24377,24378],{},"EFS - This is the file system that is mounted onto the container of the ECS task where our game's world data is stored.",[919,24380,24381,24382,24384],{},"AWS Backup (Optional) - This is an optional feature of the ",[33,24383,21565],{}," construct that can make regular backups of our EFS file system.",[919,24386,24387],{},"Events (Optional) - AWS Events can be used to scale the number of ECS tasks between 0 and 1.",[919,24389,24390,24391,24396],{},"This is the ",[15,24392,24394],{"href":21572,"rel":24393},[19],[33,24395,21565],{}," construct that I use in this project.",[919,24398,24399],{},"S3 bucket for syncing data to and from EFS with DataSync (WIP)",[919,24401,24402],{},"DataSync for moving game data between EFS and S3.",[919,24404,24405],{},"The Slash Commands that we set up",[919,24407,19031],{},[919,24409,24410,24411,24413],{},"Discord Interactions sends and a ",[33,24412,21623],{}," request",[919,24415,24416,24417,24419],{},"The API Gateway endpoint that we configured to handle Discord Interaction ",[33,24418,21623],{}," requests.",[919,24421,24422,24423,24425],{},"The Lambda function running a simple Flask app that responds to the Interaction ",[33,24424,21623],{}," request.",[919,24427,24428,24429,1172,24431,24434],{},"boto3 - This is the AWS SDK Python library included in the Python execution environment that allows us to interact with the resources in our AWS account. In particular, the interactions we use from boto3 are the ",[33,24430,21627],{},[33,24432,24433],{},"describe_servics"," methods from the ECS module. This allows us to turn our server on and off and also get the status.",[919,24436,24437,24438,24441],{},"This represents the ",[33,24439,24440],{},"valheim-server-stack"," we defined in our CDK application.",[168,24443,7912],{"id":7911},[11,24445,24446],{},"There are still some things that I'm working on finalizing.",[916,24448,24449,24452,24455,24458,24461],{},[919,24450,24451],{},"DataSync for easily moving data between S3 and EFS",[919,24453,24454],{},"Report billing data with an additional slash command sub-command",[919,24456,24457],{},"Add tagging to the resources in our stack to make the billing command easier to implement.",[919,24459,24460],{},"Get feedback from the Discord, CDK and Valheim communities about what I can improve here",[919,24462,24463,24464],{},"Contribute to ",[15,24465,24467],{"href":21572,"rel":24466},[19],[33,24468,24469],{},"gotodeploy/cdk-valheim",[11,24471,24472],{},"Thanks for reading!",[855,24474,24475],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}",{"title":35,"searchDepth":249,"depth":249,"links":24477},[24478,24480,24481,24485,24486,24491,24492,24496,24497],{"id":21562,"depth":249,"text":24479},"cdk-valheim construct on GitHub",{"id":21604,"depth":249,"text":21605},{"id":21683,"depth":249,"text":21684,"children":24482},[24483,24484],{"id":21740,"depth":312,"text":21741},{"id":21796,"depth":312,"text":21797},{"id":22098,"depth":249,"text":22099},{"id":22445,"depth":249,"text":22446,"children":24487},[24488,24489,24490],{"id":22474,"depth":312,"text":22475},{"id":22742,"depth":312,"text":22743},{"id":22957,"depth":312,"text":22958},{"id":23106,"depth":249,"text":23107},{"id":23671,"depth":249,"text":23672,"children":24493},[24494,24495],{"id":24018,"depth":312,"text":24019},{"id":24164,"depth":312,"text":24165},{"id":24341,"depth":249,"text":24342},{"id":7911,"depth":249,"text":7912},"2021-03-18","This is a detailed guide showing how to setup a dedicated Valheim server using serverless technologies on AWS","/static/valheim/hall.png",{},"/2021/03/18/on-demand-dedicated-serverless-valheim-server-with-cdk-discrod-interactions",{"title":21539,"description":24499},"2021/03/18/on-demand-dedicated-serverless-valheim-server-with-cdk-discrod-interactions",[24506,10061,1125,24507,24508,24180],"valheim","flask","serverless","j-Hl7SFibtOek-ic9mIv5wPft5eglYM_iIWCjTyFQv8",{"id":24511,"title":24512,"body":24513,"comments":872,"date":27045,"description":24512,"draft":872,"extension":873,"external":874,"image":26179,"meta":27046,"navigation":315,"path":27047,"seo":27048,"stem":27049,"tags":27050,"__hash__":27054},"blog/2021/01/16/i-scraped-analyzed-and-generated-yc-companies-founders-and-work-at-a-startup-job-postings.md","Scraping, analyzing and generating companies, founders and job postings from YC's Work at a Startup",{"type":8,"value":24514,"toc":27030},[24515,24524,24527,24530,24534,24541,24769,24773,24780,24843,24846,24877,24884,24887,25095,25099,25102,25106,25109,25163,25168,25171,25873,25876,25879,25982,25987,25991,25994,25999,26148,26151,26156,26160,26163,26168,26172,26175,26180,26388,26391,26395,26404,26407,26412,26415,26538,26541,26547,26607,26612,26616,26619,26624,26628,26631,26643,26657,26660,26666,26669,26699,26710,26713,26719,26725,26728,26737,26740,26749,26756,26771,26774,26842,26845,26851,26854,26859,26862,26867,26872,26878,26907,26917,26920,26965,26970,26974,26980,26986,26989,26995,26998,27003,27010,27017,27024,27027],[11,24516,24517,24518,24523],{},"I always enjoy reading about new batches of YC companies. I came across YC's ",[15,24519,24522],{"href":24520,"rel":24521},"https://www.workatastartup.com/",[19],"Work at a Startup"," (WaaS) recently while browsing HN and got pretty curious about all of the available data points on companies, jobs and founders.",[11,24525,24526],{},"This article will outline my process for collecting, cleaning, visualizing and analyzing the dataset.",[11,24528,24529],{},"After filling out my profile, WaaS recommended 750 matching YC startups which collectively list 1614 open positions. I think this is all of the available job openings and hiring companies, but I'm not sure.",[168,24531,24533],{"id":24532},"scraping-data","Scraping data",[11,24535,24536,24537,24540],{},"I've used a few different tools to scrape data and automate web browsers. For collecting this data, I ended up just writing some JavaScript directly in the browser console and ",[33,24538,24539],{},"Ctrl+S","aved the page HTML and assets (company logos and founder photos).",[26,24542,24544],{"className":564,"code":24543,"language":566,"meta":35,"style":35},"// expand each companies to see full details and all postings\nconst toggleDetails = document.getElementsByClassName(\"checkbox-inline\")[0]\ntoggleDetails.click()\n\n// automated scrolling, run this until it gets to the end\nconst scroll = setInterval(() => {window.scrollTo(0,document.body.scrollHeight);}, 3000)\n\n// when it no longer scrolls, clear the interval\nclearInterval(scroll)\n    \u003Cvue-apex-charts\n      v-if=\"chartOptions && series\"\n      type=\"bar\"\n      height=\"350\"\n      :options=\"chartOptions\"\n      :series=\"series\"\n    >\u003C/vue-apex-charts>\n// expand each job listing:\nconst jobs = document.getElementsByClassName(\"job-name\")\nfor (let job of jobs) {\n    job.click()\n}\n\n// now Ctrl+S to save the HTML and images\n",[33,24545,24546,24551,24578,24588,24592,24597,24631,24635,24640,24648,24655,24665,24675,24685,24690,24695,24705,24710,24730,24747,24756,24760,24764],{"__ignoreMap":35},[187,24547,24548],{"class":189,"line":190},[187,24549,24550],{"class":295},"// expand each companies to see full details and all postings\n",[187,24552,24553,24555,24558,24560,24563,24566,24568,24571,24574,24576],{"class":189,"line":249},[187,24554,574],{"class":573},[187,24556,24557],{"class":588}," toggleDetails",[187,24559,6771],{"class":573},[187,24561,24562],{"class":577}," document.",[187,24564,24565],{"class":193},"getElementsByClassName",[187,24567,615],{"class":577},[187,24569,24570],{"class":196},"\"checkbox-inline\"",[187,24572,24573],{"class":577},")[",[187,24575,10165],{"class":588},[187,24577,1437],{"class":577},[187,24579,24580,24583,24586],{"class":189,"line":312},[187,24581,24582],{"class":577},"toggleDetails.",[187,24584,24585],{"class":193},"click",[187,24587,694],{"class":577},[187,24589,24590],{"class":189,"line":319},[187,24591,316],{"emptyLinePlaceholder":315},[187,24593,24594],{"class":189,"line":325},[187,24595,24596],{"class":295},"// automated scrolling, run this until it gets to the end\n",[187,24598,24599,24601,24604,24606,24609,24611,24613,24616,24619,24621,24623,24626,24629],{"class":189,"line":686},[187,24600,574],{"class":573},[187,24602,24603],{"class":588}," scroll",[187,24605,6771],{"class":573},[187,24607,24608],{"class":193}," setInterval",[187,24610,19311],{"class":577},[187,24612,7102],{"class":573},[187,24614,24615],{"class":577}," {window.",[187,24617,24618],{"class":193},"scrollTo",[187,24620,615],{"class":577},[187,24622,10165],{"class":588},[187,24624,24625],{"class":577},",document.body.scrollHeight);}, ",[187,24627,24628],{"class":588},"3000",[187,24630,621],{"class":577},[187,24632,24633],{"class":189,"line":697},[187,24634,316],{"emptyLinePlaceholder":315},[187,24636,24637],{"class":189,"line":1291},[187,24638,24639],{"class":295},"// when it no longer scrolls, clear the interval\n",[187,24641,24642,24645],{"class":189,"line":1306},[187,24643,24644],{"class":193},"clearInterval",[187,24646,24647],{"class":577},"(scroll)\n",[187,24649,24650,24652],{"class":189,"line":1434},[187,24651,19865],{"class":577},[187,24653,24654],{"class":588},"vue-apex-charts\n",[187,24656,24657,24660,24662],{"class":189,"line":2599},[187,24658,24659],{"class":193},"      v-if",[187,24661,595],{"class":573},[187,24663,24664],{"class":196},"\"chartOptions && series\"\n",[187,24666,24667,24670,24672],{"class":189,"line":2607},[187,24668,24669],{"class":193},"      type",[187,24671,595],{"class":573},[187,24673,24674],{"class":196},"\"bar\"\n",[187,24676,24677,24680,24682],{"class":189,"line":2621},[187,24678,24679],{"class":193},"      height",[187,24681,595],{"class":573},[187,24683,24684],{"class":196},"\"350\"\n",[187,24686,24687],{"class":189,"line":2631},[187,24688,24689],{"class":7947},"      :options=\"chartOptions\"\n",[187,24691,24692],{"class":189,"line":2642},[187,24693,24694],{"class":7947},"      :series=\"series\"\n",[187,24696,24697,24700,24703],{"class":189,"line":2653},[187,24698,24699],{"class":577},"    >\u003C/",[187,24701,24702],{"class":588},"vue-apex-charts",[187,24704,6730],{"class":577},[187,24706,24707],{"class":189,"line":2665},[187,24708,24709],{"class":295},"// expand each job listing:\n",[187,24711,24712,24714,24717,24719,24721,24723,24725,24728],{"class":189,"line":2674},[187,24713,574],{"class":573},[187,24715,24716],{"class":588}," jobs",[187,24718,6771],{"class":573},[187,24720,24562],{"class":577},[187,24722,24565],{"class":193},[187,24724,615],{"class":577},[187,24726,24727],{"class":196},"\"job-name\"",[187,24729,621],{"class":577},[187,24731,24732,24734,24736,24738,24741,24744],{"class":189,"line":2684},[187,24733,16543],{"class":573},[187,24735,784],{"class":577},[187,24737,19664],{"class":573},[187,24739,24740],{"class":577}," job ",[187,24742,24743],{"class":573},"of",[187,24745,24746],{"class":577}," jobs) {\n",[187,24748,24749,24752,24754],{"class":189,"line":2694},[187,24750,24751],{"class":577},"    job.",[187,24753,24585],{"class":193},[187,24755,694],{"class":577},[187,24757,24758],{"class":189,"line":2706},[187,24759,1309],{"class":577},[187,24761,24762],{"class":189,"line":2715},[187,24763,316],{"emptyLinePlaceholder":315},[187,24765,24766],{"class":189,"line":2725},[187,24767,24768],{"class":295},"// now Ctrl+S to save the HTML and images\n",[168,24770,24772],{"id":24771},"parsing-the-html","Parsing the HTML",[11,24774,24775,24776,24779],{},"Next I'll parse the company data into a python list of dictionaries and then ",[33,24777,24778],{},"dumps"," it into a JSON file. This code is a little bit scrappy, here's the pseudo code:",[26,24781,24783],{"className":10554,"code":24782,"language":10556,"meta":35,"style":35},"# pseudo code for parsing data\nhtml = open(\"data.html\")\nparsed_html = parseHtml(html)\n\ncompanies = []\nfor company in parsed_html.find_all(\"company\")\n    # company stats, founders and jobs\n    company_details = extract_company_details(company)\n    companies.append(company_details)\n\nwith open(\"output.json\", \"wb\") as f:\n    f.write(json.dumps(companies))\n",[33,24784,24785,24790,24795,24800,24804,24809,24814,24819,24824,24829,24833,24838],{"__ignoreMap":35},[187,24786,24787],{"class":189,"line":190},[187,24788,24789],{},"# pseudo code for parsing data\n",[187,24791,24792],{"class":189,"line":249},[187,24793,24794],{},"html = open(\"data.html\")\n",[187,24796,24797],{"class":189,"line":312},[187,24798,24799],{},"parsed_html = parseHtml(html)\n",[187,24801,24802],{"class":189,"line":319},[187,24803,316],{"emptyLinePlaceholder":315},[187,24805,24806],{"class":189,"line":325},[187,24807,24808],{},"companies = []\n",[187,24810,24811],{"class":189,"line":686},[187,24812,24813],{},"for company in parsed_html.find_all(\"company\")\n",[187,24815,24816],{"class":189,"line":697},[187,24817,24818],{},"    # company stats, founders and jobs\n",[187,24820,24821],{"class":189,"line":1291},[187,24822,24823],{},"    company_details = extract_company_details(company)\n",[187,24825,24826],{"class":189,"line":1306},[187,24827,24828],{},"    companies.append(company_details)\n",[187,24830,24831],{"class":189,"line":1434},[187,24832,316],{"emptyLinePlaceholder":315},[187,24834,24835],{"class":189,"line":2599},[187,24836,24837],{},"with open(\"output.json\", \"wb\") as f:\n",[187,24839,24840],{"class":189,"line":2607},[187,24841,24842],{},"    f.write(json.dumps(companies))\n",[11,24844,24845],{},"To scrape the data I used my go-to library for this type of task: BeautifulSoup. There were a few tricky parts:",[916,24847,24848,24851,24865],{},[919,24849,24850],{},"Job details (visa requirements, salary, equity) were all labelled with the same class and they were inconsistent (sometimes salary or equity or both were excluded, for example).",[919,24852,24853,24854,24857,24858,24861,24862],{},"Equity was mostly a range of percentages such as ",[33,24855,24856],{},"1% - 2%"," and sometimes a single percentage like ",[33,24859,24860],{},"1.5%",". Some salary ranges had typos like ",[33,24863,24864],{},"$90k - $10k",[919,24866,24867,24868,637,24871,1172,24874,10006],{},"Years of experience required was also inconsistent with mixed types like ",[33,24869,24870],{},"3+ Years",[33,24872,24873],{},"Any (recent grad ok)",[33,24875,24876],{},"Senior or Juniors",[11,24878,24879,24880,24883],{},"These were all pretty easy to account for, it just required some additional logic to handle default values for ",[33,24881,24882],{},"\u003Cdiv>","s that were not included as well as mixed data types and representations where there were inconsistencies.",[11,24885,24886],{},"The resulting JSON structure for the big array of companies looks like this:",[26,24888,24890],{"className":1200,"code":24889,"language":1202,"meta":35,"style":35},"[\n    {\n        \"company_name\": \"Startup A\",\n        \"logo\": \"logo.png\",\n        \"jobs\": [\n            {\n                \"title\": \"Software Engineer\",\n                \"skills\": [\"python\", \"javascript\"],\n                \"salary\": {\n                    \"min\": 90000,\n                    \"max\": 110000,\n                    \"avg\": 100000\n                }\n            }\n        ],\n        \"founders\": [\n            {\n                \"name\": \"Founder Name\",\n                \"linkedin\": \"https://linkedin.com/founder\",\n                \"education\": \"University A\",\n                \"image\": \"abc.png\"\n            }\n        ]\n    }\n]\n",[33,24891,24892,24897,24902,24914,24926,24933,24938,24950,24967,24974,24986,24998,25008,25012,25016,25021,25028,25032,25044,25056,25068,25078,25082,25087,25091],{"__ignoreMap":35},[187,24893,24894],{"class":189,"line":190},[187,24895,24896],{"class":577},"[\n",[187,24898,24899],{"class":189,"line":249},[187,24900,24901],{"class":577},"    {\n",[187,24903,24904,24907,24909,24912],{"class":189,"line":312},[187,24905,24906],{"class":588},"        \"company_name\"",[187,24908,585],{"class":577},[187,24910,24911],{"class":196},"\"Startup A\"",[187,24913,1228],{"class":577},[187,24915,24916,24919,24921,24924],{"class":189,"line":319},[187,24917,24918],{"class":588},"        \"logo\"",[187,24920,585],{"class":577},[187,24922,24923],{"class":196},"\"logo.png\"",[187,24925,1228],{"class":577},[187,24927,24928,24931],{"class":189,"line":325},[187,24929,24930],{"class":588},"        \"jobs\"",[187,24932,18908],{"class":577},[187,24934,24935],{"class":189,"line":686},[187,24936,24937],{"class":577},"            {\n",[187,24939,24940,24943,24945,24948],{"class":189,"line":697},[187,24941,24942],{"class":588},"                \"title\"",[187,24944,585],{"class":577},[187,24946,24947],{"class":196},"\"Software Engineer\"",[187,24949,1228],{"class":577},[187,24951,24952,24955,24957,24960,24962,24965],{"class":189,"line":1291},[187,24953,24954],{"class":588},"                \"skills\"",[187,24956,2562],{"class":577},[187,24958,24959],{"class":196},"\"python\"",[187,24961,637],{"class":577},[187,24963,24964],{"class":196},"\"javascript\"",[187,24966,22232],{"class":577},[187,24968,24969,24972],{"class":189,"line":1306},[187,24970,24971],{"class":588},"                \"salary\"",[187,24973,8099],{"class":577},[187,24975,24976,24979,24981,24984],{"class":189,"line":1434},[187,24977,24978],{"class":588},"                    \"min\"",[187,24980,585],{"class":577},[187,24982,24983],{"class":588},"90000",[187,24985,1228],{"class":577},[187,24987,24988,24991,24993,24996],{"class":189,"line":2599},[187,24989,24990],{"class":588},"                    \"max\"",[187,24992,585],{"class":577},[187,24994,24995],{"class":588},"110000",[187,24997,1228],{"class":577},[187,24999,25000,25003,25005],{"class":189,"line":2607},[187,25001,25002],{"class":588},"                    \"avg\"",[187,25004,585],{"class":577},[187,25006,25007],{"class":588},"100000\n",[187,25009,25010],{"class":189,"line":2621},[187,25011,21978],{"class":577},[187,25013,25014],{"class":189,"line":2631},[187,25015,23609],{"class":577},[187,25017,25018],{"class":189,"line":2642},[187,25019,25020],{"class":577},"        ],\n",[187,25022,25023,25026],{"class":189,"line":2653},[187,25024,25025],{"class":588},"        \"founders\"",[187,25027,18908],{"class":577},[187,25029,25030],{"class":189,"line":2665},[187,25031,24937],{"class":577},[187,25033,25034,25037,25039,25042],{"class":189,"line":2674},[187,25035,25036],{"class":588},"                \"name\"",[187,25038,585],{"class":577},[187,25040,25041],{"class":196},"\"Founder Name\"",[187,25043,1228],{"class":577},[187,25045,25046,25049,25051,25054],{"class":189,"line":2684},[187,25047,25048],{"class":588},"                \"linkedin\"",[187,25050,585],{"class":577},[187,25052,25053],{"class":196},"\"https://linkedin.com/founder\"",[187,25055,1228],{"class":577},[187,25057,25058,25061,25063,25066],{"class":189,"line":2694},[187,25059,25060],{"class":588},"                \"education\"",[187,25062,585],{"class":577},[187,25064,25065],{"class":196},"\"University A\"",[187,25067,1228],{"class":577},[187,25069,25070,25073,25075],{"class":189,"line":2706},[187,25071,25072],{"class":588},"                \"image\"",[187,25074,585],{"class":577},[187,25076,25077],{"class":196},"\"abc.png\"\n",[187,25079,25080],{"class":189,"line":2715},[187,25081,23609],{"class":577},[187,25083,25084],{"class":189,"line":2725},[187,25085,25086],{"class":577},"        ]\n",[187,25088,25089],{"class":189,"line":2735},[187,25090,9799],{"class":577},[187,25092,25093],{"class":189,"line":2743},[187,25094,1437],{"class":577},[168,25096,25098],{"id":25097},"analysis","Analysis",[11,25100,25101],{},"Here are some of the biggest questions I wanted to answer along with some simple python I used for extracting data from the main dictionary/JSON object containing all companies and jobs. For the following code, assume I have read the JSON file back into a python dictionary.",[911,25103,25105],{"id":25104},"what-are-the-most-in-demand-skills-for-yc-jobs","What are the most in demand skills for YC Jobs?",[11,25107,25108],{},"I think skills are included mostly for engineering roles (not so much for sales, marketing, etc.). Here are the top skills:",[26,25110,25112],{"className":10554,"code":25111,"language":10556,"meta":35,"style":35},"skills = []\nfor company in company_list:\n    if company[\"jobs\"] is not None:\n        for job in company[\"jobs\"]:\n            if job[\"job_skills\"] is not None:\n                for skill in job[\"job_skills\"]:\n                    skills.append(skill)\n\ntop_skills = Counter(skills).most_common()\nprint(top_skills)\n",[33,25113,25114,25119,25124,25129,25134,25139,25144,25149,25153,25158],{"__ignoreMap":35},[187,25115,25116],{"class":189,"line":190},[187,25117,25118],{},"skills = []\n",[187,25120,25121],{"class":189,"line":249},[187,25122,25123],{},"for company in company_list:\n",[187,25125,25126],{"class":189,"line":312},[187,25127,25128],{},"    if company[\"jobs\"] is not None:\n",[187,25130,25131],{"class":189,"line":319},[187,25132,25133],{},"        for job in company[\"jobs\"]:\n",[187,25135,25136],{"class":189,"line":325},[187,25137,25138],{},"            if job[\"job_skills\"] is not None:\n",[187,25140,25141],{"class":189,"line":686},[187,25142,25143],{},"                for skill in job[\"job_skills\"]:\n",[187,25145,25146],{"class":189,"line":697},[187,25147,25148],{},"                    skills.append(skill)\n",[187,25150,25151],{"class":189,"line":1291},[187,25152,316],{"emptyLinePlaceholder":315},[187,25154,25155],{"class":189,"line":1306},[187,25156,25157],{},"top_skills = Counter(skills).most_common()\n",[187,25159,25160],{"class":189,"line":1434},[187,25161,25162],{},"print(top_skills)\n",[10517,25164,25165],{},[25166,25167],"skill-count",{},[11,25169,25170],{},"I'll try to briefly describe what I know about each of these if I know what it means (without Googling!):",[26,25172,25174],{"className":10554,"code":25173,"language":10556,"meta":35,"style":35},"\n[\n ('JAVASCRIPT', 330), # I have been using JS a lot recently with Vue\n ('REACT', 323), # As a prefer to use Vue, I haven't used React in a while\n ('PYTHON', 312), # I'm a big Python fan, it was the first language I touched, happy to see it near the top!\n ('AMAZON WEB SERVICES (AWS)', 200), # I like AWS a lot. I have really been enjoying using CDK to build infrastructure\n ('NODE.JS', 195), # I would to do more with node this year. I generally use Python for web apps\n ('POSTGRESQL', 132), # I have used Postgres ever since I started using Django and like it a lot\n ('TYPESCRIPT', 114), # this is another goal of mine for 2021, it seems like an inevitability\n ('JAVA', 79), # I have never used Java\n ('SQL', 74), # I usually don't write my own SQL queries; I view SQL through the lense of an ORM\n ('RUBY ON RAILS', 72), # I haven't used RoR\n ('CSS', 71), # I like CSS Frameworks. Recently I'm into Tailwind and Material UI. This site uses Tailwind\n ('HTML', 71),\n ('DOCKER', 66), # I am a big container fan! It is my preferred way to run software, locally and in the cloud\n ('KUBERNETES', 58), # I read the Manning book on k8s. I prefer ECS or Swarm, but I might try using it more\n ('GO', 58), # I haven't ever used Go, but it doesn't look too bad coming from Python\n ('REACT NATIVE', 58),\n ('C++', 55), # Also haven't used this, but I have a book on it\n ('GRAPHQL', 48), # I tried GraphQL and built a HN clone in Django. I prefer REST but I get the appeal (for frontend developers)\n ('GOOGLE CLOUD', 46), # I'm not a big GCP as I mostly use AWS and Digital Ocean but I would like to try Cloud Run\n ('RUBY', 44), # I haven't used it\n ('DJANGO', 44), # Django is my go-to tool for building web apps and APIs. I love the admin, ORM and DRF\n ('MACHINE LEARNING', 44), # I am familiar with some ML techniques but not very well practiced\n ('MONGODB', 43), # Have used it before, but I try to use the postgres JSONField for storing NoSQL data when possible\n ('IOS', 38), # I have an iPhone, but haven't used a Mac in a long time, mostly on Linux and Windows\n ('MYSQL', 36), # I tend to use Postgres, I don't think I've ever used this\n ('ANDROID', 35), # Not something I have worked with\n ('DATA ANALYTICS', 32), # I do a lot of this\n ('GIT', 30), # I'm very slowly trying to learn advanced git features. I have many abandoned \"rebase-practice\" repos\n ('ANGULAR', 29), # I tried it once for about an hour, I know that people love to hate it, I'm just not sure why\n ('SWIFT', 29), # This is apparently a very popular language and has use cases outside of mobile development, but I've never used it\n ('LINUX', 28), # I spend lot of time using Linux machines, mostly Ubuntu.\n ('SOFTWARE ARCHITECTURE', 24), # I like using diagrams.net to draw application infrastructure\n ('KOTLIN', 23), # I think this is a framework for Java/Android?\n ('TENSORFLOW', 22), # Google DL/ML library that I'll try to use later in this article\n ('DISTRIBUTED SYSTEMS', 22), # Using AWS, I guess I have technically designed distributed systems but I wouldn't call it one of my skills\n ('PHP', 22), # I almost learned it to support a WordPress site but opted to use JAMStack instead\n ('DATA WAREHOUSING', 22), # I have used Google BigQuery before which I think counts for this skill\n ('DEEP LEARNING', 20), # I'm going to try to use this later in this article\n ('DATA MODELING', 20), # To me this means writing Django models, or thinking about how to structure an API/json data, etc.\n ('C#', 19), # Micrsoft language used for different things including game dev with Unity\n ('FLASK', 19), # I'm familiar with it but mostly prefer Django's batteries included philosophy\n ('C', 19), # In learning about Linux I have read a bit of C, but never written any\n ('REDIS', 18), # Fast, in memory key-value store with multiple data types. I use it for a few different things\n ('MICROSERVICES', 18), #\n ('COMPUTER VISION', 17), # I once used OpenCV on my Raspberry Pi\n ('EXPRESS', 15), # I'd like to learn how to use this in 2021\n ('BASH/SHELL', 13), # I'm not very fluent in bash but\n ('OBJECTIVE-C', 13), # I haven't used this but I know it is popular for iOS development\n ('FIREBASE', 12), # I have played around with Firebase, but I haven't built anything with it\n ('SCALA', 11), # Functional programming language for JVM, I haven't used it\n ('SOFTWARE SECURITY', 11),\n ('UNITY', 11), # I used this once before to play around with VR development for HTC Vive\n ('R', 11), # I haven't used R before, and I would probably reach for a Python library for doing statistics or ML-related things\n ('KAFKA', 10), # I haven't used it but I'm familiar with the ideas behind Kafka.\n ('SPARK', 10), # I haven't used it and I'm not really sure what it is\n ('ELASTICSEARCH', 10), # I haven't used it before\n ('ETL', 10), # Extract, Transform and Load\n ('NATURAL LANGUAGE PROCESSING', 10),\n ('HEROKU', 10), # I used this when first learning about Django, haven't used it in a while\n ('NGINX', 9), # I use NGINX in most of my web apps as a reverse proxy\n ('JENKINS', 9), # I haven't used it, I am a big GitLab fan and will use that whenever possible\n ('RUST', 9), # I have read the Rust book and have played around with WASM\n ('IMAGE PROCESSING', 8),\n ('SERVERLESS', 8), # I currenty use Fargate and have also used Lambda and SQS and some other serverless AWS tools\n ('BLOCKCHAIN', 8),\n ('OPENCV', 8),\n ('CAD DESIGN', 7), # I am a big fan of SketchUp and I'm familiar with Blender as well, but I'm not sure if these qualify as CAD design\n ('JQUERY', 7), # It was the first library I worked with when I started learning javascript\n ('HADOOP', 6), # It is related to map-reduce, I haven't used it before and I don't really know what it is. I know it is related to HDFS\n ('.NET CORE', 6), # I haven't used it\n ('TCP/IP', 6), # I'm familiary with the basics\n ('ELIXIR', 6), # I don't know, I think it is a framework for Erlang\n ('INTERNET OF THINGS (IOT)', 5),\n ('SASS', 5), # I think it is a framework for CSS. I frequently see node-sass errors from npm\n ('OPENGL', 5),\n ('DYNAMODB', 5), # I am familiar with it but haven't used it\n ('GOOGLE APP ENGINE', 5), # I haven't used it before\n ('UNIX', 4),\n ('SPRING FRAMEWORK', 4), # A Java web framework that I haven't used\n ('CUDA', 4), # I have used it indirectly when I used nvidia-docker to used Tensorflow\n ('DART', 4), # I don't know what this is\n ('ERLANG', 4), # A language that handles concurrency very well\n ('RABBITMQ', 4), # Message queue, I have used it before but tend to use Redis as a message broker\n ('KERAS', 4), # A helper/wrapper library for Tensorflow\n ('SCSS', 4), # I think this is a language that compiles to CSS. CSS frameworks that I use use this\n ('ML', 4), # I've read some books and experimented but I'm not a regular practitioner\n ('MATLAB', 4), # A tool her programming with higher math\n ('SPRING', 4), # A Java web framework. Not sure if different from Spring Boot. Never used it.\n ('CASSANDRA', 4), # FB scalable database (NoSQL I think?) that does sharding really well\n ('HIVE', 3), # Not sure what hive is. I think it related to Hadoop\n ('PUPPET', 3), # Configuration management tool that I haven't ever used\n ('REDSHIFT', 3), # AWS version of Google BigQuery\n ('SQL SERVER', 3), # Not sure what this refers to, specifically.\n ('GROOVY', 3), # I think this is a Java framework?\n ('VERILOG', 3), # I've never heard of this.\n ('TORCH/PYTORCH', 3), # FB python deep learning library.\n ('CLOJURE', 3), # A LISP derivate, functional language\n ('MICROSOFT AZURE', 3), # I've used Azure AD and thats it.\n ('HBASE', 2), # I don't know what this is\n ('RDS/AURORA', 2), # I use RDS and experimented with Aurora but don't know when/why to use it\n ('FIRMWARE', 2), # What's between hardware and software\n ('ABAP', 2), # I don't know\n ('ARDUINO', 2), # I have one, but don't use it\n ('MICROCONTROLLERS', 2), # Arduino might be an example of what this is\n ('SOLIDITY', 2), # Don't know what this is\n ('UNREAL ENGINE', 2), # Unity competitor, used for game development\n ('COFFEESCRIPT', 2), # I think it is a dialect of JS, but I'm not sure\n ('LUA', 2), # I think this is what redis is written in, but I'm not sure how to descbribe what it is\n ('MACOS', 2), # I haven't used MacOS in a long time. I'm tempted to try M1, but I also want to buid a new PC...\n ('NEO4J', 2), # A graph database, I'm not sure what a typical use case is for this\n ('INFORMATION SECURITY', 2), # Unknown unknowns\n ('REINFORCEMENT LEARNING (RL)', 2), # Not sure what this refers to specifically\n ('DEVICE DRIVERS', 2), # Probably involves writing kernel modules\n ('EMBEDDED LINUX', 2), # Not sure if Raspberry Pi is an example of this or not\n ('ELASTIC STACK (ELK)', 1), # Useful for viewing log data (Elastic, Logstash and Kibana), haven't used it\n ('IIS', 1), # I don't know\n ('ORACLE', 1), # A big software company and a proprietary database (I think Django supports it)\n ('F#', 1), # A programming language that I don't know anything about\n ('SQLITE', 1), # A light-weight SQL-compatible file-based database\n ('HASKELL', 1), # A functional programming language that I don't know\n ('SCHEME', 1), # I'm not sure, it may be something related to LISP\n ('MS SQL', 1), # Never used this\n ('MARIADB', 1), # An open source SQL database\n ('MAVEN', 1), # I think it is a Java Framework\n ('SEARCH', 1), # I don't know what this refers to\n ('OCAML', 1), # I think I once read that high-frequency traders like to use this language, but I'm not sure why\n ('JULIA', 1), # A programming language used for math and statistics\n ('GPU PROGRAMMING', 1), # I haven't done this before, probably uses C++\n ('HACK', 1), # FB's version of PHP\n ('XAMARIN', 1), # I dont't know what this is\n ('CORDOVA', 1), # I think it is a tool for generating native apps from JS\n ('SAS', 1), # I don't know what this is\n ('ASSEMBLY', 1), # Low level language that gives instructions to CPU\n ('XML', 1), # A data format, I use it for this site's sitemap and RSS feed\n ('MEMCACHED', 1), # Used for caching. I haven't used it; I typically use redis where this might be an option\n ('LESS', 1), # I think is is related to CSS?\n ('AMAZON ECHO', 1) # I once built an open source Echo on a raspberry pi\n]\n",[33,25175,25176,25180,25184,25189,25194,25199,25204,25209,25214,25219,25224,25229,25234,25239,25244,25249,25254,25259,25264,25269,25274,25279,25284,25289,25294,25299,25304,25309,25314,25319,25324,25329,25334,25339,25344,25349,25354,25359,25364,25369,25374,25379,25384,25389,25394,25399,25404,25409,25414,25419,25424,25429,25434,25439,25444,25449,25454,25459,25464,25469,25474,25479,25484,25489,25494,25499,25504,25509,25514,25519,25524,25529,25534,25539,25544,25549,25554,25559,25564,25569,25574,25579,25584,25589,25594,25599,25604,25609,25614,25619,25624,25629,25634,25639,25644,25649,25654,25659,25664,25669,25674,25679,25684,25689,25694,25699,25704,25709,25714,25719,25724,25729,25734,25739,25744,25749,25754,25759,25764,25769,25774,25779,25784,25789,25794,25799,25804,25809,25814,25819,25824,25829,25834,25839,25844,25849,25854,25859,25864,25869],{"__ignoreMap":35},[187,25177,25178],{"class":189,"line":190},[187,25179,316],{"emptyLinePlaceholder":315},[187,25181,25182],{"class":189,"line":249},[187,25183,24896],{},[187,25185,25186],{"class":189,"line":312},[187,25187,25188],{}," ('JAVASCRIPT', 330), # I have been using JS a lot recently with Vue\n",[187,25190,25191],{"class":189,"line":319},[187,25192,25193],{}," ('REACT', 323), # As a prefer to use Vue, I haven't used React in a while\n",[187,25195,25196],{"class":189,"line":325},[187,25197,25198],{}," ('PYTHON', 312), # I'm a big Python fan, it was the first language I touched, happy to see it near the top!\n",[187,25200,25201],{"class":189,"line":686},[187,25202,25203],{}," ('AMAZON WEB SERVICES (AWS)', 200), # I like AWS a lot. I have really been enjoying using CDK to build infrastructure\n",[187,25205,25206],{"class":189,"line":697},[187,25207,25208],{}," ('NODE.JS', 195), # I would to do more with node this year. I generally use Python for web apps\n",[187,25210,25211],{"class":189,"line":1291},[187,25212,25213],{}," ('POSTGRESQL', 132), # I have used Postgres ever since I started using Django and like it a lot\n",[187,25215,25216],{"class":189,"line":1306},[187,25217,25218],{}," ('TYPESCRIPT', 114), # this is another goal of mine for 2021, it seems like an inevitability\n",[187,25220,25221],{"class":189,"line":1434},[187,25222,25223],{}," ('JAVA', 79), # I have never used Java\n",[187,25225,25226],{"class":189,"line":2599},[187,25227,25228],{}," ('SQL', 74), # I usually don't write my own SQL queries; I view SQL through the lense of an ORM\n",[187,25230,25231],{"class":189,"line":2607},[187,25232,25233],{}," ('RUBY ON RAILS', 72), # I haven't used RoR\n",[187,25235,25236],{"class":189,"line":2621},[187,25237,25238],{}," ('CSS', 71), # I like CSS Frameworks. Recently I'm into Tailwind and Material UI. This site uses Tailwind\n",[187,25240,25241],{"class":189,"line":2631},[187,25242,25243],{}," ('HTML', 71),\n",[187,25245,25246],{"class":189,"line":2642},[187,25247,25248],{}," ('DOCKER', 66), # I am a big container fan! It is my preferred way to run software, locally and in the cloud\n",[187,25250,25251],{"class":189,"line":2653},[187,25252,25253],{}," ('KUBERNETES', 58), # I read the Manning book on k8s. I prefer ECS or Swarm, but I might try using it more\n",[187,25255,25256],{"class":189,"line":2665},[187,25257,25258],{}," ('GO', 58), # I haven't ever used Go, but it doesn't look too bad coming from Python\n",[187,25260,25261],{"class":189,"line":2674},[187,25262,25263],{}," ('REACT NATIVE', 58),\n",[187,25265,25266],{"class":189,"line":2684},[187,25267,25268],{}," ('C++', 55), # Also haven't used this, but I have a book on it\n",[187,25270,25271],{"class":189,"line":2694},[187,25272,25273],{}," ('GRAPHQL', 48), # I tried GraphQL and built a HN clone in Django. I prefer REST but I get the appeal (for frontend developers)\n",[187,25275,25276],{"class":189,"line":2706},[187,25277,25278],{}," ('GOOGLE CLOUD', 46), # I'm not a big GCP as I mostly use AWS and Digital Ocean but I would like to try Cloud Run\n",[187,25280,25281],{"class":189,"line":2715},[187,25282,25283],{}," ('RUBY', 44), # I haven't used it\n",[187,25285,25286],{"class":189,"line":2725},[187,25287,25288],{}," ('DJANGO', 44), # Django is my go-to tool for building web apps and APIs. I love the admin, ORM and DRF\n",[187,25290,25291],{"class":189,"line":2735},[187,25292,25293],{}," ('MACHINE LEARNING', 44), # I am familiar with some ML techniques but not very well practiced\n",[187,25295,25296],{"class":189,"line":2743},[187,25297,25298],{}," ('MONGODB', 43), # Have used it before, but I try to use the postgres JSONField for storing NoSQL data when possible\n",[187,25300,25301],{"class":189,"line":2754},[187,25302,25303],{}," ('IOS', 38), # I have an iPhone, but haven't used a Mac in a long time, mostly on Linux and Windows\n",[187,25305,25306],{"class":189,"line":2762},[187,25307,25308],{}," ('MYSQL', 36), # I tend to use Postgres, I don't think I've ever used this\n",[187,25310,25311],{"class":189,"line":2770},[187,25312,25313],{}," ('ANDROID', 35), # Not something I have worked with\n",[187,25315,25316],{"class":189,"line":2781},[187,25317,25318],{}," ('DATA ANALYTICS', 32), # I do a lot of this\n",[187,25320,25321],{"class":189,"line":2792},[187,25322,25323],{}," ('GIT', 30), # I'm very slowly trying to learn advanced git features. I have many abandoned \"rebase-practice\" repos\n",[187,25325,25326],{"class":189,"line":2803},[187,25327,25328],{}," ('ANGULAR', 29), # I tried it once for about an hour, I know that people love to hate it, I'm just not sure why\n",[187,25330,25331],{"class":189,"line":2808},[187,25332,25333],{}," ('SWIFT', 29), # This is apparently a very popular language and has use cases outside of mobile development, but I've never used it\n",[187,25335,25336],{"class":189,"line":2816},[187,25337,25338],{}," ('LINUX', 28), # I spend lot of time using Linux machines, mostly Ubuntu.\n",[187,25340,25341],{"class":189,"line":2824},[187,25342,25343],{}," ('SOFTWARE ARCHITECTURE', 24), # I like using diagrams.net to draw application infrastructure\n",[187,25345,25346],{"class":189,"line":2834},[187,25347,25348],{}," ('KOTLIN', 23), # I think this is a framework for Java/Android?\n",[187,25350,25351],{"class":189,"line":2845},[187,25352,25353],{}," ('TENSORFLOW', 22), # Google DL/ML library that I'll try to use later in this article\n",[187,25355,25356],{"class":189,"line":2856},[187,25357,25358],{}," ('DISTRIBUTED SYSTEMS', 22), # Using AWS, I guess I have technically designed distributed systems but I wouldn't call it one of my skills\n",[187,25360,25361],{"class":189,"line":2867},[187,25362,25363],{}," ('PHP', 22), # I almost learned it to support a WordPress site but opted to use JAMStack instead\n",[187,25365,25366],{"class":189,"line":2878},[187,25367,25368],{}," ('DATA WAREHOUSING', 22), # I have used Google BigQuery before which I think counts for this skill\n",[187,25370,25371],{"class":189,"line":2886},[187,25372,25373],{}," ('DEEP LEARNING', 20), # I'm going to try to use this later in this article\n",[187,25375,25376],{"class":189,"line":2900},[187,25377,25378],{}," ('DATA MODELING', 20), # To me this means writing Django models, or thinking about how to structure an API/json data, etc.\n",[187,25380,25381],{"class":189,"line":2905},[187,25382,25383],{}," ('C#', 19), # Micrsoft language used for different things including game dev with Unity\n",[187,25385,25386],{"class":189,"line":2913},[187,25387,25388],{}," ('FLASK', 19), # I'm familiar with it but mostly prefer Django's batteries included philosophy\n",[187,25390,25391],{"class":189,"line":2921},[187,25392,25393],{}," ('C', 19), # In learning about Linux I have read a bit of C, but never written any\n",[187,25395,25396],{"class":189,"line":2931},[187,25397,25398],{}," ('REDIS', 18), # Fast, in memory key-value store with multiple data types. I use it for a few different things\n",[187,25400,25401],{"class":189,"line":2942},[187,25402,25403],{}," ('MICROSERVICES', 18), #\n",[187,25405,25406],{"class":189,"line":2953},[187,25407,25408],{}," ('COMPUTER VISION', 17), # I once used OpenCV on my Raspberry Pi\n",[187,25410,25411],{"class":189,"line":2964},[187,25412,25413],{}," ('EXPRESS', 15), # I'd like to learn how to use this in 2021\n",[187,25415,25416],{"class":189,"line":2975},[187,25417,25418],{}," ('BASH/SHELL', 13), # I'm not very fluent in bash but\n",[187,25420,25421],{"class":189,"line":2983},[187,25422,25423],{}," ('OBJECTIVE-C', 13), # I haven't used this but I know it is popular for iOS development\n",[187,25425,25426],{"class":189,"line":2992},[187,25427,25428],{}," ('FIREBASE', 12), # I have played around with Firebase, but I haven't built anything with it\n",[187,25430,25431],{"class":189,"line":3001},[187,25432,25433],{}," ('SCALA', 11), # Functional programming language for JVM, I haven't used it\n",[187,25435,25436],{"class":189,"line":3010},[187,25437,25438],{}," ('SOFTWARE SECURITY', 11),\n",[187,25440,25441],{"class":189,"line":3019},[187,25442,25443],{}," ('UNITY', 11), # I used this once before to play around with VR development for HTC Vive\n",[187,25445,25446],{"class":189,"line":3028},[187,25447,25448],{}," ('R', 11), # I haven't used R before, and I would probably reach for a Python library for doing statistics or ML-related things\n",[187,25450,25451],{"class":189,"line":3033},[187,25452,25453],{}," ('KAFKA', 10), # I haven't used it but I'm familiar with the ideas behind Kafka.\n",[187,25455,25456],{"class":189,"line":3041},[187,25457,25458],{}," ('SPARK', 10), # I haven't used it and I'm not really sure what it is\n",[187,25460,25461],{"class":189,"line":3049},[187,25462,25463],{}," ('ELASTICSEARCH', 10), # I haven't used it before\n",[187,25465,25466],{"class":189,"line":3059},[187,25467,25468],{}," ('ETL', 10), # Extract, Transform and Load\n",[187,25470,25471],{"class":189,"line":3070},[187,25472,25473],{}," ('NATURAL LANGUAGE PROCESSING', 10),\n",[187,25475,25476],{"class":189,"line":3075},[187,25477,25478],{}," ('HEROKU', 10), # I used this when first learning about Django, haven't used it in a while\n",[187,25480,25481],{"class":189,"line":3083},[187,25482,25483],{}," ('NGINX', 9), # I use NGINX in most of my web apps as a reverse proxy\n",[187,25485,25486],{"class":189,"line":3091},[187,25487,25488],{}," ('JENKINS', 9), # I haven't used it, I am a big GitLab fan and will use that whenever possible\n",[187,25490,25491],{"class":189,"line":3101},[187,25492,25493],{}," ('RUST', 9), # I have read the Rust book and have played around with WASM\n",[187,25495,25496],{"class":189,"line":3111},[187,25497,25498],{}," ('IMAGE PROCESSING', 8),\n",[187,25500,25501],{"class":189,"line":3122},[187,25502,25503],{}," ('SERVERLESS', 8), # I currenty use Fargate and have also used Lambda and SQS and some other serverless AWS tools\n",[187,25505,25506],{"class":189,"line":3132},[187,25507,25508],{}," ('BLOCKCHAIN', 8),\n",[187,25510,25511],{"class":189,"line":3143},[187,25512,25513],{}," ('OPENCV', 8),\n",[187,25515,25516],{"class":189,"line":3151},[187,25517,25518],{}," ('CAD DESIGN', 7), # I am a big fan of SketchUp and I'm familiar with Blender as well, but I'm not sure if these qualify as CAD design\n",[187,25520,25521],{"class":189,"line":3161},[187,25522,25523],{}," ('JQUERY', 7), # It was the first library I worked with when I started learning javascript\n",[187,25525,25526],{"class":189,"line":3170},[187,25527,25528],{}," ('HADOOP', 6), # It is related to map-reduce, I haven't used it before and I don't really know what it is. I know it is related to HDFS\n",[187,25530,25531],{"class":189,"line":3178},[187,25532,25533],{}," ('.NET CORE', 6), # I haven't used it\n",[187,25535,25536],{"class":189,"line":3185},[187,25537,25538],{}," ('TCP/IP', 6), # I'm familiary with the basics\n",[187,25540,25541],{"class":189,"line":3195},[187,25542,25543],{}," ('ELIXIR', 6), # I don't know, I think it is a framework for Erlang\n",[187,25545,25546],{"class":189,"line":3205},[187,25547,25548],{}," ('INTERNET OF THINGS (IOT)', 5),\n",[187,25550,25551],{"class":189,"line":3210},[187,25552,25553],{}," ('SASS', 5), # I think it is a framework for CSS. I frequently see node-sass errors from npm\n",[187,25555,25556],{"class":189,"line":3216},[187,25557,25558],{}," ('OPENGL', 5),\n",[187,25560,25561],{"class":189,"line":3224},[187,25562,25563],{}," ('DYNAMODB', 5), # I am familiar with it but haven't used it\n",[187,25565,25566],{"class":189,"line":3234},[187,25567,25568],{}," ('GOOGLE APP ENGINE', 5), # I haven't used it before\n",[187,25570,25571],{"class":189,"line":3242},[187,25572,25573],{}," ('UNIX', 4),\n",[187,25575,25576],{"class":189,"line":3252},[187,25577,25578],{}," ('SPRING FRAMEWORK', 4), # A Java web framework that I haven't used\n",[187,25580,25581],{"class":189,"line":3260},[187,25582,25583],{}," ('CUDA', 4), # I have used it indirectly when I used nvidia-docker to used Tensorflow\n",[187,25585,25586],{"class":189,"line":3270},[187,25587,25588],{}," ('DART', 4), # I don't know what this is\n",[187,25590,25591],{"class":189,"line":3275},[187,25592,25593],{}," ('ERLANG', 4), # A language that handles concurrency very well\n",[187,25595,25596],{"class":189,"line":3283},[187,25597,25598],{}," ('RABBITMQ', 4), # Message queue, I have used it before but tend to use Redis as a message broker\n",[187,25600,25601],{"class":189,"line":3291},[187,25602,25603],{}," ('KERAS', 4), # A helper/wrapper library for Tensorflow\n",[187,25605,25606],{"class":189,"line":3300},[187,25607,25608],{}," ('SCSS', 4), # I think this is a language that compiles to CSS. CSS frameworks that I use use this\n",[187,25610,25611],{"class":189,"line":3310},[187,25612,25613],{}," ('ML', 4), # I've read some books and experimented but I'm not a regular practitioner\n",[187,25615,25616],{"class":189,"line":3320},[187,25617,25618],{}," ('MATLAB', 4), # A tool her programming with higher math\n",[187,25620,25621],{"class":189,"line":3325},[187,25622,25623],{}," ('SPRING', 4), # A Java web framework. Not sure if different from Spring Boot. Never used it.\n",[187,25625,25626],{"class":189,"line":3333},[187,25627,25628],{}," ('CASSANDRA', 4), # FB scalable database (NoSQL I think?) that does sharding really well\n",[187,25630,25631],{"class":189,"line":3343},[187,25632,25633],{}," ('HIVE', 3), # Not sure what hive is. I think it related to Hadoop\n",[187,25635,25636],{"class":189,"line":3354},[187,25637,25638],{}," ('PUPPET', 3), # Configuration management tool that I haven't ever used\n",[187,25640,25641],{"class":189,"line":17135},[187,25642,25643],{}," ('REDSHIFT', 3), # AWS version of Google BigQuery\n",[187,25645,25646],{"class":189,"line":17141},[187,25647,25648],{}," ('SQL SERVER', 3), # Not sure what this refers to, specifically.\n",[187,25650,25651],{"class":189,"line":17146},[187,25652,25653],{}," ('GROOVY', 3), # I think this is a Java framework?\n",[187,25655,25656],{"class":189,"line":17152},[187,25657,25658],{}," ('VERILOG', 3), # I've never heard of this.\n",[187,25660,25661],{"class":189,"line":17164},[187,25662,25663],{}," ('TORCH/PYTORCH', 3), # FB python deep learning library.\n",[187,25665,25666],{"class":189,"line":17175},[187,25667,25668],{}," ('CLOJURE', 3), # A LISP derivate, functional language\n",[187,25670,25671],{"class":189,"line":17188},[187,25672,25673],{}," ('MICROSOFT AZURE', 3), # I've used Azure AD and thats it.\n",[187,25675,25676],{"class":189,"line":17199},[187,25677,25678],{}," ('HBASE', 2), # I don't know what this is\n",[187,25680,25681],{"class":189,"line":17207},[187,25682,25683],{}," ('RDS/AURORA', 2), # I use RDS and experimented with Aurora but don't know when/why to use it\n",[187,25685,25686],{"class":189,"line":17212},[187,25687,25688],{}," ('FIRMWARE', 2), # What's between hardware and software\n",[187,25690,25691],{"class":189,"line":17217},[187,25692,25693],{}," ('ABAP', 2), # I don't know\n",[187,25695,25696],{"class":189,"line":17223},[187,25697,25698],{}," ('ARDUINO', 2), # I have one, but don't use it\n",[187,25700,25701],{"class":189,"line":17235},[187,25702,25703],{}," ('MICROCONTROLLERS', 2), # Arduino might be an example of what this is\n",[187,25705,25706],{"class":189,"line":17248},[187,25707,25708],{}," ('SOLIDITY', 2), # Don't know what this is\n",[187,25710,25711],{"class":189,"line":17267},[187,25712,25713],{}," ('UNREAL ENGINE', 2), # Unity competitor, used for game development\n",[187,25715,25716],{"class":189,"line":17278},[187,25717,25718],{}," ('COFFEESCRIPT', 2), # I think it is a dialect of JS, but I'm not sure\n",[187,25720,25721],{"class":189,"line":11081},[187,25722,25723],{}," ('LUA', 2), # I think this is what redis is written in, but I'm not sure how to descbribe what it is\n",[187,25725,25726],{"class":189,"line":17293},[187,25727,25728],{}," ('MACOS', 2), # I haven't used MacOS in a long time. I'm tempted to try M1, but I also want to buid a new PC...\n",[187,25730,25731],{"class":189,"line":17298},[187,25732,25733],{}," ('NEO4J', 2), # A graph database, I'm not sure what a typical use case is for this\n",[187,25735,25736],{"class":189,"line":17304},[187,25737,25738],{}," ('INFORMATION SECURITY', 2), # Unknown unknowns\n",[187,25740,25741],{"class":189,"line":17332},[187,25742,25743],{}," ('REINFORCEMENT LEARNING (RL)', 2), # Not sure what this refers to specifically\n",[187,25745,25746],{"class":189,"line":17337},[187,25747,25748],{}," ('DEVICE DRIVERS', 2), # Probably involves writing kernel modules\n",[187,25750,25751],{"class":189,"line":17343},[187,25752,25753],{}," ('EMBEDDED LINUX', 2), # Not sure if Raspberry Pi is an example of this or not\n",[187,25755,25756],{"class":189,"line":17349},[187,25757,25758],{}," ('ELASTIC STACK (ELK)', 1), # Useful for viewing log data (Elastic, Logstash and Kibana), haven't used it\n",[187,25760,25761],{"class":189,"line":17361},[187,25762,25763],{}," ('IIS', 1), # I don't know\n",[187,25765,25766],{"class":189,"line":17373},[187,25767,25768],{}," ('ORACLE', 1), # A big software company and a proprietary database (I think Django supports it)\n",[187,25770,25771],{"class":189,"line":17388},[187,25772,25773],{}," ('F#', 1), # A programming language that I don't know anything about\n",[187,25775,25776],{"class":189,"line":17398},[187,25777,25778],{}," ('SQLITE', 1), # A light-weight SQL-compatible file-based database\n",[187,25780,25781],{"class":189,"line":17407},[187,25782,25783],{}," ('HASKELL', 1), # A functional programming language that I don't know\n",[187,25785,25786],{"class":189,"line":17412},[187,25787,25788],{}," ('SCHEME', 1), # I'm not sure, it may be something related to LISP\n",[187,25790,25791],{"class":189,"line":17417},[187,25792,25793],{}," ('MS SQL', 1), # Never used this\n",[187,25795,25796],{"class":189,"line":17425},[187,25797,25798],{}," ('MARIADB', 1), # An open source SQL database\n",[187,25800,25801],{"class":189,"line":17430},[187,25802,25803],{}," ('MAVEN', 1), # I think it is a Java Framework\n",[187,25805,25806],{"class":189,"line":17436},[187,25807,25808],{}," ('SEARCH', 1), # I don't know what this refers to\n",[187,25810,25811],{"class":189,"line":17453},[187,25812,25813],{}," ('OCAML', 1), # I think I once read that high-frequency traders like to use this language, but I'm not sure why\n",[187,25815,25816],{"class":189,"line":17458},[187,25817,25818],{}," ('JULIA', 1), # A programming language used for math and statistics\n",[187,25820,25821],{"class":189,"line":17464},[187,25822,25823],{}," ('GPU PROGRAMMING', 1), # I haven't done this before, probably uses C++\n",[187,25825,25826],{"class":189,"line":17476},[187,25827,25828],{}," ('HACK', 1), # FB's version of PHP\n",[187,25830,25831],{"class":189,"line":17488},[187,25832,25833],{}," ('XAMARIN', 1), # I dont't know what this is\n",[187,25835,25836],{"class":189,"line":17501},[187,25837,25838],{}," ('CORDOVA', 1), # I think it is a tool for generating native apps from JS\n",[187,25840,25841],{"class":189,"line":17511},[187,25842,25843],{}," ('SAS', 1), # I don't know what this is\n",[187,25845,25846],{"class":189,"line":17534},[187,25847,25848],{}," ('ASSEMBLY', 1), # Low level language that gives instructions to CPU\n",[187,25850,25851],{"class":189,"line":17548},[187,25852,25853],{}," ('XML', 1), # A data format, I use it for this site's sitemap and RSS feed\n",[187,25855,25856],{"class":189,"line":17553},[187,25857,25858],{}," ('MEMCACHED', 1), # Used for caching. I haven't used it; I typically use redis where this might be an option\n",[187,25860,25861],{"class":189,"line":17558},[187,25862,25863],{}," ('LESS', 1), # I think is is related to CSS?\n",[187,25865,25866],{"class":189,"line":17571},[187,25867,25868],{}," ('AMAZON ECHO', 1) # I once built an open source Echo on a raspberry pi\n",[187,25870,25871],{"class":189,"line":17576},[187,25872,1437],{},[11,25874,25875],{},"The list above gives a count of the different skills in all job postings sorted by the most common skills. But what about the most common skills listed together with any given skill? This would allow us to answer questions like \"what skills appear most frequently along with JavaScript?\"",[11,25877,25878],{},"We can find this with by doing:",[26,25880,25882],{"className":10554,"code":25881,"language":10556,"meta":35,"style":35},"skills_frequency = defaultdict(lambda: defaultdict(lambda: 0))\n\nfor company in company_list:\n    if company[\"jobs\"] is not None:\n        for job in company[\"jobs\"]:\n            if job[\"job_skills\"] is not None:\n                job_skills = job[\"job_skills\"]\n\n                skill_tuples = itertools.permutations(job_skills, 2)\n\n                for skill_tuple in skill_tuples:\n                    first = skill_tuple[0]\n                    second = skill_tuple[1]\n\n                    skills_frequency[first][second] += 1\n\npprint.pprint(\n    {\n        key: sorted(value.items(), key=lambda kv: -kv[1])\n        for key, value in skills_frequency.items()\n    }\n)\n",[33,25883,25884,25889,25893,25897,25901,25905,25909,25914,25918,25923,25927,25932,25937,25942,25946,25951,25955,25960,25964,25969,25974,25978],{"__ignoreMap":35},[187,25885,25886],{"class":189,"line":190},[187,25887,25888],{},"skills_frequency = defaultdict(lambda: defaultdict(lambda: 0))\n",[187,25890,25891],{"class":189,"line":249},[187,25892,316],{"emptyLinePlaceholder":315},[187,25894,25895],{"class":189,"line":312},[187,25896,25123],{},[187,25898,25899],{"class":189,"line":319},[187,25900,25128],{},[187,25902,25903],{"class":189,"line":325},[187,25904,25133],{},[187,25906,25907],{"class":189,"line":686},[187,25908,25138],{},[187,25910,25911],{"class":189,"line":697},[187,25912,25913],{},"                job_skills = job[\"job_skills\"]\n",[187,25915,25916],{"class":189,"line":1291},[187,25917,316],{"emptyLinePlaceholder":315},[187,25919,25920],{"class":189,"line":1306},[187,25921,25922],{},"                skill_tuples = itertools.permutations(job_skills, 2)\n",[187,25924,25925],{"class":189,"line":1434},[187,25926,316],{"emptyLinePlaceholder":315},[187,25928,25929],{"class":189,"line":2599},[187,25930,25931],{},"                for skill_tuple in skill_tuples:\n",[187,25933,25934],{"class":189,"line":2607},[187,25935,25936],{},"                    first = skill_tuple[0]\n",[187,25938,25939],{"class":189,"line":2621},[187,25940,25941],{},"                    second = skill_tuple[1]\n",[187,25943,25944],{"class":189,"line":2631},[187,25945,316],{"emptyLinePlaceholder":315},[187,25947,25948],{"class":189,"line":2642},[187,25949,25950],{},"                    skills_frequency[first][second] += 1\n",[187,25952,25953],{"class":189,"line":2653},[187,25954,316],{"emptyLinePlaceholder":315},[187,25956,25957],{"class":189,"line":2665},[187,25958,25959],{},"pprint.pprint(\n",[187,25961,25962],{"class":189,"line":2674},[187,25963,24901],{},[187,25965,25966],{"class":189,"line":2684},[187,25967,25968],{},"        key: sorted(value.items(), key=lambda kv: -kv[1])\n",[187,25970,25971],{"class":189,"line":2694},[187,25972,25973],{},"        for key, value in skills_frequency.items()\n",[187,25975,25976],{"class":189,"line":2706},[187,25977,9799],{},[187,25979,25980],{"class":189,"line":2715},[187,25981,621],{},[10517,25983,25984],{},[25985,25986],"skill-frequencies",{},[911,25988,25990],{"id":25989},"what-are-these-companies-working-on","What are these companies working on?",[11,25992,25993],{},"Here's a wordcloud made from the short company descriptions:",[11,25995,25996],{},[511,25997],{"alt":7255,"src":25998},"/static/company_desc_wc.png",[26,26000,26002],{"className":10554,"code":26001,"language":10556,"meta":35,"style":35},"import json\nimport random\n\nfrom collections import Counter\nfrom os import path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\n\nHTML_FILE = \"waas_data.json\"\nwith open(HTML_FILE, 'r') as j:\n     company_list = json.loads(j.read())\n\ncompany_names = [company.get(\"company_name\", \" \").lower() for company in company_list]\ncompany_description_list = [company.get(\"company_desc\", \" \").lower().replace(\".\", \"\") for company in company_list]\ncompany_descriptions = \" \".join(company_description_list)\n\nwc = WordCloud(background_color=\"white\", width=1920, height=1080, max_words=500, stopwords=STOPWORDS, margin=10,\n               random_state=1).generate(company_descriptions)\n\ndefault_colors = wc.to_array()\n\nplt.figure(figsize=(40, 40))\nplt.imshow(wc, interpolation=\"bilinear\")\n\nplt.axis(\"off\")\nplt.savefig('company_description_wc.png')\nplt.show()\n",[33,26003,26004,26008,26013,26017,26022,26027,26031,26036,26040,26044,26049,26054,26058,26063,26068,26073,26077,26082,26087,26092,26096,26101,26106,26110,26115,26119,26124,26129,26133,26138,26143],{"__ignoreMap":35},[187,26005,26006],{"class":189,"line":190},[187,26007,10340],{},[187,26009,26010],{"class":189,"line":249},[187,26011,26012],{},"import random\n",[187,26014,26015],{"class":189,"line":312},[187,26016,316],{"emptyLinePlaceholder":315},[187,26018,26019],{"class":189,"line":319},[187,26020,26021],{},"from collections import Counter\n",[187,26023,26024],{"class":189,"line":325},[187,26025,26026],{},"from os import path\n",[187,26028,26029],{"class":189,"line":686},[187,26030,316],{"emptyLinePlaceholder":315},[187,26032,26033],{"class":189,"line":697},[187,26034,26035],{},"import matplotlib.pyplot as plt\n",[187,26037,26038],{"class":189,"line":1291},[187,26039,10563],{},[187,26041,26042],{"class":189,"line":1306},[187,26043,316],{"emptyLinePlaceholder":315},[187,26045,26046],{"class":189,"line":1434},[187,26047,26048],{},"from PIL import Image\n",[187,26050,26051],{"class":189,"line":2599},[187,26052,26053],{},"from wordcloud import WordCloud, STOPWORDS\n",[187,26055,26056],{"class":189,"line":2607},[187,26057,316],{"emptyLinePlaceholder":315},[187,26059,26060],{"class":189,"line":2621},[187,26061,26062],{},"HTML_FILE = \"waas_data.json\"\n",[187,26064,26065],{"class":189,"line":2631},[187,26066,26067],{},"with open(HTML_FILE, 'r') as j:\n",[187,26069,26070],{"class":189,"line":2642},[187,26071,26072],{},"     company_list = json.loads(j.read())\n",[187,26074,26075],{"class":189,"line":2653},[187,26076,316],{"emptyLinePlaceholder":315},[187,26078,26079],{"class":189,"line":2665},[187,26080,26081],{},"company_names = [company.get(\"company_name\", \" \").lower() for company in company_list]\n",[187,26083,26084],{"class":189,"line":2674},[187,26085,26086],{},"company_description_list = [company.get(\"company_desc\", \" \").lower().replace(\".\", \"\") for company in company_list]\n",[187,26088,26089],{"class":189,"line":2684},[187,26090,26091],{},"company_descriptions = \" \".join(company_description_list)\n",[187,26093,26094],{"class":189,"line":2694},[187,26095,316],{"emptyLinePlaceholder":315},[187,26097,26098],{"class":189,"line":2706},[187,26099,26100],{},"wc = WordCloud(background_color=\"white\", width=1920, height=1080, max_words=500, stopwords=STOPWORDS, margin=10,\n",[187,26102,26103],{"class":189,"line":2715},[187,26104,26105],{},"               random_state=1).generate(company_descriptions)\n",[187,26107,26108],{"class":189,"line":2725},[187,26109,316],{"emptyLinePlaceholder":315},[187,26111,26112],{"class":189,"line":2735},[187,26113,26114],{},"default_colors = wc.to_array()\n",[187,26116,26117],{"class":189,"line":2743},[187,26118,316],{"emptyLinePlaceholder":315},[187,26120,26121],{"class":189,"line":2754},[187,26122,26123],{},"plt.figure(figsize=(40, 40))\n",[187,26125,26126],{"class":189,"line":2762},[187,26127,26128],{},"plt.imshow(wc, interpolation=\"bilinear\")\n",[187,26130,26131],{"class":189,"line":2770},[187,26132,316],{"emptyLinePlaceholder":315},[187,26134,26135],{"class":189,"line":2781},[187,26136,26137],{},"plt.axis(\"off\")\n",[187,26139,26140],{"class":189,"line":2792},[187,26141,26142],{},"plt.savefig('company_description_wc.png')\n",[187,26144,26145],{"class":189,"line":2803},[187,26146,26147],{},"plt.show()\n",[11,26149,26150],{},"Here's a breakdown of YC companies by category and sub category:",[10517,26152,26153],{},[26154,26155],"category-breakdown-chart",{},[911,26157,26159],{"id":26158},"salary-equity-and-years-of-experience","Salary, Equity and Years of Experience",[11,26161,26162],{},"Here's a scatterplot showing average salary and average equity for positions categorized by years of experience required.",[10517,26164,26165],{},[26166,26167],"salary-equity-scatter",{},[911,26169,26171],{"id":26170},"logos","Logos",[11,26173,26174],{},"Here's a look at about 600 of the 750 logos that were made available in the list of companies. The logos are sorted by their average hex color, which puts them on a gradient of dark to light:",[11,26176,26177],{},[511,26178],{"alt":7255,"src":26179},"/static/waas/yc.png",[26,26181,26183],{"className":10554,"code":26182,"language":10556,"meta":35,"style":35},"import os\nimport PIL\nfrom PIL import Image\nfrom IPython.display import display, Image as IPyImage\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\nLOGO_DIR = 'data/waas_full_details_dump_files/'\nyc_logos = [LOGO_DIR + x for x in os.listdir(LOGO_DIR) if x.endswith('.png')]\n\ndef average_img_hex(img):\n    \"\"\"\n    https://www.hackzine.org/getting-average-image-color-from-python.html\n    \"\"\"\n    img = Image.open(img)\n\n    # leave out images not in RGB/RGBA mode\n    if img.mode in [\"LA\", \"P\", \"L\"]:\n        return\n\n    # resize the image to 1 pixel and get the average hex value\n    img2 = img.resize((1, 1))\n    color = img2.getpixel((0, 0))\n    average_hex = '#{:02x}{:02x}{:02x}'.format(*color)\n\n    return average_hex\n\n# sort images by average hex value\nsorted_images = sorted(\n    [(average_img_hex(img), img) for img in yc_logos if average_img_hex(img) is not None],\n    key=lambda x: x[0]\n)\n\nimages = sorted_images[:600]\n\nfig, axes = plt.subplots(20, 30, figsize=(30, 15), sharex=False, sharey=False)\n\nfor img, ax in zip(images, axes.flat):\n    ax.imshow(mpimg.imread(img[1]))\n    ax.axis('off')\n\nplt.savefig('yc.png')\nplt.show()\n\n",[33,26184,26185,26189,26194,26198,26203,26207,26212,26217,26221,26226,26231,26235,26240,26244,26249,26253,26258,26262,26267,26272,26277,26281,26286,26291,26296,26301,26305,26310,26314,26319,26324,26329,26334,26338,26342,26347,26351,26356,26360,26365,26370,26375,26379,26384],{"__ignoreMap":35},[187,26186,26187],{"class":189,"line":190},[187,26188,10345],{},[187,26190,26191],{"class":189,"line":249},[187,26192,26193],{},"import PIL\n",[187,26195,26196],{"class":189,"line":312},[187,26197,26048],{},[187,26199,26200],{"class":189,"line":319},[187,26201,26202],{},"from IPython.display import display, Image as IPyImage\n",[187,26204,26205],{"class":189,"line":325},[187,26206,26035],{},[187,26208,26209],{"class":189,"line":686},[187,26210,26211],{},"import matplotlib.image as mpimg\n",[187,26213,26214],{"class":189,"line":697},[187,26215,26216],{},"%matplotlib inline\n",[187,26218,26219],{"class":189,"line":1291},[187,26220,316],{"emptyLinePlaceholder":315},[187,26222,26223],{"class":189,"line":1306},[187,26224,26225],{},"LOGO_DIR = 'data/waas_full_details_dump_files/'\n",[187,26227,26228],{"class":189,"line":1434},[187,26229,26230],{},"yc_logos = [LOGO_DIR + x for x in os.listdir(LOGO_DIR) if x.endswith('.png')]\n",[187,26232,26233],{"class":189,"line":2599},[187,26234,316],{"emptyLinePlaceholder":315},[187,26236,26237],{"class":189,"line":2607},[187,26238,26239],{},"def average_img_hex(img):\n",[187,26241,26242],{"class":189,"line":2621},[187,26243,4793],{},[187,26245,26246],{"class":189,"line":2631},[187,26247,26248],{},"    https://www.hackzine.org/getting-average-image-color-from-python.html\n",[187,26250,26251],{"class":189,"line":2642},[187,26252,4793],{},[187,26254,26255],{"class":189,"line":2653},[187,26256,26257],{},"    img = Image.open(img)\n",[187,26259,26260],{"class":189,"line":2665},[187,26261,316],{"emptyLinePlaceholder":315},[187,26263,26264],{"class":189,"line":2674},[187,26265,26266],{},"    # leave out images not in RGB/RGBA mode\n",[187,26268,26269],{"class":189,"line":2684},[187,26270,26271],{},"    if img.mode in [\"LA\", \"P\", \"L\"]:\n",[187,26273,26274],{"class":189,"line":2694},[187,26275,26276],{},"        return\n",[187,26278,26279],{"class":189,"line":2706},[187,26280,316],{"emptyLinePlaceholder":315},[187,26282,26283],{"class":189,"line":2715},[187,26284,26285],{},"    # resize the image to 1 pixel and get the average hex value\n",[187,26287,26288],{"class":189,"line":2725},[187,26289,26290],{},"    img2 = img.resize((1, 1))\n",[187,26292,26293],{"class":189,"line":2735},[187,26294,26295],{},"    color = img2.getpixel((0, 0))\n",[187,26297,26298],{"class":189,"line":2743},[187,26299,26300],{},"    average_hex = '#{:02x}{:02x}{:02x}'.format(*color)\n",[187,26302,26303],{"class":189,"line":2754},[187,26304,316],{"emptyLinePlaceholder":315},[187,26306,26307],{"class":189,"line":2762},[187,26308,26309],{},"    return average_hex\n",[187,26311,26312],{"class":189,"line":2770},[187,26313,316],{"emptyLinePlaceholder":315},[187,26315,26316],{"class":189,"line":2781},[187,26317,26318],{},"# sort images by average hex value\n",[187,26320,26321],{"class":189,"line":2792},[187,26322,26323],{},"sorted_images = sorted(\n",[187,26325,26326],{"class":189,"line":2803},[187,26327,26328],{},"    [(average_img_hex(img), img) for img in yc_logos if average_img_hex(img) is not None],\n",[187,26330,26331],{"class":189,"line":2808},[187,26332,26333],{},"    key=lambda x: x[0]\n",[187,26335,26336],{"class":189,"line":2816},[187,26337,621],{},[187,26339,26340],{"class":189,"line":2824},[187,26341,316],{"emptyLinePlaceholder":315},[187,26343,26344],{"class":189,"line":2834},[187,26345,26346],{},"images = sorted_images[:600]\n",[187,26348,26349],{"class":189,"line":2845},[187,26350,316],{"emptyLinePlaceholder":315},[187,26352,26353],{"class":189,"line":2856},[187,26354,26355],{},"fig, axes = plt.subplots(20, 30, figsize=(30, 15), sharex=False, sharey=False)\n",[187,26357,26358],{"class":189,"line":2867},[187,26359,316],{"emptyLinePlaceholder":315},[187,26361,26362],{"class":189,"line":2878},[187,26363,26364],{},"for img, ax in zip(images, axes.flat):\n",[187,26366,26367],{"class":189,"line":2886},[187,26368,26369],{},"    ax.imshow(mpimg.imread(img[1]))\n",[187,26371,26372],{"class":189,"line":2900},[187,26373,26374],{},"    ax.axis('off')\n",[187,26376,26377],{"class":189,"line":2905},[187,26378,316],{"emptyLinePlaceholder":315},[187,26380,26381],{"class":189,"line":2913},[187,26382,26383],{},"plt.savefig('yc.png')\n",[187,26385,26386],{"class":189,"line":2921},[187,26387,26147],{},[11,26389,26390],{},"There are a lot of logos that have a similar design to Stripe's logo. Rose/peach/pamplemousse colored logos also seem to be popular.",[911,26392,26394],{"id":26393},"founders","Founders",[11,26396,26397,26398,26403],{},"Let's take a look at the founders. I came across the ",[15,26399,26402],{"href":26400,"rel":26401},"https://pypi.org/project/deepface/",[19],"deepface"," PyPI project an was impressed at how accurately it can classify face data.",[11,26405,26406],{},"Here's a sample of YC Founder headshots:",[11,26408,26409],{},[511,26410],{"alt":7255,"src":26411},"/static/waas/yc_founders_sample.png",[11,26413,26414],{},"Here's how I used the deepface library to add race, gender and age data for each of the headshot images:",[26,26416,26418],{"className":10554,"code":26417,"language":10556,"meta":35,"style":35},"IMG_DIR = 'data/waas_full_details_dump_files/'\n# headshots are all .jpg files, so we can get all headshots like this:\nfounder_headshots = [x for x in os.listdir(IMG_DIR) if x.endswith('.jpg')]\nfounder_count = len(founder_headshots)\n\nimg_paths = [IMG_DIR + x for x in founder_headshots]\nresults = {}\n\nfor idx, img in enumerate(img_paths):\n    print(f\"analyzing {idx}/{founder_count}\")\n    try:\n        img_key = img.split(\"/\")[-1]\n        obj = DeepFace.analyze(\n            img_path=img,\n            actions=['age', 'gender', 'race', 'emotion'],\n            enforce_detection=False\n        )\n        obj.update({\"img\": img_key})\n        results[img_key] = obj\n\n    except ValueError as e:\n        print(e)\n\nwith open(\"founder_images.json\", \"w+\") as f:\n    f.write(json.dumps(results))\n",[33,26419,26420,26425,26430,26435,26440,26444,26449,26454,26458,26463,26468,26472,26477,26482,26487,26492,26497,26501,26506,26511,26515,26520,26524,26528,26533],{"__ignoreMap":35},[187,26421,26422],{"class":189,"line":190},[187,26423,26424],{},"IMG_DIR = 'data/waas_full_details_dump_files/'\n",[187,26426,26427],{"class":189,"line":249},[187,26428,26429],{},"# headshots are all .jpg files, so we can get all headshots like this:\n",[187,26431,26432],{"class":189,"line":312},[187,26433,26434],{},"founder_headshots = [x for x in os.listdir(IMG_DIR) if x.endswith('.jpg')]\n",[187,26436,26437],{"class":189,"line":319},[187,26438,26439],{},"founder_count = len(founder_headshots)\n",[187,26441,26442],{"class":189,"line":325},[187,26443,316],{"emptyLinePlaceholder":315},[187,26445,26446],{"class":189,"line":686},[187,26447,26448],{},"img_paths = [IMG_DIR + x for x in founder_headshots]\n",[187,26450,26451],{"class":189,"line":697},[187,26452,26453],{},"results = {}\n",[187,26455,26456],{"class":189,"line":1291},[187,26457,316],{"emptyLinePlaceholder":315},[187,26459,26460],{"class":189,"line":1306},[187,26461,26462],{},"for idx, img in enumerate(img_paths):\n",[187,26464,26465],{"class":189,"line":1434},[187,26466,26467],{},"    print(f\"analyzing {idx}/{founder_count}\")\n",[187,26469,26470],{"class":189,"line":2599},[187,26471,5405],{},[187,26473,26474],{"class":189,"line":2607},[187,26475,26476],{},"        img_key = img.split(\"/\")[-1]\n",[187,26478,26479],{"class":189,"line":2621},[187,26480,26481],{},"        obj = DeepFace.analyze(\n",[187,26483,26484],{"class":189,"line":2631},[187,26485,26486],{},"            img_path=img,\n",[187,26488,26489],{"class":189,"line":2642},[187,26490,26491],{},"            actions=['age', 'gender', 'race', 'emotion'],\n",[187,26493,26494],{"class":189,"line":2653},[187,26495,26496],{},"            enforce_detection=False\n",[187,26498,26499],{"class":189,"line":2665},[187,26500,4531],{},[187,26502,26503],{"class":189,"line":2674},[187,26504,26505],{},"        obj.update({\"img\": img_key})\n",[187,26507,26508],{"class":189,"line":2684},[187,26509,26510],{},"        results[img_key] = obj\n",[187,26512,26513],{"class":189,"line":2694},[187,26514,316],{"emptyLinePlaceholder":315},[187,26516,26517],{"class":189,"line":2706},[187,26518,26519],{},"    except ValueError as e:\n",[187,26521,26522],{"class":189,"line":2715},[187,26523,5444],{},[187,26525,26526],{"class":189,"line":2725},[187,26527,316],{"emptyLinePlaceholder":315},[187,26529,26530],{"class":189,"line":2735},[187,26531,26532],{},"with open(\"founder_images.json\", \"w+\") as f:\n",[187,26534,26535],{"class":189,"line":2743},[187,26536,26537],{},"    f.write(json.dumps(results))\n",[11,26539,26540],{},"There are more steps needed to transform this data to make it compatible for use with a histogram showing age, race and gender. Check out the Jupyter notebook linked at the end of this article to see the code used to make this data transformation. Here's a simple way to count founders grouped by race and gender:",[26,26542,26545],{"className":26543,"code":26544,"language":31},[29],"race_and_gender_count = defaultdict(lambda: 0)\nfor result in list(results):\n    obj = results[result]\n    gender = obj[\"gender\"]\n    race = obj[\"dominant_race\"]\n    # use (race, gender) tuple as defaultdict key and increment\n    race_and_gender_count[(race, gender)] += 1\n\nsorted(race_and_gender_count.items(), key=lambda x: x[1], reverse=True)\n",[33,26546,26544],{"__ignoreMap":35},[26,26548,26550],{"className":10554,"code":26549,"language":10556,"meta":35,"style":35},"[(('white', 'Man'), 771),\n (('asian', 'Man'), 217),\n (('latino hispanic', 'Man'), 134),\n (('indian', 'Man'), 117),\n (('middle eastern', 'Man'), 111),\n (('black', 'Man'), 84),\n (('white', 'Woman'), 52),\n (('asian', 'Woman'), 17),\n (('latino hispanic', 'Woman'), 13),\n (('indian', 'Woman'), 4),\n (('black', 'Woman'), 1)]\n",[33,26551,26552,26557,26562,26567,26572,26577,26582,26587,26592,26597,26602],{"__ignoreMap":35},[187,26553,26554],{"class":189,"line":190},[187,26555,26556],{},"[(('white', 'Man'), 771),\n",[187,26558,26559],{"class":189,"line":249},[187,26560,26561],{}," (('asian', 'Man'), 217),\n",[187,26563,26564],{"class":189,"line":312},[187,26565,26566],{}," (('latino hispanic', 'Man'), 134),\n",[187,26568,26569],{"class":189,"line":319},[187,26570,26571],{}," (('indian', 'Man'), 117),\n",[187,26573,26574],{"class":189,"line":325},[187,26575,26576],{}," (('middle eastern', 'Man'), 111),\n",[187,26578,26579],{"class":189,"line":686},[187,26580,26581],{}," (('black', 'Man'), 84),\n",[187,26583,26584],{"class":189,"line":697},[187,26585,26586],{}," (('white', 'Woman'), 52),\n",[187,26588,26589],{"class":189,"line":1291},[187,26590,26591],{}," (('asian', 'Woman'), 17),\n",[187,26593,26594],{"class":189,"line":1306},[187,26595,26596],{}," (('latino hispanic', 'Woman'), 13),\n",[187,26598,26599],{"class":189,"line":1434},[187,26600,26601],{}," (('indian', 'Woman'), 4),\n",[187,26603,26604],{"class":189,"line":2599},[187,26605,26606],{}," (('black', 'Woman'), 1)]\n",[10517,26608,26609],{},[26610,26611],"race-gender-age-bar-chart",{},[911,26613,26615],{"id":26614},"founder-background-wordcloud","Founder background wordcloud",[11,26617,26618],{},"Here's a wordcloudsshowing founder background, education and experience. The code for this is similar to the wordcloud shown previously for company descriptions.",[11,26620,26621],{},[511,26622],{"alt":7255,"src":26623},"/static/founders_wc.png",[168,26625,26627],{"id":26626},"generating-yc-startup-companies","Generating YC Startup Companies",[11,26629,26630],{},"Finally, I'll try to generate some plausible descriptions of YC companies based on the descriptions of companies scraped from WaaS. I have read about big advancements made in text generation with GPT-3, but otherwise I'm not familiar with text-generation or any other generative models.",[11,26632,26633,26634,26637,26638,26642],{},"My initial goal was to do this using a simple example that I could replicate locally using Tensorflow. Googling for ",[33,26635,26636],{},"text generation with python tensorflow"," led me to this tutorial: ",[15,26639,26640],{"href":26640,"rel":26641},"https://www.thepythoncode.com/article/text-generation-keras-python",[19],". I was able to run the example, but the results were not very good, at least not as good as the results used in the example of generating text from a model trained on the text of \"Alice in Wonderland\". This is probably because I ran 10 epochs instead of 30, but I didn't want to wait hours before getting results for each iteration.",[11,26644,26645,26646,26651,26652,1172,26654,752],{},"Another Google search led me to ",[15,26647,26650],{"href":26648,"rel":26649},"https://minimaxir.com/2019/09/howto-gpt2/",[19],"this article on Max Woolf's Blog"," which I was able to get started with in just a few minutes. I combined the text from the 2 sections of the longer company descriptions: ",[33,26653,6560],{},[33,26655,26656],{},"technology",[11,26658,26659],{},"Here's the link to the Google Colab (anyone can view and comment):",[11,26661,26662],{},[15,26663,26664],{"href":26664,"rel":26665},"https://colab.research.google.com/drive/1u9b-FVGgUGcfifLy7bUXgoPe-pZ81rm3?usp=sharing",[19],[11,26667,26668],{},"Google Colab gives you access to an environment with a GPU suitable for working with GPT2. Here's an overview of the code used to train GPT2 on the company descriptions:",[26,26670,26672],{"className":10554,"code":26671,"language":10556,"meta":35,"style":35},"%tensorflow_version 1.x\n!pip install -q gpt-2-simple\nimport gpt_2_simple as gpt2\nfrom datetime import datetime\nfrom google.colab import files\n",[33,26673,26674,26679,26684,26689,26694],{"__ignoreMap":35},[187,26675,26676],{"class":189,"line":190},[187,26677,26678],{},"%tensorflow_version 1.x\n",[187,26680,26681],{"class":189,"line":249},[187,26682,26683],{},"!pip install -q gpt-2-simple\n",[187,26685,26686],{"class":189,"line":312},[187,26687,26688],{},"import gpt_2_simple as gpt2\n",[187,26690,26691],{"class":189,"line":319},[187,26692,26693],{},"from datetime import datetime\n",[187,26695,26696],{"class":189,"line":325},[187,26697,26698],{},"from google.colab import files\n",[11,26700,26701,26702,26709],{},"This installs ",[15,26703,26706],{"href":26704,"rel":26705},"https://github.com/minimaxir/gpt-2-simple",[19],[33,26707,26708],{},"gpt-2-simple"," and gives us access to the Google Drive connected to the Google account used to sign in to Google Colab.",[11,26711,26712],{},"The GPU can be inspected with:",[26,26714,26717],{"className":26715,"code":26716,"language":31},[29],"!nvidia-smi\n",[33,26718,26716],{"__ignoreMap":35},[26,26720,26723],{"className":26721,"code":26722,"language":31},[29],"+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   51C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n|                               |                      |                 ERR! |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   51C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n|                               |                      |                 ERR! |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n",[33,26724,26722],{"__ignoreMap":35},[11,26726,26727],{},"Next we download the GPT-2 model:",[26,26729,26731],{"className":10554,"code":26730,"language":10556,"meta":35,"style":35},"gpt2.download_gpt2(model_name=\"124M\")\n",[33,26732,26733],{"__ignoreMap":35},[187,26734,26735],{"class":189,"line":190},[187,26736,26730],{},[11,26738,26739],{},"Then we mount Google Drive with the following command:",[26,26741,26743],{"className":10554,"code":26742,"language":10556,"meta":35,"style":35},"gpt2.mount_gdrive()\n",[33,26744,26745],{"__ignoreMap":35},[187,26746,26747],{"class":189,"line":190},[187,26748,26742],{},[11,26750,26751,26752,26755],{},"With our file (",[33,26753,26754],{},"waas.txt",") uploaded to Google Drive, the following",[26,26757,26759],{"className":10554,"code":26758,"language":10556,"meta":35,"style":35},"file_name = \"waas.txt\"\ngpt2.copy_file_from_gdrive(file_name)\n",[33,26760,26761,26766],{"__ignoreMap":35},[187,26762,26763],{"class":189,"line":190},[187,26764,26765],{},"file_name = \"waas.txt\"\n",[187,26767,26768],{"class":189,"line":249},[187,26769,26770],{},"gpt2.copy_file_from_gdrive(file_name)\n",[11,26772,26773],{},"Now the model needs to be fine-tuned:",[26,26775,26777],{"className":10554,"code":26776,"language":10556,"meta":35,"style":35},"sess = gpt2.start_tf_sess()\n\ngpt2.finetune(\n    sess,\n    dataset=file_name,\n    model_name='124M',\n    steps=1000,\n    restore_from='fresh',\n    run_name='run1',\n    print_every=10,\n    sample_every=200,\n    save_every=500\n)\n",[33,26778,26779,26784,26788,26793,26798,26803,26808,26813,26818,26823,26828,26833,26838],{"__ignoreMap":35},[187,26780,26781],{"class":189,"line":190},[187,26782,26783],{},"sess = gpt2.start_tf_sess()\n",[187,26785,26786],{"class":189,"line":249},[187,26787,316],{"emptyLinePlaceholder":315},[187,26789,26790],{"class":189,"line":312},[187,26791,26792],{},"gpt2.finetune(\n",[187,26794,26795],{"class":189,"line":319},[187,26796,26797],{},"    sess,\n",[187,26799,26800],{"class":189,"line":325},[187,26801,26802],{},"    dataset=file_name,\n",[187,26804,26805],{"class":189,"line":686},[187,26806,26807],{},"    model_name='124M',\n",[187,26809,26810],{"class":189,"line":697},[187,26811,26812],{},"    steps=1000,\n",[187,26814,26815],{"class":189,"line":1291},[187,26816,26817],{},"    restore_from='fresh',\n",[187,26819,26820],{"class":189,"line":1306},[187,26821,26822],{},"    run_name='run1',\n",[187,26824,26825],{"class":189,"line":1434},[187,26826,26827],{},"    print_every=10,\n",[187,26829,26830],{"class":189,"line":2599},[187,26831,26832],{},"    sample_every=200,\n",[187,26834,26835],{"class":189,"line":2607},[187,26836,26837],{},"    save_every=500\n",[187,26839,26840],{"class":189,"line":2621},[187,26841,621],{},[11,26843,26844],{},"Here are some results from my first attempt at using Google Colab. Training the model will output a sample after every 200 steps. I have included only the first sample from the training, but you can see the output from each step in the Colab notebook.",[26,26846,26849],{"className":26847,"code":26848,"language":31},[29],"WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nLoading checkpoint models/124M/model.ckpt\nINFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n  0%|          | 0/1 [00:00\u003C?, ?it/s]Loading dataset...\n100%|██████████| 1/1 [00:00\u003C00:00,  1.00it/s]\ndataset has 128333 tokens\nTraining...\n[10 | 28.79] loss=3.60 avg=3.60\n[20 | 50.75] loss=3.30 avg=3.45\n[30 | 73.39] loss=3.25 avg=3.38\n[40 | 96.80] loss=3.18 avg=3.33\n[50 | 121.40] loss=3.12 avg=3.29\n[60 | 145.52] loss=3.10 avg=3.26\n[70 | 169.15] loss=2.56 avg=3.16\n[80 | 193.06] loss=2.46 avg=3.07\n[90 | 217.28] loss=2.59 avg=3.01\n[100 | 241.38] loss=2.49 avg=2.96\n[110 | 265.33] loss=2.34 avg=2.90\n[120 | 289.30] loss=2.52 avg=2.86\n[130 | 313.46] loss=2.37 avg=2.82\n[140 | 337.69] loss=2.38 avg=2.79\n[150 | 361.84] loss=1.92 avg=2.73\n[160 | 385.94] loss=2.25 avg=2.70\n[170 | 410.01] loss=1.73 avg=2.63\n[180 | 434.10] loss=1.66 avg=2.58\n[190 | 458.18] loss=1.25 avg=2.50\n[200 | 482.22] loss=1.47 avg=2.44\n======== SAMPLE 1 ========\n\n to improve the usability of our Services and to provide better\n offline features through offline messaging. Our Platform\n connects users from across the country to collect data on\n non-US citizens abroad, and then gives US citizens a secure and\n authentic online presence while keeping their identities\n private. Our mission is to enable Americans to be more secure\n in online world.  We do this by putting a damper on online\n threats while preventing online attacks and slowing down the\n spread of cyberthreats. Backed by a world class Series A and B\n investments from leading technology firms, including Shas\n Ventures and Y Combinator, we\\'re an organization that believes\n in doing what is right. We\\'re built to last and provide an\n experience that\\'s built to last.\\xa0We\\'ve partnered with some\n of the world\\'s top law enforcement institutions including the\n Washington DC Metropolitan Detention Center, National Domestic\n Relations Task Force, Lambda Legal, Lambda Legal National Labs,\n NIMBYs Angels of San Francisco and Rally Labs provide legal\n technology & community education, as well as legal assistance/\n cure treatments. We\\'re a tech firm that\\'s built to last ‘lots\n of years 🙏. We\\'ve solved some really really hard problems in\n the past, and haven\\'t forgotten.We\\'re a team of passionate\n engineers, machine learning researchers, and business\n executives from Y Combinator and Naval Institute. We\\'ve just\n closed our Series B and C funding, and\\'s back at it with two\n more large-scale brothels and a bona fide law firm dedicated to\n bringing brothels to the people. Join us on this next chapter\n in our journey, and help us build the next Clark County jail.\n We\\'re looking for guys who can quickly learn leadership roles\n and fill in for leaders, not executives. We like to take what\n we learn and apply it to real problems. ReactJS, Node, Django,\n Postgres, and a sprinkle of AWS. ReactInfrastructure serves\n Express-like functions in the cloud. Our customers are law\n firms and other third-party service providers, largely\n comprised of tech startups. Our customers are generating\n billions of dollars in revenue through our platform every\n year.  Come join us and help us transform this dynamic into a\n platform-as-a-works. Rippling uses the following\n technologies:React Native JavaScriptFast, clean,\n nativeCSS3Deterministic executionReact NativeScriptWeb appLync,\n typescriptArchitecture-as-a-systemResponsible for maintaining\n and expanding the Ethereum and Bitcoin infrastructure. We\\'re\n based in New York City, and are backed by the VC-leading\n venture firms in the space. Our flagship product, Rippling, is\n an intuitive phone-in-a-person phone banking solution that\n integrates an RFID reader and password manager, as well as\n password managers dedicated to managing assets and managing\n cards. Users can create portfolios and trade on the Rippling\n marketplace, or access the technology to create their own\n portfolios on the Bitcoin and Ethereum markets. Simply put,\n Rippling is 21+ year old inventors. We\\'ve reinvented banking\n by making money available today through an open API. Banks now\n have an easy path to secure assets, including a digital wallet\n and code backed by an AllinAir’s digital asset portfolio. We\n are a small team of computer scientists building tools that\n improve customer experience, reduce wait times for businesses,\n and increase transparency in banking. Our platform runs on very\n little electricity, minimal mechanical power, and is 100x more\n robust 20 than traditional DPG/PGD systems. The system stores 1\n watt of energy and 8 watts of power per watt signal in a\n standard APS-C (Advanced Plasmon Super Capacitor Stacker).\n Cells in our cell-resistant cell-glide-based sprays secrete a\n protein that is targeted to trigger cell death, called a T\n cell. This triggered death is what causes breast cancer.\n Stacking together our diverse array of biological and chemical\n biology and chemical pathologies, and building in new energy\n efficiency technology to power our smart consumer products,\n we\\'re taking our users on an exciting journey toward modern\n energy efficiency. We are a team of computer scientists located\n in Barcelona, Spain, and we are focusing on the extremely first\n and foremost research and development applications in renewable\n energy. Current RCP is the largest R&D and R&E net for the 21st\n century, automating substantial paperwork and providing\n essential modern services. With the exception of emergency\n services and work, we are unaffected by the most common types\n of CO2 emissions including man-made nitrous oxide, human-made\n CO2, and volatile organic compounds. Current RCP delivers the\n highest quality, highest efficiency possible, including\n equipment, software, waste management, and notification\n systems, all in a modern, integrated and convenient way. By\n using RCP, consumers, businesses, civil society, researchers\n and others around the world can save money on power bills,\n reduce emissions and waste, receive timely assistance from the\n energy sector and be\n\n[210 | 517.61] loss=1.62 avg=2.40\n\n[...]\n\n[1000 | 2448.17] loss=0.07 avg=0.47\nSaving checkpoint/run1/model-1000\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix.\n",[33,26850,26848],{"__ignoreMap":35},[11,26852,26853],{},"Here's a sample generated using the trained model:",[107,26855,26856],{},[11,26857,26858],{},"We are a small team of MIT-trained researchers and entrepreneurs based in Palo Alto, California. We're creating a new form of transportation for people and gear. Based in the heart of downtown LA, our downtown L.A. location will be the last remaining obstacle preventing people from making, selling, and visiting the Smithsonian. We're building a disruptive technology that impacts the way people interact in the world in a positive way. Token Transit is digitizing much of the way we travel for real. We are building it to work for everyone, digitizing billions of things we buy and sell every day. Our platform connects real users and non-users in a global network. Our first product, UNITY, made it from scratch and is in mixed state. We are helping to develop the first truly international distributed logistics network powered by an AI and a IoT. Our mission is to revolutionize how companies move containers, trucks, and other loads across borders. Our technology is being architected and being implemented by experts across the globe. Our annual revenue:$150M/year. How we work We are a small, fast growing company with a core team based in San Francisco, CA.  Our software is being used by thousands of customers worldwide. We are building our technology platform in two key areas: (1) automated supply chain forecasting for companies and (2) internal analysis of company data to help leaders analyze and improve production. Our YearEnd blog is currently looking for Senior Electricians & Candidates. If you're excited about this exciting challenge, please apply. Burrow is a daily driver app for mobile that works on any device. With Burrow, your driver is on your phone to show your friends where you are going, which destinations you are on, and more. You can also add your phone to a list and get in-depth insights into the driving itself. We love to tie in-stream data, visualised by React Native, to the driving experience. We also take a data-driven approach to managing our customer list, managing our teams and our operations framework. Our API serves a large amount of native API functions, and using Cockroach to access these functions through microservices allowed us to interface with the relevant services (e.g. DynamoDB, Picnic and our own API). We also inserted a lot of debugging and monitoring functionality into the ecosystem. Our API serves a large amount of data - including relational and non-relational data - about where you are and what you've wanted to do the longest, what you've got, and where you are now. We also manage a lot of the operations of the platform and provide some of the data integrations with your existing relational and non-relational data. Technologies we use NodeJS, React, Apollo, Postgres, Heroku, Docker, and Tensorflow: Fluttertable, Datapoint, Pandas, Keras, Numpy, Scipy, Scikit, SciPy, scikit-learn AI-ron at MERN Automation is a Google-backed NARability company that is committed to being the leading platform for accurately measuring and diagnosing agricultural production. We have built proprietary software and software automation systems to make it easy for any government to track agricultural production and prices. Our primary technologies today are:Tables, Entry Points into Global Food Production, Pipeline and Grain Market, and GenomicData.utility.Upstream is revolutionizing how U.S. agriculture is produced and sold. We are serving the agronomists, farmers, market researchers, and data scientists in the USA. WWrendy is on a mission to enable breakthrough scientific research in agricultural technology. We are growing cassava in our bioreactors, improving crop health and producing more durables than we need to feed ourselves. We use machine learning to identify cancer-causing microbes in seeds and in consumer durables to make food more accessible and more sustainable. We are building an entirely new technical infrastructure for agricultural diagnostics: tractors, biores, and tractors-all with a singular goal of improving yields and feed the world. We have humble beginnings as a grocery delivery service and have since grown to serve millions of people every year. We are a tight-knit team of MIT-trained researchers, technologists, and businesspeople, who have built an industry-leading technology platform that uses sensors and software to analyze crop production data to create intelligent and efficient food products. Our mission is to help all farmers have a more sustainable crop, by using microbial discoveries to improve the health and prosperity of their communities. We're a well-funded, well-funded, yuletide startup. We're looking to grow and affordably upgrade our tech stack every year. We use less expensive tools and techniques to analyze and build comprehensive datasets on crop production and food safety to help farmers grow more food and avoid over-production. Rails / React / AWS We build, manage, and cybersecurity insurance through an on-premise platform. We",[11,26860,26861],{},"From this first attempt there are already a few ideas for YC startups:",[107,26863,26864],{},[11,26865,26866],{},"We are helping to develop the first truly international distributed logistics network powered by an AI and a IoT. Our mission is to revolutionize how companies move containers, trucks, and other loads across borders. Our technology is being architected and being implemented by experts across the globe.",[107,26868,26869],{},[11,26870,26871],{},"We have built proprietary software and software automation systems to make it easy for any government to track agricultural production and prices.",[11,26873,26874,26875,26877],{},"Next we can try improving the output by using some additional features of ",[33,26876,26708],{},". One important setting is the size of the model. There are three released sizes of GPT-2:",[916,26879,26880,26886,26892,26898],{},[919,26881,26882,26885],{},[33,26883,26884],{},"124M"," (default): the \"small\" model, 500MB on disk. This is the one used on my first try",[919,26887,26888,26891],{},[33,26889,26890],{},"355M",": the \"medium\" model, 1.5GB on disk.",[919,26893,26894,26897],{},[33,26895,26896],{},"774M",": the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)",[919,26899,26900,26903,26904,26906],{},[33,26901,26902],{},"1558M",": the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like ",[33,26905,26896],{},", it cannot be finetuned).",[11,26908,26909,26910,1172,26913,26916],{},"We can try again using the 355M model. The ",[33,26911,26912],{},"large",[33,26914,26915],{},"extra large"," models won't work for our use case of finetuning the model on our sample text.",[11,26918,26919],{},"This would be a great time to plug my startup, but I don't have one. Instead, here are 10,000 startup ideas I generated with GPT-2 trained on the 335M model using the YC company descriptions for finetuning:",[26,26921,26923],{"className":10554,"code":26922,"language":10556,"meta":35,"style":35},"gpt2.generate_to_file(sess,\n                      destination_path=gen_file,\n                      length=150,\n                      prefix=\"we are building the world's first\",\n                      temperature=0.7,\n                      nsamples=10000,\n                      batch_size=20,\n                      )\n",[33,26924,26925,26930,26935,26940,26945,26950,26955,26960],{"__ignoreMap":35},[187,26926,26927],{"class":189,"line":190},[187,26928,26929],{},"gpt2.generate_to_file(sess,\n",[187,26931,26932],{"class":189,"line":249},[187,26933,26934],{},"                      destination_path=gen_file,\n",[187,26936,26937],{"class":189,"line":312},[187,26938,26939],{},"                      length=150,\n",[187,26941,26942],{"class":189,"line":319},[187,26943,26944],{},"                      prefix=\"we are building the world's first\",\n",[187,26946,26947],{"class":189,"line":325},[187,26948,26949],{},"                      temperature=0.7,\n",[187,26951,26952],{"class":189,"line":686},[187,26953,26954],{},"                      nsamples=10000,\n",[187,26956,26957],{"class":189,"line":697},[187,26958,26959],{},"                      batch_size=20,\n",[187,26961,26962],{"class":189,"line":1291},[187,26963,26964],{},"                      )\n",[10517,26966,26967],{},[26968,26969],"generated-ideas",{},[911,26971,26973],{"id":26972},"credits-links-and-learning-resources","Credits, Links and Learning Resources",[11,26975,26976,26977,26979],{},"Here's a link to Max Woolf's blog which has a lot of helpful resources on using GPT-2. Max is the author of ",[33,26978,26708],{}," which is the Python package used in the Google Colab (Max is the author of that Google Colab as well):",[11,26981,26982],{},[15,26983,26984],{"href":26984,"rel":26985},"https://minimaxir.com/",[19],[11,26987,26988],{},"Here are some resources for learning more about text generation. I came across Jay Alammar's blog which has a lot of great visualizations:",[11,26990,26991],{},[15,26992,26993],{"href":26993,"rel":26994},"https://jalammar.github.io/",[19],[11,26996,26997],{},"Jay also has a great YouTube channel:",[10159,26999],{"width":10161,"height":27000,"src":27001,"frameBorder":10165,"allow":27002,"allowFullScreen":315},400,"https://www.youtube.com/embed/MQnJZuBGmSQ","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",[11,27004,27005,27006],{},"Here's the link to the source markdown file for this article: ",[15,27007,27008],{"href":27008,"rel":27009},"https://github.com/briancaffey/briancaffey.github.io/tree/master/content/2021/01/16",[19],[11,27011,27012,27013],{},"Here's a link to the repo containing all of the scraped data and Jupyter notebooks used for exploring the data: ",[15,27014,27015],{"href":27015,"rel":27016},"https://gitlab.com/briancaffey/yc-waas-data",[19],[11,27018,27019,27020],{},"Here's the link to the Google Colab used for generating company descriptions with GPT-2 and gpt-2-simple: ",[15,27021,27022],{"href":27022,"rel":27023},"https://colab.research.google.com/drive/1u9b-FVGgUGcfifLy7bUXgoPe-pZ81rm3?usp=sharing#scrollTo=8DKMc0fiej4N",[19],[11,27025,27026],{},"Thank you for reading!",[855,27028,27029],{},"html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .s7hpK, html code.shiki .s7hpK{--shiki-default:#B31D28;--shiki-default-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":27031},[27032,27033,27034,27042],{"id":24532,"depth":249,"text":24533},{"id":24771,"depth":249,"text":24772},{"id":25097,"depth":249,"text":25098,"children":27035},[27036,27037,27038,27039,27040,27041],{"id":25104,"depth":312,"text":25105},{"id":25989,"depth":312,"text":25990},{"id":26158,"depth":312,"text":26159},{"id":26170,"depth":312,"text":26171},{"id":26393,"depth":312,"text":26394},{"id":26614,"depth":312,"text":26615},{"id":26626,"depth":249,"text":26627,"children":27043},[27044],{"id":26972,"depth":312,"text":26973},"2021-01-16",{},"/2021/01/16/i-scraped-analyzed-and-generated-yc-companies-founders-and-work-at-a-startup-job-postings",{"title":24512,"description":24512},"2021/01/16/i-scraped-analyzed-and-generated-yc-companies-founders-and-work-at-a-startup-job-postings",[1125,582,27051,27052,27053],"scraping","startups","yc","lT2ne7A66Dtl5os6CYizfZqkxBj3G6GEZJZFp7gJlAw",{"id":27056,"title":27057,"body":27058,"comments":872,"date":27080,"description":27081,"draft":872,"extension":873,"external":874,"image":27082,"meta":27083,"navigation":315,"path":27084,"seo":27085,"stem":27086,"tags":27087,"__hash__":27088},"blog/2021/01/15/adding-internationalization-to-a-statically-generated-nuxt-site.md","Adding internationalization to a statically generated Nuxt site",{"type":8,"value":27059,"toc":27078},[27060,27063,27066,27072,27075],[11,27061,27062],{},"This article will give an overview of my experience adding internationalization to my statically-generated Nuxt website.",[11,27064,27065],{},"Since I migrated my site from Jekyll to Nuxt, I have had issues with my blog post URLs. My Jekyll site automatically generated URLs that included the date:",[26,27067,27070],{"className":27068,"code":27069,"language":31},[29],"https://my-site.com/2021/01/01/my-first-blog-post-of-2021\n",[33,27071,27069],{"__ignoreMap":35},[11,27073,27074],{},"To replicate this URL pattern with Nuxt, I have had to use some advanced features of the content API and Nuxt configuration options, as well as an awkward folder structure.",[11,27076,27077],{},"Because of this, I will be implementing internationalization in to phases: first I'll work on translating the core pages, next I'll work on setting up translations for individual blog posts. I don't plan on translating most of my blog posts, and in some cases I might only be providing translations for some of the locales that my site supports, falling back to the English version where a given locale is not available.",{"title":35,"searchDepth":249,"depth":249,"links":27079},[],"2021-01-15","This article will summarize my experience implementing internationalization (i18n) for a statically-generated Nuxt.js site","/static/emoji_flags.png",{},"/2021/01/15/adding-internationalization-to-a-statically-generated-nuxt-site",{"title":27057,"description":27081},"2021/01/15/adding-internationalization-to-a-statically-generated-nuxt-site",[493,882,881],"L7tDGvkvzZOtHqEPv6j8UJ09lysVEeVJf1izeHiMSmY",{"id":27090,"title":27091,"body":27092,"comments":872,"date":27551,"description":27552,"draft":872,"extension":873,"external":874,"image":27553,"meta":27554,"navigation":315,"path":27555,"seo":27556,"stem":27557,"tags":27558,"__hash__":27560},"blog/2021/01/02/using-the-stripe-api-for-recurring-monthly-saas-subscription-payments-in-django-and-vue-application.md","Using Stripe for recurring monthly SaaS subscriptions in a Django + Vue application",{"type":8,"value":27093,"toc":27541},[27094,27098,27101,27104,27108,27116,27123,27127,27139,27144,27148,27151,27155,27275,27279,27282,27446,27450,27453,27484,27488],[2215,27095,27097],{"id":27096},"using-stripe-for-recurring-monthly-payments-to-a-paid-saas-subscription-in-a-django-vuejs-application","Using Stripe for recurring monthly payments to a paid SaaS subscription in a Django + Vue.js application",[11,27099,27100],{},"This diagram shows the flow of data for the lifecycle of a paid customer subscription in a Django application with a Vue.js client. There are four stages:",[11,27102,27103],{},"I. Account setup, configuration, model and object creation\nII. Logic and data flow for starting a customer's premium monthly subscription\nIII. Automatic subscription renewal\nIV. Cancelling a premium subscription",[168,27105,27107],{"id":27106},"context","Context",[11,27109,27110,27111,27115],{},"This is my first attempt at using Stripe, or any other online payment service API. Most of what I have diagramed here comes from this article from the Stripe documentation: ",[15,27112,27113],{"href":27113,"rel":27114},"https://stripe.com/docs/billing/subscriptions/fixed-price",[19],". Knowing almost nothing about what is needed to create a SaaS subscription, I found this article very helpful. It was a lot to read at once, but each call to to the Stripe API is very clear and straightforward.",[11,27117,27118,27119,1737],{},"I made some modifications and additions to this walk-through for my use case, which is an API service called Open SEC Data, an open source project that I'm working on (",[15,27120,27121],{"href":27121,"rel":27122},"https://gitlab.com/briancaffey/sec-filings-app",[19],[168,27124,27126],{"id":27125},"diagram","Diagram",[11,27128,27129,27130,27134,27135,752],{},"Here's a read-only link to the diagram: ",[15,27131,27132],{"href":27132,"rel":27133},"https://drive.google.com/file/d/1oH2b0W-c-dI5oXzc_jvCGvXx9sJagr4a/view?usp=sharing",[19],". This diagram is made with ",[15,27136,27137],{"href":27137,"rel":27138},"https://www.diagrams.net/",[19],[11,27140,27141],{},[511,27142],{"alt":7255,"src":27143},"/static/django_vue_stripe_diagram.png",[168,27145,27147],{"id":27146},"legend","Legend",[11,27149,27150],{},"Here's a detailed description of each part of the diagram, starting with the first section.",[911,27152,27154],{"id":27153},"i-account-setup-configuration-model-and-object-creation","I. Account setup, configuration, model and object creation",[2276,27156,27157,27164,27170,27183,27190,27200,27203,27218,27231,27244,27255,27261],{},[919,27158,27159,27160,27163],{},"Setup a Stripe account. For local development, make sure you turn on ",[33,27161,27162],{},"View test data",". On your local machine, install the stripe CLI and authenticate with your Stripe account",[919,27165,27166,27167,343],{},"Create a Product in Stripe (mine is called ",[33,27168,27169],{},"Open SEC Data Premium Subscription",[919,27171,27172,27173,27175,27176,27179,27180,752],{},"Create a Price in Stripe that references the Product.",[7880,27174],{},"Instead of creating these objects in the Stripe Dashboard, you can also create them with the Stripe CLI or the Python SDK. I created a Django management command called ",[33,27177,27178],{},"create_stripe_data"," that will create a Product and related Price in Stripe. We will need the id of the Price, it looks like this: ",[33,27181,27182],{},"price_1Hx0goL67dRDwyuDh9yEWsBo",[919,27184,27185,27186,27189],{},"Add the Price ID as an environment variable ",[33,27187,27188],{},"SUBSCRIPTION_PRICE_ID"," to the backend. This will be used later when we make API calls to Stripe from inside of Django views.",[919,27191,27192,27193,27195,27196,27199],{},"For production environments, you will need to a register a Stripe webhook. This is an endpoint in Django that Stripe will POST to in order to inform the Django application of events that have happened in Stripe.",[7880,27194],{},"For local development we need to run ",[33,27197,27198],{},"stripe listen --forward-to localhost/api/stripe-webhooks/"," in order to forward webhook events to the local Django application. This works really well for local development.",[919,27201,27202],{},"In both local and production environments we need to add an environment variables to the Django application that will be used to validate the webhook event.",[919,27204,27205,27206,27209,27210,27213,27214,27217],{},"You will need to create a ",[33,27207,27208],{},"Subscription"," model or similar in your Django models. This model should be related to your user model in some way. At a minimum it should have the ",[33,27211,27212],{},"subscription_id"," (the ID of the Stripe Subscription) and ",[33,27215,27216],{},"current_period_end"," (also from the Stripe Subscription object). We will use this model in the next sections.",[919,27219,27220,27223,27224,27227,27228,1737],{},[33,27221,27222],{},"/api/stripe-webhooks/"," is the endpoint in the Django application that Stripe will send POST requests to in order to inform the Django application of events that happen in Stripe. The URL can be called anything you want, as long as you register it with that URL. In local development, you need to specify this URL in the ",[33,27225,27226],{},"stipe listen"," command (for example, ",[33,27229,27230],{},"stripe listen forward-to localhost/api/stripe-webhooks/",[919,27232,27233,27236,27237,27240,27241,752],{},[33,27234,27235],{},"STRIPE_SECRET_KEY"," is the name of the secret API key that should only be accessible by the backend. In local development, this key looks like ",[33,27238,27239],{},"sk_test_Abc123",". In production, this key will look like ",[33,27242,27243],{},"sk_Abc123",[919,27245,27246,27249,27250,784,27252,1737],{},[33,27247,27248],{},"stripe"," is the name of the PyPI package that we need to add to ",[33,27251,23123],{},[33,27253,27254],{},"requirements/base.txt",[919,27256,27257,27260],{},[33,27258,27259],{},"STRIPE_PUBLISHABLE_KEY"," is the value of the Stripe API key that can be made public and is used in the Vue application to instantiate Stripe.",[919,27262,27263,27264,27267,27268],{},"The Stripe library is included in ",[33,27265,27266],{},"index.html"," via CDN so that it is accessible anywhere in the Vue application. Stripe object is instantiated in the Vue application with:",[107,27269,27270],{},[11,27271,27272],{},[33,27273,27274],{},"let stripe = Stripe(process.env.STRIPE_PUBLISHABLE_KEY)",[911,27276,27278],{"id":27277},"ii-logic-and-data-flow-for-starting-a-customers-premium-monthly-subscription","II. Logic and data flow for starting a customer's premium monthly subscription",[11,27280,27281],{},"With everything setup and configured properly in Stripe, the backend Django application and the frontend Vue application, customers can now start paying for monthly subscriptions. In my application, a user can sign up for an account first without having a premium subscription. In other scenarios, having an active account may require a premium subscription.",[2276,27283,27284,27304,27327,27333,27345,27354,27361,27364,27367,27370,27390,27399,27404,27409,27419,27422,27425,27428,27443],{"start":2621},[919,27285,27286,27287,27290,27291,27294,27295,27300,27301,752],{},"When a logged-in user visits their ",[33,27288,27289],{},"/account"," page, they will see the status of their account: Basic (free) or Premium (paid subscription). Users on a Basic plan will see the option to upgrade to Premium. They will be redirected to a ",[33,27292,27293],{},"/premium"," page where they will be presented with a credit card form. This credit card form is generated by ",[15,27296,27299],{"href":27297,"rel":27298},"https://stripe.com/payments/elements",[19],"Stripe Elements",". The user fills out their credit card, expiration date, card security code and billing ZIP code and then clicks ",[33,27302,27303],{},"Purchase",[919,27305,27306,27307,27309,27310,27313,27314,12282,27317,27320,27321,27323,27324,752],{},"Clicking on ",[33,27308,27303],{}," calls a method ",[33,27311,27312],{},"purchase"," that calls ",[33,27315,27316],{},"stripe.CreatePaymentMethod",[33,27318,27319],{},"paymentMethodId"," token returned from ",[33,27322,27316],{}," is then passed to the method called ",[33,27325,27326],{},"createSubscription",[919,27328,27329,27330,27332],{},"Stripe creates this object and returns a response that contains a ",[33,27331,27319],{}," token.",[919,27334,27335,27337,27338,27341,27342,27344],{},[33,27336,27326],{}," sends a POST request to ",[33,27339,27340],{},"/api/stripe/create-subscription/"," in the Django application with the ",[33,27343,27319],{}," that we generated in the previous step.",[919,27346,27347,27349,27350,27353],{},[33,27348,27340],{}," calls a view called ",[33,27351,27352],{},"create_subscription"," which makes a number of API calls to Stripe and then finally saves some data in the application's Postgres database.",[919,27355,27356,27357,27360],{},"The first API call creates the Customer object in Stripe if it does not exist. ",[33,27358,27359],{},"email=request.user.email"," is used in the API call to associate the Stripe customer with the user's email.",[919,27362,27363],{},"Next the payment method is attached to Stripe Customer model.",[919,27365,27366],{},"Next the Stripe payment method is set as the default payment method for Stripe customer for future billing.",[919,27368,27369],{},"The Stripe subscription model is created with the customer ID that was created in the earlier and the price ID corresponding to the premium subscription (added in the setup stage).",[919,27371,27372,27373,637,27376,1172,27379,27382,27383,27385,27386,27389],{},"Once these Stripe API calls have finished, a new Subscription is saved in the Postgres database. ",[33,27374,27375],{},"stripe_subscription_id",[33,27377,27378],{},"stripe_customer_id",[33,27380,27381],{},"valid_through"," (DateTimeField that keeps track of the date through which the user's subscription has been paid for) are saved to the ",[33,27384,27208],{}," model and then the subscription model is saved to the user model's ",[33,27387,27388],{},"subscription"," field.",[919,27391,27392,27393,27395,27396],{},"When the ",[33,27394,27326],{}," method's POST requests returns successfully, the user's account is fetched again from ",[33,27397,27398],{},"/api/account/",[919,27400,27401,27402,752],{},"The browser makes a request to ",[33,27403,27398],{},[919,27405,27406,27408],{},[33,27407,27398],{}," returns information on the user and their subscription.",[919,27410,27411,27412,27414,27415,27418],{},"Data from ",[33,27413,27398],{}," is updated in Vuex ",[33,27416,27417],{},"user"," store.",[919,27420,27421],{},"The user is now able to make requests to resources for premium features.",[919,27423,27424],{},"In this application, one such example is the ability to request an API key for making for making API calls.",[919,27426,27427],{},"A user makes a request to an API endpoint for a premium feature.",[919,27429,27430,27431,765,27434,27437,27438,27440,27441,752],{},"When determining permissions for resources that should only be accessible to customers with valid subscriptions, we need to compare ",[33,27432,27433],{},"request.user.subscription.valid_through",[33,27435,27436],{},"timezone.now()"," and make sure that ",[33,27439,27381],{}," is greater than ",[33,27442,27436],{},[919,27444,27445],{},"Requests for protected resources are successfully returned to the browser.",[911,27447,27449],{"id":27448},"iii-automatic-subscription-renewal","III. Automatic subscription renewal",[11,27451,27452],{},"The customer's credit card is charged once each month that they are subscribed to the service. This action happens in Stripe. This section assumes that the customer's primary payment method is still valid (it has not been canceled expired or not able to be charged for some other reason).",[2276,27454,27455,27458,27469],{"start":2808},[919,27456,27457],{},"The customer's card is charged in Stripe and an event is sent to the Django backend via a webhook that we registered in the setup stage.",[919,27459,27460,27461,27464,27465,27468],{},"The webhook view checks ",[33,27462,27463],{},"event.type"," and if the event is of type ",[33,27466,27467],{},"invoice.paid"," we extend the user's subscription by one month.",[919,27470,27471,27472,27475,27476,27478,27479,27481,27482,752],{},"To extend the user's subscription, we modify the ",[33,27473,27474],{},"DateTimeField"," field on the ",[33,27477,27208],{}," that tracks the ",[33,27480,27216],{}," which is included in the webhook data object. The model field in my code is called ",[33,27483,27381],{},[911,27485,27487],{"id":27486},"iv-cancelling-a-premium-subscription","IV. Cancelling a premium subscription",[2276,27489,27490,27496,27522,27529,27535],{"start":2834},[919,27491,27492,27493,21752],{},"When a user decides to cancel their payed subscription service, they click on the ",[33,27494,27495],{},"Cancel My Subscription",[919,27497,27498,27499,27502,27503,27506,27507,27510,27511,27514,27515,27518,27519,27521],{},"This makes a POST request to ",[33,27500,27501],{},"/api/stripe/cancel-subscription"," which calls the ",[33,27504,27505],{},"cancel_subscription"," view. This view calls ",[33,27508,27509],{},"stripe.Subscription.delete(subscriptionId)",", where the ",[33,27512,27513],{},"subscriptionId"," is retrieved from ",[33,27516,27517],{},"request.user.subscription"," (the ",[33,27520,27208],{}," model created in the setup section).",[919,27523,27524,27525,27528],{},"The subscription is deleted in Stripe through the ",[33,27526,27527],{},"stripe.Subscription.delete"," API call.",[919,27530,27531,27532,752],{},"The user's subscription is deleted from the user model with ",[33,27533,27534],{},"request.user.subscription.delete()",[919,27536,27537,27538,27540],{},"The frontend responds to the deleted subscription by fetching ",[33,27539,27398],{}," again, refresh, or redirecting and the user no longer has access to their premium subscription.",{"title":35,"searchDepth":249,"depth":249,"links":27542},[27543,27544,27545],{"id":27106,"depth":249,"text":27107},{"id":27125,"depth":249,"text":27126},{"id":27146,"depth":249,"text":27147,"children":27546},[27547,27548,27549,27550],{"id":27153,"depth":312,"text":27154},{"id":27277,"depth":312,"text":27278},{"id":27448,"depth":312,"text":27449},{"id":27486,"depth":312,"text":27487},"2021-01-02","This article shares my experience learning and implementing the Stripe API for recurring monthly SaaS subscription payments in an application using Django and Vue.js","/static/django_vue_stripe.png",{},"/2021/01/02/using-the-stripe-api-for-recurring-monthly-saas-subscription-payments-in-django-and-vue-application",{"title":27091,"description":27552},"2021/01/02/using-the-stripe-api-for-recurring-monthly-saas-subscription-payments-in-django-and-vue-application",[15290,882,27559,27248],"drf","p5NxbOdUNY3TgcpBF747knZCKNoT9TntABqHUWs-eZ8",{"id":27562,"title":27563,"body":27564,"comments":872,"date":28165,"description":28166,"draft":872,"extension":873,"external":874,"image":28167,"meta":28168,"navigation":315,"path":28169,"seo":28170,"stem":28171,"tags":28172,"__hash__":28173},"blog/2021/01/01/session-authentication-with-django-django-rest-framework-and-nuxt.md","Session Authentication with Django, Django REST Framework and Nuxt",{"type":8,"value":27565,"toc":28140},[27566,27569,27572,27578,27581,27584,27587,27590,27592,27595,27598,27602,27605,27609,27612,27626,27629,27633,27636,27660,27664,27667,27670,27674,27677,27680,27683,27685,27691,27694,27702,27705,27722,27725,27739,27753,27839,27845,27877,27906,27932,27938,27969,27979,27990,27996,28077,28081,28085,28088,28092,28099,28102,28106,28109,28112,28116,28119,28123,28126,28130,28133,28137],[11,27567,27568],{},"This will be a continuation of the discussion about how data flows in Django + Nuxt applications, looking specifically at session authentication.",[11,27570,27571],{},"Here's a GitLab repo that where you can find the source code and other diagrams related to this project:",[11,27573,27574],{},[15,27575,27576],{"href":27576,"rel":27577},"https://gitlab.com/briancaffey/django-nuxt-starter",[19],[11,27579,27580],{},"This diagram focuses on the interactions between:",[11,27582,27583],{},"I. The browser",[11,27585,27586],{},"II. The Nuxt server (Node process)",[11,27588,27589],{},"III. The Django backend API server (gunicorn process)",[168,27591,27107],{"id":27106},[11,27593,27594],{},"For illustration purposes, I'm using a simple CRUD application that has two models: Users and (blog) Posts. Users can log in with email and password credentials and create, read, update and delete blog posts (CRUD). Currently I'm only doing the R (read) of CRUD: listing and viewing blog posts. Creating, updating and delete will be added later. For now, users must be logged in to see posts.",[11,27596,27597],{},"I'm still learning a lot about Nuxt and how it can be used with Django and Django REST Framework. This project is an effort at documenting my learning process, learning in public and learning from mistakes, so any feedback or guidance on what I have written here would be highly appreciated!",[168,27599,27601],{"id":27600},"why-nuxt","Why Nuxt?",[11,27603,27604],{},"Using Nuxt (with Server Side Rendering, or SSR) is one of many ways to use Vue.js with Django. Vue is a progressive framework, which means that it can be gradually adopted into a project--you don't have to go all-in on the framework or rewrite the application from scratch to fit with how Vue works.",[911,27606,27608],{"id":27607},"different-ways-to-use-vue-with-django","Different ways to use Vue with Django",[11,27610,27611],{},"In terms of Django, here are some ways that you can use Vue:",[916,27613,27614,27617,27620,27623],{},[919,27615,27616],{},"Vue as a jQuery replacement for adding basic interactivity in views served by Django templates",[919,27618,27619],{},"Build a static Vue application and serve it as a set of static assets in a Django project alongside routes that are served by other normal Django templates views.",[919,27621,27622],{},"Build a Vue SPA which consumes a Django API (usually built with Django REST Framework or similar), and serve it over a content delivery network (CDN).",[919,27624,27625],{},"Use Vue to build an Electron desktop app that uses Django as an API",[11,27627,27628],{},"In these scenarios, Vue is served as either static assets (such as in the case of serving a SPA over a CDN), or Vue code is included in an HTML response from a server (where the view library, not your application, is served over a CDN), similar to how jQuery is used.",[911,27630,27632],{"id":27631},"different-ways-to-use-nuxt","Different ways to use Nuxt",[11,27634,27635],{},"Nuxt is a Framework that can be used in a few different ways, I'll briefly discus three ways in which Nuxt can be used. Common to all three of these ways of using Nuxt is the directory structure. No matter how you use Nuxt, it provides a great way to organize Vue code.",[2276,27637,27638,27644,27647],{},[919,27639,27640,27641,1737],{},"Static mode: this mode allows you to write Vue code which is built into a static HTML, and then that HTML is deployed to a CDN or webserver like NGINX. The developer (or CI/CD process) runs a command to generate HTML files for each page in the application, and these pages are served as-is when accessed by a user. I recently migrated my personal blog from Jekyll to Nuxt with full-static mode. Check it out at (",[15,27642,743],{"href":6329,"rel":27643},[19],[919,27645,27646],{},"SPA mode: This is similar to what you might use if you started a Vue project with Vue CLI. The project is also generated as in Static Mode, but what is generated is primarily Javascript code that is executed on the browser.",[919,27648,27649,27650,27653,27654,27656,27657,27659],{},"SSR mode: Server Side Rendering is the mode that I'll be focusing on here. Unlike the other ways of using Vue that have already been discussed, this mode involves a Node.js server that will handle our requests. For example, a web request for ",[33,27651,27652],{},"/posts"," is sent to our Nuxt Server (a Node.js server process) and Node.js is responsible for returning HTML that contains all of the blog Posts that we want to show (or a paginated selection of all blog posts, which is how my example blog app is built). So the Nuxt app has to make a request to our Django API server before returning fully rendered HTML page for the ",[33,27655,27652],{}," page. The user then gets the page from Nuxt, reads all of the blog posts and then decides to check out the blog posts on the second page of posts. When the user clicks on page 2, we request the second page of data from our Django API directly, not from Nuxt. The user then sees a short loading animation followed by the second page of blog posts that are loaded in using AJAX (usually with ",[33,27658,6832],{}," or axios).",[911,27661,27663],{"id":27662},"nuxt-benefits","Nuxt Benefits",[11,27665,27666],{},"The main reason for using Nuxt is to render the first page loads on the server, returning a complete HTML response that can be beneficial for SEO, social sharing, and other scenarios where you need control over how a website's pages are delivered (specifically, the initial request made to the server).",[11,27668,27669],{},"This type of control is not possible for applications that serve Vue over CDN since they can only request backend API data once the JS client has been requested from a CDN.",[911,27671,27673],{"id":27672},"nuxt-downsides-and-tradeoffs","Nuxt Downsides and Tradeoffs",[11,27675,27676],{},"Using Nuxt for SSR introduces quite a bit of complexity in both the application deployment and our Vue code. The backend API won't have to change at all when moving to Nuxt from a static Vue SPA.",[11,27678,27679],{},"Django alone is capable of returning fully generated, SEO-optimized HTML for each request, but applications built with Vue and Django templates may be difficult to work on as the project grows larger and larger. The Django/DRF + Nuxt approach may be more appropriate for projects with dedicated backend and frontend teams.",[11,27681,27682],{},"One other potential downside is added latency because of the \"double request\". If the Nuxt server and the Django server are on the same machine, then this latency will probably be a non-issue.",[168,27684,27126],{"id":27125},[11,27686,27687],{},[511,27688],{"alt":27689,"src":27690},"Nuxt Django Auth","/static/django_nuxt_auth.png",[11,27692,27693],{},"This diagram looks at session authentication with a focus on the browser, the Nuxt server and the Django server. It looks at two simple user stories, ordered from top to bottom in the diagram.",[11,27695,27696,27697,27699,27700,7901],{},"I. An existing application user visits the site in a new browser, navigates to the Login page, logs in with credentials and then visits a protected page: ",[33,27698,27652],{},".\nII. The user closes the browser and then comes back directly to the ",[33,27701,27652],{},[11,27703,27704],{},"These two user stories sound simple, but they touch on a lot of the features of Nuxt that make it powerful, and complicated at first (for Vue users). These include:",[916,27706,27707,27711,27716,27719],{},[919,27708,27709],{},[33,27710,6712],{},[919,27712,27713],{},[33,27714,27715],{},"nuxtServerInit",[919,27717,27718],{},"Vuex on the client and server",[919,27720,27721],{},"Custom plugin for axios",[11,27723,27724],{},"Some important parts of Nuxt that this diagram does not (yet) touch on are:",[916,27726,27727,27730,27736],{},[919,27728,27729],{},"Nuxt auth module (I don't know if this is relevant for my use case)",[919,27731,27732,27733,27735],{},"Nuxt fetch property (different from the ",[33,27734,6832],{}," web API)",[919,27737,27738],{},"Nuxt middleware (I'm also not sure if this would be helpful for anything I am doing in this example project)",[911,27740,27742,27743,27745,27746,27749,27750,27752],{"id":27741},"user-story-i-a-user-tries-to-open-posts-is-redirected-to-login-logs-in-then-navigate-to-posts-and-sees-blog-posts","User story I.: A user tries to open ",[33,27744,27652],{},", is redirected to ",[33,27747,27748],{},"/login",", logs in, then navigate to ",[33,27751,27652],{}," and sees blog posts",[2276,27754,27755,27762,27777,27792,27801,27804,27807,27827,27830],{},[919,27756,27757,27758,27761],{},"User navigates to ",[33,27759,27760],{},"http://domain.com/",". This request is handled by the Nuxt server.",[919,27763,6131,27764,27766,27767,27772,27773,27776],{},[33,27765,27715],{}," action is called (",[15,27768,27771],{"href":27769,"rel":27770},"https://nuxtjs.org/docs/2.x/directory-structure/store#the-nuxtserverinit-action",[19],"read more on nuxtServerInit","). This is a special Vuex action that, if defined in ",[33,27774,27775],{},"store/index.js",", will be called once per request to the Nuxt Server (when a page is initially visited or refreshed in the browser).",[919,27778,27779,27781,27782,27784,27785,27788,27789,27791],{},[33,27780,27715],{}," dispatches a Vuex action in the ",[33,27783,27417],{}," module called ",[33,27786,27787],{},"fetchData",". This action makes an GET request to ",[33,27790,27398],{}," in the Django application.",[919,27793,27794,27795,27797,27798,343],{},"An API call to ",[33,27796,27398],{}," is made to the Django backend directly from the Nuxt container over the docker network (",[33,27799,27800],{},"backend:8000",[919,27802,27803],{},"If the request is made by an anonymous user (no user is logged in), a 403 response is returned to the Nuxt server and no account data is set in the Vuex store (on the server).",[919,27805,27806],{},"Since the user is currently not logged in, the request returns a 403 response.",[919,27808,27809,27812,27813,27815,27816,27819,27820,27822,27823,27826],{},[33,27810,27811],{},"authMiddleware"," (on the Nuxt server) redirects the user to ",[33,27814,27748],{}," based on the value of ",[33,27817,27818],{},"authenticated"," in the Vuex store. The Original request for ",[33,27821,27652],{}," returns a fully-rendered ",[33,27824,27825],{},"/login/"," page instead.",[919,27828,27829],{},"User is now on the Login page",[919,27831,6131,27832,27835,27836,752],{},[33,27833,27834],{},"created"," hook for the Login page makes a GET request to ",[33,27837,27838],{},"/api/login-set-cookie/",[11,27840,27841,27842,752],{},"10, 11. This endpoint calls a simple view that is decorated with ",[33,27843,27844],{},"@ensure_csrf_token",[2276,27846,27847,27854,27872],{"start":2607},[919,27848,27849,27850,27853],{},"When the response returns to the browser, the ",[33,27851,27852],{},"csrftoken"," is set in the browser.",[919,27855,27856,27857,27860,27861,27863,27864,27867,27868,27871],{},"The $apiCall function is defined in ",[33,27858,27859],{},"plugins/axios.js",", and it adds the ",[33,27862,27852],{}," cookie to the ",[33,27865,27866],{},"X-CSRFToken"," header of API requests. This is important for POST request where the CSRF token is required. When the user fills out their email and password in the login form, the $apiCall function is called with ",[33,27869,27870],{},"/api/login/"," and the email/password as credentials.",[919,27873,27874,27875,752],{},"The email and password are sent as data in the POST request to ",[33,27876,27870],{},[11,27878,27879,27880,27882,27883,27886,27887,585,27890,1172,27893,5857,27896,27898,27899,27901,27902,27905],{},"15, 16. The ",[33,27881,27870],{}," URL calls the ",[33,27884,27885],{},"login_view"," which makes use of two functions from ",[33,27888,27889],{},"django.contrib.auth",[33,27891,27892],{},"authenticate",[33,27894,27895],{},"login",[33,27897,27892],{}," gets a user from the provided email/password, and the ",[33,27900,27895],{}," function sets an HttpOnly ",[33,27903,27904],{},"sessionid"," session cookie on the response.",[2276,27907,27908,27917,27927],{"start":2665},[919,27909,27910,27911,27913,27914,27916],{},"The HttpOnly ",[33,27912,27904],{}," cookie is automatically set on the browser when the ",[33,27915,27870],{}," request returns successfully.",[919,27918,27919,27920,27922,27923,27926],{},"When this ",[33,27921,27870],{}," request returns successfully, a value in the ",[33,27924,27925],{},"auth"," Vuex module is set to keep track of the current user's logged in state.",[919,27928,27929,27930,752],{},"Next, a GET request is made to ",[33,27931,27398],{},[11,27933,27934,27935,27937],{},"20, 21. Since the ",[33,27936,27904],{}," cookie is set and sent along with the request automatically, this request will succeed.",[2276,27939,27940,27948,27954,27963],{"start":2715},[919,27941,27392,27942,27944,27945,27947],{},[33,27943,27398],{}," request returns, the user's account information is saved to the ",[33,27946,27417],{}," Vuex module. At this point, the client may redirect automatically to the home page, or user account page, dashboard, etc.",[919,27949,27950,27951,27953],{},"Now logged in, the user navigates (again via Vue router) to ",[33,27952,27652],{},", a page that shows a paginated view of all blog posts.",[919,27955,27956,27957,27959,27960,752],{},"This page has an ",[33,27958,6712],{}," method which is called when the page component is created and it dispatches a Vuex action ",[33,27961,27962],{},"posts/fetchData",[919,27964,27965,27966,752],{},"This Vuex action makes a GET request to ",[33,27967,27968],{},"/api/posts/",[11,27970,27971,27972,27974,27975,27978],{},"26, 27. ",[33,27973,27968],{}," uses a ",[33,27976,27977],{},"ModelViewSet"," and returns a paginated list of blog posts",[2276,27980,27981],{"start":2770},[919,27982,27392,27983,27985,27986,27989],{},[33,27984,27968],{}," request returns successfully, the blog post data is saved to the ",[33,27987,27988],{},"blog"," Vuex module.",[911,27991,27993,27994],{"id":27992},"user-story-ii-logged-in-user-opens-new-browser-window-and-revisits-posts","User story II.: Logged in user opens new browser window and revisits ",[33,27995,27652],{},[2276,27997,27998,28003,28008,28016,28030,28038,28045,28052,28064,28074],{"start":2781},[919,27999,28000,28001,752],{},"The user closes their browser and then opens a new browser window and navigates to ",[33,28002,27652],{},[919,28004,28005,28007],{},[33,28006,27715],{}," is called as usual,",[919,28009,6131,28010,28013,28014,752],{},[33,28011,28012],{},"user/fetchData"," action is called. This action makes a GET request to ",[33,28015,27398],{},[919,28017,6131,28018,28020,28021,28023,28024,28026,28027,28029],{},[33,28019,27398],{}," request returns successfully. The ",[33,28022,27904],{}," cookie is passed along from the browser to the API request that is made from the Nuxt server to the backend API (",[33,28025,27398],{},").  User account data is then set on the Vuex ",[33,28028,27417],{}," module.",[919,28031,6131,28032,28034,28035,28037],{},[33,28033,6712],{}," method for the ",[33,28036,27652],{}," pages is called.",[919,28039,28040,28042,28043],{},[33,28041,6712],{}," dispatches a Vuex action ",[33,28044,27962],{},[919,28046,28047,28049,28050,752],{},[33,28048,27962],{}," makes an API request to ",[33,28051,27968],{},[919,28053,6131,28054,28056,28057,28059,28060,28063],{},[33,28055,27968],{}," request is handled by a ",[33,28058,27977],{}," for the ",[33,28061,28062],{},"Post"," model that gets blog posts and then sets them to the Vuex store (on the server) when the request returns a response (to the Nuxt server).",[919,28065,28066,28067,1172,28069,28059,28071,28073],{},"Once the async data fetching is compete (",[33,28068,27715],{},[33,28070,6712],{},[33,28072,27652],{}," page), the page HTML is rendered using the Vuex store data stored on the server. The Vuex data is sent back with the rendered HTML (I think this is how it works).",[919,28075,28076],{},"Finally, the user sees the list of blog posts. The page is loaded \"at once\"; there is no waiting for data to load after loading the page initially.",[168,28078,28080],{"id":28079},"discussion","Discussion",[911,28082,28084],{"id":28083},"complexity","Complexity",[11,28086,28087],{},"Is this authentication process overly complicated? When I make these diagrams, I try to make simple concept as detailed as possible, but there are a lot of distinct actions being taken in many different parts of the application and getting them all into one diagram was tricky.",[911,28089,28091],{"id":28090},"httponly-session-cookies","HttpOnly Session Cookies",[11,28093,28094,28095,28098],{},"Session authentication is the officially recommended way to do authentication with Django REST Framework for clients that run in the browser. However, there seem to be lots of people using JWT with DRF and Javascript clients that run in the browser. The main argument against doing this is that the JWT must be stored in a Javascript-accessible store (localStorage or Cookies) so it can be passed with each request. Many people are also interested in trying to store JWT for authentication in HttpOnly cookies to harden client-side security. I'm very curious to know if anyone is actually doing this, and what the implementation looks like. While ",[33,28096,28097],{},"djangorestframework_simplejwt"," doesn't support HttpOnly, there seems to be lots of interest in doing this. I think it might be possible with a special middleware, so let me know if anyone is interested in proof-of-concept/diagram for that.",[11,28100,28101],{},"Some use cases for JWT and other token authentication methods with DRF might include native mobile apps or Desktop apps. For most cases, I think session authentication with Django's built in session cookies for DRF authentication is the best option. JWTs also have no clear solution for logging out, which may be important for some security considerations. The concept of stateless authentication is interesting, but for most use cases I would argue that it is not worth doing. Let me know if anyone has thoughts on this, I'm curious to see what everyone thinks.",[911,28103,28105],{"id":28104},"next-steps","Next Steps",[11,28107,28108],{},"My next steps for this project/repo are to deploy this to a production environment as soon as I have time to do so. My local setup has been working well, and I think it should work well for a simple DigitalOcean docker swarm deployment like I have done with other Django + Vue projects.",[11,28110,28111],{},"I also want to add the create, update and delete functionality for posts, improve error handling with API calls, add form validation, and maybe write some tests with Jest.",[168,28113,28115],{"id":28114},"questions","Questions",[11,28117,28118],{},"Here are some questions and areas that I still need to investigate.",[911,28120,28122],{"id":28121},"nuxt-composition-api","Nuxt Composition API",[11,28124,28125],{},"I have seen that there is a Composition API module for Nuxt. I have only just now started looking at Composition API examples and documentation for \"vanilla\" Vue, but I have heard that the Nuxt Composition API module has some additional features specifically for use with Nuxt, so I'm curious to learn what these are.",[911,28127,28129],{"id":28128},"nuxt-v3-and-vue-3","Nuxt v3 and Vue 3",[11,28131,28132],{},"Nuxt looks like it has plans to support Vue 3, so I am interested to learn more about Vue 3 as it is adopted by Vue frameworks such as Nuxt and Quasar.",[911,28134,28136],{"id":28135},"nuxts-fetch-method-server-middleware-nuxt-auth-module","Nuxt's fetch method, server middleware, Nuxt auth module",[11,28138,28139],{},"I think I am using server middleware correctly, it can be improved by redirecting to the initial requested route after successful login. I'm not sure if I should use the Nuxt auth module in this application, I have read that it doesn't support HttpOnly cookie use cases, but I could be wrong.",{"title":35,"searchDepth":249,"depth":249,"links":28141},[28142,28143,28149,28155,28160],{"id":27106,"depth":249,"text":27107},{"id":27600,"depth":249,"text":27601,"children":28144},[28145,28146,28147,28148],{"id":27607,"depth":312,"text":27608},{"id":27631,"depth":312,"text":27632},{"id":27662,"depth":312,"text":27663},{"id":27672,"depth":312,"text":27673},{"id":27125,"depth":249,"text":27126,"children":28150},[28151,28153],{"id":27741,"depth":312,"text":28152},"User story I.: A user tries to open /posts, is redirected to /login, logs in, then navigate to /posts and sees blog posts",{"id":27992,"depth":312,"text":28154},"User story II.: Logged in user opens new browser window and revisits /posts",{"id":28079,"depth":249,"text":28080,"children":28156},[28157,28158,28159],{"id":28083,"depth":312,"text":28084},{"id":28090,"depth":312,"text":28091},{"id":28104,"depth":312,"text":28105},{"id":28114,"depth":249,"text":28115,"children":28161},[28162,28163,28164],{"id":28121,"depth":312,"text":28122},{"id":28128,"depth":312,"text":28129},{"id":28135,"depth":312,"text":28136},"2021-01-01","This article shows how to use session authentication with Django + Nuxt.js applications","/static/django_nuxt_auth_og.png",{},"/2021/01/01/session-authentication-with-django-django-rest-framework-and-nuxt",{"title":27563,"description":28166},"2021/01/01/session-authentication-with-django-django-rest-framework-and-nuxt",[15290,882,881,27559,21239],"QNhKMyUFZxiHwo7hdnfz0PzsZ9ICARDle0xhC4U0Duc",{"id":28175,"title":28176,"body":28177,"comments":872,"date":28491,"description":28492,"draft":872,"extension":873,"external":874,"image":28213,"meta":28493,"navigation":315,"path":28494,"seo":28495,"stem":28496,"tags":28497,"__hash__":28498},"blog/2020/12/27/building-web-applications-with-django-drf-and-nuxt.md","Building web applications with Django, Django REST Framework, Nuxt.js and docker",{"type":8,"value":28178,"toc":28484},[28179,28182,28200,28206,28209,28214,28218,28221,28234,28237,28240,28243,28249,28252,28255,28259,28276,28286,28433,28435,28438,28441,28445,28465,28468,28471,28478,28481],[11,28180,28181],{},"Over the holidays between lots of big meals and many naps, I tried to tackle one more goal of mine before this year come to an end: building an application with Django and Nuxt.js.",[11,28183,28184,28185,28189,28190,28193,28194,28199],{},"This year I rebuilt my personal blog (",[15,28186,743],{"href":28187,"rel":28188},"https://briancaffey.github.io/",[19],") with Nuxt.js, the ",[33,28191,28192],{},"@nuxt/content"," headless git-based CMS and TailwindCSS. It is statically generated with Nuxt's full-static mode and has been really enjoyable to work with. I have also learned a lot more about SEO and how Nuxt helps improve Vue applications' SEO. I have also been working a lot with Django and Vue.js applications where Django serves as an API to a Vue.js SPA. This combination of technologies works well for a lot of use cases, but it falls short in SEO. Nuxt also provides a great way to organize large Vue.js projects which I have been finding very helpful. For these reasons, combining Django and Nuxt has been something that I have wanted to try for a while, so this article will share some of my experiences in recent efforts to build with these two frameworks. I took ",[15,28195,28198],{"href":28196,"rel":28197},"https://gitlab.com/briancaffey/django-nuxt-starter/-/blob/develop/STEP_BY_STEP.md",[19],"detailed notes of each step of the project setup"," starting from an empty repository, and I put together a diagram of my understanding of how data flows in the application.",[11,28201,28202,28203],{},"Here's the link to the project repository that I'll be referencing: ",[15,28204,27576],{"href":27576,"rel":28205},[19],[11,28207,28208],{},"This article will focus on explaining the project through the diagram shown below. I added two types of labels: letters and numbers. The letters will introduce each component of the application and its role in the application as a whole. The numbers summarize how data flows through the different components in my sample blog application.",[11,28210,28211],{},[511,28212],{"alt":27126,"src":28213},"/static/django_nuxt_app_diagram.png",[168,28215,28217],{"id":28216},"diagram-components","Diagram components",[11,28219,28220],{},"A. Your computer - Possibly also your development machine which is running the application in docker containers with docker-compose.",[11,28222,28223,28224,637,28227,637,28230,28233],{},"B. NGINX - This is the \"front desk\" of the application that does a few different things. It is the first component that web requests come to. It serves as a reverse proxy which does path-based routing. It looks at the URL request and determines where to send it. For example: ",[33,28225,28226],{},"/api/posts/1",[33,28228,28229],{},"/dashboard/",[33,28231,28232],{},"/admin/"," could all be routed differently depending on the NGINX configuration file. We will look at this again in the next section. This  component, like most of the other things in the diagram, runs in a container. NGINX can also serve static files for our Django app and do TLS termination to make our application available over a secure HTTPS connection.",[11,28235,28236],{},"C. Nuxt.JS server - The first \"S\" in SSR (server side rendering). It is a Node.js process that renders HTML from Vue components that we define in our Nuxt app, as well as data fetched from other servers/APIs before returning HTML back to the client.",[11,28238,28239],{},"D. Django server - This runs the WSGI application with a gunicorn process in a container.",[11,28241,28242],{},"E. Django REST Framework is a Django package the facilitates the creation of REST API endpoints. This is part of the Django application, it primarily takes care of data serialization (which can be thought of as translating between JSON and Python objects that represent rows of data in our Postgres database)",[11,28244,28245,28246,752],{},"F. This is the Postgres database, also a containerized service. It is on the same docker network as the Django/gunicorn application, so the Django application can connect to the Postgres database using the hostname ",[33,28247,28248],{},"postgres",[11,28250,28251],{},"G. docker-compose is used to orchestrate the docker network, containers and volumes that make up the application.",[11,28253,28254],{},"H. This box represents the docker network that allows for easy networking between services. We will come back to this the significance of this in the next section.",[168,28256,28258],{"id":28257},"data-flow-in-the-application","Data flow in the application",[11,28260,28261,28262,28265,28266,28268,28269,28272,28273,752],{},"The simple application I have built for this demonstration is a blog. There is only a list view and a detail view for simple blog post model with three fields: title, body and created date. For the list view, the frontend (Nuxt) route is ",[33,28263,28264],{},"/posts/"," and the backend route is ",[33,28267,27968],{}," for the detail view the frontend route is ",[33,28270,28271],{},"/posts/_id"," and the API route is ",[33,28274,28275],{},"/api/posts/_id/",[11,28277,28278,28279,28282,28283,1737],{},"The data flow shown here will walk through what happens when a user visits ",[33,28280,28281],{},"http://localhost/posts/",", and then show what happens when the user clicks on one of the listed posts to see the detail view of the post (",[33,28284,28285],{},"http://localhost/posts/2",[2276,28287,28288,28294,28300,28308,28314,28337,28350,28359,28362,28365,28370,28415],{"start":24352},[919,28289,28290,28293],{},[33,28291,28292],{},"docker-compose up"," is one command that is used to start the entire application in local development. This exposes the NGINX process on port 80 of the host machine (your laptop).",[919,28295,28296,28297,28299],{},"When the application is running on your machine and you navigate to ",[33,28298,28281],{},", the request is first handled by NGINX.",[919,28301,28302,28303,15754,28305,28307],{},"As we mentioned earlier, NGINX's path-based routing sends all requests that do not start with ",[33,28304,13009],{},[33,28306,13012],{}," to the Nuxt.js server.",[919,28309,28310,28311,28313],{},"When the request gets to the Nuxt server, the Nuxt lifecycle methods start. The important one that I'm using so far is ",[33,28312,6712],{},". This property is used to request data that will be used in the rendering of our HTML response.",[919,28315,28316,28317,28319,28320,28322,28323,21662,28325,28328,28329,28332,28333,28336],{},"Inside of ",[33,28318,6712],{},", the application uses axios to make a request to ",[33,28321,27968],{}," (for example). In ",[33,28324,6359],{},[33,28326,28327],{},"privateRuntimeConfig"," sets a baseUrl value for axios to ",[33,28330,28331],{},"http://backend:8000",". Since the Nuxt server is on the same docker network as the backend Django/gunicorn server, the Nuxt server is able to resolve ",[33,28334,28335],{},"http://backend"," to the address of the backend server.",[919,28338,28339,28340,28343,28344,4313,28347,752],{},"Django processes this endpoint, using the ",[33,28341,28342],{},"PostViewSet",", the views of which have been added to ",[33,28345,28346],{},"urlpatterns",[33,28348,28349],{},"blog/urls.py",[919,28351,6131,28352,28354,28355,28358],{},[33,28353,28342],{}," makes a database query on the ",[33,28356,28357],{},"posts"," which is used to serialize the data.",[919,28360,28361],{},"The Django server returns the response to the original axios call.",[919,28363,28364],{},"The data returned from Django is used to render the HTML response.",[919,28366,28367,28368,752],{},"The HTML response from the Nuxt server is sent back to the browser that originally navigated to ",[33,28369,28281],{},[919,28371,28372,28373,28376,28377,28380,28381,28384,28385,28388,28389,28392,28393,28395,28396,28399,28400,28403,28404,28407,28408,28411,28412,28414],{},"The user is presented with page that lists blog posts. Each blog posts lists to a detail view. When a blog post (let's say the post with ",[33,28374,28375],{},"id"," of 2) is clicked on, a request for ",[33,28378,28379],{},"/posts/2/"," is made directly to the Django backend. The ",[33,28382,28383],{},"browserBaseURL"," value in the ",[33,28386,28387],{},"axios"," settings under ",[33,28390,28391],{},"publicRuntimeConfig"," defined in ",[33,28394,6359],{}," is set to ",[33,28397,28398],{},"http://localhost",", so the request is made to ",[33,28401,28402],{},"http://localhost/api/posts/2/",". To clarify, since we are making this request using axios in the browser, we can't make a request to ",[33,28405,28406],{},"http://backend:8000/api/posts/2/"," like we did in step 4 (",[33,28409,28410],{},"http://backend:8000/api/posts/",") because the browser doesn't know how to resolve the ",[33,28413,12291],{}," hostname.",[919,28416,28417,28418,28420,28421,28424,28425,28428,28429,28432],{},"This request to ",[33,28419,28402],{},", like all others, first goes to NGINX which sends it to the backend since the path starts with ",[33,28422,28423],{},"/api/",". At this point the application functions like a regular Vue SPA making axios calls to a backend service. This is because we used ",[33,28426,28427],{},"\u003Cnuxt-link>"," for the posts listed in the posts list view. If we used ",[33,28430,28431],{},"\u003Ca>"," tags, we would go through the same process as in step 4 where the HTML is rendered on the Nuxt server and sent back to the browser all at once.",[168,28434,28080],{"id":28079},[11,28436,28437],{},"My main takeaway is that using Nuxt and Django together can give you good SEO and a great SPA experience at the same time. Using Django alone, or Django with traditional non SSR Vue makes this harder to do. Being a progressive framework, there are a lot of ways to use Vue with any other backend. From what I have heard, most people use Vue via CDN similar to how jQuery was and still is delivered for use in the browser.",[11,28439,28440],{},"There is additional work in setting up 3 servers for a single application (Nuxt, Django and NGINX), but the tradeoff is that I am (at least I feel) very productive writing frontend logic in Vue and backend logic with DRF. I have never liked working with Django templates and I used to know a lot more about them than I do now.",[168,28442,28444],{"id":28443},"spotlight-for-baserowios-awesome-open-source-django-nuxt-application","Spotlight for baserow.io's awesome open-source Django Nuxt application",[11,28446,28447,28448,28453,28454,28459,28460,28464],{},"Lastly I want to mention that there are some great resources in the ",[15,28449,28452],{"href":28450,"rel":28451},"https://github.com/nuxt-community/awesome-nuxt",[19],"nuxt-community/awesome-nuxt"," GitHub repo. There's one project that really stood out to me when I searched for \"django\" projects in the README, and that project is called ",[15,28455,28458],{"href":28456,"rel":28457},"https://baserow.io/",[19],"baserow.io"," (repo: ",[15,28461,28462],{"href":28462,"rel":28463},"https://gitlab.com/bramw/baserow",[19],"). Please check this repo our if you are interested in Django and Nuxt. This company is building an open source no-code database, similar to Airtable which I have worked with before.",[11,28466,28467],{},"Their entire product is open source and I have been very impressed with what I have seen. Please go give that project a star or consider becoming a Github sponsor if you are interested. I'm not affiliated with that project in any way, but I'll be referencing how they use Django and Nuxt to build their application.",[168,28469,28470],{"id":28104},"Next steps",[11,28472,28473,28474,28477],{},"There is a still a lot I have to learn about Nuxt. I'm still very new to the Framework and this is my first time using Nuxt's SSR mode. Nuxt seems to have its own way of doing lots of things that I'm used to doing in Vue. There is a very supportive community and well-maintained official packages to help with lots of things, like the ",[33,28475,28476],{},"@nuxt/axios"," package that I'm using.",[11,28479,28480],{},"My next step is to keep expanding my blog application. One thing I didn't mention is authentication. I plan on using Django session authentication for authenticating request to Django. It seems that it already works correctly in my application (logging in through Django admin and then navigating to Nuxt routes that make Django requests are working only when I'm logged in.) I think I have an idea about how Vuex, authentication and route guards will work together, but I haven't gotten there yet. If anyone has some good reference projects or recommendations on how to expand on what I already have, please let me know!",[11,28482,28483],{},"I know that Nuxt has an auth module, so I need to see if that is relevant for what I want need in my application. I also need to continue reading the Nuxt documentation. I still don't know what I don't know about Nuxt and the plugins and modules that it makes available. I also noticed that Nuxt has it's own version of the Vue 3 Composition API, something I am just now starting to learn more about, so that it another area I'll need to dig into eventually.",{"title":35,"searchDepth":249,"depth":249,"links":28485},[28486,28487,28488,28489,28490],{"id":28216,"depth":249,"text":28217},{"id":28257,"depth":249,"text":28258},{"id":28079,"depth":249,"text":28080},{"id":28443,"depth":249,"text":28444},{"id":28104,"depth":249,"text":28470},"2020-12-27","This article documents my progress combining the Django web framework with Nuxt JS to build applications that have both great SEO and a smooth SPA user experience.",{},"/2020/12/27/building-web-applications-with-django-drf-and-nuxt",{"title":28176,"description":28492},"2020/12/27/building-web-applications-with-django-drf-and-nuxt",[15290,882,881,27559,15298],"2rmd0ET1499gf_ZwBljrP4P6gk87WHP0wa3cvCPGUbo",{"id":28500,"title":28501,"body":28502,"comments":872,"date":28630,"description":28631,"draft":872,"extension":873,"external":874,"image":28632,"meta":28633,"navigation":315,"path":28634,"seo":28635,"stem":28636,"tags":28637,"__hash__":28640},"blog/2020/11/29/weekend-project-update-open-sec-data.md","Weekend project update: Open SEC Data",{"type":8,"value":28503,"toc":28628},[28504,28507,28529,28538,28541,28587,28590],[11,28505,28506],{},"Here's an early look at a project I have been working on to practice some Django and Vue.js concepts: Open SEC Data.",[916,28508,28509,28516,28522],{},[919,28510,28511,28515],{},[15,28512,28513],{"href":28513,"rel":28514},"https://opensecdata.ga",[19]," (project staging website, deployed to docker swarm cluster running on DigitalOcean)",[919,28517,28518,28521],{},[15,28519,27121],{"href":27121,"rel":28520},[19]," (main repository, requires GitLab account)",[919,28523,28524,28528],{},[15,28525,28526],{"href":28526,"rel":28527},"https://github.com/briancaffey/sec-filings-app",[19]," (mirror, no account required to view)",[11,28530,28531,28532,28537],{},"This project uses Django, DRF and Celery to read public SEC filings from ",[15,28533,28536],{"href":28534,"rel":28535},"https://www.sec.gov/Archives/edgar/full-index/",[19],"sec.gov",", build it into an API which is consumed through a Vue.js application. I'm currently focused on 13F filings which are required for large US investment funds managing over $100 million USD. There is data dating back to 1993 and it is published quarterly.",[11,28539,28540],{},"Here are some of the things I'm focusing on in this project in no particular order:",[916,28542,28543,28546,28549,28552,28555,28558,28561,28572,28581,28584],{},[919,28544,28545],{},"Getting better at Django REST Framework. This project has been helping me apply some of the parts of DRF that I have found difficult. I'm currently using ViewSets which feels function-based views inside of class-based views. They are flexible, but I would like to add more abstraction with filtering",[919,28547,28548],{},"Django admin. While this project primarily uses Django as a REST API with Django REST Framework, I have tried to take advantage of the Django admin to build out helpful views that can be used to spot check the data I'm creating. Most of my API is read-only, this makes things pretty simple.",[919,28550,28551],{},"Moderately complex paginated data tables with Vue. I work with lots of paginated table data, and I think there is a better way to do abstract some of the repeated logic that I use (getting and setting current page, rows per page). I'm using Vuex, and I have heard of module factories, but I'm thinking that there will be a better way to do this when Vue 3 officially comes to Quasar Framework (Quasar is a Vue.js framework).",[919,28553,28554],{},"Session authentication with DRF. There are a lot of guides showing how to use JWT and Token Authentication for DRF with Javascript frontends. The DRF recommends using Session Authentication for such use cases as a web-base Javascript client, so I hope I can promote some best practices around how to use Django's built-in session authentication for use with the Django REST Framework using an HttpOnly session cookie. I also understand that all security decisions have trade-offs, and I'm trying to understand what trade-offs come with handling authentication in this way.",[919,28556,28557],{},"Social authentication. I have previously setup social authentication with Google, Facebook and GitHub using Python Social Auth. I think it is a great package, and it adds a lot of flexibility with it's concept of pipelines, but I haven't done much with these yet, so I'm hoping to dig in further and better understand how I can make better use of social authentication in my app. This app uses Linkedin 0Auth2 with a custom user model. Logging in with Linkedin account gives you the ability to request an API Token (Django REST Framework's Token) to access the public API.",[919,28559,28560],{},"Automatic API documentation with OpenAPI. Swagger/OpenAPI seems like nice way to document and API, so I'm hoping to build best practices around how to document a DRF API automatically with OpenAPI and Swagger UI.",[919,28562,28563,28564,28567,28568,28571],{},"CI/CD with GitLab and docker swarm. I will admit that I am huge GitLab fan. I love how flexible their CI/CD pipelines are. Being a docker fan as well, I chose to use docker swarm for this project to keep things simple and straightforward. I think one under-appreciate feature of docker is being able to set ",[33,28565,28566],{},"DOCKER_HOST"," to an SSH connection, such as ",[33,28569,28570],{},"ssh://root@123.456.789.10",". This let's you control the remote docker host without needing to SSH to it first, and it is also how I'm able to deploy and run management commands \"manually\" through the GitLab UI.",[919,28573,28574,28575,765,28578,28580],{},"Productive development environment. To start the project, you only need to run docker-compose up (after copying ",[33,28576,28577],{},".env.template",[33,28579,9057],{}," in the root directory for storing sensitive data outside of git such as LinkedIn OAuth2 keys). The development environment is very similar to how this project runs in production with some additional utilities for monitoring and debugging such as pgadmin4, flower (for celery), redis commander (a GUI for viewing redis databases), Django debug toolbar (a must have for any Django project, I believe), runserver_plus with Werkzeug, and others. Also, the backend and frontend hot reload automatically with the help of webpack for Vue and watchdog for Django and Celery.",[919,28582,28583],{},"Automatic TLS certificate generation with Traefik. For a simple project in docker swarm, I'm really happy with how simple it is to request TLS certificates from Let's Encrypt automatically with Traefik. There are no scripts, cron jobs or one-time setup jobs, it just seems to work out of the box if configured correctly.",[919,28585,28586],{},"Testing with pytest. I have only been trying to test most of my API views so far. I really like using factory with pytest, so I use that in most of my tests.",[11,28588,28589],{},"That's all I have for now. I have a long list of questions, things I want to improve, add and experiment with, here are just a few that come to mind:",[916,28591,28592,28595,28598,28606,28609,28612,28615,28618],{},[919,28593,28594],{},"Frontend testing. I don't have any component testing or e2d tests, so this would be good to add eventually. Since I'm using a component library and my app uses these components directly, I'm not exactly sure how much testing I should be doing.",[919,28596,28597],{},"Data verification/validation. There are a lot of site that do provide similar data, WhaleWisdom is the biggest one that I know of. Once I get more data built on the site it would be good to spot check some of the values. There are some nuances to the filing data that I haven't addressed, such as Amendment filings and additions.",[919,28599,28600,28601,752],{},"Calculating period changes. One of the features that I'm not sure how best to implement is the ability to sort holdings for a filer in a given period on the percent increase from the last period. One way would be to add these as additional fields to the Holding model and then calculate these values as I process the data in celery. If I process the data from recent periods to later periods, I will have to update these values once the previous period has been processed, so it would be an additional check to do. I'll probably post this question here in more detail later. Here's ",[15,28602,28605],{"href":28603,"rel":28604},"https://whalewisdom.com/filer/ubs-ag#tabholdings_tab_link",[19],"an example of what this means from WhaleWisdom",[919,28607,28608],{},"Accessing LinkedIn profile data to populate fields on my CustomUser model.",[919,28610,28611],{},"Scaling? I have a lot more experience with deploying projects to AWS which is built around the ability to scale. I don't know a project on DigitalOcean would be scaled automatically. A single node docker swarm cluster while take some time to process all of the data. I would probably be better of scaling vertically with much bigger droplets and higher celery concurrency.",[919,28613,28614],{},"Docker swarm secrets. I'm currently using environment variables to pass secrets stored in GitLab CI when I build images and deploy to docker swarm. I would like to learn how to properly use swarm secrets and work them into my CI/CD pipeline.",[919,28616,28617],{},"As I mentioned above, I'm also interested in updating this project to Vue3 and to apply some of its new features to this project.",[919,28619,28620,28621,28624,28625,752],{},"Use pipenv, poetry or some other way of pinning secondary python dependencies. Does anyone have a recommendation on how best to do this with docker. I have always thought that docker ",[4339,28622,28623],{},"is"," the virtual environment, but I realize that some versions of indirect dependencies may change when pip installing without using a lockfile similar to ",[33,28626,28627],{},"package-lock.json",{"title":35,"searchDepth":249,"depth":249,"links":28629},[],"2020-11-29","This project uses Django, DRF and Celery to read public SEC filings from sec.gov, build it into an API which is consumed through a Vue.js application.","/static/sec-update.jpg",{},"/2020/11/29/weekend-project-update-open-sec-data",{"title":28501,"description":28631},"2020/11/29/weekend-project-update-open-sec-data",[15290,882,582,28638,28639,15298,13041],"api","gitlab","08eEPj3uympIqf4j5KknXLeDHR1mN86R_NY81yB6_U8",{"id":28642,"title":28643,"body":28644,"comments":315,"date":29011,"description":28648,"draft":872,"extension":873,"external":874,"image":29012,"meta":29013,"navigation":315,"path":29015,"seo":29016,"stem":29017,"tags":29018,"__hash__":29019},"blog/2020/11/27/how-to-authenticate-django-rest-framework-from-vue-app-with-session-authentication-httponly-cookies.md","How to authenticate Django REST Framework API calls from a (Vue) JS client using Session Authentication and HttpOnly cookies",{"type":8,"value":28645,"toc":29005},[28646,28649,28656,28673,28682,28685,28690,28703,28707,28710,28935,28937,28940,28943,28946,28949,28952,28954,28957,28971,28974],[11,28647,28648],{},"This article will describe an authentication strategy using Django REST Framework with a Javascript frontend application. I'll be demonstrating this with Vue.js (Qusar Framework, using Vue 2), but the concepts should transfer to any other Javascript framework.",[11,28650,28651,28652,752],{},"Here's a GitLab repository for a project that I will be referencing throughout this article: ",[15,28653,28654],{"href":28654,"rel":28655},"https://gitlab.com/verbose-equals-true/django-postgres-vue-gitlab-ecs",[19],[11,28657,28658,28659,28664,28665,28672],{},"When I first started learning about how to do authentication from a Vue client, I found ",[15,28660,28663],{"href":28661,"rel":28662},"https://blog.sqreen.com/authentication-best-practices-vue/",[19],"this article from Sqreen"," which describes how to use JWT to authenticate a Vue application. The example uses a mocked backend, but it is a good proxy for what you would have if you were to use a library like ",[15,28666,28669],{"href":28667,"rel":28668},"https://github.com/SimpleJWT/django-rest-framework-simplejwt",[19],[33,28670,28671],{},"django-rest-framework-simplejwt",", which I have previously used with success in Django projects.",[11,28674,28675,28676,28681],{},"JWT is an option for doing authentication with DRF ",[15,28677,28680],{"href":28678,"rel":28679},"https://www.django-rest-framework.org/api-guide/authentication/#json-web-token-authentication",[19],"listed in the authentication documentation",", but the documentation doesn't recommend when or how to use JWT authentication.",[11,28683,28684],{},"Session authentication is mentioned as well:",[107,28686,28687],{},[11,28688,28689],{},"This authentication scheme uses Django's default session backend for authentication. Session authentication is appropriate for AJAX clients that are running in the same session context as your website.",[11,28691,28692,28693,28695,28696,1172,28699,28702],{},"In my project, both in local development and in production environments, I serve the API and the Javascript clients on the same domain. ",[33,28694,13009],{}," requests go to the API, and all other request paths route to the frontend client. In other scenarios such as using ",[33,28697,28698],{},"https://mysite.com",[33,28700,28701],{},"https://api.mysite.com"," for hosting a frontend and API an different subdomains, there would need to be additional considerations for CORS, but since I have the frontend and the backend being served on the same domain (and same subdomain), this isn't a concern. You might need to watch out for this if your requirements are different.",[168,28704,28706],{"id":28705},"the-authentication-flow","The authentication flow",[11,28708,28709],{},"Now let's describe the login process at a high level.",[2276,28711,28712,28718,28727,28745,28764,28787,28812,28842,28851,28872,28884,28902],{},[919,28713,28714,28715,28717],{},"A user navigates to your site. There is currently nothing in the browser's ",[33,28716,19140],{}," or cookies related to authentication.",[919,28719,28720,28721,28724,28725,752],{},"The user navigates to the ",[33,28722,28723],{},"Login"," page at ",[33,28726,27748],{},[919,28728,28729,28730,28733,28734,28736,28737,28740,28741,28744],{},"Loading this Vue component makes a ",[33,28731,28732],{},"GET"," request to a special endpoint in our Django backend ",[33,28735,27838],{},". This request returns a simple JSON message: ",[33,28738,28739],{},"\"CSRF cookie set\"",", and as the message says, the response sets a ",[33,28742,28743],{},"csrf"," cookie on our browser.",[919,28746,28747,28748,28750,28751,28754,28755,28757,28758,28760,28761,28763],{},"Once the CSRF cookie is set by the response from ",[33,28749,27838],{},", the user is presented with a login form and enters account credentials (email and password in my example, where email is the ",[33,28752,28753],{},"USERNAME_FIELD"," on my custom user model). Clicking \"Login\" dispatches a Vuex action that uses Axios to send a send a request to ",[33,28756,27870],{}," with the ",[33,28759,28743],{}," cookie set in a ",[33,28762,27866],{}," header.",[919,28765,28766,28768,28769,28771,28772,585,28774,1172,28776,5857,28778,28780,28781,28783,28784,28786],{},[33,28767,27870],{}," is handled by the ",[33,28770,27885],{}," view which uses two important functions from ",[33,28773,27889],{},[33,28775,27892],{},[33,28777,27895],{},[33,28779,27892],{}," gets the user from the provided credentials, and ",[33,28782,27895],{}," sets a ",[33,28785,27904],{}," HttpOnly cookie on the response.",[919,28788,28789,28790,28792,28793,28795,28796,765,28798,28801,28802,28805,28806,28808,28809,343],{},"When the response from ",[33,28791,27870],{}," comes back, two things happen: first the ",[33,28794,27904],{}," HttpOnly cookie is set on our browser. Second, we set a value in both Vuex and localStorage named ",[33,28797,27818],{},[33,28799,28800],{},"success",". We are not storing any sensitive information in this value. Instead, we are using this value to signal to the rest of our Vue application that the user has authenticated. Storing this in Vuex allows us to use global Vuex ",[33,28803,28804],{},"getters"," so that we can change component state and other logic where authentication is concerned, such as route guards (for Vue router). We store it in localStorage so that when a new browser tab is opened, we can set the value of ",[33,28807,27818],{}," in Vuex based on the value in localStorage. (",[33,28810,28811],{},"authenticated: localStorage.getItem(\"authenticated\") || \"\",",[919,28813,28814,28815,28817,28818,28821,28822,28825,28826,28828,28829,28832,28833,28835,28836,765,28838,28841],{},"Since the ",[33,28816,27904],{}," cookie is HttpOnly, we can't use Javascript to interact with it, so when we want to logout the user we can't just delete the cookie. To logout the user, we make a request to ",[33,28819,28820],{},"/api/logout/"," when the user clicks on the logout button. The view for this endpoint does ",[33,28823,28824],{},"logout(request)",". This returns a response with a new value for the ",[33,28827,27904],{}," cookie: ",[33,28830,28831],{},"Set-Cookie: sessionid=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=0; Path=/; SameSite=Lax",". Since the cookie's expiration is in the past, it is removed entirely. We also remove the ",[33,28834,27818],{}," localStorage item and set the Vuex store value of ",[33,28837,27818],{},[33,28839,28840],{},"''"," (which is falsey). This lets the Vue application know that user has been logged out. One consideration for this is that your user can't logout while offline.",[919,28843,28844,28845,28850],{},"This repo also implements social authentication with the fantastic Python Social Auth library. I used Facebook, Google and GitHub, but there are lots of other providers you can choose from depending on what you need. This is a lengthy topic, and I recommend that you read ",[15,28846,28849],{"href":28847,"rel":28848},"https://www.toptal.com/django/integrate-oauth-2-into-django-drf-back-end",[19],"How to Integrate OAuth 2 Into Your Django/DRF Back-end Without Going Insane"," on which I have based my implementation. Here's the short story of how this works. First, a user clicks on one of the social sign-in links. These links are the same for sign-in and sign-up.",[919,28852,28853,28854,12282,28857,28860,28861,28864,28865,28868,28869,28871],{},"The link has a few parts, here's an example: ",[33,28855,28856],{},"https://github.com/login/oauth/authorize?client_id=r66bdfgsfsbferfef4&redirect_uri=http:%2F%2Flocalhost%2Fauth%2Fgithub%2Fcallback&login=&scope=user:email&state=ewori4t95k3vdzem",[33,28858,28859],{},"client_id"," is the app we created to allow our users to sign in. When you create this app, you specify the ",[33,28862,28863],{},"redirect_uri"," in the configuration, and you reference that here as well. ",[33,28866,28867],{},"scope"," specifies scope of access we are requesting from the user's social account. ",[33,28870,19772],{}," is used for security.",[919,28873,28874,28875,585,28877,28880,28881,28883],{},"When you click on the link above, you are redirected to GitHub and asked if you want to grant my GitHub application access to your account's associated email address. Clicking on \"Authorize\" then redirects you back to the ",[33,28876,28863],{},[33,28878,28879],{},"http://localhost/auth/github/callback?code=veroi3409e203ej&state=ewori4t95k3vdzem",". Notice the ",[33,28882,33],{}," parameter.",[919,28885,28886,28887,28890,28891,28894,28895,28898,28899,752],{},"When you navigate to ",[33,28888,28889],{},"/auth/github/callback"," on the Vue application, you see a message: \"Logging in with GitHub...\". On this page's ",[33,28892,28893],{},"mounted"," method we call ",[33,28896,28897],{},"handleOauthCallback"," which makes a request to our Django application: ",[33,28900,28901],{},"/api/social/github/?code=veroi3409e203ej",[919,28903,28904,28905,28908,28909,28912,28913,28915,28916,28919,28920,28923,28924,28927,28928,28930,28931,28934],{},"This API endpoint uses the ",[33,28906,28907],{},"exchange_token"," view which is where Python Social Auth starts to do the heavy lifting. First, we need to make an API request to GitHub (",[33,28910,28911],{},"https://github.com/login/oauth/access_token",") with the ",[33,28914,33],{}," as a URL parameter, then we pass the access code that this API call returns into Python Social Auth's ",[33,28917,28918],{},"do_auth"," function to get our Django user. Finally, similar to the email/password login approach described above, we call ",[33,28921,28922],{},"login(request, user)"," and return a simple JSON response: ",[33,28925,28926],{},"{\"detail\": \"success\"}",". This will set the ",[33,28929,27904],{}," automatically when the response returns, and we can dispatch the same Vuex action ",[33,28932,28933],{},"AUTH_SUCCESS"," to tell Vuex that a user has been logged in.",[168,28936,28080],{"id":28079},[11,28938,28939],{},"Using the default Django session authentication mechanism has some nice advantages. It allows us to easily navigate between our Javascript SPA which uses Django REST Framework, regular Django admin views that you may also be using, as well as the Django admin.",[11,28941,28942],{},"Using DRF's token authentication is still possible if you choose to use Session authentication for your JS frontend. For example, you may wish to allow users to make authenticated API requests to your public API using DRF Token Authentication.",[11,28944,28945],{},"JWT is a really interesting concept and important to know about, but it doesn't seem like a practical solution for any of my use cases with Django APIs or frontends. You also can't really \"logout\" a user if you are using this solution for authentication.",[11,28947,28948],{},"What I have described here is pretty simple scenario. It assumes that there is only one type of user and that there are no additional steps needed to make your account \"active\". Doing this would require additional logic on the Vue/Vuex side as well as the backend logic, including the User model. I also don't make use of any data from the social providers except for the user's email address.",[11,28950,28951],{},"Another thing to be aware of with this scenario is that a user can register with social authentication first, and then reset their password and login with email (I haven't implemented this client-side on this project yet). Or, you can login with an email account that was created through the Django admin and then login with a social account tied to that email. There are a lot of options for each backend in Python social auth, making it a very flexible library for handling social authentication.",[168,28953,28105],{"id":28104},[11,28955,28956],{},"There is a lot more work to do on this example project regarding authentication, but I hope it can help point some people in the right direction. Here are some areas that I would like to work on next:",[916,28958,28959,28962,28965,28968],{},[919,28960,28961],{},"Error handling for a bad authentication attempt",[919,28963,28964],{},"A signup form with email confirmation, handling cases where the user trying to signup may already have signed in with social authentication",[919,28966,28967],{},"Password reset with email",[919,28969,28970],{},"Making use of Python Social Auth settings and options, including pipelines.",[168,28972,12349],{"id":28973},"resources",[916,28975,28976,28981,28987,28993,28999],{},[919,28977,28978],{},[15,28979,28847],{"href":28847,"rel":28980},[19],[919,28982,28983],{},[15,28984,28985],{"href":28985,"rel":28986},"https://yoongkang.com/blog/cookie-based-authentication-spa-django/",[19],[919,28988,28989],{},[15,28990,28991],{"href":28991,"rel":28992},"https://github.com/encode/django-rest-framework/issues/7273",[19],[919,28994,28995],{},[15,28996,28997],{"href":28997,"rel":28998},"https://github.com/SimpleJWT/django-rest-framework-simplejwt/issues/71",[19],[919,29000,29001],{},[15,29002,29003],{"href":29003,"rel":29004},"http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/",[19],{"title":35,"searchDepth":249,"depth":249,"links":29006},[29007,29008,29009,29010],{"id":28705,"depth":249,"text":28706},{"id":28079,"depth":249,"text":28080},{"id":28104,"depth":249,"text":28105},{"id":28973,"depth":249,"text":12349},"2020-11-27","/static/padlocks.jpg",{"layout":29014},"post","/2020/11/27/how-to-authenticate-django-rest-framework-from-vue-app-with-session-authentication-httponly-cookies",{"title":28643,"description":28648},"2020/11/27/how-to-authenticate-django-rest-framework-from-vue-app-with-session-authentication-httponly-cookies",[15290,882,21239,28638],"GvTaAGutMol1B0i9VJwzVjzU5A3eb8KoWwGqhzkLUBk",{"id":29021,"title":29022,"body":29023,"comments":315,"date":29811,"description":29027,"draft":872,"extension":873,"external":874,"image":29812,"meta":29813,"navigation":315,"path":29814,"seo":29815,"stem":29816,"tags":29817,"__hash__":29818},"blog/2020/10/10/how-to-add-email-signup-form-to-nuxt-site-with-mailchimp.md","How to add an email signup form to a Nuxt site with MailChimp",{"type":8,"value":29024,"toc":29802},[29025,29028,29032,29035,29046,29048,29051,29090,29093,29097,29108,29114,29117,29127,29133,29154,29161,29700,29703,29707,29713,29717,29720,29723,29728,29737,29740,29749,29756,29770,29772,29786,29788,29799],[11,29026,29027],{},"This is a guide for setting up a MailChimp-powered newsletter signup form on a static Nuxt site hosted on GitHub Pages. I wanted to implement this on my personal blog to grow a mailing list so I can update readers when I puslish a new blog article. I haven't done too much work with MailChimp before, but I've definitely subscribed to plenty of MailChimp mailing lists.",[168,29029,29031],{"id":29030},"goals","Goals",[11,29033,29034],{},"I'm hoping to acheive some of the following:",[916,29036,29037,29040,29043],{},[919,29038,29039],{},"Build a list of emails from people who visit my site and want get",[919,29041,29042],{},"Include a simple newletter signup form (Vue) component in the Footer of my site",[919,29044,29045],{},"Send customized emails (campaigns) to a mailing list that I can track",[168,29047,28115],{"id":28114},[11,29049,29050],{},"Here are some of the questions (and answers) that I had going into this:",[916,29052,29053,29058,29071,29076,29079,29085],{},[919,29054,29055,29056,343],{},"Do I need to use the MailChimp API or MailChimp API keys? (",[338,29057,1564],{},[919,29059,29060,29061,29064,29065,29068,29069,343],{},"Is ",[338,29062,29063],{},"Double Opt-In"," possible with a static GitHub pages site hosted on a ",[33,29066,29067],{},"\u003Cusernme>.github.io"," subdomain? (",[338,29070,1596],{},[919,29072,29073,29074,343],{},"What is the signup flow? Will a new subscriber be sent to a MailChimp page first, and then back to my site? (",[338,29075,1596],{},[919,29077,29078],{},"How do I setup a \"Thank you\" page to show users after they subscribe?",[919,29080,29081,29082,343],{},"How much will this cost? (",[338,29083,29084],{},"Free up to 2,000 subscribers",[919,29086,29087,29088,343],{},"Should I create a new campaign for each new blog post email update I send out? (",[338,29089,1596],{},[11,29091,29092],{},"I'll touch on these questions as I describe how to set things up.",[168,29094,29096],{"id":29095},"creating-the-form","Creating the form",[11,29098,29099,29100,29103,29104,29107],{},"Under the ",[33,29101,29102],{},"Audience > Signup forms"," menu, I selected the ",[33,29105,29106],{},"Embedded Forms"," option:",[26,29109,29112],{"className":29110,"code":29111,"language":31},[29],"Embedded forms\nGenerate HTML code to embed in your site or blog to collect signups.\n",[33,29113,29111],{"__ignoreMap":35},[11,29115,29116],{},"Most of the MailChimp Admin that I have been using seems to have 4 different menus: 2 vertical menus and 2 horizontal.",[11,29118,29099,29119,29122,29123,29126],{},[33,29120,29121],{},"Embedded forms"," menu, I selected ",[33,29124,29125],{},"Unstyled"," since I want to add my own Tailwind CSS classes to keep the style of my signup form consistent with the different color schemes available on my site.",[11,29128,29129,29130,358],{},"I'll start with an Unstyled Embedded form. Let's go through the ",[33,29131,29132],{},"Form options",[916,29134,29135,29141,29151],{},[919,29136,29137,29140],{},[338,29138,29139],{},"Include form title"," (no, I'll add this myself)",[919,29142,29143,29146,29147,29150],{},[338,29144,29145],{},"Show only required fields"," (For now, ",[33,29148,29149],{},"email"," is the only field I will be capturing. It can be helpful to include a First and/or Last name to avoid having your emails go to users' spam folders.)",[919,29152,29153],{},"Unselect everything else",[11,29155,29156,29157,29160],{},"Here's the HTML that we can use in our ",[33,29158,29159],{},"Subscribe.vue"," component:",[26,29162,29164],{"className":6715,"code":29163,"language":6717,"meta":35,"style":35},"    \u003C!-- Begin Mailchimp Signup Form -->\n    \u003Cdiv id=\"mc_embed_signup\">\n      \u003Cform\n        action=\"https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6&amp;id=9937fe4fc5\"\n        method=\"post\"\n        id=\"mc-embedded-subscribe-form\"\n        name=\"mc-embedded-subscribe-form\"\n        class=\"validate\"\n        target=\"_blank\"\n        novalidate\n      >\n        \u003Cdiv id=\"mc_embed_signup_scroll\">\n          \u003Cdiv class=\"mc-field-group\">\n            \u003C!-- \u003Clabel for=\"mce-EMAIL\">Email Address \u003C/label> -->\n            \u003C!-- Added placeholder -->\n            \u003Cinput\n              type=\"email\"\n              value=\"\"\n              name=\"EMAIL\"\n              class=\"required email\"\n              id=\"mce-EMAIL\"\n              placeholder=\"Enter your email address\"\n            />\n          \u003C/div>\n          \u003Cdiv id=\"mce-responses\" class=\"clear\">\n            \u003Cdiv\n              class=\"response\"\n              id=\"mce-error-response\"\n              style=\"display: none\"\n            >\u003C/div>\n            \u003Cdiv\n              class=\"response\"\n              id=\"mce-success-response\"\n              style=\"display: none\"\n            >\u003C/div>\n          \u003C/div>\n          \u003C!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\n          \u003Cdiv style=\"position: absolute; left: -5000px\" aria-hidden=\"true\">\n            \u003Cinput\n              type=\"text\"\n              name=\"b_43a795784ca963e25903a0da6_9937fe4fc5\"\n              tabindex=\"-1\"\n              value=\"\"\n            />\n          \u003C/div>\n          \u003Cdiv class=\"clear\">\n            \u003Cinput\n              type=\"submit\"\n              value=\"Subscribe\"\n              name=\"subscribe\"\n              id=\"mc-embedded-subscribe\"\n              class=\"button\"\n            />\n          \u003C/div>\n        \u003C/div>\n      \u003C/form>\n    \u003C/div>\n\n    \u003C!--End mc_embed_signup-->\n",[33,29165,29166,29171,29187,29195,29211,29220,29229,29238,29248,29258,29263,29268,29283,29300,29305,29310,29317,29327,29337,29347,29357,29367,29377,29382,29391,29413,29420,29429,29438,29448,29457,29463,29471,29480,29488,29496,29504,29509,29533,29539,29548,29557,29567,29575,29579,29587,29601,29607,29616,29625,29634,29643,29652,29656,29664,29672,29682,29691,29695],{"__ignoreMap":35},[187,29167,29168],{"class":189,"line":190},[187,29169,29170],{"class":295},"    \u003C!-- Begin Mailchimp Signup Form -->\n",[187,29172,29173,29175,29177,29180,29182,29185],{"class":189,"line":249},[187,29174,19865],{"class":577},[187,29176,10229],{"class":2516},[187,29178,29179],{"class":193}," id",[187,29181,595],{"class":577},[187,29183,29184],{"class":196},"\"mc_embed_signup\"",[187,29186,6730],{"class":577},[187,29188,29189,29192],{"class":189,"line":312},[187,29190,29191],{"class":577},"      \u003C",[187,29193,29194],{"class":2516},"form\n",[187,29196,29197,29200,29202,29205,29208],{"class":189,"line":319},[187,29198,29199],{"class":193},"        action",[187,29201,595],{"class":577},[187,29203,29204],{"class":196},"\"https://github.us2.list-manage.com/subscribe/post?u=43a795784ca963e25903a0da6",[187,29206,29207],{"class":588},"&amp;",[187,29209,29210],{"class":196},"id=9937fe4fc5\"\n",[187,29212,29213,29215,29217],{"class":189,"line":325},[187,29214,2624],{"class":193},[187,29216,595],{"class":577},[187,29218,29219],{"class":196},"\"post\"\n",[187,29221,29222,29224,29226],{"class":189,"line":686},[187,29223,13330],{"class":193},[187,29225,595],{"class":577},[187,29227,29228],{"class":196},"\"mc-embedded-subscribe-form\"\n",[187,29230,29231,29234,29236],{"class":189,"line":697},[187,29232,29233],{"class":193},"        name",[187,29235,595],{"class":577},[187,29237,29228],{"class":196},[187,29239,29240,29243,29245],{"class":189,"line":1291},[187,29241,29242],{"class":193},"        class",[187,29244,595],{"class":577},[187,29246,29247],{"class":196},"\"validate\"\n",[187,29249,29250,29253,29255],{"class":189,"line":1306},[187,29251,29252],{"class":193},"        target",[187,29254,595],{"class":577},[187,29256,29257],{"class":196},"\"_blank\"\n",[187,29259,29260],{"class":189,"line":1434},[187,29261,29262],{"class":193},"        novalidate\n",[187,29264,29265],{"class":189,"line":2599},[187,29266,29267],{"class":577},"      >\n",[187,29269,29270,29272,29274,29276,29278,29281],{"class":189,"line":2607},[187,29271,7944],{"class":577},[187,29273,10229],{"class":2516},[187,29275,29179],{"class":193},[187,29277,595],{"class":577},[187,29279,29280],{"class":196},"\"mc_embed_signup_scroll\"",[187,29282,6730],{"class":577},[187,29284,29285,29288,29290,29293,29295,29298],{"class":189,"line":2621},[187,29286,29287],{"class":577},"          \u003C",[187,29289,10229],{"class":2516},[187,29291,29292],{"class":193}," class",[187,29294,595],{"class":577},[187,29296,29297],{"class":196},"\"mc-field-group\"",[187,29299,6730],{"class":577},[187,29301,29302],{"class":189,"line":2631},[187,29303,29304],{"class":295},"            \u003C!-- \u003Clabel for=\"mce-EMAIL\">Email Address \u003C/label> -->\n",[187,29306,29307],{"class":189,"line":2642},[187,29308,29309],{"class":295},"            \u003C!-- Added placeholder -->\n",[187,29311,29312,29314],{"class":189,"line":2653},[187,29313,7955],{"class":577},[187,29315,29316],{"class":2516},"input\n",[187,29318,29319,29322,29324],{"class":189,"line":2665},[187,29320,29321],{"class":193},"              type",[187,29323,595],{"class":577},[187,29325,29326],{"class":196},"\"email\"\n",[187,29328,29329,29332,29334],{"class":189,"line":2674},[187,29330,29331],{"class":193},"              value",[187,29333,595],{"class":577},[187,29335,29336],{"class":196},"\"\"\n",[187,29338,29339,29342,29344],{"class":189,"line":2684},[187,29340,29341],{"class":193},"              name",[187,29343,595],{"class":577},[187,29345,29346],{"class":196},"\"EMAIL\"\n",[187,29348,29349,29352,29354],{"class":189,"line":2694},[187,29350,29351],{"class":193},"              class",[187,29353,595],{"class":577},[187,29355,29356],{"class":196},"\"required email\"\n",[187,29358,29359,29362,29364],{"class":189,"line":2706},[187,29360,29361],{"class":193},"              id",[187,29363,595],{"class":577},[187,29365,29366],{"class":196},"\"mce-EMAIL\"\n",[187,29368,29369,29372,29374],{"class":189,"line":2715},[187,29370,29371],{"class":193},"              placeholder",[187,29373,595],{"class":577},[187,29375,29376],{"class":196},"\"Enter your email address\"\n",[187,29378,29379],{"class":189,"line":2725},[187,29380,29381],{"class":577},"            />\n",[187,29383,29384,29387,29389],{"class":189,"line":2735},[187,29385,29386],{"class":577},"          \u003C/",[187,29388,10229],{"class":2516},[187,29390,6730],{"class":577},[187,29392,29393,29395,29397,29399,29401,29404,29406,29408,29411],{"class":189,"line":2743},[187,29394,29287],{"class":577},[187,29396,10229],{"class":2516},[187,29398,29179],{"class":193},[187,29400,595],{"class":577},[187,29402,29403],{"class":196},"\"mce-responses\"",[187,29405,29292],{"class":193},[187,29407,595],{"class":577},[187,29409,29410],{"class":196},"\"clear\"",[187,29412,6730],{"class":577},[187,29414,29415,29417],{"class":189,"line":2754},[187,29416,7955],{"class":577},[187,29418,29419],{"class":2516},"div\n",[187,29421,29422,29424,29426],{"class":189,"line":2762},[187,29423,29351],{"class":193},[187,29425,595],{"class":577},[187,29427,29428],{"class":196},"\"response\"\n",[187,29430,29431,29433,29435],{"class":189,"line":2770},[187,29432,29361],{"class":193},[187,29434,595],{"class":577},[187,29436,29437],{"class":196},"\"mce-error-response\"\n",[187,29439,29440,29443,29445],{"class":189,"line":2781},[187,29441,29442],{"class":193},"              style",[187,29444,595],{"class":577},[187,29446,29447],{"class":196},"\"display: none\"\n",[187,29449,29450,29453,29455],{"class":189,"line":2792},[187,29451,29452],{"class":577},"            >\u003C/",[187,29454,10229],{"class":2516},[187,29456,6730],{"class":577},[187,29458,29459,29461],{"class":189,"line":2803},[187,29460,7955],{"class":577},[187,29462,29419],{"class":2516},[187,29464,29465,29467,29469],{"class":189,"line":2808},[187,29466,29351],{"class":193},[187,29468,595],{"class":577},[187,29470,29428],{"class":196},[187,29472,29473,29475,29477],{"class":189,"line":2816},[187,29474,29361],{"class":193},[187,29476,595],{"class":577},[187,29478,29479],{"class":196},"\"mce-success-response\"\n",[187,29481,29482,29484,29486],{"class":189,"line":2824},[187,29483,29442],{"class":193},[187,29485,595],{"class":577},[187,29487,29447],{"class":196},[187,29489,29490,29492,29494],{"class":189,"line":2834},[187,29491,29452],{"class":577},[187,29493,10229],{"class":2516},[187,29495,6730],{"class":577},[187,29497,29498,29500,29502],{"class":189,"line":2845},[187,29499,29386],{"class":577},[187,29501,10229],{"class":2516},[187,29503,6730],{"class":577},[187,29505,29506],{"class":189,"line":2856},[187,29507,29508],{"class":295},"          \u003C!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->\n",[187,29510,29511,29513,29515,29518,29520,29523,29526,29528,29531],{"class":189,"line":2867},[187,29512,29287],{"class":577},[187,29514,10229],{"class":2516},[187,29516,29517],{"class":193}," style",[187,29519,595],{"class":577},[187,29521,29522],{"class":196},"\"position: absolute; left: -5000px\"",[187,29524,29525],{"class":193}," aria-hidden",[187,29527,595],{"class":577},[187,29529,29530],{"class":196},"\"true\"",[187,29532,6730],{"class":577},[187,29534,29535,29537],{"class":189,"line":2878},[187,29536,7955],{"class":577},[187,29538,29316],{"class":2516},[187,29540,29541,29543,29545],{"class":189,"line":2886},[187,29542,29321],{"class":193},[187,29544,595],{"class":577},[187,29546,29547],{"class":196},"\"text\"\n",[187,29549,29550,29552,29554],{"class":189,"line":2900},[187,29551,29341],{"class":193},[187,29553,595],{"class":577},[187,29555,29556],{"class":196},"\"b_43a795784ca963e25903a0da6_9937fe4fc5\"\n",[187,29558,29559,29562,29564],{"class":189,"line":2905},[187,29560,29561],{"class":193},"              tabindex",[187,29563,595],{"class":577},[187,29565,29566],{"class":196},"\"-1\"\n",[187,29568,29569,29571,29573],{"class":189,"line":2913},[187,29570,29331],{"class":193},[187,29572,595],{"class":577},[187,29574,29336],{"class":196},[187,29576,29577],{"class":189,"line":2921},[187,29578,29381],{"class":577},[187,29580,29581,29583,29585],{"class":189,"line":2931},[187,29582,29386],{"class":577},[187,29584,10229],{"class":2516},[187,29586,6730],{"class":577},[187,29588,29589,29591,29593,29595,29597,29599],{"class":189,"line":2942},[187,29590,29287],{"class":577},[187,29592,10229],{"class":2516},[187,29594,29292],{"class":193},[187,29596,595],{"class":577},[187,29598,29410],{"class":196},[187,29600,6730],{"class":577},[187,29602,29603,29605],{"class":189,"line":2953},[187,29604,7955],{"class":577},[187,29606,29316],{"class":2516},[187,29608,29609,29611,29613],{"class":189,"line":2964},[187,29610,29321],{"class":193},[187,29612,595],{"class":577},[187,29614,29615],{"class":196},"\"submit\"\n",[187,29617,29618,29620,29622],{"class":189,"line":2975},[187,29619,29331],{"class":193},[187,29621,595],{"class":577},[187,29623,29624],{"class":196},"\"Subscribe\"\n",[187,29626,29627,29629,29631],{"class":189,"line":2983},[187,29628,29341],{"class":193},[187,29630,595],{"class":577},[187,29632,29633],{"class":196},"\"subscribe\"\n",[187,29635,29636,29638,29640],{"class":189,"line":2992},[187,29637,29361],{"class":193},[187,29639,595],{"class":577},[187,29641,29642],{"class":196},"\"mc-embedded-subscribe\"\n",[187,29644,29645,29647,29649],{"class":189,"line":3001},[187,29646,29351],{"class":193},[187,29648,595],{"class":577},[187,29650,29651],{"class":196},"\"button\"\n",[187,29653,29654],{"class":189,"line":3010},[187,29655,29381],{"class":577},[187,29657,29658,29660,29662],{"class":189,"line":3019},[187,29659,29386],{"class":577},[187,29661,10229],{"class":2516},[187,29663,6730],{"class":577},[187,29665,29666,29668,29670],{"class":189,"line":3028},[187,29667,8051],{"class":577},[187,29669,10229],{"class":2516},[187,29671,6730],{"class":577},[187,29673,29674,29677,29680],{"class":189,"line":3033},[187,29675,29676],{"class":577},"      \u003C/",[187,29678,29679],{"class":2516},"form",[187,29681,6730],{"class":577},[187,29683,29684,29687,29689],{"class":189,"line":3041},[187,29685,29686],{"class":577},"    \u003C/",[187,29688,10229],{"class":2516},[187,29690,6730],{"class":577},[187,29692,29693],{"class":189,"line":3049},[187,29694,316],{"emptyLinePlaceholder":315},[187,29696,29697],{"class":189,"line":3059},[187,29698,29699],{"class":295},"    \u003C!--End mc_embed_signup-->\n",[11,29701,29702],{},"Now we can create a Vue component that contains the HTML form generated by the MailChimp admin. For now, just put the embed HTML in the template of a Vue component.",[168,29704,29706],{"id":29705},"styling-the-form","Styling the form",[11,29708,29709,29710,29712],{},"Next you can add styles to the form. You can reference the ",[33,29711,29159],{}," file in the repo for this site to see how I have added styles using TailwindCSS.",[168,29714,29716],{"id":29715},"settings","Settings",[11,29718,29719],{},"Next let's look at some settings around Double Opt-in.",[11,29721,29722],{},"To navigate to the page where these settings can be set, go to:",[11,29724,29725],{},[33,29726,29727],{},"Audience > Signup forms > Settings > Audience name and defaults",[11,29729,12674,29730,29733,29734],{},[33,29731,29732],{},"Form Settings"," of this menu, you can select ",[33,29735,29736],{},"Enable double opt-in",[11,29738,29739],{},"Next, let's configure the redirect to a custom \"Thank you for subscribing page\" on our static site. Go to",[11,29741,29742,29745,29746],{},[33,29743,29744],{},"Audience > Signup forms > Signup forms"," and select ",[33,29747,29748],{},"Form builder",[11,29750,29751,29752,29755],{},"Select ",[338,29753,29754],{},"Signup thank you page"," from the dropdown menu and add a custom URL that you will show users when they first submit their email to the form.",[11,29757,29758,29759,29762,29763,29766,29767],{},"Next, also on the ",[33,29760,29761],{},"Form Builder"," menu, select ",[338,29764,29765],{},"Confirmation thank you page"," from the dropdown menu and enter the URL of the page that you want to redirect to after a user confirms their subscription where it says: ",[33,29768,29769],{},"Instead of showing this thank you page, send subscribers to another URL",[168,29771,28105],{"id":28104},[916,29773,29774,29780,29783],{},[919,29775,29776,29779],{},[338,29777,29778],{},"Form validation",": we can validate that the user has entered a valid email address",[919,29781,29782],{},"GDPR considerations",[919,29784,29785],{},"Captcha",[168,29787,12349],{"id":28973},[916,29789,29790],{},[919,29791,29792,585,29795],{},[338,29793,29794],{},"Single vs Double Opt-in",[15,29796,29797],{"href":29797,"rel":29798},"https://www.sendinblue.com/blog/double-opt-in/",[19],[855,29800,29801],{},"html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":29803},[29804,29805,29806,29807,29808,29809,29810],{"id":29030,"depth":249,"text":29031},{"id":28114,"depth":249,"text":28115},{"id":29095,"depth":249,"text":29096},{"id":29705,"depth":249,"text":29706},{"id":29715,"depth":249,"text":29716},{"id":28104,"depth":249,"text":28105},{"id":28973,"depth":249,"text":12349},"2020-10-11","/static/chimps.webp",{"layout":29014},"/2020/10/10/how-to-add-email-signup-form-to-nuxt-site-with-mailchimp",{"title":29022,"description":29027},"2020/10/10/how-to-add-email-signup-form-to-nuxt-site-with-mailchimp",[881,20780,882],"iaQVA6SoryjtXuHqJFeDvsIoQM6JtI6Jg1tarGbflnY",{"id":29820,"title":29821,"body":29822,"comments":315,"date":30158,"description":29826,"draft":872,"extension":873,"external":874,"image":30020,"meta":30159,"navigation":315,"path":30160,"seo":30161,"stem":30162,"tags":30163,"__hash__":30167},"blog/2020/09/17/migrating-from-jekyll-to-nuxt-static-site.md","Migrating my personal GitHub pages blog from Jekyll to Nuxt",{"type":8,"value":29823,"toc":30151},[29824,29827,29830,29841,29845,29854,30000,30003,30009,30015,30021,30025,30028,30071,30075,30085,30119,30123,30142,30144,30149],[11,29825,29826],{},"Jekyll has served me well for a long time, but I've decided to switch the static site generator I use for my personal blog from Jekyll to Nuxt. This article will hopefully be the first new article in my Nuxt blog.",[11,29828,29829],{},"Here are some goals for what I want to do with this new site:",[916,29831,29832,29835,29838],{},[919,29833,29834],{},"Learn more about Nuxt and Nuxt Content, JAM Stack and static site generation",[919,29836,29837],{},"Master TailwindCSS for building responsive layouts",[919,29839,29840],{},"Better understand and measure SEO for my site and the content I publish on it",[168,29842,29844],{"id":29843},"creating-a-nuxt-project","Creating a nuxt project",[11,29846,29847,29848,29851,29852,358],{},"I'm going to start off with a ",[33,29849,29850],{},"feature-nuxt"," feature branch. Since I can't create the nuxt project in a non-empty directory, I'll create a new nuxt project in a folder called ",[33,29853,881],{},[26,29855,29859],{"className":29856,"code":29857,"language":29858,"meta":35,"style":35},"language-txt shiki shiki-themes github-light github-dark","brian@x1:~/github/briancaffey.github.io/nuxt$ npx create-nuxt-app .\n\ncreate-nuxt-app v3.2.0\n✨  Generating Nuxt.js project in .\n(node:8073) ExperimentalWarning: Conditional exports is an experimental feature. This feature could change at any time\n? Project name: brian-caffey\n? Programming language: JavaScript\n? Package manager: Yarn\n? UI framework: Tailwind CSS\n? Nuxt.js modules: Axios, Content\n? Linting tools: ESLint, Prettier\n? Testing framework: None\n? Rendering mode: Universal (SSR / SSG)\n? Deployment target: Static (Static/JAMStack hosting)\n? Development tools: jsconfig.json (Recommended for VS Code if you're not using typescript)\nyarn run v1.22.5\n$ eslint --ext .js,.vue --ignore-path .gitignore . --fix\nDone in 1.57s.\n\n🎉  Successfully created project brian-caffey\n\n  To get started:\n\n        yarn dev\n\n  To build & start for production:\n\n        yarn build\n        yarn start\n","txt",[33,29860,29861,29866,29870,29875,29880,29885,29890,29895,29900,29905,29910,29915,29920,29925,29930,29935,29940,29945,29950,29954,29959,29963,29968,29972,29977,29981,29986,29990,29995],{"__ignoreMap":35},[187,29862,29863],{"class":189,"line":190},[187,29864,29865],{},"brian@x1:~/github/briancaffey.github.io/nuxt$ npx create-nuxt-app .\n",[187,29867,29868],{"class":189,"line":249},[187,29869,316],{"emptyLinePlaceholder":315},[187,29871,29872],{"class":189,"line":312},[187,29873,29874],{},"create-nuxt-app v3.2.0\n",[187,29876,29877],{"class":189,"line":319},[187,29878,29879],{},"✨  Generating Nuxt.js project in .\n",[187,29881,29882],{"class":189,"line":325},[187,29883,29884],{},"(node:8073) ExperimentalWarning: Conditional exports is an experimental feature. This feature could change at any time\n",[187,29886,29887],{"class":189,"line":686},[187,29888,29889],{},"? Project name: brian-caffey\n",[187,29891,29892],{"class":189,"line":697},[187,29893,29894],{},"? Programming language: JavaScript\n",[187,29896,29897],{"class":189,"line":1291},[187,29898,29899],{},"? Package manager: Yarn\n",[187,29901,29902],{"class":189,"line":1306},[187,29903,29904],{},"? UI framework: Tailwind CSS\n",[187,29906,29907],{"class":189,"line":1434},[187,29908,29909],{},"? Nuxt.js modules: Axios, Content\n",[187,29911,29912],{"class":189,"line":2599},[187,29913,29914],{},"? Linting tools: ESLint, Prettier\n",[187,29916,29917],{"class":189,"line":2607},[187,29918,29919],{},"? Testing framework: None\n",[187,29921,29922],{"class":189,"line":2621},[187,29923,29924],{},"? Rendering mode: Universal (SSR / SSG)\n",[187,29926,29927],{"class":189,"line":2631},[187,29928,29929],{},"? Deployment target: Static (Static/JAMStack hosting)\n",[187,29931,29932],{"class":189,"line":2642},[187,29933,29934],{},"? Development tools: jsconfig.json (Recommended for VS Code if you're not using typescript)\n",[187,29936,29937],{"class":189,"line":2653},[187,29938,29939],{},"yarn run v1.22.5\n",[187,29941,29942],{"class":189,"line":2665},[187,29943,29944],{},"$ eslint --ext .js,.vue --ignore-path .gitignore . --fix\n",[187,29946,29947],{"class":189,"line":2674},[187,29948,29949],{},"Done in 1.57s.\n",[187,29951,29952],{"class":189,"line":2684},[187,29953,316],{"emptyLinePlaceholder":315},[187,29955,29956],{"class":189,"line":2694},[187,29957,29958],{},"🎉  Successfully created project brian-caffey\n",[187,29960,29961],{"class":189,"line":2706},[187,29962,316],{"emptyLinePlaceholder":315},[187,29964,29965],{"class":189,"line":2715},[187,29966,29967],{},"  To get started:\n",[187,29969,29970],{"class":189,"line":2725},[187,29971,316],{"emptyLinePlaceholder":315},[187,29973,29974],{"class":189,"line":2735},[187,29975,29976],{},"        yarn dev\n",[187,29978,29979],{"class":189,"line":2743},[187,29980,316],{"emptyLinePlaceholder":315},[187,29982,29983],{"class":189,"line":2754},[187,29984,29985],{},"  To build & start for production:\n",[187,29987,29988],{"class":189,"line":2762},[187,29989,316],{"emptyLinePlaceholder":315},[187,29991,29992],{"class":189,"line":2770},[187,29993,29994],{},"        yarn build\n",[187,29996,29997],{"class":189,"line":2781},[187,29998,29999],{},"        yarn start\n",[11,30001,30002],{},"Running the following:",[26,30004,30007],{"className":30005,"code":30006,"language":31},[29],"yarn generate\nyarn start\n",[33,30008,30006],{"__ignoreMap":35},[11,30010,30011,30012,358],{},"Starts my project on ",[33,30013,30014],{},"localhost:3000",[11,30016,30017],{},[511,30018],{"alt":30019,"src":30020},"Nuxt app","/static/nuxt-app.png",[168,30022,30024],{"id":30023},"tricky-parts","Tricky parts",[11,30026,30027],{},"Here are some of the parts of migrating that I need to think about:",[916,30029,30030,30050],{},[919,30031,30032,30035,30036,30039,30040,30043,30044,30046,30047,30049],{},[338,30033,30034],{},"Disqus comments",": I previously based the ",[33,30037,30038],{},"disqus_identifier"," used to link Disqus threads to specific pages on Jekyll's page URLs (of the form ",[33,30041,30042],{},"/YYYY/MM/DD/name-of-article.html","). I could transform the slug value with a hook in ",[33,30045,6359],{},", or manually add a ",[33,30048,30038],{}," value to the frontmatter of markdown files.",[919,30051,30052,30055,30056,30059,30060,30062,30063,30066,30067,30070],{},[338,30053,30054],{},"Static Content",": In Jekyll I had static content in a few different places. With Nuxt, all static content will live under the top level folder ",[33,30057,30058],{},"/static"," that is mapped to the root URL where my site is hosted. Since I had lots of content under ",[33,30061,30058],{}," in my Jekyll site, such as ",[33,30064,30065],{},"/static/my-image.png",", I ended up putting conent in ",[33,30068,30069],{},"/static/static",", hopefully this won't be too confusing.",[168,30072,30074],{"id":30073},"nuxt-community","Nuxt Community",[11,30076,30077,30078,30084],{},"There are a lot of great packages from the ",[15,30079,30082],{"href":30080,"rel":30081},"https://github.com/nuxt-community",[19],[33,30083,30073],{}," GitHub organization that are widely used in many Nuxt projects. Here are a few of the Nuxt community plugins and other Nuxt extensions that I have used so far:",[916,30086,30087,30095,30100,30105,30110,30115],{},[919,30088,30089,30094],{},[15,30090,30093],{"href":30091,"rel":30092},"https://content.nuxtjs.org/",[19],"nuxt/content",": an official Nuxt project that provides a Git-based Headless CMS",[919,30096,30097],{},[33,30098,30099],{},"@nuxtjs/tailwindcss",[919,30101,30102],{},[33,30103,30104],{},"@nuxtjs/color-mode",[919,30106,30107],{},[33,30108,30109],{},"@nuxtjs/google-analytics",[919,30111,30112],{},[33,30113,30114],{},"@nuxtjs/sitemap",[919,30116,30117],{},[33,30118,20746],{},[168,30120,30122],{"id":30121},"issues","Issues",[916,30124,30125,30132,30135],{},[919,30126,30127,30128,343],{},"Tailwind removes markdown styles in blog articles from nuxt/content (solved by adding some extra css with ",[15,30129,14293],{"href":30130,"rel":30131},"https://github.com/iandinwoodie/github-markdown-tailwindcss/blob/master/markdown.css",[19],[919,30133,30134],{},"No support for automatically adding markdown anchors (GitHub Flavored Markdown supports this)",[919,30136,30137,30138],{},"Not able to do true server-side redirects, but you can do something like this: ",[15,30139,30140],{"href":30140,"rel":30141},"https://github.com/nuxt-community/redirect-module/issues/1#issuecomment-615070920",[19],[168,30143,7912],{"id":7911},[916,30145,30146],{},[919,30147,30148],{},"Add tags, categories, search to blog",[855,30150,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":30152},[30153,30154,30155,30156,30157],{"id":29843,"depth":249,"text":29844},{"id":30023,"depth":249,"text":30024},{"id":30073,"depth":249,"text":30074},{"id":30121,"depth":249,"text":30122},{"id":7911,"depth":249,"text":7912},"2020-09-17",{"layout":29014},"/2020/09/17/migrating-from-jekyll-to-nuxt-static-site",{"title":29821,"description":29826},"2020/09/17/migrating-from-jekyll-to-nuxt-static-site",[881,882,30164,30165,30166],"content","static-site","web","wFHbiWq9j3tJI5MNrbsj9veLs5clTRX8oFPDQfAoLRA",{"id":30169,"title":30170,"body":30171,"comments":315,"date":32530,"description":32531,"draft":872,"extension":873,"external":874,"image":32532,"meta":32533,"navigation":315,"path":32534,"seo":32535,"stem":32536,"tags":32537,"__hash__":32541},"blog/2020/08/09/digital-ocean-docker-swarm-django-traefik-nginx.md","Deploying Django applications with docker swarm on DigitalOcean using GitLab CI, Traefik, NGINX and REX-Ray",{"type":8,"value":30172,"toc":32504},[30173,30194,30197,30202,30209,30212,30214,30217,30255,30276,30280,30308,30312,30332,30335,30338,30341,30345,30351,30391,30395,30408,30412,30415,30421,30427,30431,30438,30441,30447,30458,30461,30467,30475,30480,30483,30488,30505,30526,30535,30540,30634,30647,30666,30700,30706,30718,30759,30765,30775,30814,30822,30826,30844,30850,30865,30884,30888,30891,30897,30902,30922,30925,30931,30938,30947,30953,30963,30969,30979,30984,30996,31126,31159,31185,31204,31210,31221,31224,31229,31237,31243,31248,31259,31265,31280,31282,31402,31410,31415,31421,31440,31449,31455,31477,31486,31522,31575,31590,31593,31596,31602,31613,31618,31727,31737,31752,31758,31788,31794,31804,31816,31822,31858,31864,31955,31958,31968,31978,31984,31987,31993,32009,32025,32028,32033,32042,32045,32056,32059,32064,32067,32081,32090,32107,32110,32294,32299,32310,32358,32364,32369,32375,32380,32386,32392,32401,32407,32410,32416,32419,32422,32428,32437,32441,32444,32453,32458,32461,32463,32466,32470,32473,32477,32480,32484,32487,32491,32494,32498,32501],[11,30174,30175,30176,30181,30182,30187,30188,30193],{},"I recently wrote two articles about deploying Django applications to AWS serverless environments: one on ",[15,30177,30180],{"href":30178,"rel":30179},"https://briancaffey.github.io/2020/06/02/django-postgres-vue-gitlab-ecs.html",[19],"AWS Fargate"," (CloudFront, ALB and ECS Fargate containers) and one on ",[15,30183,30186],{"href":30184,"rel":30185},"https://briancaffey.github.io/2020/08/01/django-and-lambda-with-cdk-and-api-gateway.html",[19],"AWS Lambda"," (Lambda + API Gateway, without using Zappa or Serverless Framework). Both projects focused on automating as much of the setup and operation as possible using DevOps patterns: Infrastructure as Code, GitOps, CI/CD and docker containers. I used AWS resources exclusively (with the exception of GitLab) with the help of ",[15,30189,30192],{"href":30190,"rel":30191},"https://aws.amazon.com/cdk/",[19],"AWS Cloud Development Kit (CDK)",", an awesome tool that I have really come to like. In general I really like AWS, and the more I use it I start to think about what I would do without it. Also, a lot of the feedback I got on these projects recommended to \"just use a VPS\" instead of bothering with AWS because it is complicated, expensive, overkill, etc. This got me thinking about how far I could get in deploying a Django application on a server with little or no external services that AWS has spoiled me with. After a little bit of discomfort and confusion, I was able to check off most of what I was hoping to and came away with a few questions as well. If you are interested to know how I got things setup and and hear some of my thoughts on running Django applications in production, continue reading!",[11,30195,30196],{},"In this article, I'm going to go over my approach to deploying and running Django applications using DigitalOcean Droplets (Linux-based virtual machine that runs on top of virtualized hardware) and block storage volumes (network-based block devices that provide additional data storage for Droplets).",[107,30198,30199],{},[11,30200,30201],{},"I'll also touch on trade-offs between DigitalOcean and AWS and emphasize aspects of the project that confuse/d me with these block quotes.",[11,30203,30204,30205,752],{},"Here's a link to my project that I'll be referencing: ",[15,30206,30207],{"href":30207,"rel":30208},"https://gitlab.com/briancaffey/digital-ocean-docker-swarm-django-traefik",[19],[11,30210,30211],{},"The project setup is a combination of some of the best practices I have picked up along the way as well as some very helpful guides, repositories and blog posts that I'll do my best to reference throughout this article.",[168,30213,24342],{"id":24341},[11,30215,30216],{},"Here are some of the key parts of the project that I'll go over:",[916,30218,30219,30222,30225,30228,30231,30234,30240,30243,30246,30249,30252],{},[919,30220,30221],{},"DigitalOcean and GitLab setup",[919,30223,30224],{},"Creating an A Record that points to our Droplet IP",[919,30226,30227],{},"Using a prebuilt VM image that ships with docker and docker-compose",[919,30229,30230],{},"Setting up the REX-Ray storage driver to automatically provision Digital Ocean block storage volumes",[919,30232,30233],{},"Setting up a docker swarm cluster",[919,30235,30236,30237,30239],{},"Setting up a ",[33,30238,22749],{}," file to build images and push them to a private GitLab CI project registry",[919,30241,30242],{},"Writing a docker-compose file to configure the services (containers) that will support the application",[919,30244,30245],{},"Deploying a stack to the docker swarm cluster on DigitalOcean from our GitLab CI environment over SSH",[919,30247,30248],{},"Django project settings and management commands for our Postgres database and static files",[919,30250,30251],{},"Monitoring, logging and debugging",[919,30253,30254],{},"Destroying the environment + cleanup",[11,30256,30257,30258,30263,30264,30269,30270,30275],{},"Before I dig into all of this, I recommend that you check out ",[15,30259,30262],{"href":30260,"rel":30261},"https://mattsegal.dev/django-prod-architectures.html",[19],"this article about Django production architectures by Matt Segal",". This is a great primer for a lot of what I'll be talking about and it includes some great visualizations. ",[15,30265,30268],{"href":30266,"rel":30267},"https://mattsegal.dev",[19],"mattsegal.dev"," has lots of good content related to Django, I also recommend checking out ",[15,30271,30274],{"href":30272,"rel":30273},"https://mattsegal.dev/nginx-django-reverse-proxy-config.html",[19],"this article about how NGINX is used with Django",". Thanks for the great resources, Matt!",[168,30277,30279],{"id":30278},"digitalocean-setup","DigitalOcean setup",[916,30281,30282,30285,30292,30299],{},[919,30283,30284],{},"Sign up for a new DigitalOcean account if you don't already have one",[919,30286,30287,30288],{},"Create a DigitalOcean project ",[15,30289,30290],{"href":30290,"rel":30291},"https://cloud.digitalocean.com/projects/new",[19],[919,30293,30294,30295],{},"Create a personal access token (we will use this to configure a docker addon that will provision block storage volumes automatically) ",[15,30296,30297],{"href":30297,"rel":30298},"https://cloud.digitalocean.com/account/api/tokens",[19],[919,30300,30301,30302,30307],{},"Create and add an SSH key to your account. This is a pretty simple step, but DigitalOcean still has really thorough documentation on how to do this (see ",[15,30303,30306],{"href":30304,"rel":30305},"https://www.digitalocean.com/docs/droplets/how-to/add-ssh-keys/",[19],"this article"," for more information)",[168,30309,30311],{"id":30310},"prebuilt-docker-vm-image","Prebuilt Docker VM image",[11,30313,30314,30315,30320,30321,30324,30325,30327,30328,30331],{},"From the ",[15,30316,30319],{"href":30317,"rel":30318},"https://cloud.digitalocean.com/droplets/new",[19],"Create Droplets"," page, select ",[33,30322,30323],{},"Marketplace"," and search for ",[33,30326,15298],{},". Select the ",[33,30329,30330],{},"Docker 5:19.03.1~3 18.04"," image. Note that this VM is Ubuntun 18.04 with Docker Community Edition and docker-compose pre-installed.",[11,30333,30334],{},"Select the basic plan, and then scroll to the left to choose the $5.00/month option. Select a datacenter region. Most of these regions should be OK, but you should verify that the region you have selected supports volumes (they may all support volumes, but there are some DO features that are not supported accross all regiongs, similar to AWS). Do not select a VPC or any of the additional options.",[11,30336,30337],{},"For Authentication, select the SSH key that you created earlier.",[11,30339,30340],{},"Take note of the Droplet's IP address; we will use this in the next step.",[168,30342,30344],{"id":30343},"gitlab-setup","GitLab setup",[11,30346,30347,30348,30350],{},"Create a new GitLab project and clone it locally. You can also clone or fork my project and use that as a starting point. Go to ",[33,30349,22904],{}," in your GitLab project and add the following environment variables:",[916,30352,30353,30365,30371,30377,30383],{},[919,30354,30355,30357,30358,30361,30362],{},[33,30356,18293],{},": the value should start with ",[33,30359,30360],{},"-----BEGIN RSA PRIVATE KEY-----"," and end with ",[33,30363,30364],{},"-----END RSA PRIVATE KEY-----",[919,30366,30367,30370],{},[33,30368,30369],{},"DROPLET_IP",": the IP address of the droplet you just created",[919,30372,30373,30376],{},[33,30374,30375],{},"POSTGRES_PASSWORD",": a secure password that we will use for our Postgres database (we will share this with our Django application later on)",[919,30378,30379,30382],{},[33,30380,30381],{},"SECRET_KEY",": a random secret key to use for our Django application.",[919,30384,30385,30388,30389],{},[33,30386,30387],{},"DEBUG",": the number ",[33,30390,10165],{},[168,30392,30394],{"id":30393},"a-record","A Record",[11,30396,30397,30398,30401,30402,30407],{},"By the end of this project you will be able to deploy your Django application to a live domain name provided that you have one. All you need to do is create an A Record that points to the Droplet IP. There are no DNS configuration changes to make inside of DigitalOcean. I'm using a domain that I purchased through Route53. You can get a free ",[33,30399,30400],{},".tk"," domain from ",[15,30403,30406],{"href":30404,"rel":30405},"https://www.freenom.com/en/freeandpaiddomains.html",[19],"freenom",". I have used this before and it is a great option for testing things out.",[168,30409,30411],{"id":30410},"ssh-into-your-digitalocean-droplet","SSH into your DigitalOcean Droplet",[11,30413,30414],{},"You can do this with the following command:",[26,30416,30419],{"className":30417,"code":30418,"language":31},[29],"ssh -i ~/.ssh/a1_rsa root@123.45.578.91\n",[33,30420,30418],{"__ignoreMap":35},[11,30422,30423,30426],{},[33,30424,30425],{},"a1_rsa"," is the private key I added to GitHub. You can logout for now, but keep this command handy, because we will be coming back to our Droplet via SSH shortly.",[168,30428,30430],{"id":30429},"add-the-rex-ray-docker-plugin","Add the REX-Ray docker plugin",[11,30432,30433,30434,752],{},"This step is very simple, you can follow along with this short guide: ",[15,30435,30436],{"href":30436,"rel":30437},"https://www.digitalocean.com/community/questions/how-to-attach-digitalocean-block-storage-to-docker-container",[19],[11,30439,30440],{},"There is basically one command to run:",[26,30442,30445],{"className":30443,"code":30444,"language":31},[29],"docker plugin install rexray/dobs DOBS_TOKEN=YOUR_DIGITALOCEAN_TOKEN DOBS_REGION=nyc1 LINUX_VOLUME_FILEMODE=0775\n",[33,30446,30444],{"__ignoreMap":35},[11,30448,30449,30450,30453,30454,30457],{},"You will need to make sure that you replace ",[33,30451,30452],{},"YOUR_DIGITALOCEAN_TOKEN"," with the personal access token you added earlier. Also, ",[33,30455,30456],{},"DOBS_REGION"," should be the region you selected for your Droplet earlier.",[11,30459,30460],{},"Check that the plugin was installed correctly with:",[26,30462,30465],{"className":30463,"code":30464,"language":31},[29],"docker plugin ls\n",[33,30466,30464],{"__ignoreMap":35},[11,30468,30469,30470,358],{},"Here's a quick intro to REX-Ray from ",[15,30471,30474],{"href":30472,"rel":30473},"https://rexray.readthedocs.io/en/stable/",[19],"rexray.readthedocs.io",[107,30476,30477],{},[11,30478,30479],{},"REX-Ray is an open source, storage management solution designed to support container runtimes such as Docker and Mesos. REX-Ray enables stateful applications, such as databases, to persist and maintain its data after the life cycle of the container has ended. Built-in high availability enables orchestrators such as Docker Swarm, Kubernetes, and Mesos Frameworks like Marathon to automatically orchestrate storage tasks between hosts in a cluster.",[11,30481,30482],{},"In the context of this project, REX-Ray will automate the creation of DigitalOcean Block Storage Volumes. We will talk about volumes and how they are used later on in this article.",[168,30484,30486],{"id":30485},"gitlab-ciyml",[33,30487,22749],{},[11,30489,30490,30492,30493,30498,30499,30504],{},[33,30491,22749],{}," is a file that configures pipelines when code is pushed to GitLab, similar to how GitHub Actions work with GitHub. This single file is a huge topic, if you are unfamiliar with GitLab CI, you might want to have a look over ",[15,30494,30497],{"href":30495,"rel":30496},"https://docs.gitlab.com/ee/ci/yaml/",[19],"this page from the GitLab documentation"," which goes over all of the configuration options with many examples. Also, ",[15,30500,30503],{"href":30501,"rel":30502},"https://docs.gitlab.com/ee/ci/variables/predefined_variables.html",[19],"this documentation page"," covers the predefined environment variables that are made available to GitLab CI pipelines. I'm using these in a few different places as we will see shortly.",[11,30506,30507,30508,30510,30511,637,30514,1172,30517,30520,30521,1172,30523,30525],{},"CI/CD pipelines that I define with ",[33,30509,22749],{}," typically contain three stages: ",[33,30512,30513],{},"test",[33,30515,30516],{},"build",[33,30518,30519],{},"deploy",". We will focus on the ",[33,30522,30516],{},[33,30524,30519],{}," stages for now (reference the article on my Fargate project linked above for reference on setting up unit tests with pytest).",[11,30527,30528,30531,30532,30534],{},[33,30529,30530],{},"build-backend"," is the name of a GitLab CI job that builds and tags a docker image from the source code in the ",[33,30533,12291],{}," directory of this project and pushes the tagged container image to a private image registry on gitlab.com that we will use later.",[11,30536,30537,30538,358],{},"Here's the YAML code for ",[33,30539,30530],{},[26,30541,30543],{"className":8656,"code":30542,"language":8658,"meta":35,"style":35},"build-backend:\n  stage: build\n  image: docker:19.03.1\n  services:\n    - docker:19.03.5-dind\n  before_script:\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n  script:\n    - |\n      docker build \\\n        -t $CI_REGISTRY_IMAGE/backend:$CI_COMMIT_SHORT_SHA \\\n        -f backend/docker/Dockerfile.prod \\\n        ./backend/\n    - docker push $CI_REGISTRY_IMAGE/backend:$CI_COMMIT_SHORT_SHA\n",[33,30544,30545,30551,30559,30568,30575,30582,30588,30595,30601,30607,30612,30617,30622,30627],{"__ignoreMap":35},[187,30546,30547,30549],{"class":189,"line":190},[187,30548,30530],{"class":2516},[187,30550,2520],{"class":577},[187,30552,30553,30555,30557],{"class":189,"line":249},[187,30554,22797],{"class":2516},[187,30556,585],{"class":577},[187,30558,24040],{"class":196},[187,30560,30561,30563,30565],{"class":189,"line":312},[187,30562,8954],{"class":2516},[187,30564,585],{"class":577},[187,30566,30567],{"class":196},"docker:19.03.1\n",[187,30569,30570,30573],{"class":189,"line":319},[187,30571,30572],{"class":2516},"  services",[187,30574,2520],{"class":577},[187,30576,30577,30579],{"class":189,"line":325},[187,30578,14709],{"class":577},[187,30580,30581],{"class":196},"docker:19.03.5-dind\n",[187,30583,30584,30586],{"class":189,"line":686},[187,30585,22834],{"class":2516},[187,30587,2520],{"class":577},[187,30589,30590,30592],{"class":189,"line":697},[187,30591,14709],{"class":577},[187,30593,30594],{"class":196},"docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n",[187,30596,30597,30599],{"class":189,"line":1291},[187,30598,22861],{"class":2516},[187,30600,2520],{"class":577},[187,30602,30603,30605],{"class":189,"line":1306},[187,30604,14709],{"class":577},[187,30606,7477],{"class":573},[187,30608,30609],{"class":189,"line":1434},[187,30610,30611],{"class":196},"      docker build \\\n",[187,30613,30614],{"class":189,"line":2599},[187,30615,30616],{"class":196},"        -t $CI_REGISTRY_IMAGE/backend:$CI_COMMIT_SHORT_SHA \\\n",[187,30618,30619],{"class":189,"line":2607},[187,30620,30621],{"class":196},"        -f backend/docker/Dockerfile.prod \\\n",[187,30623,30624],{"class":189,"line":2621},[187,30625,30626],{"class":196},"        ./backend/\n",[187,30628,30629,30631],{"class":189,"line":2631},[187,30630,14709],{"class":577},[187,30632,30633],{"class":196},"docker push $CI_REGISTRY_IMAGE/backend:$CI_COMMIT_SHORT_SHA\n",[11,30635,30636,30639,30640,30643,30644,30646],{},[33,30637,30638],{},"build-nginx"," is almost identical, but the ",[33,30641,30642],{},"docker build"," arguments are slightly different. There are three arguments for the ",[33,30645,30642],{}," command that I'm using here:",[2276,30648,30649,30654,30660],{},[919,30650,30651,30653],{},[33,30652,24141],{},": the tag to tag the built image with",[919,30655,30656,30659],{},[33,30657,30658],{},"-f",": the Dockerfile to be used for building the image",[919,30661,30662,30663,30665],{},"the ",[33,30664,27106],{}," that is sent to the docker daemon when we build the image",[11,30667,30668,30670,30671,1172,30674,5857,30677,30680,30681,23120,30684,30686,30687,30690,30691,30693,30694,30696,30697,30699],{},[33,30669,24141],{}," makes use of two predefined GitLab CI variables: ",[33,30672,30673],{},"CI_REGISTRY_IMAGE",[33,30675,30676],{},"CI_COMMIT_SHORT_SHA",[33,30678,30679],{},"$CI_REGISTRY_IMAGE"," is the URL for the private image registry on gitlab.com that we push our images to that is specific to our project: ",[33,30682,30683],{},"registry.gitlab.com/\u003Cgitlab_username>/\u003Cproject_name>",[33,30685,30676],{}," is an character alphanumeric value that contains the truncated name of the commit hash, this is known as the ",[33,30688,30689],{},"tag",", even though we pass in more than just this value. We combine these two values with ",[33,30692,20174],{}," and the name of the image we are building, such as ",[33,30695,12291],{},", so the full value being passed to ",[33,30698,24141],{}," is:",[26,30701,30704],{"className":30702,"code":30703,"language":31},[29],"registry.gitlab.com/gitlab-username/my-project/backend:abcd1234\n",[33,30705,30703],{"__ignoreMap":35},[11,30707,30708,30710,30711,30714,30715,30717],{},[33,30709,30658],{}," is the path to the ",[33,30712,30713],{},"Dockerfile"," we are using relative to the directory where we are running the ",[33,30716,30642],{}," command, which is the root directory of the project.",[11,30719,30720,30721,30723,30724,1172,30727,30730,30731,1172,30733,12272,30736,30738,30739,30741,30742,30744,30745,30741,30747,30749,30750,30752,30753,30755,30756,30758],{},"The final argument defines the context that we are using to build the image, and this is an important part for understanding how Docker works. This argument defines the directory that is zipped up and sent to the docker daemon via the docker API. When we build an image with ",[33,30722,30642],{},", we are essentially using the docker CLI to make a POST request to our docker daemon (server) where the POST data contains all of the files that we will have access to in the steps of the Dockerfile (such as ",[33,30725,30726],{},"ADD",[33,30728,30729],{},"COPY"," -- we will get to these soon). There's a key difference between the ",[33,30732,12291],{},[33,30734,30735],{},"nginx",[33,30737,30642],{}," commands: the context for ",[33,30740,12291],{}," is ",[33,30743,12291],{},", but the context for ",[33,30746,30735],{},[33,30748,752],{}," (the root of the project). This is because we may want access to another top level directory in our project that contains, for example, a Vue.js or React application, that we will build into our NGINX container. In order to be able to access both files in the ",[33,30751,30735],{}," and the folder containing our frontend app, we need to send a context that contains both of these directories. Sending too many files to to the docker daemon when you run docker build will usually cause the ",[33,30754,30642],{}," command to hang. The first line of output from a ",[33,30757,30642],{}," command should be something like this:",[26,30760,30763],{"className":30761,"code":30762,"language":31},[29],"Sending build context to Docker daemon  24.58kB\n",[33,30764,30762],{"__ignoreMap":35},[11,30766,30767,30768,30771,30772,30774],{},"If this number is too high, you should use a ",[33,30769,30770],{},".dockerignore"," file that ignores any files or directories you don't want to send to the docker daemon (similar to how ",[33,30773,783],{}," works with git).",[11,30776,30777,30778,30781,30782,30785,30786,1172,30789,30792,30793,30796,30797,30799,30800,30802,30803,1172,30805,30807,30808,30810,30811,30813],{},"To be able to pull and push (read and write) images to our private project container image registry, we need login with our docker client using the ",[33,30779,30780],{},"docker login"," command in the ",[33,30783,30784],{},"before_script"," as well two other predefined GitLab CI variables: ",[33,30787,30788],{},"CI_JOB_TOKEN",[33,30790,30791],{},"CI_REGISTRY",". This all happens using a special service called ",[33,30794,30795],{},"docker-in-docker"," which I won't go into too much detail here, but it is a common practice when working with containers in a CI/CD environment that itself which is also based on containers, such as GitLab CI (each job runs in a container -- the key ",[33,30798,6570],{}," -- and can define additional containers -- the ",[33,30801,3613],{}," key -- to help with the CI job). Once the two images for ",[33,30804,12291],{},[33,30806,30735],{}," have been built and pushed, our GitLab CI pipeline moves on to the next stage: ",[33,30809,30519],{},". In the ",[33,30812,30519],{}," stage, we will start these and other containers on our DigitalOcean droplet, so we are getting close, but there is a lot more to explain. Before we deploy our containers, we need to do some one-time setup:",[2276,30815,30816,30819],{},[919,30817,30818],{},"initialize a single-node docker swarm cluster on our Droplet and",[919,30820,30821],{},"create a docker network that our cluster's services (containers) will use",[168,30823,30825],{"id":30824},"setup-a-docker-swarm-cluster","Setup a docker swarm cluster",[11,30827,30828,30829,30832,30833,30835,30836,30839,30840,30843],{},"To setup a docker swarm cluster, SSH into the Droplet with the command we introduced above (",[33,30830,30831],{},"ssh -i ~/.ssh/a1_rsa root@123.45.578.91"," where ",[33,30834,30425],{}," is the name of the private key file -- you can ignore the ",[33,30837,30838],{},"-i ~/.ssh/a1_rsa"," part if you are using an SSH key called ",[33,30841,30842],{},"id_rsa","), and run the following command:",[26,30845,30848],{"className":30846,"code":30847,"language":31},[29],"docker swarm init --advertise-addr DROPLET_IP\n",[33,30849,30847],{"__ignoreMap":35},[11,30851,30852,30853,30855,30856,30859,30860,30864],{},"Replace ",[33,30854,30369],{}," with your Droplet's IP address (e.g. ",[33,30857,30858],{},"123.45.578.91","). Check out ",[15,30861,30306],{"href":30862,"rel":30863},"https://www.digitalocean.com/community/tutorials/how-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-ubuntu-16-04",[19]," for some additional information about using docker swarm on GitLab. It is a little bit outdated, but the main ideas should still hold up. Docker swarm is designed to orchestrate containers running on a group (or swarm) of multiple machines. However, it is perfectly fine to run a single-node cluster as we are doing here.",[11,30866,30867,30868,30870,30871,30874,30875,30877,30878,30880,30881,30883],{},"Docker swarm uses docker-compose files, but using docker swarm is very different from running ",[33,30869,28292],{},", a command which you might see people running both locally and in production and which also uses docker-compose files. As a best practice, you should not be using ",[33,30872,30873],{},"docker-compose"," (the command) in production. Many people do this, and several official tutorials will often end with \"now just run ",[33,30876,28292],{}," and you are done\". The first time I ran containers in the cloud I pulled my git repo into a VM, installed docker and docker-compose and ran ",[33,30879,28292],{},". It is pretty easy and it works very similarly in both local and production environments, but this guide will be using ",[33,30882,30873],{}," in production. There is more I could say here, but the main point is that docker swarm is a simplified version of something like Kubernetes, but it comes built-in to docker and is very simple to use.",[168,30885,30887],{"id":30886},"defining-an-overlay-network","Defining an overlay network",[11,30889,30890],{},"SSH into your droplet and run the following command:",[26,30892,30895],{"className":30893,"code":30894,"language":31},[29],"docker network create --driver=overlay traefik-public\n",[33,30896,30894],{"__ignoreMap":35},[107,30898,30899],{},[11,30900,30901],{},"Usually we define networks in our docker-compose file, but this network needs to be defined first and then referenced in our docker-compose file. Here's a thread on SO that goes into a little bit more on why this is necessary, but I still don't have a very clear idea of why this is the case. With another configuration or perhaps docker-compose version, this may not be needed. I'll update this part of the article if I figure anything out.",[11,30903,30904,30905,30908,30909,30911,30912,30915,30916,30921],{},"Let's go over one more docker concept that will helpful in the next few steps. When you run ",[33,30906,30907],{},"docker ps"," on your local machine, the docker CLI first looks to see if the ",[33,30910,28566],{}," environment variable is set. If it is not, then docker defaults to ",[33,30913,30914],{},"unix:///var/run/docker.sock",", a UNIX socket. Check out ",[15,30917,30920],{"href":30918,"rel":30919},"https://stackoverflow.com/questions/35110146/can-anyone-explain-docker-sock",[19],"this SO post"," titled \"Can anyone explain docker.sock?\"",[11,30923,30924],{},"We change the docker host that our local docker CLI is talking to by setting this environment variable, and one nice way to set this environment variables uses an SSH connection:",[26,30926,30929],{"className":30927,"code":30928,"language":31},[29],"DOCKER_HOST=ssh://root@$DOCKER_IP\n",[33,30930,30928],{"__ignoreMap":35},[11,30932,30933,30934,752],{},"See this article for a more in-depth discussion: ",[15,30935,30936],{"href":30936,"rel":30937},"https://www.digitalocean.com/community/tutorials/how-to-use-a-remote-docker-server-to-speed-up-your-workflow",[19],[11,30939,30940,30941,30943,30944,30946],{},"You can try this out locally. Run a container locally, check it with ",[33,30942,30907],{},", then export the ",[33,30945,28566],{}," environment variable with the following command:",[26,30948,30951],{"className":30949,"code":30950,"language":31},[29],"export DOCKER_HOST=ssh://root@123.45.678.91\n",[33,30952,30950],{"__ignoreMap":35},[11,30954,30955,30956,30959,30960,30962],{},"Replacing ",[33,30957,30958],{},"123.45.678.91"," with your Droplet IP. Run ",[33,30961,30907],{}," again and you should see nothing (or any other containers that you started on your Droplet). Finally, run:",[26,30964,30967],{"className":30965,"code":30966,"language":31},[29],"unset DOCKER_HOST\n",[33,30968,30966],{"__ignoreMap":35},[11,30970,30971,30972,30974,30975,30978],{},"Running ",[33,30973,30907],{}," again you should see the containers on your machine. We will be using this idea in the next step when we look at the ",[33,30976,30977],{},"docker stack deploy"," command.",[168,30980,30982],{"id":30981},"docker-stack-deploy",[33,30983,30977],{},[11,30985,30986,30987,30989,30990,30992,30993,18410],{},"Now that we have done our one-time-setup steps, let's look at the ",[33,30988,30519],{}," stage of ",[33,30991,22749],{},", the GitLab CI job that will get our containers running on our Droplet. First, let's break down the ",[33,30994,30995],{},"deploy-digital-ocean",[26,30997,30999],{"className":8656,"code":30998,"language":8658,"meta":35,"style":35},"deploy-digital-ocean:\n  stage: deploy\n  image: docker:19.03.1\n  services:\n    - docker:19.03.5-dind\n  variables:\n    DOCKER_HOST: 'ssh://root@$DROPLET_IP'\n  before_script:\n    - apk update && apk add openssh-client bash\n    - mkdir -p ~/.ssh\n    - echo \"$SSH_PRIVATE_KEY\" > ~/.ssh/id_rsa\n    - chmod 600 ~/.ssh/id_rsa\n    - eval \"$(ssh-agent -s)\"\n    - ssh-add ~/.ssh/id_rsa\n    - ssh-keyscan -H $DROPLET_IP >> ~/.ssh/known_hosts\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n  script:\n    - docker stack deploy -c stack.yml my-stack --with-registry-auth\n",[33,31000,31001,31007,31015,31023,31029,31035,31042,31052,31058,31065,31072,31079,31086,31093,31100,31107,31113,31119],{"__ignoreMap":35},[187,31002,31003,31005],{"class":189,"line":190},[187,31004,30995],{"class":2516},[187,31006,2520],{"class":577},[187,31008,31009,31011,31013],{"class":189,"line":249},[187,31010,22797],{"class":2516},[187,31012,585],{"class":577},[187,31014,7546],{"class":196},[187,31016,31017,31019,31021],{"class":189,"line":312},[187,31018,8954],{"class":2516},[187,31020,585],{"class":577},[187,31022,30567],{"class":196},[187,31024,31025,31027],{"class":189,"line":319},[187,31026,30572],{"class":2516},[187,31028,2520],{"class":577},[187,31030,31031,31033],{"class":189,"line":325},[187,31032,14709],{"class":577},[187,31034,30581],{"class":196},[187,31036,31037,31040],{"class":189,"line":686},[187,31038,31039],{"class":2516},"  variables",[187,31041,2520],{"class":577},[187,31043,31044,31047,31049],{"class":189,"line":697},[187,31045,31046],{"class":2516},"    DOCKER_HOST",[187,31048,585],{"class":577},[187,31050,31051],{"class":196},"'ssh://root@$DROPLET_IP'\n",[187,31053,31054,31056],{"class":189,"line":1291},[187,31055,22834],{"class":2516},[187,31057,2520],{"class":577},[187,31059,31060,31062],{"class":189,"line":1306},[187,31061,14709],{"class":577},[187,31063,31064],{"class":196},"apk update && apk add openssh-client bash\n",[187,31066,31067,31069],{"class":189,"line":1434},[187,31068,14709],{"class":577},[187,31070,31071],{"class":196},"mkdir -p ~/.ssh\n",[187,31073,31074,31076],{"class":189,"line":2599},[187,31075,14709],{"class":577},[187,31077,31078],{"class":196},"echo \"$SSH_PRIVATE_KEY\" > ~/.ssh/id_rsa\n",[187,31080,31081,31083],{"class":189,"line":2607},[187,31082,14709],{"class":577},[187,31084,31085],{"class":196},"chmod 600 ~/.ssh/id_rsa\n",[187,31087,31088,31090],{"class":189,"line":2621},[187,31089,14709],{"class":577},[187,31091,31092],{"class":196},"eval \"$(ssh-agent -s)\"\n",[187,31094,31095,31097],{"class":189,"line":2631},[187,31096,14709],{"class":577},[187,31098,31099],{"class":196},"ssh-add ~/.ssh/id_rsa\n",[187,31101,31102,31104],{"class":189,"line":2642},[187,31103,14709],{"class":577},[187,31105,31106],{"class":196},"ssh-keyscan -H $DROPLET_IP >> ~/.ssh/known_hosts\n",[187,31108,31109,31111],{"class":189,"line":2653},[187,31110,14709],{"class":577},[187,31112,30594],{"class":196},[187,31114,31115,31117],{"class":189,"line":2665},[187,31116,22861],{"class":2516},[187,31118,2520],{"class":577},[187,31120,31121,31123],{"class":189,"line":2674},[187,31122,14709],{"class":577},[187,31124,31125],{"class":196},"docker stack deploy -c stack.yml my-stack --with-registry-auth\n",[11,31127,31128,31129,1172,31131,31133,31134,31136,31137,765,31139,31142,31143,31145,31146,31149,31150,15754,31153,31155,31156,358],{},"This job uses the same ",[33,31130,6570],{},[33,31132,3613],{}," that our ",[33,31135,30516],{}," stage jobs used. Notice that we set ",[33,31138,28566],{},[33,31140,31141],{},"\"ssh://root@$DROPLET_IP\"",", this means that any docker CLI commands in this job will be communicating with the docker daemon on our Droplet. The ",[33,31144,30784],{}," has a lot going on, but all we are doing is preparing to use the SSH private key that we previously added to our GitLab project's CI/CD environment variables. The base image for this job, ",[33,31147,31148],{},"docker:19.03.1"," is based on Alpine Linus. This version of Linux is super light weight and doesn't come with ",[33,31151,31152],{},"openssh-client",[33,31154,183],{},", so our first step is to install these with the Alpine package manager, ",[33,31157,31158],{},"apk",[26,31160,31163],{"className":31161,"code":31064,"language":31162,"meta":35,"style":35},"language-sh shiki shiki-themes github-light github-dark","sh",[33,31164,31165],{"__ignoreMap":35},[187,31166,31167,31169,31172,31175,31177,31179,31182],{"class":189,"line":190},[187,31168,31158],{"class":193},[187,31170,31171],{"class":196}," update",[187,31173,31174],{"class":577}," && ",[187,31176,31158],{"class":193},[187,31178,243],{"class":196},[187,31180,31181],{"class":196}," openssh-client",[187,31183,31184],{"class":196}," bash\n",[11,31186,31187,31188,31190,31191,31193,31194,31197,31198,1172,31201,358],{},"Next, we add the ",[33,31189,18293],{}," environment variable into the body of the ",[33,31192,30842],{}," private key file, change the permission of this file and then add the key to our SSH agent. Here's an excerpt from ",[33,31195,31196],{},"man ssh-agent"," that provides a little bit more context into why we need to run ",[33,31199,31200],{},"eval \"$(ssh-agent -s)\"",[33,31202,31203],{},"ssh-add ~/.ssh/id_rsa",[26,31205,31208],{"className":31206,"code":31207,"language":31},[29],"DESCRIPTION\n     ssh-agent is a program to hold private keys used for public key authentication (RSA, DSA, ECDSA, Ed25519)ssh-agent is usually started in the beginning of an X-session or a login session, and all other windows or programs are started as clients to the ssh-agent program.  Through use of environment variables the agent can be located and automatically used for authentication when logging in to other machines using ssh(1).\n\n     The agent initially does not have any private keys.  Keys are added using ssh(1) (see AddKeysToAgent in ssh_config(5) for details) or ssh-add(1).  Multiple identities may be stored in ssh-agent concurrently and ssh(1) will automatically use them if present.  ssh-add(1) is also used to remove keys from ssh-agent and to query the keys that are held in one.\n",[33,31209,31207],{"__ignoreMap":35},[11,31211,31212,31213,31216,31217,31220],{},"Next, ",[33,31214,31215],{},"ssh-keyscan -H $DROPLET_IP >> ~/.ssh/known_hosts"," tells our SSH agent about our Droplet so that it doesn't prompt us with a ",[33,31218,31219],{},"Do you want to add this server to known hosts? (yes/no)",", or whatever the equivalent of that is for the docker CLI when it attempts to connect to a remote docker daemon over SSH.",[11,31222,31223],{},"Finally, we login to our our GitLab private registry using the same command from before when we built and pushed images to our private registry on gitlab.com:",[26,31225,31227],{"className":31226,"code":30594,"language":31},[29],[33,31228,30594],{"__ignoreMap":35},[11,31230,31231,31232,1172,31234,31236],{},"This essentially gives our DigitalOcean Droplet access to the ",[33,31233,12291],{},[33,31235,30735],{}," images in our private GitLab CI image registry, even tho we are running this command in a contain, in a container that is probably running in Kubernetes on GCP. Next, we are actually going to use these images.",[11,31238,31239,31240,31242],{},"The last command in the ",[33,31241,30995],{}," job is:",[26,31244,31246],{"className":31245,"code":31125,"language":31},[29],[33,31247,31125],{"__ignoreMap":35},[11,31249,31250,31251,5857,31255,31258],{},"Check out this link from the docker docs on docker stacks ",[15,31252,31253],{"href":31253,"rel":31254},"https://docs.docker.com/engine/swarm/stack-deploy/",[19],[33,31256,31257],{},"--with-registry-auth"," is important, our command will complete if this is not included, but our application won't start.",[168,31260,31262],{"id":31261},"stackyml",[33,31263,31264],{},"stack.yml",[11,31266,31267,31268,31270,31271,31273,31274,31276,31277,31279],{},"Now we are ready to tackle the last big file in our repo: ",[33,31269,31264],{},". This is a the docker-compose file that we use to deploy our project. The only reason we needed to run ",[33,31272,30780],{}," above is because ",[33,31275,31264],{}," references the two images we built and pushed to our GitLab private repo. There's a lot going on in this file, let's start with the ",[33,31278,12291],{}," service:",[911,31281,12291],{"id":12291},[26,31283,31285],{"className":8656,"code":31284,"language":8658,"meta":35,"style":35},"version: '3.4'\nservices:\n  backend:\n  image: ${CI_REGISTRY_IMAGE}/backend:${CI_COMMIT_SHORT_SHA}\n  networks:\n    - main\n  environment:\n    - POSTGRES_PASSWORD\n    - SECRET_KEY\n    - DEBUG\n  volumes:\n    - backendassets:/code/assets\n  depends_on:\n    - postgres\n    - redis\n    - web\n",[33,31286,31287,31297,31303,31310,31319,31326,31333,31340,31347,31354,31361,31368,31375,31382,31389,31395],{"__ignoreMap":35},[187,31288,31289,31292,31294],{"class":189,"line":190},[187,31290,31291],{"class":2516},"version",[187,31293,585],{"class":577},[187,31295,31296],{"class":196},"'3.4'\n",[187,31298,31299,31301],{"class":189,"line":249},[187,31300,3613],{"class":2516},[187,31302,2520],{"class":577},[187,31304,31305,31308],{"class":189,"line":312},[187,31306,31307],{"class":2516},"  backend",[187,31309,2520],{"class":577},[187,31311,31312,31314,31316],{"class":189,"line":319},[187,31313,8954],{"class":2516},[187,31315,585],{"class":577},[187,31317,31318],{"class":196},"${CI_REGISTRY_IMAGE}/backend:${CI_COMMIT_SHORT_SHA}\n",[187,31320,31321,31324],{"class":189,"line":325},[187,31322,31323],{"class":2516},"  networks",[187,31325,2520],{"class":577},[187,31327,31328,31330],{"class":189,"line":686},[187,31329,14709],{"class":577},[187,31331,31332],{"class":196},"main\n",[187,31334,31335,31338],{"class":189,"line":697},[187,31336,31337],{"class":2516},"  environment",[187,31339,2520],{"class":577},[187,31341,31342,31344],{"class":189,"line":1291},[187,31343,14709],{"class":577},[187,31345,31346],{"class":196},"POSTGRES_PASSWORD\n",[187,31348,31349,31351],{"class":189,"line":1306},[187,31350,14709],{"class":577},[187,31352,31353],{"class":196},"SECRET_KEY\n",[187,31355,31356,31358],{"class":189,"line":1434},[187,31357,14709],{"class":577},[187,31359,31360],{"class":196},"DEBUG\n",[187,31362,31363,31366],{"class":189,"line":2599},[187,31364,31365],{"class":2516},"  volumes",[187,31367,2520],{"class":577},[187,31369,31370,31372],{"class":189,"line":2607},[187,31371,14709],{"class":577},[187,31373,31374],{"class":196},"backendassets:/code/assets\n",[187,31376,31377,31380],{"class":189,"line":2621},[187,31378,31379],{"class":2516},"  depends_on",[187,31381,2520],{"class":577},[187,31383,31384,31386],{"class":189,"line":2631},[187,31385,14709],{"class":577},[187,31387,31388],{"class":196},"postgres\n",[187,31390,31391,31393],{"class":189,"line":2642},[187,31392,14709],{"class":577},[187,31394,3656],{"class":196},[187,31396,31397,31399],{"class":189,"line":2653},[187,31398,14709],{"class":577},[187,31400,31401],{"class":196},"web\n",[11,31403,6131,31404,31406,31407,31409],{},[33,31405,6570],{}," is essentially what we defined in ",[33,31408,22749],{},", but the syntax is slightly different:",[26,31411,31413],{"className":31412,"code":31318,"language":31},[29],[33,31414,31318],{"__ignoreMap":35},[11,31416,31417,31418,31420],{},"We pass environment variables that we defined in GitLab CI via the ",[33,31419,13749],{}," key.",[11,31422,31423,31424,31427,31428,31431,31432,31435,31436,31439],{},"The volume ",[33,31425,31426],{},"backendassets"," is used for storing static assets (CSS, JS, etc.) as well as media assets (images, videos, any other file type). We mount this directory at ",[33,31429,31430],{},"/code/assets"," and then define our ",[33,31433,31434],{},"STATIC_ROOT"," in Django's ",[33,31437,31438],{},"settings.py"," to be:",[26,31441,31443],{"className":10554,"code":31442,"language":10556,"meta":35,"style":35},"os.path.join(BASE_DIR, \"assets\", \"static\")\n",[33,31444,31445],{"__ignoreMap":35},[187,31446,31447],{"class":189,"line":190},[187,31448,31442],{},[11,31450,31451,31452,31454],{},"Later, when we run ",[33,31453,13071],{},", files are copied to this location in our container, and since this is the location of the volume, the files are actually copied to the volume and will be persisted if we destroy the backend container and restart it. When the container restarts, the volume is mounted again and the static files will still be available to our application.",[11,31456,31457,1172,31460,31462,31463,31465,31466,31469,31470,31472,31473,31476],{},[33,31458,31459],{},"network",[33,31461,15772],{}," related to to the other services that this application will communicate with. ",[33,31464,11677],{}," is a network defined in the ",[33,31467,31468],{},"networks"," part of ",[33,31471,31264],{},", notice that we reference the ",[33,31474,31475],{},"traefik-public"," network here that we created earlier, as well.",[107,31478,31479],{},[11,31480,31481,31482,31485],{},"Depends on helps with service startup order, but it is a better practice to use ",[33,31483,31484],{},"./wait-for-it.sh",". However, I have never had any issues related to startup order. I'll try adding this later to make things more robust.",[11,31487,31488,1172,31490,31492,31493,31495,31496,1172,31498,31500,31501,637,31503,1172,31505,31507,31508,31510,31511,31514,31515,31518,31519,31521],{},[33,31489,28248],{},[33,31491,12985],{}," will start before ",[33,31494,12291],{},". Our Django application will communicate to these services by their hostnames: ",[33,31497,28248],{},[33,31499,12985],{},". The fact that ",[33,31502,12291],{},[33,31504,28248],{},[33,31506,12985],{}," are all on the same network (",[33,31509,11677],{},") means that they can resolve each other by these names. For example, the connection string to redis will look like: ",[33,31512,31513],{},"redis://redis:6379",". Let's look at the ",[33,31516,31517],{},"DATABASES"," configuration in ",[33,31520,31438],{}," to see how we connect to Postgres:",[26,31523,31525],{"className":10554,"code":31524,"language":10556,"meta":35,"style":35},"DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.postgresql_psycopg2\",\n        \"NAME\": os.environ.get(\"POSTGRES_NAME\", \"postgres\"),\n        \"USER\": os.environ.get(\"POSTGRES_USERNAME\", \"postgres\"),\n        \"PASSWORD\": os.environ.get(\"POSTGRES_PASSWORD\", \"postgres\"),\n        \"HOST\": os.environ.get(\"POSTGRES_SERVICE_HOST\", \"postgres\"),\n        \"PORT\": os.environ.get(\"POSTGRES_SERVICE_PORT\", 5432),\n    }\n}\n",[33,31526,31527,31532,31537,31542,31547,31552,31557,31562,31567,31571],{"__ignoreMap":35},[187,31528,31529],{"class":189,"line":190},[187,31530,31531],{},"DATABASES = {\n",[187,31533,31534],{"class":189,"line":249},[187,31535,31536],{},"    \"default\": {\n",[187,31538,31539],{"class":189,"line":312},[187,31540,31541],{},"        \"ENGINE\": \"django.db.backends.postgresql_psycopg2\",\n",[187,31543,31544],{"class":189,"line":319},[187,31545,31546],{},"        \"NAME\": os.environ.get(\"POSTGRES_NAME\", \"postgres\"),\n",[187,31548,31549],{"class":189,"line":325},[187,31550,31551],{},"        \"USER\": os.environ.get(\"POSTGRES_USERNAME\", \"postgres\"),\n",[187,31553,31554],{"class":189,"line":686},[187,31555,31556],{},"        \"PASSWORD\": os.environ.get(\"POSTGRES_PASSWORD\", \"postgres\"),\n",[187,31558,31559],{"class":189,"line":697},[187,31560,31561],{},"        \"HOST\": os.environ.get(\"POSTGRES_SERVICE_HOST\", \"postgres\"),\n",[187,31563,31564],{"class":189,"line":1291},[187,31565,31566],{},"        \"PORT\": os.environ.get(\"POSTGRES_SERVICE_PORT\", 5432),\n",[187,31568,31569],{"class":189,"line":1306},[187,31570,9799],{},[187,31572,31573],{"class":189,"line":1434},[187,31574,1309],{},[11,31576,31577,31578,31581,31582,31585,31586,31589],{},"I have defined the ",[33,31579,31580],{},"HOST"," to be based on the environment variable ",[33,31583,31584],{},"POSTGRES_HOST",", but I have not defined this environment variable, so why didn't I just say ",[33,31587,31588],{},"\"HOST\": \"postgres\"","? I could have, but if I want to change the database in the future, the only change will be adding an environment variable; I won't have to worry about hardcoded values.",[11,31591,31592],{},"I'm choosing to run Postgres in a container and not use a managed database (which DigitalOcean does offer) in order to save on costs and also to get more practice managing my own database. I use RDS with AWS and it handles a lot of things that I don't have to worry about, such as backups, and it allows me to quickly restore from a snapshot. I'm interested in learning more about how I can do these tasks with a database that I run myself.",[911,31594,31595],{"id":30735},"NGINX",[11,31597,31598,31599,31601],{},"The next service we should go over is ",[33,31600,30166],{},", the service that runs the NGINX container that we pushed to our private GitLab image registry. This service has a couple of functions that I'll walk through:",[2276,31603,31604,31607,31610],{},[919,31605,31606],{},"Reverse proxy",[919,31608,31609],{},"Serve static files for Django",[919,31611,31612],{},"Serve a frontend Javascript application",[11,31614,31615,31616,358],{},"Here's the definition of this service in ",[33,31617,31264],{},[26,31619,31621],{"className":8656,"code":31620,"language":8658,"meta":35,"style":35},"services:\n  web:\n    image: ${CI_REGISTRY_IMAGE}/nginx:${CI_COMMIT_SHORT_SHA}\n    networks:\n      - traefik-public\n      - main\n    volumes:\n      - backendassets:/usr/src/app/assets\n    deploy:\n      labels:\n        - 'traefik.enable=true'\n        - 'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'\n        - 'traefik.http.routers.nginx-web.entrypoints=websecure'\n        - 'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'\n        - 'traefik.http.services.nginx-web.loadbalancer.server.port=80'\n",[33,31622,31623,31629,31636,31645,31652,31659,31665,31671,31678,31685,31692,31699,31706,31713,31720],{"__ignoreMap":35},[187,31624,31625,31627],{"class":189,"line":190},[187,31626,3613],{"class":2516},[187,31628,2520],{"class":577},[187,31630,31631,31634],{"class":189,"line":249},[187,31632,31633],{"class":2516},"  web",[187,31635,2520],{"class":577},[187,31637,31638,31640,31642],{"class":189,"line":312},[187,31639,3627],{"class":2516},[187,31641,585],{"class":577},[187,31643,31644],{"class":196},"${CI_REGISTRY_IMAGE}/nginx:${CI_COMMIT_SHORT_SHA}\n",[187,31646,31647,31650],{"class":189,"line":319},[187,31648,31649],{"class":2516},"    networks",[187,31651,2520],{"class":577},[187,31653,31654,31656],{"class":189,"line":325},[187,31655,2610],{"class":577},[187,31657,31658],{"class":196},"traefik-public\n",[187,31660,31661,31663],{"class":189,"line":686},[187,31662,2610],{"class":577},[187,31664,31332],{"class":196},[187,31666,31667,31669],{"class":189,"line":697},[187,31668,3637],{"class":2516},[187,31670,2520],{"class":577},[187,31672,31673,31675],{"class":189,"line":1291},[187,31674,2610],{"class":577},[187,31676,31677],{"class":196},"backendassets:/usr/src/app/assets\n",[187,31679,31680,31683],{"class":189,"line":1306},[187,31681,31682],{"class":2516},"    deploy",[187,31684,2520],{"class":577},[187,31686,31687,31690],{"class":189,"line":1434},[187,31688,31689],{"class":2516},"      labels",[187,31691,2520],{"class":577},[187,31693,31694,31696],{"class":189,"line":2599},[187,31695,2579],{"class":577},[187,31697,31698],{"class":196},"'traefik.enable=true'\n",[187,31700,31701,31703],{"class":189,"line":2607},[187,31702,2579],{"class":577},[187,31704,31705],{"class":196},"'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'\n",[187,31707,31708,31710],{"class":189,"line":2621},[187,31709,2579],{"class":577},[187,31711,31712],{"class":196},"'traefik.http.routers.nginx-web.entrypoints=websecure'\n",[187,31714,31715,31717],{"class":189,"line":2631},[187,31716,2579],{"class":577},[187,31718,31719],{"class":196},"'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'\n",[187,31721,31722,31724],{"class":189,"line":2642},[187,31723,2579],{"class":577},[187,31725,31726],{"class":196},"'traefik.http.services.nginx-web.loadbalancer.server.port=80'\n",[11,31728,31729,31730,31732,31733,31736],{},"For now, ignore the contents under the ",[33,31731,30519],{}," key; we will cover this next when we go over the ",[33,31734,31735],{},"traefik"," service.",[11,31738,31739,31740,15754,31742,31744,31745,31747,31748,31751],{},"NGINX acts as a reverse proxy when it sends request starting with ",[33,31741,15753],{},[33,31743,15757],{}," to the ",[33,31746,12291],{}," container. Two blocks in ",[33,31749,31750],{},"prod.conf"," enable this behavior:",[26,31753,31756],{"className":31754,"code":31755,"language":31},[29],"  upstream backend {\n    server backend:8000;\n  }\n",[33,31757,31755],{"__ignoreMap":35},[11,31759,31760,31761,31763,31764,5857,31766,31768,31769,31771,31772,31774,31775,31777,31778,15754,31780,31782,31783,31785,31786,752],{},"This hostname ",[33,31762,12291],{}," is defined as ",[33,31765,27800],{},[33,31767,27800],{}," can be resolved by the ",[33,31770,30166],{}," service because it is on the same ",[33,31773,11677],{}," network that ",[33,31776,12291],{}," is on. If either of the ",[33,31779,30166],{},[33,31781,12291],{}," wasn't on the ",[33,31784,11677],{}," network, NGINX would not be able to make sense of ",[33,31787,27800],{},[26,31789,31792],{"className":31790,"code":31791,"language":31},[29],"    # backend urls\n    location ~ ^/(admin|api) {\n      proxy_redirect off;\n      proxy_pass http://backend;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n    }\n",[33,31793,31791],{"__ignoreMap":35},[11,31795,31796,31797,31799,31800,31803],{},"This block does that actual request forwarding. ",[33,31798,28335],{}," references the ",[33,31801,31802],{},"upstream backend {}"," block defined above.",[11,31805,31423,31806,31808,31809,31812,31813,31815],{},[33,31807,31426],{}," is referenced here and mounts to ",[33,31810,31811],{},"/usr/src/app/assets",". This path is then referenced in ",[33,31814,31750],{},", the NGINX configuration file that is used in our custom NGINX-based image:",[26,31817,31820],{"className":31818,"code":31819,"language":31},[29],"    # static files\n    location /static {\n      autoindex on;\n      alias /usr/src/app/assets/static;\n    }\n",[33,31821,31819],{"__ignoreMap":35},[11,31823,31824,31825,31827,31828,31831,31832,31834,31835,31838,31839,31841,31842,31845,31846,31848,31849,31851,31852,31854,31855,31857],{},"In this block of ",[33,31826,31750],{},", we tell NGINX to serve files from ",[33,31829,31830],{},"/usr/src/app/assets/static"," for requests that start with ",[33,31833,30058],{},". A request made to ",[33,31836,31837],{},"https://mysite.com/static/base.css"," would return a file located at ",[33,31840,31830],{}," if that file existed. Remember, when we run the ",[33,31843,31844],{},"collecstatic"," management command in our Django container, it will collect our static files to ",[33,31847,31426],{},". Since ",[33,31850,31426],{}," is mounted to the ",[33,31853,30166],{}," service at ",[33,31856,31811],{},", NGINX will have access to these files by way of the volume mount and they will persist across restarts of the web service and its NGINX container.",[11,31859,31860,31861,31863],{},"Finally, NGINX can serve a Javascript SPA or similar if we choose to use one in our project. To understand how this is done, we need to understand multistage Dockerfiles. Here's the Dockerfile used for the ",[33,31862,30735],{}," container:",[26,31865,31869],{"className":31866,"code":31867,"language":31868,"meta":35,"style":35},"language-dockerfile shiki shiki-themes github-light github-dark","# # build stage\n# FROM node:10-alpine as build-stage\n# WORKDIR /app/\n# COPY frontend/package.json /app/\n# RUN npm cache verify\n# RUN npm install\n# COPY frontend /app/\n# RUN npm run build\n\n# production stage\n# FROM nginx:1.19.1-alpine as production-stage\nFROM nginx:1.19.1-alpine\nCOPY nginx/prod/prod.conf /etc/nginx/nginx.conf\nCOPY nginx/prod/index.html /dist/\n# COPY --from=build-stage /app/dist /dist/\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n","dockerfile",[33,31870,31871,31876,31881,31886,31891,31896,31901,31906,31911,31915,31920,31925,31930,31935,31940,31945,31950],{"__ignoreMap":35},[187,31872,31873],{"class":189,"line":190},[187,31874,31875],{},"# # build stage\n",[187,31877,31878],{"class":189,"line":249},[187,31879,31880],{},"# FROM node:10-alpine as build-stage\n",[187,31882,31883],{"class":189,"line":312},[187,31884,31885],{},"# WORKDIR /app/\n",[187,31887,31888],{"class":189,"line":319},[187,31889,31890],{},"# COPY frontend/package.json /app/\n",[187,31892,31893],{"class":189,"line":325},[187,31894,31895],{},"# RUN npm cache verify\n",[187,31897,31898],{"class":189,"line":686},[187,31899,31900],{},"# RUN npm install\n",[187,31902,31903],{"class":189,"line":697},[187,31904,31905],{},"# COPY frontend /app/\n",[187,31907,31908],{"class":189,"line":1291},[187,31909,31910],{},"# RUN npm run build\n",[187,31912,31913],{"class":189,"line":1306},[187,31914,316],{"emptyLinePlaceholder":315},[187,31916,31917],{"class":189,"line":1434},[187,31918,31919],{},"# production stage\n",[187,31921,31922],{"class":189,"line":2599},[187,31923,31924],{},"# FROM nginx:1.19.1-alpine as production-stage\n",[187,31926,31927],{"class":189,"line":2607},[187,31928,31929],{},"FROM nginx:1.19.1-alpine\n",[187,31931,31932],{"class":189,"line":2621},[187,31933,31934],{},"COPY nginx/prod/prod.conf /etc/nginx/nginx.conf\n",[187,31936,31937],{"class":189,"line":2631},[187,31938,31939],{},"COPY nginx/prod/index.html /dist/\n",[187,31941,31942],{"class":189,"line":2642},[187,31943,31944],{},"# COPY --from=build-stage /app/dist /dist/\n",[187,31946,31947],{"class":189,"line":2653},[187,31948,31949],{},"EXPOSE 80\n",[187,31951,31952],{"class":189,"line":2665},[187,31953,31954],{},"CMD [\"nginx\", \"-g\", \"daemon off;\"]\n",[11,31956,31957],{},"Currently I don't have a SPA setup, but this is how we could setup one using Vue.js. The important part is this line:",[26,31959,31962],{"className":31960,"code":31961,"language":30713,"meta":35,"style":35},"language-Dockerfile shiki shiki-themes github-light github-dark","COPY --from=build-stage /app/dist /dist/\n",[33,31963,31964],{"__ignoreMap":35},[187,31965,31966],{"class":189,"line":190},[187,31967,31961],{},[11,31969,31970,31971,31974,31975,31977],{},"This would copy the build files for our Javascript application into the ",[33,31972,31973],{},"/dist/"," folder of our NGINX container. Another few declarations and blocks in ",[33,31976,31750],{}," allow all other requests to be served by the contents of this folder:",[26,31979,31982],{"className":31980,"code":31981,"language":31},[29],"    root /dist/;\n    index index.html;\n",[33,31983,31981],{"__ignoreMap":35},[11,31985,31986],{},"This sets the root and the index document for our NGINX webserver.",[26,31988,31991],{"className":31989,"code":31990,"language":31},[29],"    # frontend\n    location / {\n      try_files $uri $uri/ @rewrites;\n    }\n\n    location @rewrites {\n      rewrite ^(.+)$ /index.html last;\n    }\n",[33,31992,31990],{"__ignoreMap":35},[11,31994,31995,31996,31998,31999,32001,32002,637,32004,15754,32006,32008],{},"These two blocks route all other requests to the frontend Javascript app's ",[33,31997,27266],{}," file location in ",[33,32000,31973],{}," (any request that doesn't start with ",[33,32003,30058],{},[33,32005,15753],{},[33,32007,15757],{},"). We may wish to change this behavior if you want Django to serve most of your requests and possibly serve a single page application on another path.",[11,32010,32011,32012,32014,32015,32018,32019,32022,32023,31736],{},"Lastly, the ",[33,32013,30166],{}," service's ",[33,32016,32017],{},"deployment"," key has some ",[33,32020,32021],{},"labels"," defined for Traefik. Let's come back to these after having a look at the ",[33,32024,31735],{},[911,32026,32027],{"id":31735},"Traefik",[11,32029,32030],{},[511,32031],{"alt":7255,"src":32032},"https://docs.traefik.io/assets/img/traefik-architecture.png",[107,32034,32035],{},[11,32036,32037,32038],{},"Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. -- ",[15,32039,32040],{"href":32040,"rel":32041},"https://docs.traefik.io/",[19],[11,32043,32044],{},"Traefik has three main functions in my application:",[2276,32046,32047,32050,32053],{},[919,32048,32049],{},"Request TLS certificates from Let's Encrypt that allow us to encrypt our web traffic with HTTPS",[919,32051,32052],{},"Do TLS termination",[919,32054,32055],{},"Route all requests to NGINX",[11,32057,32058],{},"The one thing that Traefik cannot do is serve static files; it is not a webserver, unlike NGINX which is a webserver. NGINX is also capable of requesting TLS certs from Let's Encrypt, so we don't technically need Traefik, but it is indeed \"fun and easy\", especially when it comes to requesting certificates.",[107,32060,32061],{},[11,32062,32063],{},"I have tried setting up Certbot with NGINX a long time ago but I never go it to work, and I didn't like the idea about how to run a chron job to refresh old certs.",[11,32065,32066],{},"There are two main ways to set up Traefik:",[2276,32068,32069,32078],{},[919,32070,32071,32072,32075,32076,343],{},"write a ",[33,32073,32074],{},"traefik.toml"," file and build this into your own custom image (similar to what we do with NGINX and ",[33,32077,31750],{},[919,32079,32080],{},"use a base image and specify all configure options through command line arguments.",[11,32082,32083,32084,32086,32087,32089],{},"I started out with the first approach, and I did get it to work, but I have decided that the second way is better. It requires one less image to build in our deployment process and it is easy to parametrize the command line arguments in ",[33,32085,31264],{}," (for now all the values I'm using in the ",[33,32088,31735],{}," service are hard-coded, this is one more item for my ToDo list on this project).",[11,32091,32092,32093,32095,32096,32101,32102,507],{},"I had a hard time finding good examples of how to use Traefik version 2 with Docker Swarm in the Traefik docs. Their official example for using docker uses ",[33,32094,28292],{},". There is a Swarm example, but it is for an older version of Traefik (1.7). This article titled ",[15,32097,32100],{"href":32098,"rel":32099},"https://blog.creekorful.com/2019/10/how-to-install-traefik-2-docker-swarm/",[19],"How to install Traefik 2.x on a Docker Swarm"," helped me a lot in figuring out how to get everything working. Thank you for the great article, ",[15,32103,32106],{"href":32104,"rel":32105},"https://github.com/creekorful",[19],"Aloïs",[11,32108,32109],{},"Here's the code that sets up the traefik service:",[26,32111,32113],{"className":8656,"code":32112,"language":8658,"meta":35,"style":35},"services:\n  traefik:\n    image: traefik:v2.0.2\n    ports:\n      - '80:80'\n      - '443:443'\n    command:\n      - '--providers.docker.endpoint=unix:///var/run/docker.sock'\n      - '--providers.docker.swarmMode=true'\n      - '--providers.docker.exposedbydefault=false'\n      - '--providers.docker.network=traefik-public'\n      - '--entrypoints.web.address=:80'\n      - '--entrypoints.websecure.address=:443'\n      - '--certificatesresolvers.letsencryptresolver.acme.httpchallenge=true'\n      - '--certificatesresolvers.letsencryptresolver.acme.httpchallenge.entrypoint=web'\n      - '--certificatesresolvers.letsencryptresolver.acme.email=brian@email.com'\n      - '--certificatesresolvers.letsencryptresolver.acme.storage=/letsencrypt/acme.json'\n    volumes:\n      - letsencrypt:/letsencrypt\n      - /var/run/docker.sock:/var/run/docker.sock\n    networks:\n      - traefik-public\n    deploy:\n      placement:\n        constraints:\n          - node.role == manager\n",[33,32114,32115,32121,32128,32137,32143,32150,32157,32164,32171,32178,32185,32192,32199,32206,32213,32220,32227,32234,32240,32247,32254,32260,32266,32272,32279,32286],{"__ignoreMap":35},[187,32116,32117,32119],{"class":189,"line":190},[187,32118,3613],{"class":2516},[187,32120,2520],{"class":577},[187,32122,32123,32126],{"class":189,"line":249},[187,32124,32125],{"class":2516},"  traefik",[187,32127,2520],{"class":577},[187,32129,32130,32132,32134],{"class":189,"line":312},[187,32131,3627],{"class":2516},[187,32133,585],{"class":577},[187,32135,32136],{"class":196},"traefik:v2.0.2\n",[187,32138,32139,32141],{"class":189,"line":319},[187,32140,3661],{"class":2516},[187,32142,2520],{"class":577},[187,32144,32145,32147],{"class":189,"line":325},[187,32146,2610],{"class":577},[187,32148,32149],{"class":196},"'80:80'\n",[187,32151,32152,32154],{"class":189,"line":686},[187,32153,2610],{"class":577},[187,32155,32156],{"class":196},"'443:443'\n",[187,32158,32159,32162],{"class":189,"line":697},[187,32160,32161],{"class":2516},"    command",[187,32163,2520],{"class":577},[187,32165,32166,32168],{"class":189,"line":1291},[187,32167,2610],{"class":577},[187,32169,32170],{"class":196},"'--providers.docker.endpoint=unix:///var/run/docker.sock'\n",[187,32172,32173,32175],{"class":189,"line":1306},[187,32174,2610],{"class":577},[187,32176,32177],{"class":196},"'--providers.docker.swarmMode=true'\n",[187,32179,32180,32182],{"class":189,"line":1434},[187,32181,2610],{"class":577},[187,32183,32184],{"class":196},"'--providers.docker.exposedbydefault=false'\n",[187,32186,32187,32189],{"class":189,"line":2599},[187,32188,2610],{"class":577},[187,32190,32191],{"class":196},"'--providers.docker.network=traefik-public'\n",[187,32193,32194,32196],{"class":189,"line":2607},[187,32195,2610],{"class":577},[187,32197,32198],{"class":196},"'--entrypoints.web.address=:80'\n",[187,32200,32201,32203],{"class":189,"line":2621},[187,32202,2610],{"class":577},[187,32204,32205],{"class":196},"'--entrypoints.websecure.address=:443'\n",[187,32207,32208,32210],{"class":189,"line":2631},[187,32209,2610],{"class":577},[187,32211,32212],{"class":196},"'--certificatesresolvers.letsencryptresolver.acme.httpchallenge=true'\n",[187,32214,32215,32217],{"class":189,"line":2642},[187,32216,2610],{"class":577},[187,32218,32219],{"class":196},"'--certificatesresolvers.letsencryptresolver.acme.httpchallenge.entrypoint=web'\n",[187,32221,32222,32224],{"class":189,"line":2653},[187,32223,2610],{"class":577},[187,32225,32226],{"class":196},"'--certificatesresolvers.letsencryptresolver.acme.email=brian@email.com'\n",[187,32228,32229,32231],{"class":189,"line":2665},[187,32230,2610],{"class":577},[187,32232,32233],{"class":196},"'--certificatesresolvers.letsencryptresolver.acme.storage=/letsencrypt/acme.json'\n",[187,32235,32236,32238],{"class":189,"line":2674},[187,32237,3637],{"class":2516},[187,32239,2520],{"class":577},[187,32241,32242,32244],{"class":189,"line":2684},[187,32243,2610],{"class":577},[187,32245,32246],{"class":196},"letsencrypt:/letsencrypt\n",[187,32248,32249,32251],{"class":189,"line":2694},[187,32250,2610],{"class":577},[187,32252,32253],{"class":196},"/var/run/docker.sock:/var/run/docker.sock\n",[187,32255,32256,32258],{"class":189,"line":2706},[187,32257,31649],{"class":2516},[187,32259,2520],{"class":577},[187,32261,32262,32264],{"class":189,"line":2715},[187,32263,2610],{"class":577},[187,32265,31658],{"class":196},[187,32267,32268,32270],{"class":189,"line":2725},[187,32269,31682],{"class":2516},[187,32271,2520],{"class":577},[187,32273,32274,32277],{"class":189,"line":2735},[187,32275,32276],{"class":2516},"      placement",[187,32278,2520],{"class":577},[187,32280,32281,32284],{"class":189,"line":2743},[187,32282,32283],{"class":2516},"        constraints",[187,32285,2520],{"class":577},[187,32287,32288,32291],{"class":189,"line":2754},[187,32289,32290],{"class":577},"          - ",[187,32292,32293],{"class":196},"node.role == manager\n",[107,32295,32296],{},[11,32297,32298],{},"The one thing I don't like about this setup is that it uses a 1GB volume to store one small JSON file. I think that 1GB is the smallest block storage volume I can request using REX-Ray. This only adds $0.10/month to our project costs which is not that bad.",[11,32300,32301,1172,32303,32306,32307,32309],{},[33,32302,30166],{},[33,32304,32305],{},"websecure"," refer to values declared on the ",[33,32308,30166],{}," service. Let's take a look at those values:",[26,32311,32313],{"className":2507,"code":32312,"language":2509,"meta":35,"style":35},"deploy:\n  labels:\n    - 'traefik.enable=true'\n    - 'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'\n    - 'traefik.http.routers.nginx-web.entrypoints=websecure'\n    - 'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'\n    - 'traefik.http.services.nginx-web.loadbalancer.server.port=80'\n",[33,32314,32315,32321,32328,32334,32340,32346,32352],{"__ignoreMap":35},[187,32316,32317,32319],{"class":189,"line":190},[187,32318,30519],{"class":2516},[187,32320,2520],{"class":577},[187,32322,32323,32326],{"class":189,"line":249},[187,32324,32325],{"class":2516},"  labels",[187,32327,2520],{"class":577},[187,32329,32330,32332],{"class":189,"line":312},[187,32331,14709],{"class":577},[187,32333,31698],{"class":196},[187,32335,32336,32338],{"class":189,"line":319},[187,32337,14709],{"class":577},[187,32339,31705],{"class":196},[187,32341,32342,32344],{"class":189,"line":325},[187,32343,14709],{"class":577},[187,32345,31712],{"class":196},[187,32347,32348,32350],{"class":189,"line":686},[187,32349,14709],{"class":577},[187,32351,31719],{"class":196},[187,32353,32354,32356],{"class":189,"line":697},[187,32355,14709],{"class":577},[187,32357,31726],{"class":196},[11,32359,32360,32361,32363],{},"I still need to setup HTTP -> HTTPS redirecting, so for now only ",[33,32362,32305],{}," is defined, but Aloïs explains this clearly in his article.",[107,32365,32366],{},[11,32367,32368],{},"For me this is the most complicated part of the setup. I'm still not familiar with exactly how Traefik and Let's Encrypt work. Hopefully I can run through this process a few more times with some variations to better understand the rough edges. Otherwise for this simple way to get TLS certificates. AWS makes this very easy with Amazon Certificate Manager (ACM) which makes the requesting of certificates very simple, especially within CDK.",[11,32370,32371,32372,32374],{},"That wraps up our overview of ",[33,32373,31264],{},", we left off with the following command:",[26,32376,32378],{"className":32377,"code":31125,"language":31},[29],[33,32379,31125],{"__ignoreMap":35},[11,32381,32382,32383,32385],{},"We can check out the status of our docker stack deployment by running a few different docker CLI commands. You can either SSH into your Droplet or configure the ",[33,32384,28566],{}," environment variable that I showed you earlier and run these commands from your local terminal:",[26,32387,32390],{"className":32388,"code":32389,"language":31},[29],"docker stack ps my-stack --no-trunc\n",[33,32391,32389],{"__ignoreMap":35},[11,32393,32394,32397,32398,752],{},[33,32395,32396],{},"--no-trunc"," is important because important error messages tend to be cut off. This option will show the full version of each column returned by ",[33,32399,32400],{},"docker stack ps",[26,32402,32405],{"className":32403,"code":32404,"language":31},[29],"docker service ls\n",[33,32406,32404],{"__ignoreMap":35},[11,32408,32409],{},"This command shows some the active services on our Droplet.",[26,32411,32414],{"className":32412,"code":32413,"language":31},[29],"docker ps\n",[33,32415,32413],{"__ignoreMap":35},[11,32417,32418],{},"This command is useful for shelling into a container to run commands and poke around for debugging.",[11,32420,32421],{},"You can get the Container ID of a running container and access it with the following command:",[26,32423,32426],{"className":32424,"code":32425,"language":31},[29],"docker exec -it 0da8370ab283 bash\n",[33,32427,32425],{"__ignoreMap":35},[11,32429,32430,32431,32433,32434,752],{},"This assumes that ",[33,32432,183],{}," is installed on the container with ID ",[33,32435,32436],{},"0da8370ab283",[168,32438,32440],{"id":32439},"management-commands","Management commands",[11,32442,32443],{},"Once the site is deployed we stil need to run a few commands to set up our Djnago application:",[2276,32445,32446,32448,32450],{},[919,32447,13071],{},[919,32449,13074],{},[919,32451,32452],{},"createsuperuser",[107,32454,32455],{},[11,32456,32457],{},"One other ToDo is to figure out how to run these commands through manual GitLab CI jobs.",[11,32459,32460],{},"That's most of what I wanted to cover on a first pass. This should be a good starting point for working with a Django application in Docker Swarm on DigitalOcean.",[168,32462,28470],{"id":28104},[11,32464,32465],{},"Here are some ideas about the next steps I could take on this project.",[911,32467,32469],{"id":32468},"local-environment","Local environment",[11,32471,32472],{},"I'll probably need to create another docker-compose file to bring up everything locally. It might be a good way to experiment with different Traefik settings.",[911,32474,32476],{"id":32475},"infrastructure-as-code-setup","Infrastructure as Code setup",[11,32478,32479],{},"There might be a good opportunity to learn more about Terreform or Ansible here. There are a number of manual, one time setup steps. Some of these can't be automated, but some of them would probably fit very neatly into one of these tools. Pulumi would also be a good option to explore as it is more analagous to CDK.",[911,32481,32483],{"id":32482},"scaling-out-docker-swarm-services-across-multiple-machines","Scaling out docker swarm services across multiple machines",[11,32485,32486],{},"I have only scratched the surface of what docker swarm can do. There are lots of other settings that would be helpful to setup for learning purposes, especially around resource limits for services. For simplicity I haven't touch on any of these options yet. I'm curious to know how many containers I can fit onto one small Droplet, and if resource limits could help with compute and memory-intensive workloads.",[911,32488,32490],{"id":32489},"kubernetes-on-digitalocean","Kubernetes on DigitalOcean",[11,32492,32493],{},"DigitalOcean now offers simplified Kubernetes solutions. It would be interesting to try this out once I get better with docker swarm. I have used Kubernetes a with minikube and to a limited extent with GCP.",[911,32495,32497],{"id":32496},"deploying-locally-without-using-gitlab-ci","Deploying locally, without using GitLab CI",[11,32499,32500],{},"CDK makes deploying locally very easy, especially with the Lambda project I put together. This project as it stands might be a little bit more difficult to deploy locally. It assumes that the images we want to deploy are private. It might be possible, but for now I am fine with deploying through GitLab CI since the pipeline only takes a few minutes to complete for the build and deploy stages.",[855,32502,32503],{},"html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}",{"title":35,"searchDepth":249,"depth":249,"links":32505},[32506,32507,32508,32509,32510,32511,32512,32513,32514,32515,32516,32517,32522,32523],{"id":24341,"depth":249,"text":24342},{"id":30278,"depth":249,"text":30279},{"id":30310,"depth":249,"text":30311},{"id":30343,"depth":249,"text":30344},{"id":30393,"depth":249,"text":30394},{"id":30410,"depth":249,"text":30411},{"id":30429,"depth":249,"text":30430},{"id":30485,"depth":249,"text":22749},{"id":30824,"depth":249,"text":30825},{"id":30886,"depth":249,"text":30887},{"id":30981,"depth":249,"text":30977},{"id":31261,"depth":249,"text":31264,"children":32518},[32519,32520,32521],{"id":12291,"depth":312,"text":12291},{"id":30735,"depth":312,"text":31595},{"id":31735,"depth":312,"text":32027},{"id":32439,"depth":249,"text":32440},{"id":28104,"depth":249,"text":28470,"children":32524},[32525,32526,32527,32528,32529],{"id":32468,"depth":312,"text":32469},{"id":32475,"depth":312,"text":32476},{"id":32482,"depth":312,"text":32483},{"id":32489,"depth":312,"text":32490},{"id":32496,"depth":312,"text":32497},"2020-08-09","A guide to deploying Django applications with docker using popular open-source tools","/static/shark.jpg",{"layout":29014},"/2020/08/09/digital-ocean-docker-swarm-django-traefik-nginx",{"title":30170,"description":32531},"2020/08/09/digital-ocean-docker-swarm-django-traefik-nginx",[15290,15298,32538,882,28639,32539,31735,30735,32540],"digital-ocean","rex-ray","swarm","ZlCLL_5gKM5_dvcX8cplnycggEchh7fQpiuEEzLGJH4",{"id":32543,"title":32544,"body":32545,"comments":315,"date":34491,"description":35,"draft":872,"extension":873,"external":874,"image":32556,"meta":34492,"navigation":315,"path":34493,"seo":34494,"stem":34495,"tags":34496,"__hash__":34497},"blog/2020/08/01/django-and-lambda-with-cdk-and-api-gateway.md","Deploying serverless Django applications to AWS with CDK on a tiny budget using Lambda, API Gateway, awsgi and the Lambda proxy pattern",{"type":8,"value":32546,"toc":34476},[32547,32549,32552,32557,32560,32564,32567,32575,32578,32585,32594,32598,32620,32663,32672,32696,32775,32785,32787,32794,32846,32872,32875,32882,32885,32904,32907,32979,32982,33085,33130,33139,33147,33192,33208,33280,33283,33286,33289,33401,33408,33427,33432,33538,33541,33585,33589,33609,33612,33692,33695,33712,33715,33792,33799,33805,33825,33841,33850,33873,33877,33884,33909,33920,33927,33931,33937,33943,33955,33973,33990,33999,34003,34006,34014,34018,34025,34031,34037,34059,34066,34072,34078,34084,34087,34093,34099,34106,34112,34119,34124,34127,34137,34142,34146,34149,34262,34269,34400,34407,34409,34412,34471,34473],[168,32548,8184],{"id":8183},[11,32550,32551],{},"One way to deploy Django apps to AWS on a budget is to use the Lambda proxy pattern. The main idea is this: web requests are made to API Gateway that calls a Lambda outside of our VPC (proxy Lambda), which then invokes a second lambda (Django Lambda) inside of our VPC so that it can access VPC resources, namely RDS. The handler for the Django Lambda translates the API Gateway event into a WSGI-compatible request, processes the request and returns the response to the user back through the proxy Lambda. The big caveat is that you can't easily access the internet in Django's request/response cycle without network address translation (NAT) services which add additional costs. One other caveat is that there is high latency associated with the initial request. There are three cold starts to wait for: the cold start for the proxy Lambda, the cold start for the Django Lambda and the cold start for the Aurora Postgres Serverless database. Here's an architecture diagram:",[11,32553,32554],{},[511,32555],{"alt":7255,"src":32556},"/static/djambda.png",[11,32558,32559],{},"Route53 and CloudFront (1 and 2), discuessed later on, are optional. Starting with 5, API Gateway requests are passed to a proxy lambda (6) which calls a Lambda in a VPC that contains our Django code and special Django handler (7). This function can access RDS (8) and S3 (4) via a VPC Gateway Endpoint (9), but it cannot access the internet.",[168,32561,32563],{"id":32562},"why","Why?",[11,32565,32566],{},"There are a few different reasons for why I put this project together. Before I get into these reasons, here is a little bit of background on how I typically deploy web apps and the motivations for this project.",[11,32568,32569,32570,32574],{},"Django is my web framework of choice, and I'm most comfortable working with Django in docker containers. I previously wrote an article about ",[15,32571,32573],{"href":30178,"rel":32572},[19],"deploying Django applications with AWS Fargate",". This approach technically is \"serverless\", but it requires the use of \"always on\" services: an Application Load Balancer, NAT Gateways and long-running Fargate tasks to run gunicorn processes for our Django application. Even if you choose to run workloads in public subnets and don't use NAT Gateways or NAT instances, the costs are prohbitively high for many types of Django projects: personal projects, proof-of-concept projects, internal projects, toy projects, etc.",[11,32576,32577],{},"The other serverless compute platform on AWS is Lambda. There is a popular framework for deploying serverless Django applications on Lambda called Zappa. I have been meaning to try out Zappa for a while now, and I still haven't used it, but it provided a good reference for how to do \"serverless Django\". I'll come back to Zappa in more detail later, but the main reason I didn't want to use it for my serverless Django project is because I already have a great deployment and Infrastructure as Code tool: CDK.",[11,32579,32580,32581,752],{},"CDK, or Cloud Development Kit, is a tool that allows you define and deploy AWS infrastructure using any popular programming language, Python in my case. It uses CloudFormation in the background, and it has great support for lots of AWS services. If you are looking for an introuction to CDK, check out ",[15,32582,32583],{"href":32583,"rel":32584},"https://cdkworkshop.com/",[19],[11,32586,32587,32588,32593],{},"Zappa makes it really easy to deploy serveless Django applications with Django, but it requires that you ",[15,32589,32592],{"href":32590,"rel":32591},"https://github.com/Miserlou/Zappa#running-tasks-in-a-vpc",[19],"setup NAT and other VPC resources before using it with RDS",". Since CDK is really good at setting up these kinds of resources, as well as the resources that Zappa creates (including Lambda functions and API Gateway), I wanted to know how far I could get in setting up a servless Django application only using CDK. I also didn't want everything abstracted away by a high level framework; I'm interested in a lower level view to get more familiar with how serverless technologies work. It's important to note that the biggest motivation of all is to learn try something out of my comfort zone, make mistakes and get feedback.",[168,32595,32597],{"id":32596},"django-lambda-djambda","Django + Lambda = djambda",[11,32599,32600,32601,32606,32607,12282,32612,32615,32616,32619],{},"The name for this project came pretty easily. I created ",[15,32602,32605],{"href":32603,"rel":32604},"https://gitlab.com/briancaffey/djambda",[19],"a repo on GitLab called djambda"," and then discovered ",[15,32608,32611],{"href":32609,"rel":32610},"https://github.com/netsome/djambda",[19],"a similar project by the same name on GitHub: netsome/djambda",[33,32613,32614],{},"netsome/djambda"," project uses Terraform, and I encourage you to check it out and leave it a GitHub star. Before I came across this project, I hacked together a very basic djambda protoype using some code from Zappa's handler. A ",[33,32617,32618],{},"handler"," is the name for the function that is called in your Lambda function's code when the Lambda function is invoked. Here's some pseudo code that shows how Zappa's handler works:",[26,32621,32623],{"className":10554,"code":32622,"language":10556,"meta":35,"style":35},"from werkzeug.wrappers import Response\n\ndef handler(event, context):\n    with Response.from_app(wsgi_app, wsgi_request(event)) as response:\n        zappa_returndict = dict()\n        zappa_returndict['body'] = response.data\n        zappa_returndict['statusCode'] = response.status_code\n        return zappa_returndict\n",[33,32624,32625,32630,32634,32638,32643,32648,32653,32658],{"__ignoreMap":35},[187,32626,32627],{"class":189,"line":190},[187,32628,32629],{},"from werkzeug.wrappers import Response\n",[187,32631,32632],{"class":189,"line":249},[187,32633,316],{"emptyLinePlaceholder":315},[187,32635,32636],{"class":189,"line":312},[187,32637,23623],{},[187,32639,32640],{"class":189,"line":319},[187,32641,32642],{},"    with Response.from_app(wsgi_app, wsgi_request(event)) as response:\n",[187,32644,32645],{"class":189,"line":325},[187,32646,32647],{},"        zappa_returndict = dict()\n",[187,32649,32650],{"class":189,"line":686},[187,32651,32652],{},"        zappa_returndict['body'] = response.data\n",[187,32654,32655],{"class":189,"line":697},[187,32656,32657],{},"        zappa_returndict['statusCode'] = response.status_code\n",[187,32659,32660],{"class":189,"line":1291},[187,32661,32662],{},"        return zappa_returndict\n",[11,32664,32665,32666,32671],{},"For reference, ",[15,32667,32670],{"href":32668,"rel":32669},"https://github.com/Miserlou/Zappa/blob/master/zappa/handler.py#L540",[19],"here is the link to the line in Zappa's source code that starts processing API Gateway requests"," on which the above psuedo code is loosly based.",[11,32673,6131,32674,32676,32677,32681,32682,32685,32686,32689,32690,32692,32693,32695],{},[33,32675,32614],{}," project makes use of a package called ",[15,32678,23667],{"href":32679,"rel":32680},"https://github.com/slank/awsgi",[19]," that has active contributions from people at AWS. Similar to djambda, it is a mashup of words (acronyms): (",[33,32683,32684],{},"AWS"," + ",[33,32687,32688],{},"wsgi"," = ",[33,32691,23667],{},"). It does most of the work that Zappa's handler does, so I replaced the adapted Zappa handler code with a very elagant handler (borrowed from ",[33,32694,32614],{},"):",[26,32697,32699],{"className":10554,"code":32698,"language":10556,"meta":35,"style":35},"# djambda/src/djambda/awsgi.py\nimport io\n\nimport awsgi\nfrom django.core import management\n\nfrom .wsgi import application\n\n\ndef lambda_handler(event, context):\n    if \"manage\" in event:\n        output = io.StringIO()\n        management.call_command(*event[\"manage\"].split(\" \"), stdout=output)\n        return {\"output\": output.getvalue()}\n    else:\n        return awsgi.response(application, event, context)\n",[33,32700,32701,32706,32711,32715,32719,32724,32728,32733,32737,32741,32746,32751,32756,32761,32766,32770],{"__ignoreMap":35},[187,32702,32703],{"class":189,"line":190},[187,32704,32705],{},"# djambda/src/djambda/awsgi.py\n",[187,32707,32708],{"class":189,"line":249},[187,32709,32710],{},"import io\n",[187,32712,32713],{"class":189,"line":312},[187,32714,316],{"emptyLinePlaceholder":315},[187,32716,32717],{"class":189,"line":319},[187,32718,23191],{},[187,32720,32721],{"class":189,"line":325},[187,32722,32723],{},"from django.core import management\n",[187,32725,32726],{"class":189,"line":686},[187,32727,316],{"emptyLinePlaceholder":315},[187,32729,32730],{"class":189,"line":697},[187,32731,32732],{},"from .wsgi import application\n",[187,32734,32735],{"class":189,"line":1291},[187,32736,316],{"emptyLinePlaceholder":315},[187,32738,32739],{"class":189,"line":1306},[187,32740,316],{"emptyLinePlaceholder":315},[187,32742,32743],{"class":189,"line":1434},[187,32744,32745],{},"def lambda_handler(event, context):\n",[187,32747,32748],{"class":189,"line":2599},[187,32749,32750],{},"    if \"manage\" in event:\n",[187,32752,32753],{"class":189,"line":2607},[187,32754,32755],{},"        output = io.StringIO()\n",[187,32757,32758],{"class":189,"line":2621},[187,32759,32760],{},"        management.call_command(*event[\"manage\"].split(\" \"), stdout=output)\n",[187,32762,32763],{"class":189,"line":2631},[187,32764,32765],{},"        return {\"output\": output.getvalue()}\n",[187,32767,32768],{"class":189,"line":2642},[187,32769,23313],{},[187,32771,32772],{"class":189,"line":2653},[187,32773,32774],{},"        return awsgi.response(application, event, context)\n",[11,32776,32777,32778,637,32780,637,32782,32784],{},"This handler can takes care of translating API Gateway requests to WSGI requests as well as management commands such as ",[33,32779,13074],{},[33,32781,32452],{},[33,32783,13071],{}," and other custom management commands. Let's talk about the management commands first as they relate to the two main AWS services that our Django application uses: RDS and S3. Then we will talk about API Gateway, the proxy Lambda and how the Lambda proxy pattern works.",[168,32786,12336],{"id":12281},[11,32788,32789,32790,32793],{},"To run the management commands for our application, we can use the AWS CLI to invoke the Lambda function with a payload that will trigger the ",[33,32791,32792],{},"if \"manage\" in event:"," code block from the above handler:",[26,32795,32797],{"className":181,"code":32796,"language":183,"meta":35,"style":35},"aws lambda invoke \\\n    --function-name my-djambda-lambda \\\n    --invocation-type RequestResponse \\\n    --payload '{\"manage\": \"migrate --no-input\"}' \\\n    resp.json\n",[33,32798,32799,32811,32821,32831,32841],{"__ignoreMap":35},[187,32800,32801,32803,32806,32809],{"class":189,"line":190},[187,32802,12748],{"class":193},[187,32804,32805],{"class":196}," lambda",[187,32807,32808],{"class":196}," invoke",[187,32810,16644],{"class":588},[187,32812,32813,32816,32819],{"class":189,"line":249},[187,32814,32815],{"class":588},"    --function-name",[187,32817,32818],{"class":196}," my-djambda-lambda",[187,32820,16644],{"class":588},[187,32822,32823,32826,32829],{"class":189,"line":312},[187,32824,32825],{"class":588},"    --invocation-type",[187,32827,32828],{"class":196}," RequestResponse",[187,32830,16644],{"class":588},[187,32832,32833,32836,32839],{"class":189,"line":319},[187,32834,32835],{"class":588},"    --payload",[187,32837,32838],{"class":196}," '{\"manage\": \"migrate --no-input\"}'",[187,32840,16644],{"class":588},[187,32842,32843],{"class":189,"line":325},[187,32844,32845],{"class":196},"    resp.json\n",[11,32847,32848,32849,32851,32852,8299,32855,32858,32859,32862,32863,1172,32865,32868,32869,32871],{},"This will run the ",[33,32850,13074],{}," command by connecting to RDS from our Django application with a special version of ",[33,32853,32854],{},"psycopg2",[33,32856,32857],{},"aws-psycopg2"," (check ",[33,32860,32861],{},"django/requirements.txt"," for this dependency). ",[33,32864,32854],{},[33,32866,32867],{},"psycopg2-binary"," both gave error messages when trying to access the database, but the ",[33,32870,32857],{}," package had no issues.",[11,32873,32874],{},"RDS can sometimes be the most expensive part of a project on AWS. To reduce costs, we are using Aurora Postgres Serverless. This AWS service is ideal for small, infrequently-used projects that are in development. The database \"goes to sleep\" after a period of inactivity (5 minutes, I think). When new requests to the database are made, it can take up to 30 seconds for the database to wake up. For this reason, we need the timeout of the Django Lambda to be able to clear the wakeup period of the Aurora Postgres Serverless database.",[11,32876,32877,32878,32881],{},"Since our Lambda function is in public subnet of our VPC and our RDS Aurora database cluster is in an isolated VPC, we need to make sure that the Lambda function can talk to the RDS cluster. CDK makes this really easy. I'll highlight a few important permission related parts of ",[33,32879,32880],{},"awscdk/vpc.py",", the code that defines the nested CloudFormation stack where we define our VPC and resources related to our application (including API Gateway, which is technically not located in our VPC).",[11,32883,32884],{},"First, we define a new security group for our Django Lambda:",[26,32886,32888],{"className":10554,"code":32887,"language":10556,"meta":35,"style":35},"        self.lambda_security_group = ec2.SecurityGroup(\n            self, \"LambdaSecurityGroup\", vpc=self.vpc\n        )\n",[33,32889,32890,32895,32900],{"__ignoreMap":35},[187,32891,32892],{"class":189,"line":190},[187,32893,32894],{},"        self.lambda_security_group = ec2.SecurityGroup(\n",[187,32896,32897],{"class":189,"line":249},[187,32898,32899],{},"            self, \"LambdaSecurityGroup\", vpc=self.vpc\n",[187,32901,32902],{"class":189,"line":312},[187,32903,4531],{},[11,32905,32906],{},"Then, we reference this security group in the list of security group ingresses for our database cluster's security group:",[26,32908,32910],{"className":10554,"code":32909,"language":10556,"meta":35,"style":35},"        self.db_security_group = ec2.CfnSecurityGroup(\n            self,\n            \"DBSecurityGroup\",\n            vpc_id=self.vpc.vpc_id,\n            group_description=\"DBSecurityGroup\",\n            security_group_ingress=[\n                ec2.CfnSecurityGroup.IngressProperty(\n                    ip_protocol=\"tcp\",\n                    to_port=5432,\n                    from_port=5432,\n                    source_security_group_id=self.lambda_security_group.security_group_id,\n                )\n            ],\n        )\n",[33,32911,32912,32917,32921,32926,32931,32936,32941,32946,32951,32956,32961,32966,32970,32975],{"__ignoreMap":35},[187,32913,32914],{"class":189,"line":190},[187,32915,32916],{},"        self.db_security_group = ec2.CfnSecurityGroup(\n",[187,32918,32919],{"class":189,"line":249},[187,32920,22649],{},[187,32922,32923],{"class":189,"line":312},[187,32924,32925],{},"            \"DBSecurityGroup\",\n",[187,32927,32928],{"class":189,"line":319},[187,32929,32930],{},"            vpc_id=self.vpc.vpc_id,\n",[187,32932,32933],{"class":189,"line":325},[187,32934,32935],{},"            group_description=\"DBSecurityGroup\",\n",[187,32937,32938],{"class":189,"line":686},[187,32939,32940],{},"            security_group_ingress=[\n",[187,32942,32943],{"class":189,"line":697},[187,32944,32945],{},"                ec2.CfnSecurityGroup.IngressProperty(\n",[187,32947,32948],{"class":189,"line":1291},[187,32949,32950],{},"                    ip_protocol=\"tcp\",\n",[187,32952,32953],{"class":189,"line":1306},[187,32954,32955],{},"                    to_port=5432,\n",[187,32957,32958],{"class":189,"line":1434},[187,32959,32960],{},"                    from_port=5432,\n",[187,32962,32963],{"class":189,"line":2599},[187,32964,32965],{},"                    source_security_group_id=self.lambda_security_group.security_group_id,\n",[187,32967,32968],{"class":189,"line":2607},[187,32969,4521],{},[187,32971,32972],{"class":189,"line":2621},[187,32973,32974],{},"            ],\n",[187,32976,32977],{"class":189,"line":2631},[187,32978,4531],{},[11,32980,32981],{},"Now let's take a look at the code for the Django lambda itself:",[26,32983,32985],{"className":10554,"code":32984,"language":10556,"meta":35,"style":35},"        self.djambda_lambda = _lambda.Function(\n            self,\n            \"DjambdaLambda\",\n            runtime=_lambda.Runtime.PYTHON_3_8,\n            code=_lambda.AssetCode('./django'),\n            function_name=f\"{scope.full_app_name}-djambda-lambda\",\n            handler=\"djambda.awsgi.lambda_handler\",\n            layers=[self.djambda_layer],\n            timeout=core.Duration.seconds(60),\n            vpc=self.vpc,\n            vpc_subnets=ec2.SubnetSelection(subnets=self.vpc.isolated_subnets),\n            environment={**self.env_vars},\n            security_groups=[self.lambda_security_group],\n        )\n\n        # Use raw override because Lambda's can't be placed in\n        # public subnets using CDK: https://github.com/aws/aws-cdk/issues/8935\n        self.djambda_lambda.node.default_child.add_override(\n            \"Properties.VpcConfig.SubnetIds\",\n            [subnet.subnet_id for subnet in self.vpc.public_subnets],\n        )\n",[33,32986,32987,32992,32996,33001,33005,33010,33015,33020,33025,33029,33034,33039,33043,33048,33052,33056,33061,33066,33071,33076,33081],{"__ignoreMap":35},[187,32988,32989],{"class":189,"line":190},[187,32990,32991],{},"        self.djambda_lambda = _lambda.Function(\n",[187,32993,32994],{"class":189,"line":249},[187,32995,22649],{},[187,32997,32998],{"class":189,"line":312},[187,32999,33000],{},"            \"DjambdaLambda\",\n",[187,33002,33003],{"class":189,"line":319},[187,33004,23768],{},[187,33006,33007],{"class":189,"line":325},[187,33008,33009],{},"            code=_lambda.AssetCode('./django'),\n",[187,33011,33012],{"class":189,"line":686},[187,33013,33014],{},"            function_name=f\"{scope.full_app_name}-djambda-lambda\",\n",[187,33016,33017],{"class":189,"line":697},[187,33018,33019],{},"            handler=\"djambda.awsgi.lambda_handler\",\n",[187,33021,33022],{"class":189,"line":1291},[187,33023,33024],{},"            layers=[self.djambda_layer],\n",[187,33026,33027],{"class":189,"line":1306},[187,33028,23793],{},[187,33030,33031],{"class":189,"line":1434},[187,33032,33033],{},"            vpc=self.vpc,\n",[187,33035,33036],{"class":189,"line":2599},[187,33037,33038],{},"            vpc_subnets=ec2.SubnetSelection(subnets=self.vpc.isolated_subnets),\n",[187,33040,33041],{"class":189,"line":2607},[187,33042,23798],{},[187,33044,33045],{"class":189,"line":2621},[187,33046,33047],{},"            security_groups=[self.lambda_security_group],\n",[187,33049,33050],{"class":189,"line":2631},[187,33051,4531],{},[187,33053,33054],{"class":189,"line":2642},[187,33055,316],{"emptyLinePlaceholder":315},[187,33057,33058],{"class":189,"line":2653},[187,33059,33060],{},"        # Use raw override because Lambda's can't be placed in\n",[187,33062,33063],{"class":189,"line":2665},[187,33064,33065],{},"        # public subnets using CDK: https://github.com/aws/aws-cdk/issues/8935\n",[187,33067,33068],{"class":189,"line":2674},[187,33069,33070],{},"        self.djambda_lambda.node.default_child.add_override(\n",[187,33072,33073],{"class":189,"line":2684},[187,33074,33075],{},"            \"Properties.VpcConfig.SubnetIds\",\n",[187,33077,33078],{"class":189,"line":2694},[187,33079,33080],{},"            [subnet.subnet_id for subnet in self.vpc.public_subnets],\n",[187,33082,33083],{"class":189,"line":2706},[187,33084,4531],{},[11,33086,33087,33090,33091,11478,33093,33096,33097,33100,33101,33104,33105,33107,33108,33111,33112,33115,33116,6867,33119,16045,33122,33125,33126,33129],{},[33,33088,33089],{},"lambda"," is a reserved word in Python, so we import ",[33,33092,33089],{},[33,33094,33095],{},"_lambda"," from ",[33,33098,33099],{},"aws_cdk",". Note that we have a reference to the ",[33,33102,33103],{},"awsgi.py"," handler function in the ",[33,33106,32618],{}," parameter of ",[33,33109,33110],{},"self.djambda_lambda",". I initially put the lambda in ",[33,33113,33114],{},"isolated_subnets"," because CDK won't let you define a ",[33,33117,33118],{},"_lambda.Function",[33,33120,33121],{},"public_subnets",[33,33123,33124],{},"vpc_subnets",". We can override this using ",[33,33127,33128],{},"add_override"," below the Lambda definition to place it in a public subnet instead.",[107,33131,33132],{},[11,33133,33134,33135,33138],{},"I'm not sure if this is necessary, or if this is recommended. Things get a little bit confusing for me here so I would love some insight if anyone knows what would be best to do here. The VPC construct for CDK doesn't allow you to have private subnets when ",[33,33136,33137],{},"nat_gateways=0",". Would I be better off placing the lambda in the isolated subnet with the RDS cluster? Would I still be able to access S3 via the VPC Gateway Endpoint from an isolated subnet?",[11,33140,33141,33142,33144,33145,30978],{},"Putting aside my uncertainties about the correct way to configure our Lambdas functions in a VPC, let's continue! Here's the Lambda invocation for ",[33,33143,32452],{}," that we can run once we have successfully invoked the ",[33,33146,13074],{},[26,33148,33150],{"className":181,"code":33149,"language":183,"meta":35,"style":35},"aws lambda invoke \\\n    --function-name dev-mysite-com-djambda-lambda \\\n    --invocation-type RequestResponse \\\n    --payload '{\"manage\": \"createsuperuser --no-input --username admin --email brian@email.com\"}' \\\n    resp.json\n",[33,33151,33152,33162,33171,33179,33188],{"__ignoreMap":35},[187,33153,33154,33156,33158,33160],{"class":189,"line":190},[187,33155,12748],{"class":193},[187,33157,32805],{"class":196},[187,33159,32808],{"class":196},[187,33161,16644],{"class":588},[187,33163,33164,33166,33169],{"class":189,"line":249},[187,33165,32815],{"class":588},[187,33167,33168],{"class":196}," dev-mysite-com-djambda-lambda",[187,33170,16644],{"class":588},[187,33172,33173,33175,33177],{"class":189,"line":312},[187,33174,32825],{"class":588},[187,33176,32828],{"class":196},[187,33178,16644],{"class":588},[187,33180,33181,33183,33186],{"class":189,"line":319},[187,33182,32835],{"class":588},[187,33184,33185],{"class":196}," '{\"manage\": \"createsuperuser --no-input --username admin --email brian@email.com\"}'",[187,33187,16644],{"class":588},[187,33189,33190],{"class":189,"line":325},[187,33191,32845],{"class":196},[11,33193,33194,33195,33197,33198,16045,33201,33204,33205,358],{},"This assumes that there is an environment variable defined in our Lambda's ",[33,33196,13749],{}," variables that we passed in ",[33,33199,33200],{},"{**self.env_vars}",[33,33202,33203],{},"DJANGO_SUPERUSER_USERNAME",". Here's what we have in ",[33,33206,33207],{},"env_vars",[26,33209,33211],{"className":10554,"code":33210,"language":10556,"meta":35,"style":35},"        self.env_vars = {\n            \"POSTGRES_SERVICE_HOST\": self.rds_cluster.get_att(\n                \"Endpoint.Address\"\n            ).to_string(),\n            \"POSTGRES_PASSWORD\": os.environ.get(\"DB_PASSWORD\", \"db-password\"),\n            \"AWS_STORAGE_BUCKET_NAME\": f\"{scope.full_app_name}-assets\",\n            \"DEBUG\": \"\",\n            \"DJANGO_SUPERUSER_PASSWORD\": os.environ.get(\n                \"DJANGO_SUPERUSER_PASSWORD\", \"Mypassword1!\"\n            ),\n            \"DJANGO_SUPERUSER_USERNAME\": os.environ.get(\n                \"DJANGO_SUPERUSER_USERNAME\", \"admin\"\n            ),\n        }\n",[33,33212,33213,33217,33222,33227,33232,33237,33242,33247,33252,33257,33262,33267,33272,33276],{"__ignoreMap":35},[187,33214,33215],{"class":189,"line":190},[187,33216,23694],{},[187,33218,33219],{"class":189,"line":249},[187,33220,33221],{},"            \"POSTGRES_SERVICE_HOST\": self.rds_cluster.get_att(\n",[187,33223,33224],{"class":189,"line":312},[187,33225,33226],{},"                \"Endpoint.Address\"\n",[187,33228,33229],{"class":189,"line":319},[187,33230,33231],{},"            ).to_string(),\n",[187,33233,33234],{"class":189,"line":325},[187,33235,33236],{},"            \"POSTGRES_PASSWORD\": os.environ.get(\"DB_PASSWORD\", \"db-password\"),\n",[187,33238,33239],{"class":189,"line":686},[187,33240,33241],{},"            \"AWS_STORAGE_BUCKET_NAME\": f\"{scope.full_app_name}-assets\",\n",[187,33243,33244],{"class":189,"line":697},[187,33245,33246],{},"            \"DEBUG\": \"\",\n",[187,33248,33249],{"class":189,"line":1291},[187,33250,33251],{},"            \"DJANGO_SUPERUSER_PASSWORD\": os.environ.get(\n",[187,33253,33254],{"class":189,"line":1306},[187,33255,33256],{},"                \"DJANGO_SUPERUSER_PASSWORD\", \"Mypassword1!\"\n",[187,33258,33259],{"class":189,"line":1434},[187,33260,33261],{},"            ),\n",[187,33263,33264],{"class":189,"line":2599},[187,33265,33266],{},"            \"DJANGO_SUPERUSER_USERNAME\": os.environ.get(\n",[187,33268,33269],{"class":189,"line":2607},[187,33270,33271],{},"                \"DJANGO_SUPERUSER_USERNAME\", \"admin\"\n",[187,33273,33274],{"class":189,"line":2621},[187,33275,33261],{},[187,33277,33278],{"class":189,"line":2631},[187,33279,9780],{},[11,33281,33282],{},"We can use environment variables to set an initial password for our superuser. We also define additionl environment variables that we will use for our datbase connection, and the S3 bucket that we will use for Django's static and media assets.",[168,33284,12327],{"id":33285},"s3",[11,33287,33288],{},"To access S3 from our Lambda function in a VPC, we need to use a VPC Gateway Endpoint for S3. This is a free service that enables us to access S3 directly from our VPC without going through the internet, and instead using a private connection . This again has to do with the fact that our Django Lambda can't access the internet because the network interfaces created by Lambda only have private IP addresses and would require NAT in order to connect to resources on the internet. Here's how we define the VPC as well as the VPC Gateway Endpoint using CDK:",[26,33290,33292],{"className":10554,"code":33291,"language":10556,"meta":35,"style":35},"        self.vpc = ec2.Vpc(\n            self,\n            \"Vpc\",\n            max_azs=2,\n            cidr=\"10.0.0.0/16\",\n            nat_gateways=0,\n            subnet_configuration=[\n                ec2.SubnetConfiguration(\n                    subnet_type=ec2.SubnetType.PUBLIC,\n                    name=\"Public\",\n                    cidr_mask=24,\n                ),\n                ec2.SubnetConfiguration(\n                    subnet_type=ec2.SubnetType.ISOLATED,\n                    name=\"Isolated\",\n                    cidr_mask=24,\n                ),\n            ],\n        )\n\n        self.vpc.add_gateway_endpoint(\n            \"S3Gateway\", service=ec2.GatewayVpcEndpointAwsService('s3')\n        )\n",[33,33293,33294,33299,33303,33308,33313,33318,33323,33328,33333,33338,33343,33348,33353,33357,33362,33367,33371,33375,33379,33383,33387,33392,33397],{"__ignoreMap":35},[187,33295,33296],{"class":189,"line":190},[187,33297,33298],{},"        self.vpc = ec2.Vpc(\n",[187,33300,33301],{"class":189,"line":249},[187,33302,22649],{},[187,33304,33305],{"class":189,"line":312},[187,33306,33307],{},"            \"Vpc\",\n",[187,33309,33310],{"class":189,"line":319},[187,33311,33312],{},"            max_azs=2,\n",[187,33314,33315],{"class":189,"line":325},[187,33316,33317],{},"            cidr=\"10.0.0.0/16\",\n",[187,33319,33320],{"class":189,"line":686},[187,33321,33322],{},"            nat_gateways=0,\n",[187,33324,33325],{"class":189,"line":697},[187,33326,33327],{},"            subnet_configuration=[\n",[187,33329,33330],{"class":189,"line":1291},[187,33331,33332],{},"                ec2.SubnetConfiguration(\n",[187,33334,33335],{"class":189,"line":1306},[187,33336,33337],{},"                    subnet_type=ec2.SubnetType.PUBLIC,\n",[187,33339,33340],{"class":189,"line":1434},[187,33341,33342],{},"                    name=\"Public\",\n",[187,33344,33345],{"class":189,"line":2599},[187,33346,33347],{},"                    cidr_mask=24,\n",[187,33349,33350],{"class":189,"line":2607},[187,33351,33352],{},"                ),\n",[187,33354,33355],{"class":189,"line":2621},[187,33356,33332],{},[187,33358,33359],{"class":189,"line":2631},[187,33360,33361],{},"                    subnet_type=ec2.SubnetType.ISOLATED,\n",[187,33363,33364],{"class":189,"line":2642},[187,33365,33366],{},"                    name=\"Isolated\",\n",[187,33368,33369],{"class":189,"line":2653},[187,33370,33347],{},[187,33372,33373],{"class":189,"line":2665},[187,33374,33352],{},[187,33376,33377],{"class":189,"line":2674},[187,33378,32974],{},[187,33380,33381],{"class":189,"line":2684},[187,33382,4531],{},[187,33384,33385],{"class":189,"line":2694},[187,33386,316],{"emptyLinePlaceholder":315},[187,33388,33389],{"class":189,"line":2706},[187,33390,33391],{},"        self.vpc.add_gateway_endpoint(\n",[187,33393,33394],{"class":189,"line":2715},[187,33395,33396],{},"            \"S3Gateway\", service=ec2.GatewayVpcEndpointAwsService('s3')\n",[187,33398,33399],{"class":189,"line":2725},[187,33400,4531],{},[11,33402,33403,33404,33407],{},"With this VPC endpoint in place, we are almost ready to run our collectstatic command, but it won't work yet. This is because we haven't given our Lambda function access to write files to the S3 assets bucket for our Django application's static and media files. We can grant this permission with the following snippet from ",[33,33405,33406],{},"awscdk/app_stack.py",", the file that defines the root CloudFormation stack for our application:",[26,33409,33411],{"className":10554,"code":33410,"language":10556,"meta":35,"style":35},"        self.backend_assets_bucket.grant_read_write(\n            self.vpc_stack.djambda_lambda\n        )\n",[33,33412,33413,33418,33423],{"__ignoreMap":35},[187,33414,33415],{"class":189,"line":190},[187,33416,33417],{},"        self.backend_assets_bucket.grant_read_write(\n",[187,33419,33420],{"class":189,"line":249},[187,33421,33422],{},"            self.vpc_stack.djambda_lambda\n",[187,33424,33425],{"class":189,"line":312},[187,33426,4531],{},[11,33428,33429,33430,358],{},"Finally, we need to add the following settings to ",[33,33431,31438],{},[26,33433,33435],{"className":10554,"code":33434,"language":10556,"meta":35,"style":35},"STATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, 'static')\nSTATICFILES_STORAGE = \"djambda.storage_backends.StaticStorage\"\n\nAWS_DEFAULT_ACL = None\nAWS_STORAGE_BUCKET_NAME = os.environ.get(\n    \"AWS_STORAGE_BUCKET_NAME\", \"bucketname\"\n)\nAWS_S3_OBJECT_PARAMETERS = {\n    \"CacheControl\": \"max-age=86400\",\n}\nAWS_PRIVATE_MEDIA_LOCATION = \"media/private\"\nAWS_STATIC_LOCATION = \"static\"\nPRIVATE_FILE_STORAGE = \"backend.storage_backends.PrivateMediaStorage\"\nAWS_S3_CUSTOM_DOMAIN = f\"{AWS_STORAGE_BUCKET_NAME}.s3.amazonaws.com\"\n\nif not DEBUG:\n    MEDIA_ROOT = \"media\"\n    MEDIA_URL = f\"https://{AWS_S3_CUSTOM_DOMAIN}/{MEDIA_ROOT}/\"\n    STATIC_URL = f\"https://{AWS_S3_CUSTOM_DOMAIN}/{AWS_STATIC_LOCATION}/\"\n    FORCE_SCRIPT_NAME = \"/prod\"\n",[33,33436,33437,33442,33447,33452,33456,33461,33466,33471,33475,33480,33485,33489,33494,33499,33504,33509,33513,33518,33523,33528,33533],{"__ignoreMap":35},[187,33438,33439],{"class":189,"line":190},[187,33440,33441],{},"STATIC_URL = '/static/'\n",[187,33443,33444],{"class":189,"line":249},[187,33445,33446],{},"STATIC_ROOT = os.path.join(BASE_DIR, 'static')\n",[187,33448,33449],{"class":189,"line":312},[187,33450,33451],{},"STATICFILES_STORAGE = \"djambda.storage_backends.StaticStorage\"\n",[187,33453,33454],{"class":189,"line":319},[187,33455,316],{"emptyLinePlaceholder":315},[187,33457,33458],{"class":189,"line":325},[187,33459,33460],{},"AWS_DEFAULT_ACL = None\n",[187,33462,33463],{"class":189,"line":686},[187,33464,33465],{},"AWS_STORAGE_BUCKET_NAME = os.environ.get(\n",[187,33467,33468],{"class":189,"line":697},[187,33469,33470],{},"    \"AWS_STORAGE_BUCKET_NAME\", \"bucketname\"\n",[187,33472,33473],{"class":189,"line":1291},[187,33474,621],{},[187,33476,33477],{"class":189,"line":1306},[187,33478,33479],{},"AWS_S3_OBJECT_PARAMETERS = {\n",[187,33481,33482],{"class":189,"line":1434},[187,33483,33484],{},"    \"CacheControl\": \"max-age=86400\",\n",[187,33486,33487],{"class":189,"line":2599},[187,33488,1309],{},[187,33490,33491],{"class":189,"line":2607},[187,33492,33493],{},"AWS_PRIVATE_MEDIA_LOCATION = \"media/private\"\n",[187,33495,33496],{"class":189,"line":2621},[187,33497,33498],{},"AWS_STATIC_LOCATION = \"static\"\n",[187,33500,33501],{"class":189,"line":2631},[187,33502,33503],{},"PRIVATE_FILE_STORAGE = \"backend.storage_backends.PrivateMediaStorage\"\n",[187,33505,33506],{"class":189,"line":2642},[187,33507,33508],{},"AWS_S3_CUSTOM_DOMAIN = f\"{AWS_STORAGE_BUCKET_NAME}.s3.amazonaws.com\"\n",[187,33510,33511],{"class":189,"line":2653},[187,33512,316],{"emptyLinePlaceholder":315},[187,33514,33515],{"class":189,"line":2665},[187,33516,33517],{},"if not DEBUG:\n",[187,33519,33520],{"class":189,"line":2674},[187,33521,33522],{},"    MEDIA_ROOT = \"media\"\n",[187,33524,33525],{"class":189,"line":2684},[187,33526,33527],{},"    MEDIA_URL = f\"https://{AWS_S3_CUSTOM_DOMAIN}/{MEDIA_ROOT}/\"\n",[187,33529,33530],{"class":189,"line":2694},[187,33531,33532],{},"    STATIC_URL = f\"https://{AWS_S3_CUSTOM_DOMAIN}/{AWS_STATIC_LOCATION}/\"\n",[187,33534,33535],{"class":189,"line":2706},[187,33536,33537],{},"    FORCE_SCRIPT_NAME = \"/prod\"\n",[11,33539,33540],{},"We can run the collectstatic command with the following Lambda invocation:",[26,33542,33544],{"className":181,"code":33543,"language":183,"meta":35,"style":35},"aws lambda invoke \\\n    --function-name dev-mysite-com-djambda-lambda \\\n    --invocation-type RequestResponse \\\n    --payload '{\"manage\": \"collectstatic --no-input\"}' \\\n    resp.json\n",[33,33545,33546,33556,33564,33572,33581],{"__ignoreMap":35},[187,33547,33548,33550,33552,33554],{"class":189,"line":190},[187,33549,12748],{"class":193},[187,33551,32805],{"class":196},[187,33553,32808],{"class":196},[187,33555,16644],{"class":588},[187,33557,33558,33560,33562],{"class":189,"line":249},[187,33559,32815],{"class":588},[187,33561,33168],{"class":196},[187,33563,16644],{"class":588},[187,33565,33566,33568,33570],{"class":189,"line":312},[187,33567,32825],{"class":588},[187,33569,32828],{"class":196},[187,33571,16644],{"class":588},[187,33573,33574,33576,33579],{"class":189,"line":319},[187,33575,32835],{"class":588},[187,33577,33578],{"class":196}," '{\"manage\": \"collectstatic --no-input\"}'",[187,33580,16644],{"class":588},[187,33582,33583],{"class":189,"line":325},[187,33584,32845],{"class":196},[168,33586,33588],{"id":33587},"api-gateway-and-the-lambda-proxy-pattern","API Gateway and the Lambda proxy pattern",[11,33590,33591,33592,33597,33598,33602,33603,33608],{},"I first read about the Lambda proxy pattern in an article titled ",[15,33593,33596],{"href":33594,"rel":33595},"https://serverlessfirst.com/lambda-vpc-internet-access-no-nat-gateway/",[19],"How to access VPC and internet resources from Lambda without paying for a NAT Gateway"," on Paul Swail's website ",[15,33599,33600],{"href":33600,"rel":33601},"https://serverlessfirst.com/",[19],". This site has tons of great serverless content, check out the ",[15,33604,33607],{"href":33605,"rel":33606},"https://serverlessfirst.com/articles/",[19],"list of articles",". Thank you Paul for putting out great serverless resources!",[11,33610,33611],{},"Let's take a look at the Proxy Lambda function next. The function itself is pretty straightforward:",[26,33613,33615],{"className":10554,"code":33614,"language":10556,"meta":35,"style":35},"import json\nimport os\nimport boto3\n\nlambda_client = boto3.client('lambda', region_name='us-east-1')\n\n\ndef handler(event, context):\n    invoke_response = lambda_client.invoke(\n        FunctionName=os.environ.get(\"FUNCTION_NAME\", None),\n        InvocationType='RequestResponse',\n        Payload=json.dumps(event),\n    )\n\n    data = invoke_response['Payload'].read()\n\n    return data\n",[33,33616,33617,33621,33625,33629,33633,33638,33642,33646,33650,33655,33660,33665,33670,33674,33678,33683,33687],{"__ignoreMap":35},[187,33618,33619],{"class":189,"line":190},[187,33620,10340],{},[187,33622,33623],{"class":189,"line":249},[187,33624,10345],{},[187,33626,33627],{"class":189,"line":312},[187,33628,23196],{},[187,33630,33631],{"class":189,"line":319},[187,33632,316],{"emptyLinePlaceholder":315},[187,33634,33635],{"class":189,"line":325},[187,33636,33637],{},"lambda_client = boto3.client('lambda', region_name='us-east-1')\n",[187,33639,33640],{"class":189,"line":686},[187,33641,316],{"emptyLinePlaceholder":315},[187,33643,33644],{"class":189,"line":697},[187,33645,316],{"emptyLinePlaceholder":315},[187,33647,33648],{"class":189,"line":1291},[187,33649,23623],{},[187,33651,33652],{"class":189,"line":1306},[187,33653,33654],{},"    invoke_response = lambda_client.invoke(\n",[187,33656,33657],{"class":189,"line":1434},[187,33658,33659],{},"        FunctionName=os.environ.get(\"FUNCTION_NAME\", None),\n",[187,33661,33662],{"class":189,"line":2599},[187,33663,33664],{},"        InvocationType='RequestResponse',\n",[187,33666,33667],{"class":189,"line":2607},[187,33668,33669],{},"        Payload=json.dumps(event),\n",[187,33671,33672],{"class":189,"line":2621},[187,33673,23653],{},[187,33675,33676],{"class":189,"line":2631},[187,33677,316],{"emptyLinePlaceholder":315},[187,33679,33680],{"class":189,"line":2642},[187,33681,33682],{},"    data = invoke_response['Payload'].read()\n",[187,33684,33685],{"class":189,"line":2653},[187,33686,316],{"emptyLinePlaceholder":315},[187,33688,33689],{"class":189,"line":2665},[187,33690,33691],{},"    return data\n",[11,33693,33694],{},"There are just a few things to setup in our CDK code to make the proxy pattern work:",[2276,33696,33697,33700,33703],{},[919,33698,33699],{},"Define the Lambda function",[919,33701,33702],{},"Give the proxy Lambda permission to invoke the Django Lambda",[919,33704,33705,33706,33709,33710],{},"Define a ",[33,33707,33708],{},"LambdaRestApi"," construct with the Proxy Lambda as the ",[33,33711,32618],{},[11,33713,33714],{},"Here's what that code looks like:",[26,33716,33718],{"className":10554,"code":33717,"language":10556,"meta":35,"style":35},"        self.proxy_lambda = _lambda.Function(\n            self,\n            \"ProxyLambda\",\n            code=_lambda.AssetCode(\"./awslambda\"),\n            runtime=_lambda.Runtime.PYTHON_3_8,\n            layers=[self.djambda_layer],\n            handler=\"proxy_lambda.handler\",\n            timeout=core.Duration.seconds(60),\n            environment={\"FUNCTION_NAME\": self.djambda_lambda.function_name},\n        )\n\n        self.djambda_lambda.grant_invoke(self.proxy_lambda)\n\n        self.apigw = apigw.LambdaRestApi(\n            self, 'DjambdaEndpoint', handler=self.proxy_lambda,\n        )\n",[33,33719,33720,33725,33729,33734,33739,33743,33747,33752,33756,33761,33765,33769,33774,33778,33783,33788],{"__ignoreMap":35},[187,33721,33722],{"class":189,"line":190},[187,33723,33724],{},"        self.proxy_lambda = _lambda.Function(\n",[187,33726,33727],{"class":189,"line":249},[187,33728,22649],{},[187,33730,33731],{"class":189,"line":312},[187,33732,33733],{},"            \"ProxyLambda\",\n",[187,33735,33736],{"class":189,"line":319},[187,33737,33738],{},"            code=_lambda.AssetCode(\"./awslambda\"),\n",[187,33740,33741],{"class":189,"line":325},[187,33742,23768],{},[187,33744,33745],{"class":189,"line":686},[187,33746,33024],{},[187,33748,33749],{"class":189,"line":697},[187,33750,33751],{},"            handler=\"proxy_lambda.handler\",\n",[187,33753,33754],{"class":189,"line":1291},[187,33755,23793],{},[187,33757,33758],{"class":189,"line":1306},[187,33759,33760],{},"            environment={\"FUNCTION_NAME\": self.djambda_lambda.function_name},\n",[187,33762,33763],{"class":189,"line":1434},[187,33764,4531],{},[187,33766,33767],{"class":189,"line":2599},[187,33768,316],{"emptyLinePlaceholder":315},[187,33770,33771],{"class":189,"line":2607},[187,33772,33773],{},"        self.djambda_lambda.grant_invoke(self.proxy_lambda)\n",[187,33775,33776],{"class":189,"line":2621},[187,33777,316],{"emptyLinePlaceholder":315},[187,33779,33780],{"class":189,"line":2631},[187,33781,33782],{},"        self.apigw = apigw.LambdaRestApi(\n",[187,33784,33785],{"class":189,"line":2642},[187,33786,33787],{},"            self, 'DjambdaEndpoint', handler=self.proxy_lambda,\n",[187,33789,33790],{"class":189,"line":2653},[187,33791,4531],{},[11,33793,33794,33795,33798],{},"That's it! We can now access our Django project at the API Gateway ",[33,33796,33797],{},"execute-api"," endpoint. This is a special URL that has the format:",[26,33800,33803],{"className":33801,"code":33802,"language":31},[29],"https://abc123.execute-api.us-east-1.amazonaws.com/prod\n",[33,33804,33802],{"__ignoreMap":35},[916,33806,33807,33813,33819],{},[919,33808,33809,33812],{},[33,33810,33811],{},"abc123"," is the id of the API Gateway that we created (you can find the value of this id in the API Gateway section of the AWS management console)]",[919,33814,33815,33816,33818],{},"The region ",[33,33817,18270],{}," is the region where you deployed the API Gateway",[919,33820,33821,33824],{},[33,33822,33823],{},"/prod"," is the stage name",[11,33826,33827,33828,33830,33831,33833,33834,33836,33837,33840],{},"The stage name part is a little confusing for me. It is meant to be a URL suffix that indicates production, staging, etc., but I typically separate environments at the level of the CloudFormation stack, so all of my environments use the ",[33,33829,33823],{}," suffix. In order for Django to work properly, we need to tell it that our site will be served at this ",[33,33832,33823],{}," path (for example ",[33,33835,15757],{}," will now be served at ",[33,33838,33839],{},"/prod/admin",") by setting a special value in our settings module:",[26,33842,33844],{"className":10554,"code":33843,"language":10556,"meta":35,"style":35},"FORCE_SCRIPT_NAME = \"/prod\"\n",[33,33845,33846],{"__ignoreMap":35},[187,33847,33848],{"class":189,"line":190},[187,33849,33843],{},[11,33851,33852,33853,33856,33857,33859,33860,33862,33863,33865,33866,33869,33870,33872],{},"This will make dealing with URLs easier, mostly because we don't actually have to change anything in our ",[33,33854,33855],{},"urls.py",". I tried adding ",[33,33858,33823],{}," to my URL paths in ",[33,33861,33855],{},", but the Django admin doesn't work properly because going to ",[33,33864,33839],{}," will redirect you to ",[33,33867,33868],{},"/admin/login"," which will thrown an error with API Gateway since our application must be on the ",[33,33871,33823],{}," subpath.",[168,33874,33876],{"id":33875},"setting-a-custom-url-for-api-gateway","Setting a custom URL for API Gateway",[11,33878,33879,33880,33883],{},"If you don't mind having a URL in the form of ",[33,33881,33882],{},"https://abc123.execute-api.us-east-1.amazonaws.com/prod",", then everything should be good to go. If you do want a custom URL for API Gateway, there are only two things you need to do:",[2276,33885,33886,33889,33897,33900],{},[919,33887,33888],{},"Get a domain and hosted zone set up in Route53 (this typically costs about $12/year)",[919,33890,18856,33891,14372,33893,33896],{},[33,33892,18281],{},[33,33894,33895],{},"HOSTED_ZONE_ID"," to environment variables (see below for how to do this)",[919,33898,33899],{},"Define an ACM certificate in our CDK code and",[919,33901,18533,33902,33904,33905,33908],{},[33,33903,33708],{},"'s ",[33,33906,33907],{},"add_domain_name"," method to add the domain name and certificate to the API Gateway endpoint.",[11,33910,33911,33912,33917,33918,752],{},"Here's a ",[15,33913,33916],{"href":33914,"rel":33915},"https://docs.aws.amazon.com/cdk/api/latest/python/aws_cdk.aws_apigateway/LambdaRestApi.html#aws_cdk.aws_apigateway.LambdaRestApi.add_domain_name",[19],"link to the CDK Python documentation"," on ",[33,33919,33907],{},[11,33921,33922,33923,33926],{},"When you configure a custom domain name, you don't need to set the ",[33,33924,33925],{},"FORCE_SCRIPT_NAME"," setting in Django settings.",[168,33928,33930],{"id":33929},"project-directory-structure","Project directory structure",[11,33932,33933,33934,358],{},"Let's take a quick look at the structure of the project using ",[33,33935,33936],{},"tree",[26,33938,33941],{"className":33939,"code":33940,"language":31},[29],"tree -L 3 .\n.\n├── awscdk\n│   ├── app.py\n│   ├── awscdk\n│   │   ├── app_stack.py  \u003C----------------- CDK application overview\n│   │   ├── backend_assets.py\n│   │   ├── cert.py\n│   │   ├── cloudfront.py\n│   │   ├── hosted_zone.py\n│   │   ├── __init__.py\n│   │   ├── static_site_bucket.py\n│   │   └── vpc.py \u003C------------------------ VPC, Lambdas, API Gateway and more\n│   ├── cdk.json                             defined here\n│   ├── README.md\n│   ├── requirements.txt\n│   ├── setup.py\n│   └── source.bat\n├── awslambda\n│   └── proxy_lambda.py \u003C------------------- Proxy Lambda\n├── django\n│   ├── core \u003C------------------------------ A sample Django app\n│   │   ├── admin.py\n│   │   ├── apps.py\n│   │   ├── __init__.py\n│   │   ├── migrations\n│   │   ├── models.py\n│   │   ├── tests.py\n│   │   ├── urls.py\n│   │   └── views.py\n│   ├── djambda\n│   │   ├── asgi.py\n│   │   ├── awsgi.py \u003C---------------------- Django Lambda handler\n│   │   ├── __init__.py\n│   │   ├── settings.py \u003C------------------- Django settings (RDS connections, S3 static/media)\n│   │   ├── storage_backends.py\n│   │   ├── urls.py\n│   │   └── wsgi.py\n│   ├── manage.py\n│   └── requirements.txt\n├── layers \u003C-------------------------------- Lambda Layers (python dependencies)\n│   └── django                               Note: This folder is not committed to git\n│       └── python\n├── ARTICLE.md \u003C---------------------------- This article\n├── gitlab-ci.yml\n├── .variables \u003C---------------------------- Environment variables needed for deployment\n└── README.md\n",[33,33942,33940],{"__ignoreMap":35},[11,33944,33945,33946,637,33949,1172,33952,33954],{},"The three top level folders are ",[33,33947,33948],{},"awscdk",[33,33950,33951],{},"awslambda",[33,33953,15290],{},". To deploy our CDK code, we run the following command from the root of the project:",[26,33956,33958],{"className":181,"code":33957,"language":183,"meta":35,"style":35},"cdk deploy --app awscdk/app.py\n",[33,33959,33960],{"__ignoreMap":35},[187,33961,33962,33964,33967,33970],{"class":189,"line":190},[187,33963,15291],{"class":193},[187,33965,33966],{"class":196}," deploy",[187,33968,33969],{"class":588}," --app",[187,33971,33972],{"class":196}," awscdk/app.py\n",[11,33974,33975,33976,33979,33980,33983,33984,1172,33987,33989],{},"We can also run ",[33,33977,33978],{},"cdk synth --app awscdk/app.py"," to preview changes to our CDK code by reviewing the CloudFormation templates generated by ",[33,33981,33982],{},"cdk synth",". Running the ",[33,33985,33986],{},"cdk deploy",[33,33988,33982],{}," commands at root of the project means that we need to specificy lambda code assets from the root of the project as well. This is why the Django application's Lambda function references the code with:",[26,33991,33993],{"className":10554,"code":33992,"language":10556,"meta":35,"style":35},"code=_lambda.AssetCode('./django'),\n",[33,33994,33995],{"__ignoreMap":35},[187,33996,33997],{"class":189,"line":190},[187,33998,33992],{},[168,34000,34002],{"id":34001},"deploying","Deploying",[11,34004,34005],{},"CDK is easy to use both locally and in a CI/CD tool such as GitLab CI, my CI/CD tool of choice. There are a few things to coordinate when deploying in both of these environments:",[2276,34007,34008,34011],{},[919,34009,34010],{},"Environment variables",[919,34012,34013],{},"Dependencies",[911,34015,34017],{"id":34016},"deploying-from-a-terminal","Deploying from a terminal",[11,34019,34020,34021,34024],{},"If you are new to CDK, you will need to make sure that you have ran the ",[33,34022,34023],{},"cdk bootstrap"," command at least once on your account. This command sets up a CloudFormation stack that CDK uses internally to manage deployments.",[11,34026,34027,34028,358],{},"The environment variables needed for deployment are defined in ",[33,34029,34030],{},".variables.template",[26,34032,34035],{"className":34033,"code":34034,"language":31},[29],"export APP_NAME=\nexport AWS_ACCESS_KEY_ID=\nexport AWS_ACCOUNT_ID=\nexport AWS_DEFAULT_REGION=\nexport AWS_SECRET_ACCESS_KEY=\nexport DOMAIN_NAME=\nexport HOSTED_ZONE_ID=\n",[33,34036,34034],{"__ignoreMap":35},[916,34038,34039,34052],{},[919,34040,34041,34044,34045,34048,34049,34051],{},[33,34042,34043],{},"APP_NAME"," should be a URL compatible name, such as ",[33,34046,34047],{},"my-app"," (don't put ",[33,34050,752],{}," in this variable)",[919,34053,34054,1172,34056,34058],{},[33,34055,18281],{},[33,34057,33895],{}," are needed for adding a custom domain name to API Gateway",[11,34060,34061,34062,34065],{},"Copy this template into a file in the root of the project called ",[33,34063,34064],{},".variables"," and set these environment variables with the following command:",[26,34067,34070],{"className":34068,"code":34069,"language":31},[29],". .variables\n",[33,34071,34069],{"__ignoreMap":35},[11,34073,34074,34075,34077],{},"Next, we need to install our Python dependencies locally with the ",[33,34076,24141],{}," target flag so that the Lambda layers we define in CDK can find the files needed to be packaged into our Lambda layer and made available to our Lambda function at runtime. Install dependencies to the target directory by running the following command from the root of the project:",[26,34079,34082],{"className":34080,"code":34081,"language":31},[29],"pip install -r django/requirements.txt -t layers/django/python\n",[33,34083,34081],{"__ignoreMap":35},[11,34085,34086],{},"Now we can deploy our serverless application to AWS using CDK. First, activate the CDK Python virtual environment with:",[26,34088,34091],{"className":34089,"code":34090,"language":31},[29],"source awscdk/.env/bin/activate\n",[33,34092,34090],{"__ignoreMap":35},[11,34094,34095,34096,752],{},"Also make sure that you are using a version of Node.js greater or equal to 10. I do this with ",[33,34097,34098],{},"nvm use 13",[11,34100,34101,34102,34105],{},"Make sure that all of the dependencies defined in ",[33,34103,34104],{},"awscdk/setup.py"," are up-to-date. Make sure that there are no issues witht the CDK code by running:",[26,34107,34110],{"className":34108,"code":34109,"language":31},[29],"cdk synth --app awscdk/app.py\n",[33,34111,34109],{"__ignoreMap":35},[11,34113,34114,34115,34118],{},"Inspect the contents of ",[33,34116,34117],{},"cdk.out","; these are the CloudFormation templates generated by CDK that will be used to deploy all of our resources. If everything looks good to go, deploy with the following command:",[26,34120,34122],{"className":34121,"code":34109,"language":31},[29],[33,34123,34109],{"__ignoreMap":35},[11,34125,34126],{},"Follow the output of this command and ensure that it completes successfully. You can also follow along in the CloudFormation section of the AWS management console to make sure that things are deploying properly.",[11,34128,34129,34130,637,34132,1172,34134,34136],{},"Once everything has finished deploying, you will need to run ",[33,34131,13074],{},[33,34133,32452],{},[33,34135,31844],{}," after the first deployment.",[107,34138,34139],{},[11,34140,34141],{},"TODO: consolidate these steps with a Makefile or bash script",[911,34143,34145],{"id":34144},"deploying-with-gitlab-ci","Deploying with GitLab CI",[11,34147,34148],{},"Deploying locally is fine, but it is better to run CDK commands from a CI/CD process so we can keep track of the commits, pipeline results, commit messages, etc. The process is very similar to everything we do locally, but you will need to add environment variables to the GitLab project's Settings > CI/CD > Variables. I have broken out dependency installation into a separate stage. This is not entirely necessary, but it helps keep things easy to follow:",[26,34150,34152],{"className":2507,"code":34151,"language":2509,"meta":35,"style":35},"pip_install:\n  stage: build\n  artifacts:\n    paths:\n      - layers/django/python\n  script:\n    - pip install -r django/requirements.txt -t layers/django/python\n\ncdk_deploy:\n  stage: deploy\n  before_script:\n    - apt-get -qq update && apt-get -y install nodejs npm\n    - npm i -g aws-cdk\n    - pip3 install -e awscdk\n  script:\n    - cdk bootstrap --app awscdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n    - cdk deploy --app awscdk/app.py --require-approval never\n",[33,34153,34154,34160,34168,34174,34180,34187,34193,34199,34203,34209,34217,34223,34229,34235,34242,34248,34255],{"__ignoreMap":35},[187,34155,34156,34158],{"class":189,"line":190},[187,34157,24067],{"class":2516},[187,34159,2520],{"class":577},[187,34161,34162,34164,34166],{"class":189,"line":249},[187,34163,22797],{"class":2516},[187,34165,585],{"class":577},[187,34167,24040],{"class":196},[187,34169,34170,34172],{"class":189,"line":312},[187,34171,24106],{"class":2516},[187,34173,2520],{"class":577},[187,34175,34176,34178],{"class":189,"line":319},[187,34177,24113],{"class":2516},[187,34179,2520],{"class":577},[187,34181,34182,34184],{"class":189,"line":325},[187,34183,2610],{"class":577},[187,34185,34186],{"class":196},"layers/django/python\n",[187,34188,34189,34191],{"class":189,"line":686},[187,34190,22861],{"class":2516},[187,34192,2520],{"class":577},[187,34194,34195,34197],{"class":189,"line":697},[187,34196,14709],{"class":577},[187,34198,34081],{"class":196},[187,34200,34201],{"class":189,"line":1291},[187,34202,316],{"emptyLinePlaceholder":315},[187,34204,34205,34207],{"class":189,"line":1306},[187,34206,22790],{"class":2516},[187,34208,2520],{"class":577},[187,34210,34211,34213,34215],{"class":189,"line":1434},[187,34212,22797],{"class":2516},[187,34214,585],{"class":577},[187,34216,7546],{"class":196},[187,34218,34219,34221],{"class":189,"line":2599},[187,34220,22834],{"class":2516},[187,34222,2520],{"class":577},[187,34224,34225,34227],{"class":189,"line":2607},[187,34226,14709],{"class":577},[187,34228,22843],{"class":196},[187,34230,34231,34233],{"class":189,"line":2621},[187,34232,14709],{"class":577},[187,34234,22457],{"class":196},[187,34236,34237,34239],{"class":189,"line":2631},[187,34238,14709],{"class":577},[187,34240,34241],{"class":196},"pip3 install -e awscdk\n",[187,34243,34244,34246],{"class":189,"line":2642},[187,34245,22861],{"class":2516},[187,34247,2520],{"class":577},[187,34249,34250,34252],{"class":189,"line":2653},[187,34251,14709],{"class":577},[187,34253,34254],{"class":196},"cdk bootstrap --app awscdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n",[187,34256,34257,34259],{"class":189,"line":2665},[187,34258,14709],{"class":577},[187,34260,34261],{"class":196},"cdk deploy --app awscdk/app.py --require-approval never\n",[11,34263,34264,34265,34268],{},"For management commands, we use a ",[33,34266,34267],{},".base_task"," template and reuse this for each command:",[26,34270,34272],{"className":2507,"code":34271,"language":2509,"meta":35,"style":35},".base_task: &task\n  image: python:3.8\n  stage: deploy\n  rules:\n    - when: manual\n  before_script:\n    - pip install awscli\n  after_script:\n    - cat invoke_response.json\n\nmigrate:\n  \u003C\u003C: *task\n  script:\n    - |\n      aws lambda invoke \\\n        --function-name ${ENVIRONMENT}-${APP_NAME}-djambda-lambda \\\n        --payload '{\"manage\": \"migrate --no-input\"}' \\\n        invoke_response.json\n",[33,34273,34274,34286,34294,34302,34308,34320,34326,34333,34340,34347,34351,34357,34368,34374,34380,34385,34390,34395],{"__ignoreMap":35},[187,34275,34276,34278,34280,34283],{"class":189,"line":190},[187,34277,34267],{"class":2516},[187,34279,585],{"class":577},[187,34281,34282],{"class":573},"&",[187,34284,34285],{"class":193},"task\n",[187,34287,34288,34290,34292],{"class":189,"line":249},[187,34289,8954],{"class":2516},[187,34291,585],{"class":577},[187,34293,22781],{"class":196},[187,34295,34296,34298,34300],{"class":189,"line":312},[187,34297,22797],{"class":2516},[187,34299,585],{"class":577},[187,34301,7546],{"class":196},[187,34303,34304,34306],{"class":189,"line":319},[187,34305,22806],{"class":2516},[187,34307,2520],{"class":577},[187,34309,34310,34312,34315,34317],{"class":189,"line":325},[187,34311,14709],{"class":577},[187,34313,34314],{"class":2516},"when",[187,34316,585],{"class":577},[187,34318,34319],{"class":196},"manual\n",[187,34321,34322,34324],{"class":189,"line":686},[187,34323,22834],{"class":2516},[187,34325,2520],{"class":577},[187,34327,34328,34330],{"class":189,"line":697},[187,34329,14709],{"class":577},[187,34331,34332],{"class":196},"pip install awscli\n",[187,34334,34335,34338],{"class":189,"line":1291},[187,34336,34337],{"class":2516},"  after_script",[187,34339,2520],{"class":577},[187,34341,34342,34344],{"class":189,"line":1306},[187,34343,14709],{"class":577},[187,34345,34346],{"class":196},"cat invoke_response.json\n",[187,34348,34349],{"class":189,"line":1434},[187,34350,316],{"emptyLinePlaceholder":315},[187,34352,34353,34355],{"class":189,"line":2599},[187,34354,13074],{"class":2516},[187,34356,2520],{"class":577},[187,34358,34359,34362,34364,34366],{"class":189,"line":2607},[187,34360,34361],{"class":588},"  \u003C\u003C",[187,34363,585],{"class":577},[187,34365,22953],{"class":573},[187,34367,34285],{"class":577},[187,34369,34370,34372],{"class":189,"line":2621},[187,34371,22861],{"class":2516},[187,34373,2520],{"class":577},[187,34375,34376,34378],{"class":189,"line":2631},[187,34377,14709],{"class":577},[187,34379,7477],{"class":573},[187,34381,34382],{"class":189,"line":2642},[187,34383,34384],{"class":196},"      aws lambda invoke \\\n",[187,34386,34387],{"class":189,"line":2653},[187,34388,34389],{"class":196},"        --function-name ${ENVIRONMENT}-${APP_NAME}-djambda-lambda \\\n",[187,34391,34392],{"class":189,"line":2665},[187,34393,34394],{"class":196},"        --payload '{\"manage\": \"migrate --no-input\"}' \\\n",[187,34396,34397],{"class":189,"line":2674},[187,34398,34399],{"class":196},"        invoke_response.json\n",[11,34401,34402,34403,34406],{},"These are ",[33,34404,34405],{},"manual"," commands, and can be started through the GitLab CI interface by pressing the \"Play\" button on the pipeline.",[168,34408,28105],{"id":28104},[11,34410,34411],{},"I still have lots of ideas and things to try out with this Django/Lambda architecture. Here are a few things that would be good to try:",[916,34413,34414,34424,34431,34438,34450,34464],{},[919,34415,34416,34417,15754,34420,34423],{},"Using another Lambda function that proxies web requests to a Lambda outside of the VPC for basic network requests using either ",[33,34418,34419],{},"urllib.request",[33,34421,34422],{},"requests",". This would be the easiest way to add simple internet access without needing a NAT Gateway",[919,34425,34426,34427,34430],{},"Use a NAT provider to add a cheap NAT instance using a ",[33,34428,34429],{},"t3a.nano"," instance ( about $5.00/month) to allow for internet access in the Django request/response cycle",[919,34432,34433,34434,34437],{},"Figure out a good solution for async processing. Zappa has a ",[33,34435,34436],{},"@task"," wrapper that allows you to run tasks asynchronously in separate Lambda functions. It would be interesting to experiment with direct invocation of async tasks as well as task queueing with SQS. Using SQS would involve a VPC Endpoint which does cost extra, or we could setup a dedicated SQS proxy function to do this in a way similar to how we would handle requests.",[919,34439,34440,34441,34443,34444,34449],{},"Lambda tuning. I'm pretty new to using Lambda and I would like to better understand how to fine-tune Lambda settings. There are lots of options in the ",[33,34442,33118],{}," construct, which would be a good place to start. This would be especially important for async tasks that require high memeory. From the ",[15,34445,34448],{"href":34446,"rel":34447},"https://docs.aws.amazon.com/lambda/latest/dg/lambda-dg.pdf",[19],"Lambda Developer Guide"," it looks like memory can be configured from 128 MB to 3,008 MB in 64 MB increments.",[919,34451,34452,34453,34455,34456,34459,34460,34463],{},"I typically setup Vue.js frontends with S3/CloudFront and use Django only for the admin and API with Django REST Framework. From what I understand, API Gateway uses CloudFront in the backround to do custom domains, but you don't have access to this CloudFront distribution. You can add the ",[33,34454,33797],{}," URL as a Custom Origin for a single CloudFront distribution, or keep a CloudFront distribution separate from the API Gateway custom domain and serve these on different subdomains, such as ",[33,34457,34458],{},"api.mysite.com"," for the API Gateway domain name and ",[33,34461,34462],{},"mysite.com"," for the CloudFront distribution serving Vue.js files.",[919,34465,34466,34467],{},"Pricing. I'm still unsure about exactly how much this setup costs. The only major costs should be Aurora Postgres Serverless if it is used heavily, but I'm still not sure about how the pricing for this service works. Here's an in-depth article from Jeremy Daly that has more information on Aurora Serverless: ",[15,34468,34469],{"href":34469,"rel":34470},"https://www.jeremydaly.com/aurora-serverless-the-good-the-bad-and-the-scalable/",[19],[11,34472,24472],{},[855,34474,34475],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}",{"title":35,"searchDepth":249,"depth":249,"links":34477},[34478,34479,34480,34481,34482,34483,34484,34485,34486,34490],{"id":8183,"depth":249,"text":8184},{"id":32562,"depth":249,"text":32563},{"id":32596,"depth":249,"text":32597},{"id":12281,"depth":249,"text":12336},{"id":33285,"depth":249,"text":12327},{"id":33587,"depth":249,"text":33588},{"id":33875,"depth":249,"text":33876},{"id":33929,"depth":249,"text":33930},{"id":34001,"depth":249,"text":34002,"children":34487},[34488,34489],{"id":34016,"depth":312,"text":34017},{"id":34144,"depth":312,"text":34145},{"id":28104,"depth":249,"text":28105},"2020-08-01",{"layout":29014},"/2020/08/01/django-and-lambda-with-cdk-and-api-gateway",{"title":32544,"description":35},"2020/08/01/django-and-lambda-with-cdk-and-api-gateway",[24508,15290,12748,33089,15291],"EgdK3zkBTLvFkcZke2gGZ0c_ajWVN4KYPPO3wNXIAFc",{"id":34499,"title":34500,"body":34501,"comments":315,"date":36974,"description":35,"draft":872,"extension":873,"external":874,"image":34612,"meta":36975,"navigation":315,"path":36976,"seo":36977,"stem":36978,"tags":36979,"__hash__":36980},"blog/2020/06/02/django-postgres-vue-gitlab-ecs.md","Using AWS CDK, GitLab, Fargate and CloudFront for Django + Vue.js applications",{"type":8,"value":34502,"toc":36935},[34503,34517,34521,34528,34552,34555,34559,34562,34604,34608,34613,34615,34624,34627,34630,34633,34636,34639,34642,34645,34648,34651,34654,34657,34660,34663,34666,34669,34672,34675,34678,34681,34684,34687,34690,34693,34696,34699,34702,34705,34708,34711,34714,34717,34719,34741,34745,34769,34771,34785,34790,34829,34833,34876,34880,34902,34906,34915,34918,34922,34962,34966,34999,35005,35009,35012,35016,35027,35031,35034,35048,35052,35064,35067,35095,35099,35107,35124,35142,35146,35149,35157,35160,35164,35173,35183,35254,35259,35463,35469,35503,35510,35548,35554,35566,35572,35588,35599,35608,35736,35749,35798,35804,35810,35815,35819,35824,35867,35885,35910,35920,35924,35934,35953,35970,35980,35986,35992,35997,36005,36010,36016,36021,36039,36044,36059,36064,36067,36072,36075,36080,36087,36094,36113,36123,36129,36135,36139,36142,36146,36164,36169,36172,36183,36189,36195,36200,36203,36208,36215,36250,36255,36262,36267,36270,36286,36291,36294,36299,36308,36320,36323,36327,36334,36354,36357,36361,36364,36368,36371,36381,36384,36392,36396,36399,36403,36406,36420,36434,36437,36443,36450,36453,36476,36479,36482,36488,36491,36501,36504,36546,36549,36633,36647,36650,36752,36755,36890,36894,36913,36915,36932],[107,34504,34505],{},[11,34506,34507,34508,34512,34513],{},"This article was originally posted on ",[15,34509,34510],{"href":34510,"rel":34511},"https://dev.to/briancaffey/building-a-django-vue-js-application-with-aws-cdk-and-gitlab-ci-also-how-to-scale-celery-workers-to-zero-1o6i",[19],". It is also available on the documentation site for the project: ",[15,34514,34515],{"href":34515,"rel":34516},"https://verbose-equals-true.gitlab.io/django-postgres-vue-gitlab-ecs/start/overview/",[19],[2215,34518,34520],{"id":34519},"project-overview","Project Overview",[11,34522,34523,34524,34527],{},"This is an overview of a Proof-of-Concept web application I'm working on called ",[33,34525,34526],{},"django-postgres-vue-gitlab-ecs",". This project aims to demonstrate the development and deployment of a web application using some of my favorite tools, languages and frameworks including:",[916,34529,34530,34533,34536,34539,34542,34545,34547],{},[919,34531,34532],{},"Python",[919,34534,34535],{},"Django",[919,34537,34538],{},"JavaScript",[919,34540,34541],{},"Vue.js/Quasar Framework",[919,34543,34544],{},"GitLab",[919,34546,32684],{},[919,34548,34549,34550],{},"and last but not least, ",[338,34551,30192],{},[11,34553,34554],{},"This README will start by describing some features of the application I'm building and how the different technologies are used together. I also share my experience in adopting CDK for managing cloud infrastructure on AWS. Finally, I discuss my solution to a specific question I have been trying to answer: what's the best way to scale Celery workers to zero to reduce Total Cost of Ownership?",[168,34556,34558],{"id":34557},"development-philosophies-and-best-practices","Development Philosophies and Best Practices",[11,34560,34561],{},"Here are some of the best practices that this project aims to use:",[916,34563,34564,34567,34569,34572,34575,34578,34581,34588,34591,34594,34597],{},[919,34565,34566],{},"Open-source, MIT-Licensed",[919,34568,11451],{},[919,34570,34571],{},"Infrastructure as Code",[919,34573,34574],{},"Containerization with Docker",[919,34576,34577],{},"Testing",[919,34579,34580],{},"GitOps",[919,34582,34583,34584],{},"Serverless* ",[338,34585,34586],{},[187,34587,15625],{},[919,34589,34590],{},"Project documentation",[919,34592,34593],{},"Cost containment and tracking",[919,34595,34596],{},"KISS & DRY",[919,34598,34599,34600,34603],{},"Initial AWS console interaction is strictly limited to what can ",[4339,34601,34602],{},"only"," be done through the AWS console, otherwise AWS CDK and AWS CLI (preferably in CI/CD pipelines) are the primary means of interacting with AWS resources and the AWS Console is treated as a \"read-only\" convenience.",[168,34605,34607],{"id":34606},"topics","Topics",[11,34609,34610],{},[511,34611],{"alt":7255,"src":34612},"/static/architecture_verbose_equals_true.png",[911,34614,27147],{"id":27146},[107,34616,34617],{},[11,34618,34619,34620],{},"This diagram was created with draw.io. Here's the link to the a read-only version of the diagram on draw.io: ",[15,34621,34622],{"href":34622,"rel":34623},"https://drive.google.com/file/d/1gU61zjoW80fCusUcswU1zhEE5VFB1Z5U/view?usp=sharing",[19],[11,34625,34626],{},"1 - GitLab is used to host the source code, test the source code and deploy the application to AWS.",[11,34628,34629],{},"2 - Unit testing (see .gitlab-ci.yml)",[11,34631,34632],{},"2a - Pytest",[11,34634,34635],{},"2b - Jest",[11,34637,34638],{},"2c - Cypress",[11,34640,34641],{},"3 - Deployment phase (see /gitlab-ci/aws/cdk.yml)",[11,34643,34644],{},"3a - Quasar PWA assets are built if there are changes in the quasar directory",[11,34646,34647],{},"3b - AWS Cloud Development Kit (CDK) defines all infrastructure in AWS (4a - 12)",[11,34649,34650],{},"3c - AWS CLI is used to run Fargate tasks through manual GitLab CI jobs",[11,34652,34653],{},"4 - CDK Assets (ECR and S3 buckets that CDK uses internally to manage build assets and artifacts)",[11,34655,34656],{},"4a - Elastic Container Repository is used to manage the Django docker image used in various parts of the application",[11,34658,34659],{},"4b - S3 bucket used to store files associated with CDK and CloudFormation",[11,34661,34662],{},"5 - Route53 is used to route traffic to the CloudFront distribution",[11,34664,34665],{},"6 - CloudFront distribution that serves as the \"front desk\" of the application. It routes requests to to the correct CloudFront Origin",[11,34667,34668],{},"7 - CloudFront Origin Configurations",[11,34670,34671],{},"7a - S3 bucket for Quasar PWA assets",[11,34673,34674],{},"7b - Application Load Balancer for Django application (/api/, /admin/, /flower/, /ws/, /graphql/)",[11,34676,34677],{},"7c - S3 bucket for Django assets (static files, public media and private media)",[11,34679,34680],{},"8 - Web server and websocket servers",[11,34682,34683],{},"8a - Fargate service running uvicorn process (REST, GraphQL, Django Channels)",[11,34685,34686],{},"8b - Autoscaling Group for Fargate Service that serves Django API",[11,34688,34689],{},"9 - Celery and celery worker autoscaling",[11,34691,34692],{},"9a - Fargate service that is autoscaled between 0 and N Fargate tasks for a given celery queue",[11,34694,34695],{},"9b - Scheduled Event that triggers a Lambda to make a request to Django backend which collects celery queue metrics and published\nmetrics to CloudWatch using boto3",[11,34697,34698],{},"9c - Lambda event the makes a request to /api/celery-metrics/",[11,34700,34701],{},"9d - CloudWatch alarm that is used to scale the Fargate service for a celery queue",[11,34703,34704],{},"9e - Autoscaling group for celery Fargate service",[11,34706,34707],{},"10 - Fargate tasks that run Django management commands such as migrate and collectstatic. These are triggered from manual GitLab CI\njobs using the AWS CLI (3c)",[11,34709,34710],{},"11 - ElastiCache for Redis, used for Caching, Celery Broker, Channels Layer, etc.",[11,34712,34713],{},"12 - Aurora Postgres Serverless",[11,34715,34716],{},"Here's a list of some of the major topics covered:",[911,34718,14752],{"id":14751},[916,34720,34721,34726,34729,34732,34735,34738],{},[919,34722,34723,34724],{},"Setting up a local development environment using docker with one easy command: ",[33,34725,28292],{},[919,34727,34728],{},"Hot-reloading for all parts of the application in local development",[919,34730,34731],{},"Structuring a Django application (apps, settings modules, environment variables)",[919,34733,34734],{},"Automatic code formatting with black and prettier",[919,34736,34737],{},"Monitoring services (flower, pgadmin4, redis-commander)",[919,34739,34740],{},"VSCode settings and extensions",[911,34742,34744],{"id":34743},"gitlab-ci","GitLab CI",[916,34746,34747,34750,34753,34756,34759,34766],{},[919,34748,34749],{},"GitLab CI for running unit and integration tests",[919,34751,34752],{},"GitLab CI configuration for deployment to multiple environments (dev, prod) using AWS CDK",[919,34754,34755],{},"GitLab CI scheduled jobs for updating all project dependencies through automated merge requests with Renovate",[919,34757,34758],{},"GitLab CI manual jobs for running admin and management commands (database migrations, collectstatic, etc.)",[919,34760,34761,34762,34765],{},"How to deploy to multiple environments (master -> staging, tags -> prod pattern as well as optional ephemeral environments per commit) using the new GitLab CI ",[33,34763,34764],{},"rules"," syntax",[919,34767,34768],{},"Recording Cypress test videos with GitLab CI artifacts for manual review",[911,34770,32684],{"id":12748},[916,34772,34773,34776,34779,34782],{},[919,34774,34775],{},"Initial account setup",[919,34777,34778],{},"Generating keys used in GitLab CI for deployment",[919,34780,34781],{},"Opting in to new ARN format for ECS services and tasks (used for resource tagging) and container insights",[919,34783,34784],{},"AWS Cloud Development Kit (CDK) for Infrastructure as Code to manage all AWS resources with CloudFormation Stacks - Nested stacks for grouping related resources under parent tasks (one parent task represent on environment for the application, such as production)",[34786,34787,34789],"h4",{"id":34788},"aws-resources","AWS Resources",[916,34791,34792,34795,34802,34805,34808,34811,34814,34817,34820,34823,34826],{},[919,34793,34794],{},"Automated provisioning of Amazon Certificate Manager certificates for TLS (HTTPS)",[919,34796,34797,34798,343],{},"Multi-AZ VPC with security groups and NACLs (no NAT Gateways/NAT instances used* ",[338,34799,34800],{},[187,34801,22129],{},[919,34803,34804],{},"S3 buckets for static site hosting and Django static and media assets",[919,34806,34807],{},"CloudFront for serving Vue.js SPA (PWA) static assets, Django static/media files and proxy to Application Load Balancer which forwards to ECS",[919,34809,34810],{},"Fargate for multiple Django processes: gunicorn for backend API, daphne for Django Channels, celery for asynchronous tasks and celery beat for scheduled tasks",[919,34812,34813],{},"Autoscaling (between 0 and N Fargate tasks) for infrequent, compute/memory intensive and long-running celery tasks with custom CloudWatch metrics",[919,34815,34816],{},"Hosted Redis with ElastiCache for celery broker, application caching, django-constance, etc.",[919,34818,34819],{},"Aurora Postgres and Aurora Postgres Serverless for cost savings in staging environments",[919,34821,34822],{},"CDK Assets for automating S3 asset deployment and automated container building during deployment with AWS CDK",[919,34824,34825],{},"Thorough resource tagging by both application and environment (dev, prod, etc.) for comprehensive cost tracking",[919,34827,34828],{},"Optional bastion host for SSH access to bring up a Django shell in a container running on a container instance (this requires an EC2 instance)",[911,34830,34832],{"id":34831},"django-application","Django Application",[916,34834,34835,34838,34841,34844,34851,34854,34857,34864,34867,34870,34873],{},[919,34836,34837],{},"Django REST Framework",[919,34839,34840],{},"JWT-based authentication with django-rest-framework-simplejwt",[919,34842,34843],{},"Custom user model with email as username",[919,34845,34846,34847,34850],{},"Social authentication with ",[33,34848,34849],{},"python-social-auth"," (Google, Facebook, GitHub)",[919,34852,34853],{},"Pytest for unit testing",[919,34855,34856],{},"Settings broken into separate modules (development, ci, production)",[919,34858,34859,34860,34863],{},"Django apps organized in ",[33,34861,34862],{},"apps"," directory",[919,34865,34866],{},"S3 for static assets, public media and private media files in production, local file system for assets in local development",[919,34868,34869],{},"Multiple Celery queues for asynchronous tasks",[919,34871,34872],{},"Django Channels for websocket support with daphne",[919,34874,34875],{},"Graphene for GraphQL",[911,34877,34879],{"id":34878},"vuejs-application","Vue.js application",[916,34881,34882,34885,34888,34891,34894,34897,34899],{},[919,34883,34884],{},"Quasar Framework for creating a Vue.js app with multiple build targets (SPA, PWA, Electron, SSR and Cordova)",[919,34886,34887],{},"Internationalization",[919,34889,34890],{},"Dark/Light mode",[919,34892,34893],{},"Vuex for state management",[919,34895,34896],{},"vue-router",[919,34898,28387],{},[919,34900,34901],{},"Vue Apollo",[911,34903,34905],{"id":34904},"aws-django-vuejs","AWS ∩ Django ∩ Vue.js",[11,34907,34908,34909,34914],{},"CloudFront is used as a CDN and proxy in order to serve the frontend Vue.js PWA* ",[338,34910,34911],{},[187,34912,34913],{},"3",", static and media assets, Django API and other monitoring services all on the same URL with environments namespaced by subdomain (or a combination of domain and subdomains by production and non-production environments, respectively). The PWA uses Workbox to route certain URL paths to network only (not served by the local cache).",[11,34916,34917],{},"Here are some examples:",[34786,34919,34921],{"id":34920},"staging-environment-urls","Staging environment URLs",[916,34923,34924,34930,34936,34942,34948,34954],{},[919,34925,34926,34929],{},[33,34927,34928],{},"https://dev.mysite.com/api/*"," - Django REST Framework",[919,34931,34932,34935],{},[33,34933,34934],{},"https://dev.mysite.com/admin/*"," - Django Admin",[919,34937,34938,34941],{},[33,34939,34940],{},"https://dev.mysite.com/graphql/"," - Graphene/GraphQL",[919,34943,34944,34947],{},[33,34945,34946],{},"https://dev.mysite.com/flower/*"," - Flower (Celery monitoring utility)",[919,34949,34950,34953],{},[33,34951,34952],{},"https://dev.mysite.com/media/private/private-file.csv"," - private media files",[919,34955,34956,34959,34960,343],{},[33,34957,34958],{},"https://dev.mysite.com/*"," - Vue.js PWA (all other routes load ",[33,34961,27266],{},[34786,34963,34965],{"id":34964},"production-environment-urls","Production environment URLs",[916,34967,34968,34973,34978,34983,34988,34993],{},[919,34969,34970,34929],{},[33,34971,34972],{},"https://mysite.com/api/*",[919,34974,34975,34935],{},[33,34976,34977],{},"https://mysite.com/admin/*",[919,34979,34980,34941],{},[33,34981,34982],{},"https://mysite.com/graphql/",[919,34984,34985,34947],{},[33,34986,34987],{},"https://mysite.com/flower/*",[919,34989,34990,34953],{},[33,34991,34992],{},"https://mysite.com/media/private/private-file.csv",[919,34994,34995,34959,34997,343],{},[33,34996,28698],{},[33,34998,27266],{},[11,35000,35001,35002,752],{},"Alternatively, the production domain name could be configured to use a dedicated subdomain such as ",[33,35003,35004],{},"https://app.mysite.com",[911,35006,35008],{"id":35007},"sample-application-logic","Sample application logic",[11,35010,35011],{},"While this is mostly a Proof-of-Concept project that focuses on architecture, I have included some light-weight examples of what you can do with this application. These examples are all WIP.",[34786,35013,35015],{"id":35014},"social-authentication","Social Authentication",[916,35017,35018,35024],{},[919,35019,35020,35021,35023],{},"Uses ",[33,35022,34849],{}," to allow users to Sign Up/Login with Google, Facebook and GitHub accounts",[919,35025,35026],{},"Makes use of the custom user model",[34786,35028,35030],{"id":35029},"credit-card-statement-app","Credit Card Statement App",[11,35032,35033],{},"This is a simple application for users to upload credit card statements in CSV format.",[916,35035,35036,35039,35042,35045],{},[919,35037,35038],{},"Credit card transactions in CSV files are created using bulk inserts via the Django ORM in a celery task",[919,35040,35041],{},"CSV files are saved in private S3 storage",[919,35043,35044],{},"Basic visualization of spend over time",[919,35046,35047],{},"Download consolidated CSV file",[34786,35049,35051],{"id":35050},"hacker-news-clone","Hacker News Clone",[916,35053,35054,35061],{},[919,35055,35056,35057],{},"An implementation of the application from this tutorial: ",[15,35058,35059],{"href":35059,"rel":35060},"https://www.howtographql.com/graphql-python/0-introduction/",[19],[919,35062,35063],{},"Uses Vue Apollo GraphQL client",[911,35065,34577],{"id":35066},"testing",[916,35068,35069,35072,35075,35078,35081,35087],{},[919,35070,35071],{},"Unit testing with pytest and Jest",[919,35073,35074],{},"Test coverage reports",[919,35076,35077],{},"Integration testing with Cypress",[919,35079,35080],{},"Capture integration test run recordings as GitLab CI job artifacts",[919,35082,35083,35084],{},"Testing GitLab CI jobs locally with ",[33,35085,35086],{},"gitlab-runner",[919,35088,35089,35090],{},"Tests for CDK?* ",[338,35091,35092],{},[187,35093,35094],{},"4",[911,35096,35098],{"id":35097},"caveats-questions-confusion-and-footnotes","Caveats, Questions, Confusion and Footnotes",[2276,35100,35101,35104],{},[919,35102,35103],{},"In the context of this project, it is serverless in the sense that AWS Fargate is serverless: there are no EC2 instances to manage. It is not serverless in the way that Zappa can deploy a Django application as an AWS Lambda function with one invocation per request. There are \"always on\" processes that listen for incoming requests. I'm interested in trying Zappa, but I'm confused about how CDK, zappa-cli, SAM and Serverless Framework would all play together. Aurora Postgres Serverless is another nice \"serverless\" aspect of this project and contributes significantly to cost savings.",[919,35105,35106],{},"This project currently uses no NAT Gateways. The Fargate services and tasks running Django processes (gunicorn, celery, daphne as well as migration, collectstatic and other management commands) are launched in public subnets and the databases (RDS and ElastiCache) are placed in private subnets. This is primarily done to avoid the cost of running a NAT Gateway.",[916,35108,35109,35116],{},[919,35110,35111,35112,752],{},"I think it would fairly easy to switch to using a NAT Gatway to add an additional layer of security that is recommended in this article: ",[15,35113,35114],{"href":35114,"rel":35115},"https://aws.amazon.com/blogs/compute/task-networking-in-aws-fargate/",[19],[919,35117,35118,35119,35123],{},"I asked a question about this on the Stack Exchange Information Security forum: ",[15,35120,35121],{"href":35121,"rel":35122},"https://security.stackexchange.com/questions/232055/security-implications-of-using-public-subnets-in-aws-vpc-for-hosting-web-and-job",[19],"\nI'm curious to know if this is a reasonable tradeoff to make, as well as how secure the proposed solution is (using security groups and network ACLs).",[2276,35125,35126,35132],{"start":312},[919,35127,35128,35129,35131],{},"I'm not focusing on SEO in this project. I'm using ",[33,35130,27266],{}," as the error document for the S3 website that CloudFront uses as the default behavior. This means that nested routes for the PWA have 404 response codes. I have heard about using lambda@edge to rewrite the the response code, I'm also curious about using this project in SSR mode (and also \"serverless-side rendering\"), but for now that is outside of the scope of what I want to do with this project.",[919,35133,35134,35137,35138,752],{},[338,35135,35136],{},"CDK is an awesome tool!"," Here is a great introduction by Nathan Peck, Senior Developer Advocate at Amazon Web Services: ",[15,35139,35140],{"href":35140,"rel":35141},"https://www.youtube.com/watch?v=184S7ki6fJA",[19],[168,35143,35145],{"id":35144},"my-experience-with-aws-cdk","My experience with AWS CDK",[11,35147,35148],{},"Having worked with CloudFormation for almost a year, I was not looking forward to learning a another Infrastructure as Code (IaC) tool. I struggled with CloudFormation and the whole concept of using an extended version of YAML to define infrastructure for multiple environments. I have limited experience with Terraform, but learning another DSL seemed like a lateral move that didn't seem worth it. I'm glad I did spend time learning and using CloudFormation, because CDK is an abstraction layer over CloudFormation. Here are some of my thoughts on adopting CDK.",[911,35150,35152,35153],{"id":35151},"start-here-httpscdkworkshopcom","Start here: ",[15,35154,35155],{"href":35155,"rel":35156},"https://cdkworkshop.com",[19],[11,35158,35159],{},"This is a great resource that was my first exposure to CDK. It focuses on using Lambda, DynamoDB and API Gateway, three technologies that I haven't used in production environments. In my mind this stack is the antithesis of the stack paradigm I am most experienced with: EC2, RDS and ELB. Regardless, I went ahead with it and was pretty much instantly hooked on CDK.",[911,35161,35163],{"id":35162},"the-stack-is-defined-by-a-small-number-of-inputs-used-for-namespacing","The stack is defined by a small number of inputs used for namespacing",[11,35165,35166,35169,35170,35172],{},[33,35167,35168],{},"awscdk/app.py"," is the entrypoint for ",[33,35171,15291],{}," commands. Here's what it contains:",[11,35174,35175,35176,35179,35180,35182],{},"Here are the main parameters for ",[33,35177,35178],{},"ApplicationStack",", which represents all of the resources for a specific environment environment. Each value is composed of environment variables set in GitLab CI when ",[33,35181,33986],{}," is called:",[916,35184,35185,35202,35208,35220,35235,35249],{},[919,35186,35187,35190,35191,637,35193,637,35196,637,35199,35201],{},[33,35188,35189],{},"environment_name"," - Possible examples could be ",[33,35192,6303],{},[33,35194,35195],{},"qa",[33,35197,35198],{},"some-feature-branch",[33,35200,11949],{},", etc.",[919,35203,35204,35207],{},[33,35205,35206],{},"base_domain_name"," - The domain name of the Hosted Zone that needs to be setup manually in Route53",[919,35209,35210,35213,35214,35217,35218],{},[33,35211,35212],{},"full_domain_name"," - ",[33,35215,35216],{},"f\"{environment_name}.{base_domain_name}\""," or optionally just ",[33,35219,35206],{},[919,35221,35222,35225,35226,35228,35229,35231,35232,35234],{},[33,35223,35224],{},"base_app_name"," - Based on the ",[33,35227,35206],{},", but ",[33,35230,752],{}," is replaced with ",[33,35233,677],{}," in order to be used for naming certain AWS resources, used for resources tagging so we can easily look at the costs of all environments for our application with one tag.",[919,35236,35237,35240,35241,637,35243,35245,35246,35248],{},[33,35238,35239],{},"full_app_name"," - Base on ",[33,35242,35212],{},[33,35244,752],{}," replaced with ",[33,35247,677],{}," as well.",[919,35250,35251],{},[33,35252,35253],{},"aws_region",[11,35255,35256,35257,358],{},"Expand to the following to view ",[33,35258,35168],{},[35260,35261,35262],"details",{},[26,35263,35265],{"className":1383,"code":35264,"language":1125,"meta":35,"style":35},"#!/usr/bin/env python3\nimport os\n\nfrom aws_cdk import core\n\nfrom awscdk.cdk_app_root import ApplicationStack\n\n# naming conventions, also used for ACM certs, DNS Records, resource naming\n# Dynamically generated resource names created in CDK are used in GitLab CI\n# such as cluster name, task definitions, etc.\nenvironment_name = f\"{os.environ.get('ENVIRONMENT', 'dev')}\"\nbase_domain_name = os.environ.get(\"DOMAIN_NAME\", \"mysite.com\")\n# if the the production environent subdomain should nott be included in the URL\n# redefine `full_domain_name` to `base_domain_name` for that environment\nfull_domain_name = f\"{environment_name}.{base_domain_name}\"  # dev.mysite.com\n# if environment_name == \"prod\":\n#     full_domain_name = base_domain_name\nbase_app_name = os.environ.get(\"APP_NAME\", \"mysite-com\")\nfull_app_name = f\"{environment_name}-{base_app_name}\"  # dev-mysite-com\naws_region = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n\n\napp = core.App()\nstack = ApplicationStack(\n    app,\n    f\"{full_app_name}-stack\",\n    environment_name=environment_name,\n    base_domain_name=base_domain_name,\n    full_domain_name=full_domain_name,\n    base_app_name=base_app_name,\n    full_app_name=full_app_name,\n    env={\"region\": aws_region},\n)\n\n# in order to be able to tag ECS resources, you need to go to\n# the ECS Console > Account Settings > Amazon ECS ARN and resource ID settings\n# and enable at least Service and Task. Optionally enable\n# CloudWatch Container Insights\nstack.node.apply_aspect(core.Tag(\"StackName\", full_app_name))\nstack.node.apply_aspect(core.Tag(\"StackName\", base_app_name))\n\napp.synth()\n",[33,35266,35267,35271,35275,35279,35283,35287,35292,35296,35301,35306,35311,35316,35321,35326,35331,35336,35341,35346,35351,35356,35360,35364,35368,35373,35378,35382,35387,35392,35397,35402,35407,35412,35417,35421,35425,35430,35435,35440,35445,35450,35455,35459],{"__ignoreMap":35},[187,35268,35269],{"class":189,"line":190},[187,35270,22974],{},[187,35272,35273],{"class":189,"line":249},[187,35274,10345],{},[187,35276,35277],{"class":189,"line":312},[187,35278,316],{"emptyLinePlaceholder":315},[187,35280,35281],{"class":189,"line":319},[187,35282,23019],{},[187,35284,35285],{"class":189,"line":325},[187,35286,316],{"emptyLinePlaceholder":315},[187,35288,35289],{"class":189,"line":686},[187,35290,35291],{},"from awscdk.cdk_app_root import ApplicationStack\n",[187,35293,35294],{"class":189,"line":697},[187,35295,316],{"emptyLinePlaceholder":315},[187,35297,35298],{"class":189,"line":1291},[187,35299,35300],{},"# naming conventions, also used for ACM certs, DNS Records, resource naming\n",[187,35302,35303],{"class":189,"line":1306},[187,35304,35305],{},"# Dynamically generated resource names created in CDK are used in GitLab CI\n",[187,35307,35308],{"class":189,"line":1434},[187,35309,35310],{},"# such as cluster name, task definitions, etc.\n",[187,35312,35313],{"class":189,"line":2599},[187,35314,35315],{},"environment_name = f\"{os.environ.get('ENVIRONMENT', 'dev')}\"\n",[187,35317,35318],{"class":189,"line":2607},[187,35319,35320],{},"base_domain_name = os.environ.get(\"DOMAIN_NAME\", \"mysite.com\")\n",[187,35322,35323],{"class":189,"line":2621},[187,35324,35325],{},"# if the the production environent subdomain should nott be included in the URL\n",[187,35327,35328],{"class":189,"line":2631},[187,35329,35330],{},"# redefine `full_domain_name` to `base_domain_name` for that environment\n",[187,35332,35333],{"class":189,"line":2642},[187,35334,35335],{},"full_domain_name = f\"{environment_name}.{base_domain_name}\"  # dev.mysite.com\n",[187,35337,35338],{"class":189,"line":2653},[187,35339,35340],{},"# if environment_name == \"prod\":\n",[187,35342,35343],{"class":189,"line":2665},[187,35344,35345],{},"#     full_domain_name = base_domain_name\n",[187,35347,35348],{"class":189,"line":2674},[187,35349,35350],{},"base_app_name = os.environ.get(\"APP_NAME\", \"mysite-com\")\n",[187,35352,35353],{"class":189,"line":2684},[187,35354,35355],{},"full_app_name = f\"{environment_name}-{base_app_name}\"  # dev-mysite-com\n",[187,35357,35358],{"class":189,"line":2694},[187,35359,23037],{},[187,35361,35362],{"class":189,"line":2706},[187,35363,316],{"emptyLinePlaceholder":315},[187,35365,35366],{"class":189,"line":2715},[187,35367,316],{"emptyLinePlaceholder":315},[187,35369,35370],{"class":189,"line":2725},[187,35371,35372],{},"app = core.App()\n",[187,35374,35375],{"class":189,"line":2735},[187,35376,35377],{},"stack = ApplicationStack(\n",[187,35379,35380],{"class":189,"line":2743},[187,35381,23065],{},[187,35383,35384],{"class":189,"line":2754},[187,35385,35386],{},"    f\"{full_app_name}-stack\",\n",[187,35388,35389],{"class":189,"line":2762},[187,35390,35391],{},"    environment_name=environment_name,\n",[187,35393,35394],{"class":189,"line":2770},[187,35395,35396],{},"    base_domain_name=base_domain_name,\n",[187,35398,35399],{"class":189,"line":2781},[187,35400,35401],{},"    full_domain_name=full_domain_name,\n",[187,35403,35404],{"class":189,"line":2792},[187,35405,35406],{},"    base_app_name=base_app_name,\n",[187,35408,35409],{"class":189,"line":2803},[187,35410,35411],{},"    full_app_name=full_app_name,\n",[187,35413,35414],{"class":189,"line":2808},[187,35415,35416],{},"    env={\"region\": aws_region},\n",[187,35418,35419],{"class":189,"line":2816},[187,35420,621],{},[187,35422,35423],{"class":189,"line":2824},[187,35424,316],{"emptyLinePlaceholder":315},[187,35426,35427],{"class":189,"line":2834},[187,35428,35429],{},"# in order to be able to tag ECS resources, you need to go to\n",[187,35431,35432],{"class":189,"line":2845},[187,35433,35434],{},"# the ECS Console > Account Settings > Amazon ECS ARN and resource ID settings\n",[187,35436,35437],{"class":189,"line":2856},[187,35438,35439],{},"# and enable at least Service and Task. Optionally enable\n",[187,35441,35442],{"class":189,"line":2867},[187,35443,35444],{},"# CloudWatch Container Insights\n",[187,35446,35447],{"class":189,"line":2878},[187,35448,35449],{},"stack.node.apply_aspect(core.Tag(\"StackName\", full_app_name))\n",[187,35451,35452],{"class":189,"line":2886},[187,35453,35454],{},"stack.node.apply_aspect(core.Tag(\"StackName\", base_app_name))\n",[187,35456,35457],{"class":189,"line":2900},[187,35458,316],{"emptyLinePlaceholder":315},[187,35460,35461],{"class":189,"line":2905},[187,35462,23088],{},[911,35464,18073,35466,35468],{"id":35465},"using-cdk-synth-in-development",[33,35467,33982],{}," in development",[11,35470,35471,35472,35475,35476,35479,35480,35479,35483,35479,35486,35489,35490,35493,35494,35497,35498,35500,35501,30978],{},"When I first started using CDK, I defined a root stack containing multiple CDK constructs that each included groups of related CDK resources. For example, I had an ",[33,35473,35474],{},"RDSConstruct"," that contained a ",[33,35477,35478],{},"Secret",", a ",[33,35481,35482],{},"StringParameter",[33,35484,35485],{},"CfnSecurityGroup",[33,35487,35488],{},"CfnDBSubnetGroup"," and a ",[33,35491,35492],{},"CfnDBCluster",". Whenever I added a new section of CDK code, I would run ",[33,35495,35496],{},"cdk synth > stack.yml"," to generate a \"snapshot\" of my infrastructure in one file. ",[33,35499,31264],{}," was committed in my repo, and I could easily see how changes in CDK code changed the resulting CloudFormation YAML by repeating this ",[33,35502,33982],{},[911,35504,35506,35509],{"id":35505},"nestedstacks",[33,35507,35508],{},"NestedStack","s",[11,35511,35512,35514,35515,35517,35518,6867,35521,35524,35525,31799,35527,35530,35531,35534,35535,35537,35538,35540,35541,35544,35545,35547],{},[33,35513,31264],{}," grew larger and larger as I added more resources in my main \"master stack\" or \"skeleton stack\", and I realized that I was going to reach the hard limit of 200 resources per CloudFormation stack. Refactoring to use ",[33,35516,35508],{},"s was pretty straightforward, all I had to do was replace ",[33,35519,35520],{},"core.Construct",[33,35522,35523],{},"cloudformation.NestedStack",". With nested stacks, running ",[33,35526,35496],{},[33,35528,35529],{},"AWS::CloudFormation::Stack","s that are created, with ",[33,35532,35533],{},"TemplateURL"," and parameters from other stacks. It is more useful to look into contents of ",[33,35536,34117],{}," when working with ",[33,35539,35508],{},"s. The following command (executed from the root of the project) update the contents of the ",[33,35542,35543],{},"awscdk/cdk.out"," directory with templates for each of the ",[33,35546,35508],{},"s:",[26,35549,35552],{"className":35550,"code":35551,"language":31},[29],"cdk synth --app awscdk/app.py --output awscdk/cdk.out\n",[33,35553,35551],{"__ignoreMap":35},[11,35555,35556,35557,30741,35559,35561,35562,35565],{},"The result is JSON, not YAML, and ",[33,35558,34117],{},[33,35560,783],{},"d, but it can still be helpful in verifying that your CDK code is generating the correct CloudFormation templates. (",[33,35563,35564],{},"cdk diff"," may be the best option for seeing how CDK code changes will change your infrastructure).",[911,35567,35569,35571],{"id":35568},"cdk-deploy-and-cdk-in-gitlab-ci-pipelines",[33,35570,33986],{}," and CDK in GitLab CI pipelines",[11,35573,35574,35575,12272,35577,35580,35581,35584,35585,30978],{},"I like how ",[33,35576,33986],{},[33,35578,35579],{},"tail","s the output of CloudFormation events until the stack update finishes or fails. I previously had been using the AWS CLI to call ",[33,35582,35583],{},"aws cloudformation update-stack",". This command kicks off a stack update and the GitLab pipeline will succeed regardless of the success of failure of the ",[33,35586,35587],{},"update-stack",[107,35589,35590],{},[11,35591,35592,35593,35595,35596,35598],{},"With CDK, the CI pipeline that calls ",[33,35594,33986],{}," fails if ",[33,35597,33986],{}," results in a stack rollback.",[11,35600,35601,35602,35604,35605,35607],{},"I couldn't find any examples of how to run ",[33,35603,33986],{}," in a GitLab CI pipeline (using the Python version of CDK, or any version of CDK). It would be nice if there was an official image maintained for the different languages that are ready to run CDK deploy. Here's how I got ",[33,35606,33986],{}," working correctly in my CI/CD pipline:",[26,35609,35611],{"className":2507,"code":35610,"language":2509,"meta":35,"style":35},"cdk deploy:\n  image: docker:19.03.1\n  services:\n    - docker:19.03.5-dind\n  stage: deploy\n  only:\n    - master\n  variables:\n    ENVIRONMENT: dev\n    DOCKER_TLS_CERTDIR: ''\n  before_script:\n    - apk add nodejs-current npm\n    - npm i -g aws-cdk\n    - apk add --no-cache python3\n    - pip3 install -e awscdk\n  script:\n    - cdk bootstrap --app awscdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n    - cdk deploy --app awscdk/app.py --require-approval never\n",[33,35612,35613,35619,35627,35633,35639,35647,35654,35660,35666,35676,35686,35692,35699,35705,35712,35718,35724,35730],{"__ignoreMap":35},[187,35614,35615,35617],{"class":189,"line":190},[187,35616,33986],{"class":2516},[187,35618,2520],{"class":577},[187,35620,35621,35623,35625],{"class":189,"line":249},[187,35622,8954],{"class":2516},[187,35624,585],{"class":577},[187,35626,30567],{"class":196},[187,35628,35629,35631],{"class":189,"line":312},[187,35630,30572],{"class":2516},[187,35632,2520],{"class":577},[187,35634,35635,35637],{"class":189,"line":319},[187,35636,14709],{"class":577},[187,35638,30581],{"class":196},[187,35640,35641,35643,35645],{"class":189,"line":325},[187,35642,22797],{"class":2516},[187,35644,585],{"class":577},[187,35646,7546],{"class":196},[187,35648,35649,35652],{"class":189,"line":686},[187,35650,35651],{"class":2516},"  only",[187,35653,2520],{"class":577},[187,35655,35656,35658],{"class":189,"line":697},[187,35657,14709],{"class":577},[187,35659,7321],{"class":196},[187,35661,35662,35664],{"class":189,"line":1291},[187,35663,31039],{"class":2516},[187,35665,2520],{"class":577},[187,35667,35668,35671,35673],{"class":189,"line":1306},[187,35669,35670],{"class":2516},"    ENVIRONMENT",[187,35672,585],{"class":577},[187,35674,35675],{"class":196},"dev\n",[187,35677,35678,35681,35683],{"class":189,"line":1434},[187,35679,35680],{"class":2516},"    DOCKER_TLS_CERTDIR",[187,35682,585],{"class":577},[187,35684,35685],{"class":196},"''\n",[187,35687,35688,35690],{"class":189,"line":2599},[187,35689,22834],{"class":2516},[187,35691,2520],{"class":577},[187,35693,35694,35696],{"class":189,"line":2607},[187,35695,14709],{"class":577},[187,35697,35698],{"class":196},"apk add nodejs-current npm\n",[187,35700,35701,35703],{"class":189,"line":2621},[187,35702,14709],{"class":577},[187,35704,22457],{"class":196},[187,35706,35707,35709],{"class":189,"line":2631},[187,35708,14709],{"class":577},[187,35710,35711],{"class":196},"apk add --no-cache python3\n",[187,35713,35714,35716],{"class":189,"line":2642},[187,35715,14709],{"class":577},[187,35717,34241],{"class":196},[187,35719,35720,35722],{"class":189,"line":2653},[187,35721,22861],{"class":2516},[187,35723,2520],{"class":577},[187,35725,35726,35728],{"class":189,"line":2665},[187,35727,14709],{"class":577},[187,35729,34254],{"class":196},[187,35731,35732,35734],{"class":189,"line":2674},[187,35733,14709],{"class":577},[187,35735,34261],{"class":196},[11,35737,35738,35739,35741,35742,35744,35745,35748],{},"I have all of my CDK code in a top level directory called ",[33,35740,33948],{},", my Django code is in the top level ",[33,35743,12291],{}," directory and my frontend code is in the top level ",[33,35746,35747],{},"quasar"," directory. This directory structure is important, I'll come back to it shortly. Here are some things to note about this GitLab CI job definition:",[916,35750,35751,35774,35786],{},[919,35752,35753,35754,35756,35757,35760,35761,35763,35764,35767,35768,9054,35771,35773],{},"It uses the base ",[33,35755,15298],{}," image with ",[33,35758,35759],{},"docker:dind"," as a dependent service. This is necessary only if you hare using CDK constructs that build docker images and make use of ",[33,35762,34023],{},", such as the ",[33,35765,35766],{},"aws_ecs.AssetImage"," that I use to define ",[33,35769,35770],{},"self.image",[33,35772,35178],{}," class that defines that main stack of my application.",[919,35775,35776,35777,35779,35780,35782,35783,35785],{},"It requires node, npm python and pip, so these all need to be installed via ",[33,35778,31158],{},", the package manager for Alpine Linux. These are done in the ",[33,35781,30784],{},", the setup for the main \"script\" where ",[33,35784,33986],{}," is called.",[919,35787,35788,35789,35791,35792,35794,35795,35797],{},"The main ",[33,35790,6727],{}," section calls ",[33,35793,34023],{},". You only need to call ",[33,35796,34023],{}," once to initialize resources that CDK will use, so placing it here in my CI script is more of a reminder that we are using CDK assets (S3 buckets and ECR images); calling it again once it has been called initially on your AWS account does nothing, and you will see a message that communicates this:",[26,35799,35802],{"className":35800,"code":35801,"language":31},[29]," $ cdk bootstrap --app awscdk/app.py aws://$AWS_ACCOUNT_ID/$AWS_DEFAULT_REGION\n  ⏳  Bootstrapping environment aws://XXXXXXXXXXXX/us-east-1...\n  ✅  Environment aws://XXXXXXXXXXXX/us-east-1 bootstrapped (no changes).\n",[33,35803,35801],{"__ignoreMap":35},[911,35805,35807],{"id":35806},"cdk-bootstap",[33,35808,35809],{},"cdk bootstap",[11,35811,35812,35814],{},[33,35813,34023],{}," is one of my favorite features of CDK. As I mentioned earlier, it is used with S3 and ECR.",[34786,35816,35818],{"id":35817},"s3-assets","S3 Assets",[11,35820,35821,35822,358],{},"With S3, it allows you to populate the contents of an S3 bucket. Here's an example from ",[33,35823,35178],{},[26,35825,35827],{"className":1383,"code":35826,"language":1125,"meta":35,"style":35},"        if os.path.isdir(\"./quasar/dist/pwa\"):\n            s3_deployment.BucketDeployment(\n                self,\n                \"BucketDeployment\",\n                destination_bucket=self.static_site_bucket,\n                sources=[s3_deployment.Source.asset(\"./quasar/dist/pwa\")],\n                distribution=self.cloudfront.distribution,\n            )\n",[33,35828,35829,35834,35839,35843,35848,35853,35858,35863],{"__ignoreMap":35},[187,35830,35831],{"class":189,"line":190},[187,35832,35833],{},"        if os.path.isdir(\"./quasar/dist/pwa\"):\n",[187,35835,35836],{"class":189,"line":249},[187,35837,35838],{},"            s3_deployment.BucketDeployment(\n",[187,35840,35841],{"class":189,"line":312},[187,35842,23821],{},[187,35844,35845],{"class":189,"line":319},[187,35846,35847],{},"                \"BucketDeployment\",\n",[187,35849,35850],{"class":189,"line":325},[187,35851,35852],{},"                destination_bucket=self.static_site_bucket,\n",[187,35854,35855],{"class":189,"line":686},[187,35856,35857],{},"                sources=[s3_deployment.Source.asset(\"./quasar/dist/pwa\")],\n",[187,35859,35860],{"class":189,"line":697},[187,35861,35862],{},"                distribution=self.cloudfront.distribution,\n",[187,35864,35865],{"class":189,"line":1291},[187,35866,3474],{},[11,35868,35869,35870,35873,35874,35877,35878,35881,35882,35884],{},"If the directory ",[33,35871,35872],{},"./quasar/dist/pwa"," exists, CDK will upload the contents of that directory to the ",[33,35875,35876],{},"static_site_bucket"," and then invalidate the cache for ",[33,35879,35880],{},"self.cloudfront.distribution",", all in one shot. ",[33,35883,35872],{}," is the directory where assets from my frontend PWA are placed when compiled. GitLab CI passes these files between the jobs using artifacts:",[26,35886,35888],{"className":2507,"code":35887,"language":2509,"meta":35,"style":35},"artifacts:\n  paths:\n    - quasar/dist/pwa\n",[33,35889,35890,35896,35903],{"__ignoreMap":35},[187,35891,35892,35894],{"class":189,"line":190},[187,35893,24155],{"class":2516},[187,35895,2520],{"class":577},[187,35897,35898,35901],{"class":189,"line":249},[187,35899,35900],{"class":2516},"  paths",[187,35902,2520],{"class":577},[187,35904,35905,35907],{"class":189,"line":312},[187,35906,14709],{"class":577},[187,35908,35909],{"class":196},"quasar/dist/pwa\n",[11,35911,35912,35913,35915,35916,35919],{},"The job to build PWA assets can be set to only run when there are changes in the ",[33,35914,35747],{}," directory, so the ",[33,35917,35918],{},".quasar/dist/pwa"," directory will only exist if the job is executed. This helps speed up our CI/CD pipeline.",[34786,35921,35923],{"id":35922},"ecr-assets","ECR Assets",[11,35925,35926,35927,35930,35931,35933],{},"I use ",[33,35928,35929],{},"cdk boostrap"," and CDK assets in a similar way for building and pushing my application container. In ",[33,35932,35178],{},", I define the following:",[26,35935,35937],{"className":1383,"code":35936,"language":1125,"meta":35,"style":35},"        self.image = ecs.AssetImage(\n            \"./backend\", file=\"scripts/prod/Dockerfile\", target=\"production\",\n        )\n",[33,35938,35939,35944,35949],{"__ignoreMap":35},[187,35940,35941],{"class":189,"line":190},[187,35942,35943],{},"        self.image = ecs.AssetImage(\n",[187,35945,35946],{"class":189,"line":249},[187,35947,35948],{},"            \"./backend\", file=\"scripts/prod/Dockerfile\", target=\"production\",\n",[187,35950,35951],{"class":189,"line":312},[187,35952,4531],{},[11,35954,35955,35956,35958,35959,637,35962,637,35965,35967,35968,20840],{},"This image is then referenced by multiple ",[33,35957,35508],{},"s that define Fargate services and tasks (",[33,35960,35961],{},"gunicorn",[33,35963,35964],{},"daphne",[33,35966,13041],{}," workers, etc.) This keeps our CDK code DRY.. Don't Repeat Yourself! Similarly, we aren't rebuilding, pushing and pulling the backend container when there are no changes to the code in ",[33,35969,12291],{},[11,35971,35972,35973,35976,35977,752],{},"I'm not setting up an ECR image repository for my application, but I believe there is a way to do this. One question that I have about using ",[33,35974,35975],{},"ecs.AssetImage"," is about image lifecycle management. I know that you can implement rules about how many images you want to keep in an ECR image repository, but ",[338,35978,35979],{},"I'm not sure how this works with CDK Image Assets",[911,35981,35983,35984],{"id":35982},"quick-tour-of-applicationstack","Quick tour of ",[33,35985,35178],{},[11,35987,35988,35989,35991],{},"Here's a very quick look at the structure of my CDK code, focusing on the ",[33,35990,35178],{},", the \"master stack\" or \"skeleton stack\" that contains.",[34786,35993,35995],{"id":35994},"hosted_zone",[33,35996,35994],{},[11,35998,35999,36000,1172,36002,36004],{},"We get the hosted zone using the ",[33,36001,18281],{},[33,36003,33895],{},". This is not a nested stack.",[34786,36006,36008],{"id":36007},"site_certificate",[33,36009,36007],{},[11,36011,36012,36013,36015],{},"The ACM Certificate that will be used for the given environment. This references the ",[33,36014,35212],{}," (environment + application).",[34786,36017,36019],{"id":36018},"vpc_stack",[33,36020,36018],{},[11,36022,2303,36023,36025,36026,36029,36030,1172,36033,36036,36037,752],{},[33,36024,35508],{}," for defining VPC resources. This construct generates lots of CloudFormation resources. I currently have ",[33,36027,36028],{},"nat_gateways"," set to zero, and I'm ",[33,36031,36032],{},"PUBLIC",[33,36034,36035],{},"PRIVATE"," subnets spread over 2 AZs. As I mentioned earlier, this is primarily for cost considerations and it is a best practice to use the tiered security model and run our Fargate tasks in private subnets instead of public subnets. I think I need to add NACL resources in this ",[33,36038,35508],{},[34786,36040,36042],{"id":36041},"alb_stack",[33,36043,36041],{},[11,36045,36046,36047,1172,36050,36053,36054,36056,36057,752],{},"This defines the load balancer, configures that will send traffic to our Fargate services (such as our Django API). I was a little bit unclear about needing a ",[33,36048,36049],{},"listener",[33,36051,36052],{},"https_listener",". I might be able to get away with removing the ",[33,36055,36049],{}," and only using ",[33,36058,36052],{},[34786,36060,36062],{"id":36061},"static_site_stack",[33,36063,36061],{},[11,36065,36066],{},"This stack defines the S3 bucket and policies that will be used for hosting our static site (Quasar PWA).",[34786,36068,36070],{"id":36069},"backend_assets",[33,36071,36069],{},[11,36073,36074],{},"This stack defines the bucket and policies for managing the bucket that holds static and media assets for Django.",[34786,36076,36078],{"id":36077},"cloudfront",[33,36079,36077],{},[11,36081,36082,36083,36086],{},"This defines the CloudFront distribution that ties together several different parts of the application. It is the \"front desk\" of the application, and acts as a CDN and proxy. There is a separate CloudFront distribution for each environment (dev, staging, production). This stack also defines the Route53 ",[33,36084,36085],{},"ARecord"," that will be used to send traffic to a specific subdomain to the correct CloudFront distribution.",[11,36088,36089,36090,36093],{},"There are three ",[33,36091,36092],{},"origin_configs"," for each distribution:",[2276,36095,36096,36102,36107],{},[919,36097,36098,36101],{},[33,36099,36100],{},"CustomOriginConfig"," for the ALB",[919,36103,36104,36106],{},[33,36105,36100],{}," for the S3 bucket website",[919,36108,36109,36112],{},[33,36110,36111],{},"S3OriginConfig"," for the Django static assets",[11,36114,36115,36116,36118,36119,36122],{},"Note that these ",[33,36117,36092],{}," each have different ",[33,36120,36121],{},"behaviors",", and that list comprehension is used to keep this code DRY.",[34786,36124,36126],{"id":36125},"bucketdeployment",[33,36127,36128],{},"BucketDeployment",[11,36130,36131,36132,36134],{},"This will deploy our static site assets to the S3 bucket defined in ",[33,36133,36061],{}," if the static site assets are present at the time of deployment. If they are not present, this means that there were no changes made to the frontend site.",[34786,36136,36137],{"id":15295},[33,36138,15295],{},[11,36140,36141],{},"Defines the ECS Cluster.",[34786,36143,36144],{"id":12281},[33,36145,12281],{},[11,36147,36148,36149,36152,36153,36155,36156,14372,36159,12272,36161,752],{},"There is no L2 construct for ",[33,36150,36151],{},"DBCluster",", so I used ",[33,36154,35492],{}," in order to use the Aurora Postgres ",[33,36157,36158],{},"engine",[33,36160,24508],{},[33,36162,36163],{},"engine_mode",[34786,36165,36167],{"id":36166},"elasticache",[33,36168,36166],{},[11,36170,36171],{},"I also had to use L1 constructs for ElastiCache, but this one is pretty straightforward.",[11,36173,36174,36175,36178,36179,36182],{},"For both RDS and ElastiCache I used the ",[33,36176,36177],{},"vpc_default_security_group"," as the ",[33,36180,36181],{},"source_security_group",". It might be a better idea to define another security group altogether, but this approach works.",[34786,36184,36186],{"id":36185},"assetimage",[33,36187,36188],{},"AssetImage",[11,36190,36191,36192,36194],{},"The docker image that references Django application code in the ",[33,36193,12291],{}," directory. This image is referenced in Fargate services and tasks.",[34786,36196,36198],{"id":36197},"variables",[33,36199,36197],{},[11,36201,36202],{},"This section defines and organizes all of the environment variables and secrets for my application.",[34786,36204,36206],{"id":36205},"backend_service",[33,36207,36205],{},[11,36209,36210,36211,36214],{},"It might be a better idea to replace this with ",[33,36212,36213],{},"NetworkLoadBalancedFargateService",", but instead I implemented this with lower-level constructs just to be clear about what I'm doing. To add a load balanced service, here is what I did:",[2276,36216,36217,36220,36226,36233,36236,36239,36247],{},[919,36218,36219],{},"Define the Fargate task",[919,36221,36222,36223,15626],{},"Add the container to this task with other information (secrets, logging, ",[33,36224,36225],{},"command",[919,36227,36228,36229,36232],{},"Give the task role permissions it needs such as access to Secrets, S3 permissions. (It might be a good idea to refactor this into a function that can be called on ",[33,36230,36231],{},"task_role",", but for now I am explicitly granting all permissions)",[919,36234,36235],{},"Create and add a port mapping",[919,36237,36238],{},"Define an ECS Fargate Service that reference the previously defined Fargate task, configure security group",[919,36240,36241,36242,36244,36245,752],{},"Add the service as a target to the ",[33,36243,36052],{}," defined previously in ",[33,36246,36041],{},[919,36248,36249],{},"Optionally configure autoscaling for the Fargate service",[34786,36251,36253],{"id":36252},"flower_service",[33,36254,36252],{},[11,36256,36257,36258],{},"Flower is a monitoring utility for Celery. I had trouble getting this to work correctly, but I managed to make it work by adding a simple nginx container that passes traffic to the flower container running in the same task. ",[15,36259,36260],{"href":36260,"rel":36261},"https://flower.readthedocs.io/en/latest/reverse-proxy.html",[19],[34786,36263,36265],{"id":36264},"celery_default_service",[33,36266,36264],{},[11,36268,36269],{},"This stack defines the default celery queue. This is discussed later in more detail, but the basic idea is to:",[2276,36271,36272,36274,36277,36280,36283],{},[919,36273,36219],{},[919,36275,36276],{},"Add the container",[919,36278,36279],{},"Define the Fargate service",[919,36281,36282],{},"Grant permissions",[919,36284,36285],{},"Configure autoscaling",[34786,36287,36289],{"id":36288},"celery_autoscaling",[33,36290,36288],{},[11,36292,36293],{},"This stack defines the Lambda function and schedule on which this Lambda is called. This stack is discussed in more detail later on.",[34786,36295,36297],{"id":36296},"backend_tasks",[33,36298,36296],{},[11,36300,36301,36302,637,36304,1172,36306,752],{},"These are administrative tasks that are executed by running manual GitLab CI jobs such as ",[33,36303,13074],{},[33,36305,13071],{},[33,36307,32452],{},[168,36309,36311,36312,36315,36316,36319],{"id":36310},"why-x-why-not-y","Why ",[33,36313,36314],{},"X","? Why not ",[33,36317,36318],{},"Y","?",[11,36321,36322],{},"This section will compare some of the technology choices I have made in this project to other popular alternatives.",[911,36324,36326],{"id":36325},"why-ecs-why-not-kubernetes","Why ECS? Why not Kubernetes?",[11,36328,36329,36330,36333],{},"I like Kubernetes. I have never used it to support production workloads, but I have explored it in a limited capacity. I have set up this project in Kubernetes locally using ",[33,36331,36332],{},"minikube",", there is an article on this in the documentation. There are also lots of options for how you do Kubernetes, here are a few off of the top of my head:",[916,36335,36336,36339,36342,36345,36348,36351],{},[919,36337,36338],{},"KOPS",[919,36340,36341],{},"EKS",[919,36343,36344],{},"k3s",[919,36346,36347],{},"cdk8s",[919,36349,36350],{},"Kubernetes on Fargate",[919,36352,36353],{},"\"Kuberetes the Hard Way\"",[11,36355,36356],{},"With any of these options you are probably going to want to use Helm to do deployments, which adds another layer of abstraction that also has several different ways to be managed. On the other hand, ECS is \"just ECS\"; there are not a lot of other considerations to make when running workloads in ECS. You have to choose between the two available launch types: EC2 and Fargate. Comparisons of ECS and Kubernetes often mention that ECS integrates nicely with other AWS Services, something I have generally found to be true in setting up this project. Granting permissions to S3 buckets or CloudWatch, or using security groups to give ECS tasks access to certain resources in your VPC are some examples of what this \"tight integration\" has meant for me so far.",[911,36358,36360],{"id":36359},"why-django-why-not-flask","Why Django? Why not Flask?",[11,36362,36363],{},"I like Django for lots of reasons. I get why people say it can be \"overkill\", and there are definitely lots of parts of Django that don't use. I use Django primarily for the ORM, migrations system and the Django Admin. I also use the Django REST Framework, which gives me another big productivity boost when building APIs. I dislike Django Forms and Django templates, but that wasn't always the case. Before that, I disliked JavaScript frameworks and single page applications. That changes when I started working with Vue.js and Quasar.",[911,36365,36367],{"id":36366},"why-quasar-framework-why-not-nuxtjs-or-vanilla-vuejs-why-not-react","Why Quasar Framework? Why not Nuxt.js or vanilla Vue.js? Why not React?",[11,36369,36370],{},"Quasar Framework is a few different things, and just like Vue.js itself, Quasar can be incrementally adopted. Primarily, Quasar is a CLI for creating Vue.js projects. It offers some opinions on how to organize files and folders. It handles SPA, PWA, SSR, Electron, Cordova and other build targets. It implements the MaterialUI spec and it has an awesome and active community (but Django, GitLab and AWS do, too!)",[11,36372,36373,36374,36377,36378,36380],{},"Quasar does things a little bit differently than vanilla Vue.js. There is no ",[33,36375,36376],{},"main.js"," file in a typical Quasar application. Instead, bootfiles are used to initialize things that would typically go into ",[33,36379,36376],{},". I believe this helps Quasar manage multiple build targets easily.",[11,36382,36383],{},"I really haven't worked a lot with Nuxt.js, but I would probably be drawn to it if Quasar was not an option. I like how it helps structure your application. In the same way that Django is \"batteries included\", Quasar is also very much \"batteries included\".",[11,36385,36386,36387,1172,36389,1737],{},"I think React is neat, but I have similar feelings between React and Vue that I have between Django and Flask. One requires you make more decisions and therefore has a heavy mental load. The biggest example of this is the tight integration of an official router and state management system for Vue (",[33,36388,34896],{},[33,36390,36391],{},"vuex",[911,36393,36395],{"id":36394},"why-celery-why-not-heuy-or-django-rq","Why Celery? Why not Heuy or django-rq?",[11,36397,36398],{},"I think celery is probably the most heavy-weight option for managing asynchronous tasks in Django. It is very flexible and \"pluggable\" which makes it slightly more challenging to get setup. I would be interested in trying another option, but celery is a mature option that has a large community of users.",[168,36400,36402],{"id":36401},"scaling-celery-workers-to-zero","Scaling Celery workers to zero",[11,36404,36405],{},"Django is used in a few different ways in this application:",[916,36407,36408,36411,36414,36417],{},[919,36409,36410],{},"a backend API supported by Django REST Framework, Postgres and static files stored in AWS S3, served over CloudFront",[919,36412,36413],{},"a websocket server supported by Django Channels",[919,36415,36416],{},"an administrative backend that is automatically generated by Django (Django Admin)",[919,36418,36419],{},"asynchronous task workers supported by Celery",[11,36421,36422,36423,36425,36426,637,36428,1172,36431,36433],{},"The API server, websocket server and Django admin can technically be served by the same ",[33,36424,35964],{}," process. In terms of our application and AWS architecture, this means that requests for URLs starting with ",[33,36427,28423],{},[33,36429,36430],{},"/ws/",[33,36432,28232],{}," can all be sent to the same Fargate service target group.",[11,36435,36436],{},"Alternatively, we could split these up into two or three different process that can then be scaled individually.",[11,36438,36439,36440,752],{},"Celery processes should be run as separate processes. If you have multiple queues, you may have workers that are dedicated to processing tasks from certain queues which run as individual Fargate tasks, each with certain CPU and memory allocations and other celery settings, such as ",[33,36441,36442],{},"max_concurrency",[11,36444,36445,36446,36449],{},"To manage the total cost of ownership, I wanted to know how to scale Celery workers between 0 and ",[33,36447,36448],{},"N",". A celery worker is typically an \"always on\" process that watches for new messages that arrive in the queue and then processes message specified (the messages delivered to the queue contain information on which function to call, and what arguments that function should be called with.",[11,36451,36452],{},"Let's image that we have a celery task to process with the following properties:",[916,36454,36455,36458,36461,36464,36467,36470,36473],{},[919,36456,36457],{},"It takes a long time to process (between 15 minutes and 1 or 2 hours)",[919,36459,36460],{},"It has lots of dependencies (such as pandas, scikit-learn, etc.)",[919,36462,36463],{},"It requires a high amount of CPU and memory",[919,36465,36466],{},"It cannot easily be broken down into smaller sub-tasks",[919,36468,36469],{},"The time and frequency at which this task will be called is not predictable",[919,36471,36472],{},"A 2 - 3 minute delay between calling the task and starting work on the task is acceptable",[919,36474,36475],{},"This task might not be very easy to process with AWS Lambda without lots of additional logic, if not impossible.",[11,36477,36478],{},"To manage our project's TCO, we want scale down the number of Fargate tasks that process the queue this task is sent to. If there are no messages queued for this worker and no messages currently being processed by any of the workers for the queue, the number of Fargate tasks (celery workers) should be scaled down.",[11,36480,36481],{},"I haven't done much with autoscaling on AWS before, but I found that CDK provides some very nice abstractions that make scaling with Fargate task very straightforward.",[11,36483,36484,36485,36487],{},"ECS allows you to scale based on some built in metrics, such as CPU Utilization. Scaling between 0 and ",[33,36486,36448],{}," workers based on CPU utilization metrics wouldn't work because there would be no CPU utilization after the Fargate tasks are scaled to zero; messages in the queue would remain unprocessed and no new workers would be brought online.",[11,36489,36490],{},"Using the number of tasks in the queue would be a better option. I tried a few different options to get this to work.",[11,36492,36493,36494,8299,36497,36500],{},"First, I tried using one of the high-level constructs (L3 construct) from ",[33,36495,36496],{},"ecs_patterns",[33,36498,36499],{},"QueueProcessingFargateService",", but this would require that I replace Redis with SQS as the broker to be used with Celery. Some people prefer to use SQS, but I like having the ability to inspect and control, which requires a broker like Redis and is not possible with SQS.",[11,36502,36503],{},"My current solution involves:",[2276,36505,36506,36516,36523,36531,36540],{},[919,36507,36508,36509,36512,36513,343],{},"Creating a CloudWatch metric (the namespace is ",[33,36510,36511],{},"FULL_APP_NAME"," and the metric name is the name of the queue, ",[33,36514,36515],{},"default",[919,36517,36518,36519,36522],{},"Calling ",[33,36520,36521],{},"auto_scale_task_count"," on the Fargate service to create a an \"autoscaling Fargate task\"",[919,36524,36518,36525,36528,36529],{},[33,36526,36527],{},"scale_on_metric"," on the \"autoscaling Fargate task\" with the CloudWatch metric created in ",[33,36530,15625],{},[919,36532,30236,36533,36536,36537,36539],{},[33,36534,36535],{},"celery-metrics"," API endpoint on my Django API server that, when ",[33,36538,21623],{},"ed to, collects celery metrics per queue and publishes these metrics to CloudWatch.",[919,36541,36542,36543,36545],{},"Scheduling a Lambda function to post to the ",[33,36544,36535],{}," endpoint every 5 minutes.",[11,36547,36548],{},"Here's the code that takes care of steps 1, 2 and 3:",[26,36550,36552],{"className":1383,"code":36551,"language":1125,"meta":35,"style":35},"        self.default_celery_queue_cw_metric = cw.Metric(\n            namespace=scope.full_app_name, metric_name=\"default\"\n        )\n\n        self.celery_default_queue_asg = self.celery_default_worker_service.auto_scale_task_count(\n            min_capacity=0, max_capacity=2\n        )\n\n        self.celery_default_queue_asg.scale_on_metric(\n            \"CeleryDefaultQueueAutoscaling\",\n            metric=self.default_celery_queue_cw_metric,\n            scaling_steps=[\n                aas.ScalingInterval(change=-1, lower=0),\n                aas.ScalingInterval(change=1, lower=1),\n            ],\n            adjustment_type=aas.AdjustmentType.CHANGE_IN_CAPACITY,\n        )\n",[33,36553,36554,36559,36564,36568,36572,36577,36582,36586,36590,36595,36600,36605,36610,36615,36620,36624,36629],{"__ignoreMap":35},[187,36555,36556],{"class":189,"line":190},[187,36557,36558],{},"        self.default_celery_queue_cw_metric = cw.Metric(\n",[187,36560,36561],{"class":189,"line":249},[187,36562,36563],{},"            namespace=scope.full_app_name, metric_name=\"default\"\n",[187,36565,36566],{"class":189,"line":312},[187,36567,4531],{},[187,36569,36570],{"class":189,"line":319},[187,36571,316],{"emptyLinePlaceholder":315},[187,36573,36574],{"class":189,"line":325},[187,36575,36576],{},"        self.celery_default_queue_asg = self.celery_default_worker_service.auto_scale_task_count(\n",[187,36578,36579],{"class":189,"line":686},[187,36580,36581],{},"            min_capacity=0, max_capacity=2\n",[187,36583,36584],{"class":189,"line":697},[187,36585,4531],{},[187,36587,36588],{"class":189,"line":1291},[187,36589,316],{"emptyLinePlaceholder":315},[187,36591,36592],{"class":189,"line":1306},[187,36593,36594],{},"        self.celery_default_queue_asg.scale_on_metric(\n",[187,36596,36597],{"class":189,"line":1434},[187,36598,36599],{},"            \"CeleryDefaultQueueAutoscaling\",\n",[187,36601,36602],{"class":189,"line":2599},[187,36603,36604],{},"            metric=self.default_celery_queue_cw_metric,\n",[187,36606,36607],{"class":189,"line":2607},[187,36608,36609],{},"            scaling_steps=[\n",[187,36611,36612],{"class":189,"line":2621},[187,36613,36614],{},"                aas.ScalingInterval(change=-1, lower=0),\n",[187,36616,36617],{"class":189,"line":2631},[187,36618,36619],{},"                aas.ScalingInterval(change=1, lower=1),\n",[187,36621,36622],{"class":189,"line":2642},[187,36623,32974],{},[187,36625,36626],{"class":189,"line":2653},[187,36627,36628],{},"            adjustment_type=aas.AdjustmentType.CHANGE_IN_CAPACITY,\n",[187,36630,36631],{"class":189,"line":2665},[187,36632,4531],{},[11,36634,36635,36636,36639,36640,36643,36644,752],{},"Step 4 inspects active and reserved tasks, filters the tasks by the ",[33,36637,36638],{},"routing_key"," (which is the same as the queue name) and the combines this with any queued tasks by calling ",[33,36641,36642],{},"llen(queue_name)",". Finally, the queue names and combined active, reserved and queued task totals are sent to CloudWatch via boto3. The code for this is in ",[33,36645,36646],{},"backend/apps/core/utils/celery_utils.py",[11,36648,36649],{},"Here's the code for the Lambda function in step 5:",[26,36651,36653],{"className":1383,"code":36652,"language":1125,"meta":35,"style":35},"import json\nimport os\nimport urllib.request\n\nFULL_DOMAIN_NAME = os.environ.get(\"FULL_DOMAIN_NAME\")\nCELERY_METRICS_PATH = \"api/celery-metrics/\"\n\nCELERY_METRICS_URL = f\"https://{FULL_DOMAIN_NAME}/{CELERY_METRICS_PATH}\"\nCELERY_METRICS_TOKEN = os.environ.get(\"CELERY_METRICS_TOKEN\")\n\n\ndef lambda_handler(event, context):\n    data = {\"celery_metrics_token\": CELERY_METRICS_TOKEN}\n    params = json.dumps(data).encode('utf8')\n    req = urllib.request.Request(\n        CELERY_METRICS_URL,\n        data=params,\n        headers={'content-type': 'application/json'},\n    )\n    response = urllib.request.urlopen(req)\n    return response.read()\n",[33,36654,36655,36659,36663,36668,36672,36677,36682,36686,36691,36696,36700,36704,36708,36713,36718,36723,36728,36733,36738,36742,36747],{"__ignoreMap":35},[187,36656,36657],{"class":189,"line":190},[187,36658,10340],{},[187,36660,36661],{"class":189,"line":249},[187,36662,10345],{},[187,36664,36665],{"class":189,"line":312},[187,36666,36667],{},"import urllib.request\n",[187,36669,36670],{"class":189,"line":319},[187,36671,316],{"emptyLinePlaceholder":315},[187,36673,36674],{"class":189,"line":325},[187,36675,36676],{},"FULL_DOMAIN_NAME = os.environ.get(\"FULL_DOMAIN_NAME\")\n",[187,36678,36679],{"class":189,"line":686},[187,36680,36681],{},"CELERY_METRICS_PATH = \"api/celery-metrics/\"\n",[187,36683,36684],{"class":189,"line":697},[187,36685,316],{"emptyLinePlaceholder":315},[187,36687,36688],{"class":189,"line":1291},[187,36689,36690],{},"CELERY_METRICS_URL = f\"https://{FULL_DOMAIN_NAME}/{CELERY_METRICS_PATH}\"\n",[187,36692,36693],{"class":189,"line":1306},[187,36694,36695],{},"CELERY_METRICS_TOKEN = os.environ.get(\"CELERY_METRICS_TOKEN\")\n",[187,36697,36698],{"class":189,"line":1434},[187,36699,316],{"emptyLinePlaceholder":315},[187,36701,36702],{"class":189,"line":2599},[187,36703,316],{"emptyLinePlaceholder":315},[187,36705,36706],{"class":189,"line":2607},[187,36707,32745],{},[187,36709,36710],{"class":189,"line":2621},[187,36711,36712],{},"    data = {\"celery_metrics_token\": CELERY_METRICS_TOKEN}\n",[187,36714,36715],{"class":189,"line":2631},[187,36716,36717],{},"    params = json.dumps(data).encode('utf8')\n",[187,36719,36720],{"class":189,"line":2642},[187,36721,36722],{},"    req = urllib.request.Request(\n",[187,36724,36725],{"class":189,"line":2653},[187,36726,36727],{},"        CELERY_METRICS_URL,\n",[187,36729,36730],{"class":189,"line":2665},[187,36731,36732],{},"        data=params,\n",[187,36734,36735],{"class":189,"line":2674},[187,36736,36737],{},"        headers={'content-type': 'application/json'},\n",[187,36739,36740],{"class":189,"line":2684},[187,36741,23653],{},[187,36743,36744],{"class":189,"line":2694},[187,36745,36746],{},"    response = urllib.request.urlopen(req)\n",[187,36748,36749],{"class":189,"line":2706},[187,36750,36751],{},"    return response.read()\n",[11,36753,36754],{},"Finally, here is the code for defining the lambda function and scheduled invocation of the lambda function:",[26,36756,36758],{"className":1383,"code":36757,"language":1125,"meta":35,"style":35},"class CeleryAutoscalingStack(cloudformation.NestedStack):\n    def __init__(self, scope: core.Construct, id: str, **kwargs) -> None:\n        super().__init__(\n            scope, id, **kwargs,\n        )\n\n        self.lambda_function = aws_lambda.Function(\n            self,\n            \"CeleryMetricsLambdaFunction\",\n            code=aws_lambda.Code.asset(\"awslambda\"),\n            handler=\"publish_celery_metrics.lambda_handler\",\n            runtime=aws_lambda.Runtime.PYTHON_3_7,\n            environment=scope.variables.regular_variables,\n        )\n\n        self.celery_default_cw_metric_schedule = events.Rule(\n            self,\n            \"CeleryDefaultCWMetricSchedule\",\n            schedule=events.Schedule.rate(core.Duration.minutes(5)),\n            targets=[\n                events_targets.LambdaFunction(handler=self.lambda_function)\n            ],\n        )\n\n        # TODO: refactor this to loop through CloudWatch metrics multiple celery queues\n        scope.celery_default_service.default_celery_queue_cw_metric.grant_put_metric_data(\n            scope.backend_service.backend_task.task_role\n        )\n",[33,36759,36760,36765,36770,36775,36780,36784,36788,36793,36797,36802,36807,36812,36817,36822,36826,36830,36835,36839,36844,36849,36854,36859,36863,36867,36871,36876,36881,36886],{"__ignoreMap":35},[187,36761,36762],{"class":189,"line":190},[187,36763,36764],{},"class CeleryAutoscalingStack(cloudformation.NestedStack):\n",[187,36766,36767],{"class":189,"line":249},[187,36768,36769],{},"    def __init__(self, scope: core.Construct, id: str, **kwargs) -> None:\n",[187,36771,36772],{"class":189,"line":312},[187,36773,36774],{},"        super().__init__(\n",[187,36776,36777],{"class":189,"line":319},[187,36778,36779],{},"            scope, id, **kwargs,\n",[187,36781,36782],{"class":189,"line":325},[187,36783,4531],{},[187,36785,36786],{"class":189,"line":686},[187,36787,316],{"emptyLinePlaceholder":315},[187,36789,36790],{"class":189,"line":697},[187,36791,36792],{},"        self.lambda_function = aws_lambda.Function(\n",[187,36794,36795],{"class":189,"line":1291},[187,36796,22649],{},[187,36798,36799],{"class":189,"line":1306},[187,36800,36801],{},"            \"CeleryMetricsLambdaFunction\",\n",[187,36803,36804],{"class":189,"line":1434},[187,36805,36806],{},"            code=aws_lambda.Code.asset(\"awslambda\"),\n",[187,36808,36809],{"class":189,"line":2599},[187,36810,36811],{},"            handler=\"publish_celery_metrics.lambda_handler\",\n",[187,36813,36814],{"class":189,"line":2607},[187,36815,36816],{},"            runtime=aws_lambda.Runtime.PYTHON_3_7,\n",[187,36818,36819],{"class":189,"line":2621},[187,36820,36821],{},"            environment=scope.variables.regular_variables,\n",[187,36823,36824],{"class":189,"line":2631},[187,36825,4531],{},[187,36827,36828],{"class":189,"line":2642},[187,36829,316],{"emptyLinePlaceholder":315},[187,36831,36832],{"class":189,"line":2653},[187,36833,36834],{},"        self.celery_default_cw_metric_schedule = events.Rule(\n",[187,36836,36837],{"class":189,"line":2665},[187,36838,22649],{},[187,36840,36841],{"class":189,"line":2674},[187,36842,36843],{},"            \"CeleryDefaultCWMetricSchedule\",\n",[187,36845,36846],{"class":189,"line":2684},[187,36847,36848],{},"            schedule=events.Schedule.rate(core.Duration.minutes(5)),\n",[187,36850,36851],{"class":189,"line":2694},[187,36852,36853],{},"            targets=[\n",[187,36855,36856],{"class":189,"line":2706},[187,36857,36858],{},"                events_targets.LambdaFunction(handler=self.lambda_function)\n",[187,36860,36861],{"class":189,"line":2715},[187,36862,32974],{},[187,36864,36865],{"class":189,"line":2725},[187,36866,4531],{},[187,36868,36869],{"class":189,"line":2735},[187,36870,316],{"emptyLinePlaceholder":315},[187,36872,36873],{"class":189,"line":2743},[187,36874,36875],{},"        # TODO: refactor this to loop through CloudWatch metrics multiple celery queues\n",[187,36877,36878],{"class":189,"line":2754},[187,36879,36880],{},"        scope.celery_default_service.default_celery_queue_cw_metric.grant_put_metric_data(\n",[187,36882,36883],{"class":189,"line":2762},[187,36884,36885],{},"            scope.backend_service.backend_task.task_role\n",[187,36887,36888],{"class":189,"line":2770},[187,36889,4531],{},[168,36891,36893],{"id":36892},"miscellaneous-grievances","Miscellaneous Grievances",[916,36895,36896,36899,36905],{},[919,36897,36898],{},"Fargate tasks cannot access Secrets that use JSON template",[919,36900,36901,36902,36904],{},"You cannot select ",[33,36903,12843],{}," on a CDK-created ECS cluster. This option seems to only be available from the ECS Wizard in the AWS Console",[919,36906,36907,36908,36912],{},"It's not super clear how to package dependencies in Lambda with CDK. Here's an interesting approach that I would like to try: ",[15,36909,36910],{"href":36910,"rel":36911},"https://github.com/aws-samples/aws-cdk-examples/issues/130#issuecomment-554097487",[19],". The Lambda functions I'm using don't require anything outside of the standard library, but if they did it would require some additional work to make sure dependencies can be added as Lambda Layers.",[168,36914,7912],{"id":7911},[916,36916,36917,36920,36923,36926],{},[919,36918,36919],{},"Create an IAM role template that can be used to create an IAM role that has access to setup infrastructure through CDK. Currently I'm using an admin account which is not a best practice.",[919,36921,36922],{},"Move this Wiki Article to the project documentation site and main README",[919,36924,36925],{},"Expand on each topic, replace existing documentation",[919,36927,36928,36929,36931],{},"Add a summary of each ",[33,36930,35508],{}," from CDK code",[855,36933,36934],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}",{"title":35,"searchDepth":249,"depth":249,"links":36936},[36937,36938,36950,36964,36971,36972,36973],{"id":34557,"depth":249,"text":34558},{"id":34606,"depth":249,"text":34607,"children":36939},[36940,36941,36942,36943,36944,36945,36946,36947,36948,36949],{"id":27146,"depth":312,"text":27147},{"id":14751,"depth":312,"text":14752},{"id":34743,"depth":312,"text":34744},{"id":12748,"depth":312,"text":32684},{"id":34831,"depth":312,"text":34832},{"id":34878,"depth":312,"text":34879},{"id":34904,"depth":312,"text":34905},{"id":35007,"depth":312,"text":35008},{"id":35066,"depth":312,"text":34577},{"id":35097,"depth":312,"text":35098},{"id":35144,"depth":249,"text":35145,"children":36951},[36952,36954,36955,36957,36959,36961,36962],{"id":35151,"depth":312,"text":36953},"Start here: https://cdkworkshop.com",{"id":35162,"depth":312,"text":35163},{"id":35465,"depth":312,"text":36956},"Using cdk synth in development",{"id":35505,"depth":312,"text":36958},"NestedStacks",{"id":35568,"depth":312,"text":36960},"cdk deploy and CDK in GitLab CI pipelines",{"id":35806,"depth":312,"text":35809},{"id":35982,"depth":312,"text":36963},"Quick tour of ApplicationStack",{"id":36310,"depth":249,"text":36965,"children":36966},"Why X? Why not Y?",[36967,36968,36969,36970],{"id":36325,"depth":312,"text":36326},{"id":36359,"depth":312,"text":36360},{"id":36366,"depth":312,"text":36367},{"id":36394,"depth":312,"text":36395},{"id":36401,"depth":249,"text":36402},{"id":36892,"depth":249,"text":36893},{"id":7911,"depth":249,"text":7912},"2020-06-02",{"layout":29014},"/2020/06/02/django-postgres-vue-gitlab-ecs",{"title":34500,"description":35},"2020/06/02/django-postgres-vue-gitlab-ecs",[15296,15290,28639,882,15291],"U7jqz0ocPtApQEHwM0vKqM7fVpElt05VgC92B0yZIqM",{"id":36982,"title":36983,"body":36984,"comments":315,"date":37040,"description":37041,"draft":872,"extension":873,"external":874,"image":37042,"meta":37043,"navigation":315,"path":37044,"seo":37045,"stem":37046,"tags":37047,"__hash__":37048},"blog/2019/01/09/django-docker-vue-nginx-drf-traefik-celery.md","Setup, Development and Deployment of a web app using Django, VueJS, VuePress, Docker, nginx, traefik and GitLab",{"type":8,"value":36985,"toc":37038},[36986,36998,37001,37023,37026,37031],[11,36987,36988,36989,36993,36994,752],{},"This project is currently deployed on ",[15,36990,36991],{"href":36991,"rel":36992},"https://verbose-equals-true.tk",[19],". The source code is available at ",[15,36995,36996],{"href":36996,"rel":36997},"https://gitlab.com/briancaffey/verbose-equals-true",[19],[11,36999,37000],{},"The goal of this project is to explain how to setup a project starting with a fresh installation of 16.04. Setup includes local development environment, GitLab CI/CD, VSCode settings and configuration of the different containers that make up the application:",[916,37002,37003,37005,37008,37010,37012,37015,37017,37020],{},[919,37004,34535],{},[919,37006,37007],{},"Node (for local development with VueJS)",[919,37009,30735],{},[919,37011,32027],{},[919,37013,37014],{},"Postgres",[919,37016,13042],{},[919,37018,37019],{},"flower",[919,37021,37022],{},"portainer",[11,37024,37025],{},"Here's an overview of the architecture used in the application:",[11,37027,37028],{},[511,37029],{"alt":7255,"src":37030},"/static/architecture.png",[11,37032,37033,37034,752],{},"Extensive documentation for this project can be found at ",[15,37035,37036],{"href":37036,"rel":37037},"https://verbose-equals-true.tk/docs",[19],{"title":35,"searchDepth":249,"depth":249,"links":37039},[],"2019-01-09","This project is currently deployed on https://verbose-equals-true.tk. The source code is available at https://gitlab.com/briancaffey/verbose-equals-true.","/static/technologies.png",{"layout":29014},"/2019/01/09/django-docker-vue-nginx-drf-traefik-celery",{"title":36983,"description":37041},"2019/01/09/django-docker-vue-nginx-drf-traefik-celery",[15290,882,15298,28639],"chUpawMMoieGHXW3jOUznjOHny1Ne3eaXE-H0D6iZxE",{"id":37050,"title":37051,"body":37052,"comments":315,"date":37998,"description":37999,"draft":872,"extension":873,"external":874,"image":38000,"meta":38001,"navigation":315,"path":38002,"seo":38003,"stem":38004,"tags":38005,"__hash__":38009},"blog/2018/04/26/generating-music-from-guitar-tabs-with-python.md","Generating MIDI files from guitar tablatures with Python and regular expressions",{"type":8,"value":37053,"toc":37991},[37054,37061,37067,37070,37073,37076,37083,37086,37090,37093,37097,37103,37109,37115,37121,37127,37133,37136,37209,37213,37216,37226,37232,37236,37239,37247,37252,37255,37360,37363,37370,37373,37382,37385,37391,37397,37403,37409,37414,37420,37423,37429,37432,37435,37438,37443,37449,37537,37543,37546,37944,37947,37986,37989],[11,37055,37056,37057,37060],{},"Recently I have been playing lots of classical guitar. I use ",[15,37058,37059],{"href":37059},"classtab.org",", a website with hundreds of classical guitar pieces in tab form. Here's an example of a guitar tab:",[26,37062,37065],{"className":37063,"code":37064,"language":31},[29],"|---------------0---|---3-2-3-----------|-----0-------------|\n|-------0-3-1-1---3-|-3-------3-0-----0-|-1-3---3-1-0---0---|\n|---0-2-0-----------|-------------0-2---|-------------2---0-|\n|-------------2-----|-------2-----------|-------------------|\n|-------------------|-2-----------------|-0-----2-----3-----|\n|-3-----------------|-------------0-----|-------------------|\n",[33,37066,37064],{"__ignoreMap":35},[11,37068,37069],{},"It is a representation of where and when strings are played. The six lines here represent the six strings of the guitar e, A, D, G, B and E and the numbers indicate where on the fretboard to press for each note. The vertical bars represent one bar of music.",[11,37071,37072],{},"My goal is to write an application that can read in guitar tabs and then produce a corresponding MIDI file for the song.",[11,37074,37075],{},"To start, I will use regular expressions to parse the bars of music in a guitar tab text file.",[11,37077,37078,37079,37082],{},"Next, I will use a Python package called MIDIUtil to generate ",[33,37080,37081],{},".mid"," files from the guitar tabs.",[11,37084,37085],{},"Finally, I'll put this into a simple web app that lets users copy paste guitar tab text or enter a URL from a site like classtab.org that will similarly generate a MIDI file.",[168,37087,37089],{"id":37088},"regex","Regex",[11,37091,37092],{},"First, let's look at a few different examples of guitar tabs to get a better sense of what our goal should be in using regular expressions to parse notes.",[911,37094,37096],{"id":37095},"tab-examples","Tab examples",[11,37098,37099],{},[15,37100,37101],{"href":37101,"rel":37102},"http://www.classtab.org/barrios_la_catedral_3_allegro.txt",[19],[26,37104,37107],{"className":37105,"code":37106,"language":31},[29],"La Catedral\nAuthor: Agustin Barrios\nTablature by: Gianpiero Ciammaricone\n              giancian@yahoo.com\n              http://www.geocities.com/vienna/6619\n\nTablature Explanation at the end.\n\n[key corrected from \"D Major\" Jan 99]\n\nThis is Revision 3 (5/2/98), I have added a modification made by\nBarrios in line 5 of the TAB, both ways of playing it are included,\nso you can choose the best one for you.\nRevision 2: Made a few corrections.\nRevision 1: The first release of the tab.\n\n3rd Movement (Allegro):\nKey: B Minor\nTime signature: 6/8\n\nE-|-------------------------|-----------------------------|\nB-|-----------------3-------|-----------------3-----------|\nG-|---------0---4-------4---|---------0---4-------4-------|\nD-|---4-3h4---4---4---4---4-|---4-3h4---4---4---4---4-----|\nA-|-2-----------------------|-2---------------------------|\nE-|-------------------------|-----------------------------|\n    p m i   m i m i a i m i\n\nE-|-------------------------|-----------------------------|\nB-|-----------------3-------|-----------------3-----------|\nG-|---------2---4-------4---|---------2---4-------4-------|\nD-|---5-4h5---5---5---5---5-|---5-4h5---5---5---5---5-----|\nA-|-2-----------------------|-2---------------------------|\nE-|-------------------------|-----------------------------|\n    p m i   m i m i a i m i\n\nE-|-------------------------|-----------------------------|\nB-|-----------------5-------|-----------------5-----------|\nG-|---------4---6-------6---|---------4---6-------6-------|\nD-|---7-6h7---7---7---7---7-|---7-6h7---7---7---7---7-----|\nA-|-4-----------------------|-4---------------------------|\nE-|-------------------------|-----------------------------|\n    p m i   m i m i a i m i\n",[33,37108,37106],{"__ignoreMap":35},[11,37110,37111],{},[15,37112,37113],{"href":37113,"rel":37114},"http://www.classtab.org/moreno_torroba_siete_piezas_de_album_6_chisperada.txt",[19],[26,37116,37119],{"className":37117,"code":37118,"language":31},[29],"SIETE PIEZAS DE ALBUM: VI. CHISPERADA\nAs recorded by Federico Moreno Torroba\n\nMusic by Federico Moreno Torroba\n\n|-----0-----0-|-----------0-|-----0-----0-|\n|---1-----0---|-----3-----0-|---1-----0---|\n|---2-----0---|---2-------1-|---2-----0---|\n|---2---------|---3---------|---2---------|\n|-0-----------|---------2---|-0-----------|\n|-------3-----|-1-----0-----|-------3-----|\n\n|-----------0-|-----0-----0-|-----------0-|-----------0--|\n|-----3-----0-|---1-----0---|-----3-----0-|-----3-----0--|\n|---2-------1-|---2-----0---|---2-------1-|---2-------1--|\n|---3---------|---2---------|---3---------|---3----------|\n|---------2---|-0-----------|-0-------2---|-0-------2----|\n|-1-----0-----|-------3-----|-------0-----|-------0------|\n\n|---------------0--|-0-----0-----|-1-----1-3p1-0---|\n|---1-0---------0--|-1-----------|---------------3-|\n|-2-----2-0-----1--|-2-----1-----|-2-----2---------|\n|-----------3------|-------------|-------0---------|\n|-3----------------|---0-0---2-2-|---3-3-----------|\n|-------------0----|-------------|-----------------|\n",[33,37120,37118],{"__ignoreMap":35},[11,37122,37123],{},[15,37124,37125],{"href":37125,"rel":37126},"http://www.classtab.org/moreno_torroba_sonatina_in_a_2_andante.txt",[19],[26,37128,37131],{"className":37129,"code":37130,"language":31},[29],"Sonatina in A - II - Andante\n\nF. Moreno Torroba (1891-1982)\n\ntuning DADGBE\n\nE|-----2--3---5----------10--8--7-------|-----8----7^8^7-----------------------|\nB|-----3--5---7----------7--------------|-----8-----------10---------8---------|\nG|-----2---------------------9--7-------|-----9--------------------------11----|\nD|-----0--------------4-----------------|--------------------------10----------|\nA|-----0----------0---------------------|----------------------7---------------|\nD|-----0--------------------------------|-----0--------------------------------|\n\nE|--------------------------------------|--------------------------------------|\nB|------------------7--8^10-8---7^5-----|---5---------7-8-7-5-----5------------|\nG|--9^11^9----7--9------------------7---|---6-----------------7---6------------|\nD|-------8------------------------------|---5---------------------5--7----19°--|\nA|-------10-----------------------------|------0--12°--------------------------|\nD|-------0------------------------------|---0---------------------0------------|\n",[33,37132,37130],{"__ignoreMap":35},[11,37134,37135],{},"Here are a few things that we need to consider:",[916,37137,37138,37163,37180,37183,37195],{},[919,37139,37140,37141,15754,37144,637,37147,637,37150,37153,37154,37156,37157,37159,37160,752],{},"The start of each line sometimes starts with a the note of the string (",[33,37142,37143],{},"E",[33,37145,37146],{},"e",[33,37148,37149],{},"A",[33,37151,37152],{},"D",", etc.) followed by ",[33,37155,16679],{}," or just simply start with ",[33,37158,16679],{},". Some lines may start with ",[33,37161,37162],{},"E-|",[919,37164,37165,37166,37168,37169,637,37171,15754,37174,37177,37178,752],{},"There is generally a ",[33,37167,677],{}," character between each note. When this is not the case, we may have a ",[33,37170,11],{},[33,37172,37173],{},"h",[33,37175,37176],{},"^"," which indicates \"pull off\" or \"hammer on\". This means that you are playing the string with the hand on the fretboard, not the hand over the sound hole. We should replace these with ",[33,37179,677],{},[919,37181,37182],{},"A tab may have an inconsistant number of characters in each bar. I'm not too concerned about this. I'm not going for perfection just yet, but it could make this task very difficult if you want to get the timing of your sounds just right.",[919,37184,37185,37186,37189,37190,37192,37193,752],{},"Notes played on the 10th fret and higher must be read together. For example, ",[33,37187,37188],{},"---12---"," should be played on the 12 fret; it does not mean play ",[33,37191,15625],{}," and then play ",[33,37194,22129],{},[919,37196,37197,37198,15754,37201,20174,37203,37206,37207,35248],{},"There are some other special characters such as ",[33,37199,37200],{},"°",[33,37202,6724],{},[33,37204,37205],{},">",". I think these both denote harmonics. This is when you place your finger on the string at a position along the fretboard, but do not press down. We probably want to replace these characters with ",[33,37208,677],{},[911,37210,37212],{"id":37211},"tab-text-file-processing","Tab text file processing",[11,37214,37215],{},"Here are some things that may be helpful to do as soon as we read a text file:",[916,37217,37218,37221],{},[919,37219,37220],{},"convert everything to upper or lower case",[919,37222,37223,37224],{},"replace non-numeric characters with ",[33,37225,677],{},[11,37227,37228,37229,752],{},"When I work with regex, I usually go straight to ",[15,37230,37231],{"href":37231},"regex101.com",[168,37233,37235],{"id":37234},"midiutil","MIDIUtil",[11,37237,37238],{},"At this point, I think it will be helpful to examine MIDIUtil and create and play a basic MIDI file.",[11,37240,37241,37242,752],{},"Before you do that, you might want to read over the ",[15,37243,37246],{"href":37244,"rel":37245},"https://wiki.archlinux.org/index.php/MIDI",[19],"MIDI Arch Wiki article",[107,37248,37249],{},[11,37250,37251],{},"MIDIUtil is a pure Python library that allows one to write multi-track Musical Instrument Digital Interface (MIDI) files from within Python programs (both format 1 and format 2 files are now supported). It is object-oriented and allows one to create and write these files with a minimum of fuss.",[11,37253,37254],{},"Here's a basic example of MIDIUtil. This script generates a MIDI file that contains a major scale.",[26,37256,37258],{"className":1383,"code":37257,"language":1125,"meta":35,"style":35},"#!/usr/bin/env python\n\nfrom midiutil import MIDIFile\n\ndegrees  = [60, 62, 64, 65, 67, 69, 71, 72]  # MIDI note number\ntrack    = 0\nchannel  = 0\ntime     = 0    # In beats\nduration = 1    # In beats\ntempo    = 60   # In BPM\nvolume   = 100  # 0-127, as per the MIDI standard\n\nMyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track is created\n                      # automatically)\nMyMIDI.addTempo(track, time, tempo)\n\nfor i, pitch in enumerate(degrees):\n    MyMIDI.addNote(track, channel, pitch, time + i, duration, volume)\n\nwith open(\"major-scale.mid\", \"wb\") as output_file:\n    MyMIDI.writeFile(output_file)\n",[33,37259,37260,37265,37269,37274,37278,37283,37288,37293,37298,37303,37308,37313,37317,37322,37327,37332,37336,37341,37346,37350,37355],{"__ignoreMap":35},[187,37261,37262],{"class":189,"line":190},[187,37263,37264],{},"#!/usr/bin/env python\n",[187,37266,37267],{"class":189,"line":249},[187,37268,316],{"emptyLinePlaceholder":315},[187,37270,37271],{"class":189,"line":312},[187,37272,37273],{},"from midiutil import MIDIFile\n",[187,37275,37276],{"class":189,"line":319},[187,37277,316],{"emptyLinePlaceholder":315},[187,37279,37280],{"class":189,"line":325},[187,37281,37282],{},"degrees  = [60, 62, 64, 65, 67, 69, 71, 72]  # MIDI note number\n",[187,37284,37285],{"class":189,"line":686},[187,37286,37287],{},"track    = 0\n",[187,37289,37290],{"class":189,"line":697},[187,37291,37292],{},"channel  = 0\n",[187,37294,37295],{"class":189,"line":1291},[187,37296,37297],{},"time     = 0    # In beats\n",[187,37299,37300],{"class":189,"line":1306},[187,37301,37302],{},"duration = 1    # In beats\n",[187,37304,37305],{"class":189,"line":1434},[187,37306,37307],{},"tempo    = 60   # In BPM\n",[187,37309,37310],{"class":189,"line":2599},[187,37311,37312],{},"volume   = 100  # 0-127, as per the MIDI standard\n",[187,37314,37315],{"class":189,"line":2607},[187,37316,316],{"emptyLinePlaceholder":315},[187,37318,37319],{"class":189,"line":2621},[187,37320,37321],{},"MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track is created\n",[187,37323,37324],{"class":189,"line":2631},[187,37325,37326],{},"                      # automatically)\n",[187,37328,37329],{"class":189,"line":2642},[187,37330,37331],{},"MyMIDI.addTempo(track, time, tempo)\n",[187,37333,37334],{"class":189,"line":2653},[187,37335,316],{"emptyLinePlaceholder":315},[187,37337,37338],{"class":189,"line":2665},[187,37339,37340],{},"for i, pitch in enumerate(degrees):\n",[187,37342,37343],{"class":189,"line":2674},[187,37344,37345],{},"    MyMIDI.addNote(track, channel, pitch, time + i, duration, volume)\n",[187,37347,37348],{"class":189,"line":2684},[187,37349,316],{"emptyLinePlaceholder":315},[187,37351,37352],{"class":189,"line":2694},[187,37353,37354],{},"with open(\"major-scale.mid\", \"wb\") as output_file:\n",[187,37356,37357],{"class":189,"line":2706},[187,37358,37359],{},"    MyMIDI.writeFile(output_file)\n",[11,37361,37362],{},"Now let's try to play this file. Well, if you read the MIDI article, you would know that this isn't really what we are doing. We want to see what our major scale sounds like. In order to do that, we need to install and configure both a MIDI synthesizer as well as a soundfont.",[11,37364,37365,37366,752],{},"We can use timidity++ for our MIDI synth. There are very simple instructions on how to do this ",[15,37367,1321],{"href":37368,"rel":37369},"https://wiki.archlinux.org/index.php/timidity",[19],[11,37371,37372],{},"Here are the steps I followed:",[11,37374,37375,37376,1172,37379,752],{},"From the AUR, install ",[33,37377,37378],{},"timidity++",[33,37380,37381],{},"timidity-freepats",[11,37383,37384],{},"Add yourself to the audio group:",[26,37386,37389],{"className":37387,"code":37388,"language":31},[29],"# gpasswd -a brian audio\n",[33,37390,37388],{"__ignoreMap":35},[11,37392,37393,37394,358],{},"Add the following line to ",[33,37395,37396],{},"/etc/timidity++/timidity.cfg",[26,37398,37401],{"className":37399,"code":37400,"language":31},[29],"soundfont /usr/share/soundfonts/timidity-freepats.sf2\n",[33,37402,37400],{"__ignoreMap":35},[26,37404,37407],{"className":37405,"code":37406,"language":31},[29],"sudo systemctl start timidity.service\nsudo systemctl enable timidity.service\n",[33,37408,37406],{"__ignoreMap":35},[11,37410,1164,37411,358],{},[33,37412,37413],{},"aplaymidi -l",[26,37415,37418],{"className":37416,"code":37417,"language":31},[29]," $ aplaymidi -l\n Port    Client name                      Port name\n 14:0    Midi Through                     Midi Through Port-0\n130:0    TiMidity                         TiMidity port 0\n130:1    TiMidity                         TiMidity port 1\n130:2    TiMidity                         TiMidity port 2\n130:3    TiMidity                         TiMidity port 3\n",[33,37419,37417],{"__ignoreMap":35},[11,37421,37422],{},"Finally, we can play our MIDI file that we generated by running the python script above:",[26,37424,37427],{"className":37425,"code":37426,"language":31},[29]," $ aplaymidi major-scale.mid --port 130:0\n\u003Cmusic plays...>\n",[33,37428,37426],{"__ignoreMap":35},[11,37430,37431],{},"For this project, playing files using a synthesizer is useful to verify that the MIDI was successfully created. If/when we get to the point of making a web app to convert midi files to music files, we will generate the midifile in-memory, convert it to a WAV file and then return that to the user.",[11,37433,37434],{},"Let's try to process a very simple tab file with just one string and a few notes, and then generate a simple MIDI file based on the tab.",[11,37436,37437],{},"Our sample tab can be:",[11,37439,37440],{},[4339,37441,37442],{},"simple-tab.txt",[26,37444,37447],{"className":37445,"code":37446,"language":31},[29],"--0--2--4--\n",[33,37448,37446],{"__ignoreMap":35},[26,37450,37451],{"className":1383,"code":37257,"language":1125,"meta":35,"style":35},[33,37452,37453,37457,37461,37465,37469,37473,37477,37481,37485,37489,37493,37497,37501,37505,37509,37513,37517,37521,37525,37529,37533],{"__ignoreMap":35},[187,37454,37455],{"class":189,"line":190},[187,37456,37264],{},[187,37458,37459],{"class":189,"line":249},[187,37460,316],{"emptyLinePlaceholder":315},[187,37462,37463],{"class":189,"line":312},[187,37464,37273],{},[187,37466,37467],{"class":189,"line":319},[187,37468,316],{"emptyLinePlaceholder":315},[187,37470,37471],{"class":189,"line":325},[187,37472,37282],{},[187,37474,37475],{"class":189,"line":686},[187,37476,37287],{},[187,37478,37479],{"class":189,"line":697},[187,37480,37292],{},[187,37482,37483],{"class":189,"line":1291},[187,37484,37297],{},[187,37486,37487],{"class":189,"line":1306},[187,37488,37302],{},[187,37490,37491],{"class":189,"line":1434},[187,37492,37307],{},[187,37494,37495],{"class":189,"line":2599},[187,37496,37312],{},[187,37498,37499],{"class":189,"line":2607},[187,37500,316],{"emptyLinePlaceholder":315},[187,37502,37503],{"class":189,"line":2621},[187,37504,37321],{},[187,37506,37507],{"class":189,"line":2631},[187,37508,37326],{},[187,37510,37511],{"class":189,"line":2642},[187,37512,37331],{},[187,37514,37515],{"class":189,"line":2653},[187,37516,316],{"emptyLinePlaceholder":315},[187,37518,37519],{"class":189,"line":2665},[187,37520,37340],{},[187,37522,37523],{"class":189,"line":2674},[187,37524,37345],{},[187,37526,37527],{"class":189,"line":2684},[187,37528,316],{"emptyLinePlaceholder":315},[187,37530,37531],{"class":189,"line":2694},[187,37532,37354],{},[187,37534,37535],{"class":189,"line":2706},[187,37536,37359],{},[26,37538,37541],{"className":37539,"code":37540,"language":31},[29]," $ echo \"--0--2--4--\" > simple-tab.txt\n",[33,37542,37540],{"__ignoreMap":35},[11,37544,37545],{},"Here's the script I used to generate some fairly accurate MIDI files. There are still some bugs in the script that generates the MIDI file, but this meets the goal I had last weekend of writing a simple script to generate music from guitar tabs.",[26,37547,37549],{"className":1383,"code":37548,"language":1125,"meta":35,"style":35},"#!/usr/bin/env python\n\nfrom midiutil import MIDIFile\nimport sys\nimport re\n\nstring_notes = [\"high_e\", \"B\", \"G\", \"D\", \"A\", \"E\"]\n\nguitar_strings = {\n    'E':{'note_val':52, 'track_num':0},\n    'A':{'note_val':57, 'track_num':1},\n    'D':{'note_val':62, 'track_num':2},\n    'G':{'note_val':67, 'track_num':3},\n    'B':{'note_val':71, 'track_num':4},\n    'high_e':{'note_val':76, 'track_num':5},\n}\n\n# read the tab file\nfile_name = sys.argv[1]\nif file_name.split(\".\")[-1] != 'txt':\n    print(\"Please select a text file\")\n\nwith open(file_name) as f:\n    contents = f.read()\n\ncontents = contents.replace(\"h\", \"-\")\ncontents = contents.replace(\"p\", \"-\")\ncontents = contents.replace(\"/\", \"-\")\ncontents = contents.replace(\"*\", \"-\")\ncontents = contents.upper()\nbar_group = re.findall(r\"(?:[E,B,G,D,A,-]+\\|[0-9-h|]+\\n){6}\",contents)\n\n#bar_group = re.findall(r\"(?:\\|[0-9-\\*h\\|]+\\n){6}\",contents)\n\n\ntrack    = 0\nchannel  = 0\ntime     = 0    # In beats\nduration = 1    # In beats\ntempo    = 1000   # In BPM\nvolume   = 100  # 0-127, as per the MIDI standard\n\nMyMIDI = MIDIFile(6)  # One track, defaults to format 1 (tempo track is created\n                      # automatically)\nMyMIDI.addTempo(track, time, tempo)\n\ninterval = len(bar_group[0].split(\"\\n\")) - 1\n\nfor b in bar_group:\n\n    strings = b.split(\"\\n\")\n    strings = [x for x in strings if x != '']\n    e_count = 0\n    for i,s in enumerate(strings):\n\n        current_string = strings[i][0]\n        if current_string not in guitar_strings.keys():\n            current_string = string_notes[i]\n        if current_string == \"E\":\n            e_count += 1\n        if e_count == 2:\n            current_string == \"high_e\"\n\n        track = guitar_strings[current_string]['track_num']\n\n        s = s[1:]\n        s = s.replace('|', '')\n        s = list(s)\n\n        for i, pitch in enumerate(s):\n            volume = 100\n\n            if pitch == \"\\n\":\n                break\n            if pitch == \"-\":\n                volume = 0\n                pitch = 50\n            print(\"adding note\")\n            pitch = int(pitch) + guitar_strings[current_string]['note_val']\n            MyMIDI.addNote(track, channel, pitch, time + i, duration, volume)\n\n    time += interval*8\n\nwith open(\"major-scale.mid\", \"wb\") as output_file:\n    MyMIDI.writeFile(output_file)\n",[33,37550,37551,37555,37559,37563,37568,37573,37577,37582,37586,37591,37596,37601,37606,37611,37616,37621,37625,37629,37634,37639,37644,37649,37653,37658,37663,37667,37672,37677,37682,37687,37692,37697,37701,37706,37710,37714,37718,37722,37726,37730,37735,37739,37743,37748,37752,37756,37760,37765,37769,37774,37778,37783,37788,37793,37798,37802,37807,37812,37817,37822,37827,37832,37837,37841,37846,37850,37855,37860,37865,37869,37874,37879,37883,37888,37893,37898,37903,37908,37913,37918,37923,37927,37932,37936,37940],{"__ignoreMap":35},[187,37552,37553],{"class":189,"line":190},[187,37554,37264],{},[187,37556,37557],{"class":189,"line":249},[187,37558,316],{"emptyLinePlaceholder":315},[187,37560,37561],{"class":189,"line":312},[187,37562,37273],{},[187,37564,37565],{"class":189,"line":319},[187,37566,37567],{},"import sys\n",[187,37569,37570],{"class":189,"line":325},[187,37571,37572],{},"import re\n",[187,37574,37575],{"class":189,"line":686},[187,37576,316],{"emptyLinePlaceholder":315},[187,37578,37579],{"class":189,"line":697},[187,37580,37581],{},"string_notes = [\"high_e\", \"B\", \"G\", \"D\", \"A\", \"E\"]\n",[187,37583,37584],{"class":189,"line":1291},[187,37585,316],{"emptyLinePlaceholder":315},[187,37587,37588],{"class":189,"line":1306},[187,37589,37590],{},"guitar_strings = {\n",[187,37592,37593],{"class":189,"line":1434},[187,37594,37595],{},"    'E':{'note_val':52, 'track_num':0},\n",[187,37597,37598],{"class":189,"line":2599},[187,37599,37600],{},"    'A':{'note_val':57, 'track_num':1},\n",[187,37602,37603],{"class":189,"line":2607},[187,37604,37605],{},"    'D':{'note_val':62, 'track_num':2},\n",[187,37607,37608],{"class":189,"line":2621},[187,37609,37610],{},"    'G':{'note_val':67, 'track_num':3},\n",[187,37612,37613],{"class":189,"line":2631},[187,37614,37615],{},"    'B':{'note_val':71, 'track_num':4},\n",[187,37617,37618],{"class":189,"line":2642},[187,37619,37620],{},"    'high_e':{'note_val':76, 'track_num':5},\n",[187,37622,37623],{"class":189,"line":2653},[187,37624,1309],{},[187,37626,37627],{"class":189,"line":2665},[187,37628,316],{"emptyLinePlaceholder":315},[187,37630,37631],{"class":189,"line":2674},[187,37632,37633],{},"# read the tab file\n",[187,37635,37636],{"class":189,"line":2684},[187,37637,37638],{},"file_name = sys.argv[1]\n",[187,37640,37641],{"class":189,"line":2694},[187,37642,37643],{},"if file_name.split(\".\")[-1] != 'txt':\n",[187,37645,37646],{"class":189,"line":2706},[187,37647,37648],{},"    print(\"Please select a text file\")\n",[187,37650,37651],{"class":189,"line":2715},[187,37652,316],{"emptyLinePlaceholder":315},[187,37654,37655],{"class":189,"line":2725},[187,37656,37657],{},"with open(file_name) as f:\n",[187,37659,37660],{"class":189,"line":2735},[187,37661,37662],{},"    contents = f.read()\n",[187,37664,37665],{"class":189,"line":2743},[187,37666,316],{"emptyLinePlaceholder":315},[187,37668,37669],{"class":189,"line":2754},[187,37670,37671],{},"contents = contents.replace(\"h\", \"-\")\n",[187,37673,37674],{"class":189,"line":2762},[187,37675,37676],{},"contents = contents.replace(\"p\", \"-\")\n",[187,37678,37679],{"class":189,"line":2770},[187,37680,37681],{},"contents = contents.replace(\"/\", \"-\")\n",[187,37683,37684],{"class":189,"line":2781},[187,37685,37686],{},"contents = contents.replace(\"*\", \"-\")\n",[187,37688,37689],{"class":189,"line":2792},[187,37690,37691],{},"contents = contents.upper()\n",[187,37693,37694],{"class":189,"line":2803},[187,37695,37696],{},"bar_group = re.findall(r\"(?:[E,B,G,D,A,-]+\\|[0-9-h|]+\\n){6}\",contents)\n",[187,37698,37699],{"class":189,"line":2808},[187,37700,316],{"emptyLinePlaceholder":315},[187,37702,37703],{"class":189,"line":2816},[187,37704,37705],{},"#bar_group = re.findall(r\"(?:\\|[0-9-\\*h\\|]+\\n){6}\",contents)\n",[187,37707,37708],{"class":189,"line":2824},[187,37709,316],{"emptyLinePlaceholder":315},[187,37711,37712],{"class":189,"line":2834},[187,37713,316],{"emptyLinePlaceholder":315},[187,37715,37716],{"class":189,"line":2845},[187,37717,37287],{},[187,37719,37720],{"class":189,"line":2856},[187,37721,37292],{},[187,37723,37724],{"class":189,"line":2867},[187,37725,37297],{},[187,37727,37728],{"class":189,"line":2878},[187,37729,37302],{},[187,37731,37732],{"class":189,"line":2886},[187,37733,37734],{},"tempo    = 1000   # In BPM\n",[187,37736,37737],{"class":189,"line":2900},[187,37738,37312],{},[187,37740,37741],{"class":189,"line":2905},[187,37742,316],{"emptyLinePlaceholder":315},[187,37744,37745],{"class":189,"line":2913},[187,37746,37747],{},"MyMIDI = MIDIFile(6)  # One track, defaults to format 1 (tempo track is created\n",[187,37749,37750],{"class":189,"line":2921},[187,37751,37326],{},[187,37753,37754],{"class":189,"line":2931},[187,37755,37331],{},[187,37757,37758],{"class":189,"line":2942},[187,37759,316],{"emptyLinePlaceholder":315},[187,37761,37762],{"class":189,"line":2953},[187,37763,37764],{},"interval = len(bar_group[0].split(\"\\n\")) - 1\n",[187,37766,37767],{"class":189,"line":2964},[187,37768,316],{"emptyLinePlaceholder":315},[187,37770,37771],{"class":189,"line":2975},[187,37772,37773],{},"for b in bar_group:\n",[187,37775,37776],{"class":189,"line":2983},[187,37777,316],{"emptyLinePlaceholder":315},[187,37779,37780],{"class":189,"line":2992},[187,37781,37782],{},"    strings = b.split(\"\\n\")\n",[187,37784,37785],{"class":189,"line":3001},[187,37786,37787],{},"    strings = [x for x in strings if x != '']\n",[187,37789,37790],{"class":189,"line":3010},[187,37791,37792],{},"    e_count = 0\n",[187,37794,37795],{"class":189,"line":3019},[187,37796,37797],{},"    for i,s in enumerate(strings):\n",[187,37799,37800],{"class":189,"line":3028},[187,37801,316],{"emptyLinePlaceholder":315},[187,37803,37804],{"class":189,"line":3033},[187,37805,37806],{},"        current_string = strings[i][0]\n",[187,37808,37809],{"class":189,"line":3041},[187,37810,37811],{},"        if current_string not in guitar_strings.keys():\n",[187,37813,37814],{"class":189,"line":3049},[187,37815,37816],{},"            current_string = string_notes[i]\n",[187,37818,37819],{"class":189,"line":3059},[187,37820,37821],{},"        if current_string == \"E\":\n",[187,37823,37824],{"class":189,"line":3070},[187,37825,37826],{},"            e_count += 1\n",[187,37828,37829],{"class":189,"line":3075},[187,37830,37831],{},"        if e_count == 2:\n",[187,37833,37834],{"class":189,"line":3083},[187,37835,37836],{},"            current_string == \"high_e\"\n",[187,37838,37839],{"class":189,"line":3091},[187,37840,316],{"emptyLinePlaceholder":315},[187,37842,37843],{"class":189,"line":3101},[187,37844,37845],{},"        track = guitar_strings[current_string]['track_num']\n",[187,37847,37848],{"class":189,"line":3111},[187,37849,316],{"emptyLinePlaceholder":315},[187,37851,37852],{"class":189,"line":3122},[187,37853,37854],{},"        s = s[1:]\n",[187,37856,37857],{"class":189,"line":3132},[187,37858,37859],{},"        s = s.replace('|', '')\n",[187,37861,37862],{"class":189,"line":3143},[187,37863,37864],{},"        s = list(s)\n",[187,37866,37867],{"class":189,"line":3151},[187,37868,316],{"emptyLinePlaceholder":315},[187,37870,37871],{"class":189,"line":3161},[187,37872,37873],{},"        for i, pitch in enumerate(s):\n",[187,37875,37876],{"class":189,"line":3170},[187,37877,37878],{},"            volume = 100\n",[187,37880,37881],{"class":189,"line":3178},[187,37882,316],{"emptyLinePlaceholder":315},[187,37884,37885],{"class":189,"line":3185},[187,37886,37887],{},"            if pitch == \"\\n\":\n",[187,37889,37890],{"class":189,"line":3195},[187,37891,37892],{},"                break\n",[187,37894,37895],{"class":189,"line":3205},[187,37896,37897],{},"            if pitch == \"-\":\n",[187,37899,37900],{"class":189,"line":3210},[187,37901,37902],{},"                volume = 0\n",[187,37904,37905],{"class":189,"line":3216},[187,37906,37907],{},"                pitch = 50\n",[187,37909,37910],{"class":189,"line":3224},[187,37911,37912],{},"            print(\"adding note\")\n",[187,37914,37915],{"class":189,"line":3234},[187,37916,37917],{},"            pitch = int(pitch) + guitar_strings[current_string]['note_val']\n",[187,37919,37920],{"class":189,"line":3242},[187,37921,37922],{},"            MyMIDI.addNote(track, channel, pitch, time + i, duration, volume)\n",[187,37924,37925],{"class":189,"line":3252},[187,37926,316],{"emptyLinePlaceholder":315},[187,37928,37929],{"class":189,"line":3260},[187,37930,37931],{},"    time += interval*8\n",[187,37933,37934],{"class":189,"line":3270},[187,37935,316],{"emptyLinePlaceholder":315},[187,37937,37938],{"class":189,"line":3275},[187,37939,37354],{},[187,37941,37942],{"class":189,"line":3283},[187,37943,37359],{},[11,37945,37946],{},"Here's me playing one of my favorite songs, you might recognize it!",[107,37948,37953],{"className":37949,"dataInstgrmCaptioned":35,"dataInstgrmPermalink":37951,"dataInstgrmVersion":10522,"style":37952},[37950],"instagram-media","https://www.instagram.com/p/rg2gmyyFdC/"," background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);",[10229,37954,12272,37956,12272,37962,12272,37970],{"style":37955},"padding:8px;",[10229,37957,12272,37959],{"style":37958}," background:#F8F8F8; line-height:0; margin-top:40px; padding:50% 0; text-align:center; width:100%;",[10229,37960],{"style":37961}," background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;",[11,37963,12272,37965],{"style":37964}," margin:8px 0 0 0; padding:0 4px;",[15,37966,37969],{"href":37951,"style":37967,"target":37968}," color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;","_blank","Written in 1902, this catchy tune from Fransico Tárrega's 'Gran Vals' is now heard by an estimated 22,000 per second worldwide.. it is also a registered sound trademark of a billion dollar Finnish company 🎵 #franciscotárrega #tarrega #tárrega #classicalguitar #spanishguitarist #spain #instamusic #granvals #grandevalse #grandwaltz #crownmolding",[11,37971,37973,37974,37979,37980],{"style":37972}," color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;","A post shared by ",[15,37975,37978],{"href":37976,"style":37977,"target":37968},"https://www.instagram.com/briancaffey/"," color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;"," Brian"," (@briancaffey) on ",[37981,37982,37985],"time",{"style":37983,"dateTime":37984}," font-family:Arial,sans-serif; font-size:14px; line-height:17px;","2014-08-10T09:48:59+00:00","Aug 10, 2014 at 2:48am PDT",[6727,37987],{"async":315,"defer":315,"src":37988},"//www.instagram.com/embed.js",[855,37990,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":37992},[37993,37997],{"id":37088,"depth":249,"text":37089,"children":37994},[37995,37996],{"id":37095,"depth":312,"text":37096},{"id":37211,"depth":312,"text":37212},{"id":37234,"depth":249,"text":37235},"2018-04-26","Recently I have been playing lots of classical guitar. I use classtab.org, a website with hundreds of classical guitar pieces in tab form. Here's an example of a guitar tab:","/static/guitar.jpg",{"layout":29014},"/2018/04/26/generating-music-from-guitar-tabs-with-python",{"title":37051,"description":37999},"2018/04/26/generating-music-from-guitar-tabs-with-python",[38006,1125,37088,38007,38008],"midi","guitar","music","UcCGQ2K8ITO1wARcqTQUwaLjqJD-XXTo07mf_BWk3do",{"id":38011,"title":38012,"body":38013,"comments":315,"date":41602,"description":35,"draft":872,"extension":873,"external":874,"image":41603,"meta":41604,"navigation":315,"path":41606,"seo":41607,"stem":41608,"tags":41609,"__hash__":41615},"blog/2018/02/19/leaflet-maps-with-django.md","Display, filter and export geographical data in a Django app with Leaflet, Mapbox, DataTables, Bootstrap 4 and Travis-CI",{"type":8,"value":38014,"toc":41590},[38015,38020,38028,38036,38041,38048,38061,38064,38118,38121,38124,38128,38131,38137,38167,38170,38174,38180,38186,38189,38314,38321,38330,38336,38389,38392,38395,38397,39047,39049,39053,39060,39063,39143,39149,39151,39307,39309,39322,39333,39342,39351,39355,39362,39381,39391,39401,39431,39434,39441,39443,39500,39502,39509,39513,39529,39532,39687,39701,39719,39883,39889,39940,39944,39947,39966,40055,40061,40063,40568,40570,40588,40598,40604,40614,40617,40662,40665,40679,40683,40686,40691,40696,40699,40795,40798,40803,40810,40843,40849,40963,41015,41018,41022,41025,41035,41149,41152,41167,41177,41181,41184,41197,41202,41432,41437,41568,41571,41577,41580,41587],[11,38016,38017],{},[511,38018],{"alt":7255,"src":38019},"/static/map_entire.png",[911,38021,38023],{"id":38022},"live-demo-on-digitalocean",[15,38024,38027],{"href":38025,"rel":38026},"http://159.89.235.193/books/",[19],"Live Demo on DigitalOcean",[11,38029,38030,38031,358],{},"This post is a review of my first attempt at using geographical data in a Django project. I have been interested in working with map APIs, and I once looked into the Google Maps API. For this project I chose to use ",[15,38032,38035],{"href":38033,"rel":38034},"http://leafletjs.com/",[19],"Leaflet",[107,38037,38038],{},[11,38039,38040],{},"an open-source JavaScript library for mobile-friendly interactive maps",[11,38042,38043,38044,752],{},"Getting started with Leaflet is easy. All you need to do is request a public Mapbox API key which is free (with no credit card required). You can get a key from ",[15,38045,38046],{"href":38046,"rel":38047},"https://www.mapbox.com/account/access-tokens/",[19],[11,38049,38050,38051,38056,38057,38060],{},"Then you will follow steps on the ",[15,38052,38055],{"href":38053,"rel":38054},"http://leafletjs.com/examples/quick-start/",[19],"quickstart guide"," and replace ",[33,38058,38059],{},"your.mapbox.access.token"," with your Mapbox API key.",[11,38062,38063],{},"{% raw %}",[26,38065,38067],{"className":6362,"code":38066,"language":6364,"meta":35,"style":35},"L.tileLayer(\n  'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}',\n  {\n    attribution:\n      'Map data &copy; \u003Ca href=\"http://openstreetmap.org\">OpenStreetMap\u003C/a> contributors, \u003Ca href=\"http://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA\u003C/a>, Imagery © \u003Ca href=\"http://mapbox.com\">Mapbox\u003C/a>',\n    maxZoom: 18,\n    id: 'mapbox.streets',\n    accessToken: 'your.mapbox.access.token',\n  }\n).addTo(mymap)\n",[33,38068,38069,38074,38079,38084,38089,38094,38099,38104,38109,38113],{"__ignoreMap":35},[187,38070,38071],{"class":189,"line":190},[187,38072,38073],{},"L.tileLayer(\n",[187,38075,38076],{"class":189,"line":249},[187,38077,38078],{},"  'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}',\n",[187,38080,38081],{"class":189,"line":312},[187,38082,38083],{},"  {\n",[187,38085,38086],{"class":189,"line":319},[187,38087,38088],{},"    attribution:\n",[187,38090,38091],{"class":189,"line":325},[187,38092,38093],{},"      'Map data &copy; \u003Ca href=\"http://openstreetmap.org\">OpenStreetMap\u003C/a> contributors, \u003Ca href=\"http://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA\u003C/a>, Imagery © \u003Ca href=\"http://mapbox.com\">Mapbox\u003C/a>',\n",[187,38095,38096],{"class":189,"line":686},[187,38097,38098],{},"    maxZoom: 18,\n",[187,38100,38101],{"class":189,"line":697},[187,38102,38103],{},"    id: 'mapbox.streets',\n",[187,38105,38106],{"class":189,"line":1291},[187,38107,38108],{},"    accessToken: 'your.mapbox.access.token',\n",[187,38110,38111],{"class":189,"line":1306},[187,38112,6847],{},[187,38114,38115],{"class":189,"line":1434},[187,38116,38117],{},").addTo(mymap)\n",[11,38119,38120],{},"{% endraw %}",[11,38122,38123],{},"I'll come back to using Leaflet later on, and also show how to use Leaflet plugins that can be used to add functionality such as aggregating markers on a map.",[168,38125,38127],{"id":38126},"django-model","Django model",[11,38129,38130],{},"Next, let's look at the Django setup. For now I'm simply working with a local project and SQLite3 backend.",[11,38132,38133,38134,358],{},"To store latitude and longitude data, we can use ",[33,38135,38136],{},"DecimalField",[26,38138,38140],{"className":1383,"code":38139,"language":1125,"meta":35,"style":35},"class Book(models.Model):\n    title = models.CharField(max_length=300)\n    lat = models.DecimalField(max_digits=9, decimal_places=6)\n    lon = models.DecimalField(max_digits=9, decimal_places=6)\n    [...]\n",[33,38141,38142,38147,38152,38157,38162],{"__ignoreMap":35},[187,38143,38144],{"class":189,"line":190},[187,38145,38146],{},"class Book(models.Model):\n",[187,38148,38149],{"class":189,"line":249},[187,38150,38151],{},"    title = models.CharField(max_length=300)\n",[187,38153,38154],{"class":189,"line":312},[187,38155,38156],{},"    lat = models.DecimalField(max_digits=9, decimal_places=6)\n",[187,38158,38159],{"class":189,"line":319},[187,38160,38161],{},"    lon = models.DecimalField(max_digits=9, decimal_places=6)\n",[187,38163,38164],{"class":189,"line":325},[187,38165,38166],{},"    [...]\n",[11,38168,38169],{},"This allows us to store numbers to six decimal places. The difference between 0 and 0.00001 is about 3 feet, so this gives us plenty of accuracy for our geographical coordinate data.",[168,38171,38173],{"id":38172},"plotting-books-on-a-map","Plotting books on a map",[11,38175,38176,38177,38179],{},"Plotting our books on a map is fairly straightforward, but there were a few obstacles to work around. Initially I setup an additional endpoint that returned an AJAX response with the longitude, latitude, title and link for each book. The ",[33,38178,28800],{}," function in the AJAX call then passed the returned JSON into a function that populated the book data in the map.",[11,38181,35788,38182,38185],{},[33,38183,38184],{},"books"," page displays book data in two places: on the map and on the DataTable below the map. Since I was already making one database query for DataTables, I decided to reuse this data for the map and not use a separate request and database query. There are a few helpful idioms in Python and JavaScript to accomplish this.",[11,38187,38188],{},"First, let's look at the request with some added comments:",[26,38190,38192],{"className":1383,"code":38191,"language":1125,"meta":35,"style":35},"def all_books(request):\n    \"\"\"\n    Main view for books. request.GET parameters are used to filter books\n    \"\"\"\n    books = Book.objects.all()\n    form = QueryForm(request.GET or None)\n    paramDict = request.GET\n\n    books = filter_books(books, paramDict)\n\n    page_count = books.aggregate(Sum('pages'))\n\n    # This takes the first book query an reformats the data so it can be read\n    # by the map script on the frontend.\n    map_books = [{'loc':[float(book.lon), float(book.lat)],\n                  'title':book.title,\n                  'url':book.get_absolute_url()} for book in books]\n    context = {\n        'books':books,\n        # Here, we apply `json.dumps`, `escapejs` and `marksafe` for security\n        # and proper formatting\n        'map_books': mark_safe(escapejs(json.dumps(map_books))),\n        'page_count':page_count['pages__sum'],\n        'form':form}\n    return render(request, 'books/books.html', context)\n\n",[33,38193,38194,38199,38203,38208,38212,38217,38222,38227,38231,38236,38240,38245,38249,38254,38259,38264,38269,38274,38279,38284,38289,38294,38299,38304,38309],{"__ignoreMap":35},[187,38195,38196],{"class":189,"line":190},[187,38197,38198],{},"def all_books(request):\n",[187,38200,38201],{"class":189,"line":249},[187,38202,4793],{},[187,38204,38205],{"class":189,"line":312},[187,38206,38207],{},"    Main view for books. request.GET parameters are used to filter books\n",[187,38209,38210],{"class":189,"line":319},[187,38211,4793],{},[187,38213,38214],{"class":189,"line":325},[187,38215,38216],{},"    books = Book.objects.all()\n",[187,38218,38219],{"class":189,"line":686},[187,38220,38221],{},"    form = QueryForm(request.GET or None)\n",[187,38223,38224],{"class":189,"line":697},[187,38225,38226],{},"    paramDict = request.GET\n",[187,38228,38229],{"class":189,"line":1291},[187,38230,316],{"emptyLinePlaceholder":315},[187,38232,38233],{"class":189,"line":1306},[187,38234,38235],{},"    books = filter_books(books, paramDict)\n",[187,38237,38238],{"class":189,"line":1434},[187,38239,316],{"emptyLinePlaceholder":315},[187,38241,38242],{"class":189,"line":2599},[187,38243,38244],{},"    page_count = books.aggregate(Sum('pages'))\n",[187,38246,38247],{"class":189,"line":2607},[187,38248,316],{"emptyLinePlaceholder":315},[187,38250,38251],{"class":189,"line":2621},[187,38252,38253],{},"    # This takes the first book query an reformats the data so it can be read\n",[187,38255,38256],{"class":189,"line":2631},[187,38257,38258],{},"    # by the map script on the frontend.\n",[187,38260,38261],{"class":189,"line":2642},[187,38262,38263],{},"    map_books = [{'loc':[float(book.lon), float(book.lat)],\n",[187,38265,38266],{"class":189,"line":2653},[187,38267,38268],{},"                  'title':book.title,\n",[187,38270,38271],{"class":189,"line":2665},[187,38272,38273],{},"                  'url':book.get_absolute_url()} for book in books]\n",[187,38275,38276],{"class":189,"line":2674},[187,38277,38278],{},"    context = {\n",[187,38280,38281],{"class":189,"line":2684},[187,38282,38283],{},"        'books':books,\n",[187,38285,38286],{"class":189,"line":2694},[187,38287,38288],{},"        # Here, we apply `json.dumps`, `escapejs` and `marksafe` for security\n",[187,38290,38291],{"class":189,"line":2706},[187,38292,38293],{},"        # and proper formatting\n",[187,38295,38296],{"class":189,"line":2715},[187,38297,38298],{},"        'map_books': mark_safe(escapejs(json.dumps(map_books))),\n",[187,38300,38301],{"class":189,"line":2725},[187,38302,38303],{},"        'page_count':page_count['pages__sum'],\n",[187,38305,38306],{"class":189,"line":2735},[187,38307,38308],{},"        'form':form}\n",[187,38310,38311],{"class":189,"line":2743},[187,38312,38313],{},"    return render(request, 'books/books.html', context)\n",[11,38315,38316,38317,38320],{},"On the frontend, I passed the ",[33,38318,38319],{},"map_books"," variable to the template like this:",[26,38322,38324],{"className":6362,"code":38323,"language":6364,"meta":35,"style":35},"var map_books = JSON.parse('{{ map_books }}')\n",[33,38325,38326],{"__ignoreMap":35},[187,38327,38328],{"class":189,"line":190},[187,38329,38323],{},[11,38331,38332,38333,38335],{},"Now that I have ",[33,38334,38319],{}," as a JavaScript object, I can simply pass it into the map function that populates data in the Leaflet map. Here's the function that does that:",[26,38337,38339],{"className":6362,"code":38338,"language":6364,"meta":35,"style":35},"function populateMap(data) {\n  for (i in data) {\n    var title = data[i].title, //value searched\n      loc = data[i].loc, //position found\n      url = data[i].url,\n      marker = new L.Marker(new L.latLng(loc), { title: title, icon: bookIcon }) //se property searched\n    marker.bindPopup('\u003Cp>\u003Ca href=\"' + url + '\">' + title + '\u003C/a>\u003C/p>')\n    markersLayer.addLayer(marker)\n  }\n}\n",[33,38340,38341,38346,38351,38356,38361,38366,38371,38376,38381,38385],{"__ignoreMap":35},[187,38342,38343],{"class":189,"line":190},[187,38344,38345],{},"function populateMap(data) {\n",[187,38347,38348],{"class":189,"line":249},[187,38349,38350],{},"  for (i in data) {\n",[187,38352,38353],{"class":189,"line":312},[187,38354,38355],{},"    var title = data[i].title, //value searched\n",[187,38357,38358],{"class":189,"line":319},[187,38359,38360],{},"      loc = data[i].loc, //position found\n",[187,38362,38363],{"class":189,"line":325},[187,38364,38365],{},"      url = data[i].url,\n",[187,38367,38368],{"class":189,"line":686},[187,38369,38370],{},"      marker = new L.Marker(new L.latLng(loc), { title: title, icon: bookIcon }) //se property searched\n",[187,38372,38373],{"class":189,"line":697},[187,38374,38375],{},"    marker.bindPopup('\u003Cp>\u003Ca href=\"' + url + '\">' + title + '\u003C/a>\u003C/p>')\n",[187,38377,38378],{"class":189,"line":1291},[187,38379,38380],{},"    markersLayer.addLayer(marker)\n",[187,38382,38383],{"class":189,"line":1306},[187,38384,6847],{},[187,38386,38387],{"class":189,"line":1434},[187,38388,1309],{},[11,38390,38391],{},"This adds our markers to the map, and also passes link and title data to the popup box when a marker is clicked.",[11,38393,38394],{},"Here's the entire script that is used to populate map data:",[11,38396,38063],{},[26,38398,38400],{"className":6715,"code":38399,"language":6717,"meta":35,"style":35},"\u003Cscript>\n  var mymap = new L.Map('mapid', { zoom: 9, center: new L.latLng([40, 13]) }) //set center from first location\n\n  L.tileLayer(\n    'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}',\n    {\n      attribution:\n        'Map data &copy; \u003Ca href=\"http://openstreetmap.org\">OpenStreetMap\u003C/a> contributors, \u003Ca href=\"http://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA\u003C/a>, Imagery © \u003Ca href=\"http://mapbox.com\">Mapbox\u003C/a>',\n      maxZoom: 18,\n      id: 'mapbox.streets',\n      accessToken:\n        'pk.eyJ1IjoiYnJpYW5jYWZmZXkiLCJhIjoiY2pkczJycjl5MmhqbTMzbzQ3bHJuaHA3aiJ9.cvRYYPNjQJVpFjdUcmIHzA',\n    }\n  ).addTo(mymap)\n\n  var bookIcon = L.icon({\n    iconUrl: '/static/images/marker-icon.png',\n    iconSize: [35, 35], // size of the icon\n    iconAnchor: [17, 35], // point of the icon which will correspond to marker's location\n    popupAnchor: [0, -30], // point from which the popup should open relative to the iconAnchor\n  })\n\n  var markersLayer = L.markerClusterGroup()\n\n  mymap.addLayer(markersLayer)\n\n  var controlSearch = new L.Control.Search({\n    position: 'topright',\n    layer: markersLayer,\n    initial: false,\n    zoom: 12,\n    marker: false,\n  })\n\n  mymap.addControl(controlSearch)\n\n  function populateMap(data) {\n    for (i in data) {\n      var title = data[i].title, //value searched\n        loc = data[i].loc, //position found\n        url = data[i].url,\n        marker = new L.Marker(new L.latLng(loc), {\n          title: title,\n          icon: bookIcon,\n        }) //se property searched\n      marker.bindPopup('\u003Cp>\u003Ca href=\"' + url + '\">' + title + '\u003C/a>\u003C/p>')\n      markersLayer.addLayer(marker)\n    }\n  }\n\n  // use context variable instead of making AJAX call\n  var map_books = JSON.parse('{{ map_books }}')\n  var new_lat = map_books[0].loc[0]\n  var new_lon = map_books[0].loc[1]\n  mymap.setView([0, 0], 2)\n  populateMap(map_books)\n\u003C/script>\n",[33,38401,38402,38410,38469,38473,38486,38493,38497,38502,38509,38519,38528,38533,38540,38544,38555,38559,38577,38587,38605,38622,38641,38646,38650,38668,38672,38683,38687,38708,38718,38723,38732,38742,38751,38755,38759,38769,38773,38787,38800,38816,38829,38839,38868,38873,38878,38886,38921,38931,38935,38939,38943,38948,38970,38991,39010,39031,39039],{"__ignoreMap":35},[187,38403,38404,38406,38408],{"class":189,"line":190},[187,38405,6724],{"class":577},[187,38407,6727],{"class":2516},[187,38409,6730],{"class":577},[187,38411,38412,38415,38418,38420,38422,38425,38427,38430,38432,38435,38438,38440,38443,38445,38447,38449,38452,38455,38458,38460,38463,38466],{"class":189,"line":249},[187,38413,38414],{"class":573},"  var",[187,38416,38417],{"class":577}," mymap ",[187,38419,595],{"class":573},[187,38421,12437],{"class":573},[187,38423,38424],{"class":588}," L",[187,38426,752],{"class":577},[187,38428,38429],{"class":193},"Map",[187,38431,615],{"class":577},[187,38433,38434],{"class":196},"'mapid'",[187,38436,38437],{"class":577},", { zoom: ",[187,38439,668],{"class":588},[187,38441,38442],{"class":577},", center: ",[187,38444,19392],{"class":573},[187,38446,38424],{"class":588},[187,38448,752],{"class":577},[187,38450,38451],{"class":193},"latLng",[187,38453,38454],{"class":577},"([",[187,38456,38457],{"class":588},"40",[187,38459,637],{"class":577},[187,38461,38462],{"class":588},"13",[187,38464,38465],{"class":577},"]) }) ",[187,38467,38468],{"class":295},"//set center from first location\n",[187,38470,38471],{"class":189,"line":312},[187,38472,316],{"emptyLinePlaceholder":315},[187,38474,38475,38478,38480,38483],{"class":189,"line":319},[187,38476,38477],{"class":588},"  L",[187,38479,752],{"class":577},[187,38481,38482],{"class":193},"tileLayer",[187,38484,38485],{"class":577},"(\n",[187,38487,38488,38491],{"class":189,"line":325},[187,38489,38490],{"class":196},"    'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token={accessToken}'",[187,38492,1228],{"class":577},[187,38494,38495],{"class":189,"line":686},[187,38496,24901],{"class":577},[187,38498,38499],{"class":189,"line":697},[187,38500,38501],{"class":577},"      attribution:\n",[187,38503,38504,38507],{"class":189,"line":1291},[187,38505,38506],{"class":196},"        'Map data &copy; \u003Ca href=\"http://openstreetmap.org\">OpenStreetMap\u003C/a> contributors, \u003Ca href=\"http://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA\u003C/a>, Imagery © \u003Ca href=\"http://mapbox.com\">Mapbox\u003C/a>'",[187,38508,1228],{"class":577},[187,38510,38511,38514,38517],{"class":189,"line":1306},[187,38512,38513],{"class":577},"      maxZoom: ",[187,38515,38516],{"class":588},"18",[187,38518,1228],{"class":577},[187,38520,38521,38523,38526],{"class":189,"line":1434},[187,38522,18972],{"class":577},[187,38524,38525],{"class":196},"'mapbox.streets'",[187,38527,1228],{"class":577},[187,38529,38530],{"class":189,"line":2599},[187,38531,38532],{"class":577},"      accessToken:\n",[187,38534,38535,38538],{"class":189,"line":2607},[187,38536,38537],{"class":196},"        'pk.eyJ1IjoiYnJpYW5jYWZmZXkiLCJhIjoiY2pkczJycjl5MmhqbTMzbzQ3bHJuaHA3aiJ9.cvRYYPNjQJVpFjdUcmIHzA'",[187,38539,1228],{"class":577},[187,38541,38542],{"class":189,"line":2621},[187,38543,9799],{"class":577},[187,38545,38546,38549,38552],{"class":189,"line":2631},[187,38547,38548],{"class":577},"  ).",[187,38550,38551],{"class":193},"addTo",[187,38553,38554],{"class":577},"(mymap)\n",[187,38556,38557],{"class":189,"line":2642},[187,38558,316],{"emptyLinePlaceholder":315},[187,38560,38561,38563,38566,38568,38570,38572,38575],{"class":189,"line":2653},[187,38562,38414],{"class":573},[187,38564,38565],{"class":577}," bookIcon ",[187,38567,595],{"class":573},[187,38569,38424],{"class":588},[187,38571,752],{"class":577},[187,38573,38574],{"class":193},"icon",[187,38576,6380],{"class":577},[187,38578,38579,38582,38585],{"class":189,"line":2665},[187,38580,38581],{"class":577},"    iconUrl: ",[187,38583,38584],{"class":196},"'/static/images/marker-icon.png'",[187,38586,1228],{"class":577},[187,38588,38589,38592,38595,38597,38599,38602],{"class":189,"line":2674},[187,38590,38591],{"class":577},"    iconSize: [",[187,38593,38594],{"class":588},"35",[187,38596,637],{"class":577},[187,38598,38594],{"class":588},[187,38600,38601],{"class":577},"], ",[187,38603,38604],{"class":295},"// size of the icon\n",[187,38606,38607,38610,38613,38615,38617,38619],{"class":189,"line":2684},[187,38608,38609],{"class":577},"    iconAnchor: [",[187,38611,38612],{"class":588},"17",[187,38614,637],{"class":577},[187,38616,38594],{"class":588},[187,38618,38601],{"class":577},[187,38620,38621],{"class":295},"// point of the icon which will correspond to marker's location\n",[187,38623,38624,38627,38629,38631,38633,38636,38638],{"class":189,"line":2694},[187,38625,38626],{"class":577},"    popupAnchor: [",[187,38628,10165],{"class":588},[187,38630,637],{"class":577},[187,38632,677],{"class":573},[187,38634,38635],{"class":588},"30",[187,38637,38601],{"class":577},[187,38639,38640],{"class":295},"// point from which the popup should open relative to the iconAnchor\n",[187,38642,38643],{"class":189,"line":2706},[187,38644,38645],{"class":577},"  })\n",[187,38647,38648],{"class":189,"line":2715},[187,38649,316],{"emptyLinePlaceholder":315},[187,38651,38652,38654,38657,38659,38661,38663,38666],{"class":189,"line":2725},[187,38653,38414],{"class":573},[187,38655,38656],{"class":577}," markersLayer ",[187,38658,595],{"class":573},[187,38660,38424],{"class":588},[187,38662,752],{"class":577},[187,38664,38665],{"class":193},"markerClusterGroup",[187,38667,694],{"class":577},[187,38669,38670],{"class":189,"line":2735},[187,38671,316],{"emptyLinePlaceholder":315},[187,38673,38674,38677,38680],{"class":189,"line":2743},[187,38675,38676],{"class":577},"  mymap.",[187,38678,38679],{"class":193},"addLayer",[187,38681,38682],{"class":577},"(markersLayer)\n",[187,38684,38685],{"class":189,"line":2754},[187,38686,316],{"emptyLinePlaceholder":315},[187,38688,38689,38691,38694,38696,38698,38700,38703,38706],{"class":189,"line":2762},[187,38690,38414],{"class":573},[187,38692,38693],{"class":577}," controlSearch ",[187,38695,595],{"class":573},[187,38697,12437],{"class":573},[187,38699,38424],{"class":588},[187,38701,38702],{"class":577},".Control.",[187,38704,38705],{"class":193},"Search",[187,38707,6380],{"class":577},[187,38709,38710,38713,38716],{"class":189,"line":2770},[187,38711,38712],{"class":577},"    position: ",[187,38714,38715],{"class":196},"'topright'",[187,38717,1228],{"class":577},[187,38719,38720],{"class":189,"line":2781},[187,38721,38722],{"class":577},"    layer: markersLayer,\n",[187,38724,38725,38728,38730],{"class":189,"line":2792},[187,38726,38727],{"class":577},"    initial: ",[187,38729,12660],{"class":588},[187,38731,1228],{"class":577},[187,38733,38734,38737,38740],{"class":189,"line":2803},[187,38735,38736],{"class":577},"    zoom: ",[187,38738,38739],{"class":588},"12",[187,38741,1228],{"class":577},[187,38743,38744,38747,38749],{"class":189,"line":2808},[187,38745,38746],{"class":577},"    marker: ",[187,38748,12660],{"class":588},[187,38750,1228],{"class":577},[187,38752,38753],{"class":189,"line":2816},[187,38754,38645],{"class":577},[187,38756,38757],{"class":189,"line":2824},[187,38758,316],{"emptyLinePlaceholder":315},[187,38760,38761,38763,38766],{"class":189,"line":2834},[187,38762,38676],{"class":577},[187,38764,38765],{"class":193},"addControl",[187,38767,38768],{"class":577},"(controlSearch)\n",[187,38770,38771],{"class":189,"line":2845},[187,38772,316],{"emptyLinePlaceholder":315},[187,38774,38775,38778,38781,38783,38785],{"class":189,"line":2856},[187,38776,38777],{"class":573},"  function",[187,38779,38780],{"class":193}," populateMap",[187,38782,615],{"class":577},[187,38784,582],{"class":581},[187,38786,19272],{"class":577},[187,38788,38789,38792,38795,38797],{"class":189,"line":2867},[187,38790,38791],{"class":573},"    for",[187,38793,38794],{"class":577}," (i ",[187,38796,16549],{"class":573},[187,38798,38799],{"class":577}," data) {\n",[187,38801,38802,38805,38808,38810,38813],{"class":189,"line":2878},[187,38803,38804],{"class":573},"      var",[187,38806,38807],{"class":577}," title ",[187,38809,595],{"class":573},[187,38811,38812],{"class":577}," data[i].title, ",[187,38814,38815],{"class":295},"//value searched\n",[187,38817,38818,38821,38823,38826],{"class":189,"line":2886},[187,38819,38820],{"class":577},"        loc ",[187,38822,595],{"class":573},[187,38824,38825],{"class":577}," data[i].loc, ",[187,38827,38828],{"class":295},"//position found\n",[187,38830,38831,38834,38836],{"class":189,"line":2900},[187,38832,38833],{"class":577},"        url ",[187,38835,595],{"class":573},[187,38837,38838],{"class":577}," data[i].url,\n",[187,38840,38841,38844,38846,38848,38850,38852,38855,38857,38859,38861,38863,38865],{"class":189,"line":2905},[187,38842,38843],{"class":577},"        marker ",[187,38845,595],{"class":573},[187,38847,12437],{"class":573},[187,38849,38424],{"class":588},[187,38851,752],{"class":577},[187,38853,38854],{"class":193},"Marker",[187,38856,615],{"class":577},[187,38858,19392],{"class":573},[187,38860,38424],{"class":588},[187,38862,752],{"class":577},[187,38864,38451],{"class":193},[187,38866,38867],{"class":577},"(loc), {\n",[187,38869,38870],{"class":189,"line":2913},[187,38871,38872],{"class":577},"          title: title,\n",[187,38874,38875],{"class":189,"line":2921},[187,38876,38877],{"class":577},"          icon: bookIcon,\n",[187,38879,38880,38883],{"class":189,"line":2931},[187,38881,38882],{"class":577},"        }) ",[187,38884,38885],{"class":295},"//se property searched\n",[187,38887,38888,38891,38894,38896,38899,38902,38905,38907,38910,38912,38914,38916,38919],{"class":189,"line":2942},[187,38889,38890],{"class":577},"      marker.",[187,38892,38893],{"class":193},"bindPopup",[187,38895,615],{"class":577},[187,38897,38898],{"class":196},"'\u003Cp>\u003Ca href=\"'",[187,38900,38901],{"class":573}," +",[187,38903,38904],{"class":577}," url ",[187,38906,9585],{"class":573},[187,38908,38909],{"class":196}," '\">'",[187,38911,38901],{"class":573},[187,38913,38807],{"class":577},[187,38915,9585],{"class":573},[187,38917,38918],{"class":196}," '\u003C/a>\u003C/p>'",[187,38920,621],{"class":577},[187,38922,38923,38926,38928],{"class":189,"line":2953},[187,38924,38925],{"class":577},"      markersLayer.",[187,38927,38679],{"class":193},[187,38929,38930],{"class":577},"(marker)\n",[187,38932,38933],{"class":189,"line":2964},[187,38934,9799],{"class":577},[187,38936,38937],{"class":189,"line":2975},[187,38938,6847],{"class":577},[187,38940,38941],{"class":189,"line":2983},[187,38942,316],{"emptyLinePlaceholder":315},[187,38944,38945],{"class":189,"line":2992},[187,38946,38947],{"class":295},"  // use context variable instead of making AJAX call\n",[187,38949,38950,38952,38955,38957,38959,38961,38963,38965,38968],{"class":189,"line":3001},[187,38951,38414],{"class":573},[187,38953,38954],{"class":577}," map_books ",[187,38956,595],{"class":573},[187,38958,14471],{"class":588},[187,38960,752],{"class":577},[187,38962,14476],{"class":193},[187,38964,615],{"class":577},[187,38966,38967],{"class":196},"'{{ map_books }}'",[187,38969,621],{"class":577},[187,38971,38972,38974,38977,38979,38982,38984,38987,38989],{"class":189,"line":3010},[187,38973,38414],{"class":573},[187,38975,38976],{"class":577}," new_lat ",[187,38978,595],{"class":573},[187,38980,38981],{"class":577}," map_books[",[187,38983,10165],{"class":588},[187,38985,38986],{"class":577},"].loc[",[187,38988,10165],{"class":588},[187,38990,1437],{"class":577},[187,38992,38993,38995,38998,39000,39002,39004,39006,39008],{"class":189,"line":3019},[187,38994,38414],{"class":573},[187,38996,38997],{"class":577}," new_lon ",[187,38999,595],{"class":573},[187,39001,38981],{"class":577},[187,39003,10165],{"class":588},[187,39005,38986],{"class":577},[187,39007,15625],{"class":588},[187,39009,1437],{"class":577},[187,39011,39012,39014,39017,39019,39021,39023,39025,39027,39029],{"class":189,"line":3028},[187,39013,38676],{"class":577},[187,39015,39016],{"class":193},"setView",[187,39018,38454],{"class":577},[187,39020,10165],{"class":588},[187,39022,637],{"class":577},[187,39024,10165],{"class":588},[187,39026,38601],{"class":577},[187,39028,22129],{"class":588},[187,39030,621],{"class":577},[187,39032,39033,39036],{"class":189,"line":3033},[187,39034,39035],{"class":193},"  populateMap",[187,39037,39038],{"class":577},"(map_books)\n",[187,39040,39041,39043,39045],{"class":189,"line":3041},[187,39042,6856],{"class":577},[187,39044,6727],{"class":2516},[187,39046,6730],{"class":577},[11,39048,38120],{},[168,39050,39052],{"id":39051},"login-redirect","Login Redirect",[11,39054,39055,39056,39059],{},"I wanted to include a small note on a simple issue that I previously had trouble with, which is using the ",[33,39057,39058],{},"next"," parameter to do a redirect to a page that an unauthenticated user will be redirected to after they successfully login.",[11,39061,39062],{},"Here's the login view:",[26,39064,39066],{"className":1383,"code":39065,"language":1125,"meta":35,"style":35},"def login_view(request):\n    next_redirect = request.GET.get('next')\n    form = UserLoginForm(request.POST or None)\n    if form.is_valid():\n        next_redirect = request.POST.get('next')\n        username = form.cleaned_data.get('username')\n        password = form.cleaned_data.get('password')\n        user = authenticate(username=username, password=password)\n        login(request, user)\n        print(next_redirect)\n        if next_redirect != 'None':\n            return redirect(next_redirect)\n        return redirect('books:all')\n    context = { 'form':form, 'next':next_redirect }\n    return render(request, 'accounts/login_form.html', context)\n",[33,39067,39068,39073,39078,39083,39088,39093,39098,39103,39108,39113,39118,39123,39128,39133,39138],{"__ignoreMap":35},[187,39069,39070],{"class":189,"line":190},[187,39071,39072],{},"def login_view(request):\n",[187,39074,39075],{"class":189,"line":249},[187,39076,39077],{},"    next_redirect = request.GET.get('next')\n",[187,39079,39080],{"class":189,"line":312},[187,39081,39082],{},"    form = UserLoginForm(request.POST or None)\n",[187,39084,39085],{"class":189,"line":319},[187,39086,39087],{},"    if form.is_valid():\n",[187,39089,39090],{"class":189,"line":325},[187,39091,39092],{},"        next_redirect = request.POST.get('next')\n",[187,39094,39095],{"class":189,"line":686},[187,39096,39097],{},"        username = form.cleaned_data.get('username')\n",[187,39099,39100],{"class":189,"line":697},[187,39101,39102],{},"        password = form.cleaned_data.get('password')\n",[187,39104,39105],{"class":189,"line":1291},[187,39106,39107],{},"        user = authenticate(username=username, password=password)\n",[187,39109,39110],{"class":189,"line":1306},[187,39111,39112],{},"        login(request, user)\n",[187,39114,39115],{"class":189,"line":1434},[187,39116,39117],{},"        print(next_redirect)\n",[187,39119,39120],{"class":189,"line":2599},[187,39121,39122],{},"        if next_redirect != 'None':\n",[187,39124,39125],{"class":189,"line":2607},[187,39126,39127],{},"            return redirect(next_redirect)\n",[187,39129,39130],{"class":189,"line":2621},[187,39131,39132],{},"        return redirect('books:all')\n",[187,39134,39135],{"class":189,"line":2631},[187,39136,39137],{},"    context = { 'form':form, 'next':next_redirect }\n",[187,39139,39140],{"class":189,"line":2642},[187,39141,39142],{},"    return render(request, 'accounts/login_form.html', context)\n",[11,39144,39145,39146,39148],{},"Notice that I pass ",[33,39147,39058],{}," as a context variable. I use this in the login form here:",[11,39150,38063],{},[26,39152,39154],{"className":6715,"code":39153,"language":6717,"meta":35,"style":35},"\u003Cform method=\"POST\" action=\".\">\n  {% csrf_token %} {{ form | crispy }}\n  \u003Cinput type=\"hidden\" value=\"{{ next }}\" name=\"next\" />\n  \u003Cdiv class=\"login-center\">\n    \u003Cinput class=\"btn btn-success login-center\" type=\"submit\" value=\"Login\" />\n  \u003C/div>\n  \u003Cbr />\n  or \u003Ca href=\"{% url 'accounts:register' %}\">create an account\u003C/a>\n\u003C/form>\n",[33,39155,39156,39180,39185,39217,39232,39261,39269,39277,39299],{"__ignoreMap":35},[187,39157,39158,39160,39162,39165,39167,39170,39173,39175,39178],{"class":189,"line":190},[187,39159,6724],{"class":577},[187,39161,29679],{"class":2516},[187,39163,39164],{"class":193}," method",[187,39166,595],{"class":577},[187,39168,39169],{"class":196},"\"POST\"",[187,39171,39172],{"class":193}," action",[187,39174,595],{"class":577},[187,39176,39177],{"class":196},"\".\"",[187,39179,6730],{"class":577},[187,39181,39182],{"class":189,"line":249},[187,39183,39184],{"class":577},"  {% csrf_token %} {{ form | crispy }}\n",[187,39186,39187,39189,39191,39194,39196,39199,39202,39204,39207,39210,39212,39215],{"class":189,"line":312},[187,39188,19179],{"class":577},[187,39190,15028],{"class":2516},[187,39192,39193],{"class":193}," type",[187,39195,595],{"class":577},[187,39197,39198],{"class":196},"\"hidden\"",[187,39200,39201],{"class":193}," value",[187,39203,595],{"class":577},[187,39205,39206],{"class":196},"\"{{ next }}\"",[187,39208,39209],{"class":193}," name",[187,39211,595],{"class":577},[187,39213,39214],{"class":196},"\"next\"",[187,39216,19871],{"class":577},[187,39218,39219,39221,39223,39225,39227,39230],{"class":189,"line":319},[187,39220,19179],{"class":577},[187,39222,10229],{"class":2516},[187,39224,29292],{"class":193},[187,39226,595],{"class":577},[187,39228,39229],{"class":196},"\"login-center\"",[187,39231,6730],{"class":577},[187,39233,39234,39236,39238,39240,39242,39245,39247,39249,39252,39254,39256,39259],{"class":189,"line":325},[187,39235,19865],{"class":577},[187,39237,15028],{"class":2516},[187,39239,29292],{"class":193},[187,39241,595],{"class":577},[187,39243,39244],{"class":196},"\"btn btn-success login-center\"",[187,39246,39193],{"class":193},[187,39248,595],{"class":577},[187,39250,39251],{"class":196},"\"submit\"",[187,39253,39201],{"class":193},[187,39255,595],{"class":577},[187,39257,39258],{"class":196},"\"Login\"",[187,39260,19871],{"class":577},[187,39262,39263,39265,39267],{"class":189,"line":686},[187,39264,19938],{"class":577},[187,39266,10229],{"class":2516},[187,39268,6730],{"class":577},[187,39270,39271,39273,39275],{"class":189,"line":697},[187,39272,19179],{"class":577},[187,39274,7880],{"class":2516},[187,39276,19871],{"class":577},[187,39278,39279,39282,39284,39287,39289,39292,39295,39297],{"class":189,"line":1291},[187,39280,39281],{"class":577},"  or \u003C",[187,39283,15],{"class":2516},[187,39285,39286],{"class":193}," href",[187,39288,595],{"class":577},[187,39290,39291],{"class":196},"\"{% url 'accounts:register' %}\"",[187,39293,39294],{"class":577},">create an account\u003C/",[187,39296,15],{"class":2516},[187,39298,6730],{"class":577},[187,39300,39301,39303,39305],{"class":189,"line":1306},[187,39302,6856],{"class":577},[187,39304,29679],{"class":2516},[187,39306,6730],{"class":577},[11,39308,38120],{},[11,39310,39311,39312,39314,39315,39317,39318,39321],{},"By passing this as a hidden value to the form, when a ",[33,39313,21623],{}," request is made, we are redirected to the value of ",[33,39316,39058],{}," if it is not ",[33,39319,39320],{},"None",", or if there is no redirect.",[11,39323,39324,39325,39328,39329,39332],{},"When a ",[33,39326,39327],{},"@login_required"," decorator is used, and we want to access a url such as ",[33,39330,39331],{},"/books/add",", we are redirected to a url that looks like this:",[26,39334,39336],{"className":6715,"code":39335,"language":6717,"meta":35,"style":35},"http://localhost:8000/accounts/login/?next=/books/new/\n",[33,39337,39338],{"__ignoreMap":35},[187,39339,39340],{"class":189,"line":190},[187,39341,39335],{"class":577},[11,39343,39344,39345,39347,39348,39350],{},"In this way we can pass the next parameter from the ",[33,39346,28732],{}," request to the ",[33,39349,21623],{}," request that is made when the login view is hit.",[168,39352,39354],{"id":39353},"authors","Authors",[11,39356,39357,39358,39361],{},"For illustration purposes, I included an ",[33,39359,39360],{},"Author"," model. Since a book can have zero, one or many Authors, I used a many-to-many relationship to link authors to books. Here is how we do this in our Book model:",[26,39363,39365],{"className":1383,"code":39364,"language":1125,"meta":35,"style":35},"class Book(models.Model):\n    [...other fields...]\n    authors = models.ManyToManyField('authors.Author')\n",[33,39366,39367,39371,39376],{"__ignoreMap":35},[187,39368,39369],{"class":189,"line":190},[187,39370,38146],{},[187,39372,39373],{"class":189,"line":249},[187,39374,39375],{},"    [...other fields...]\n",[187,39377,39378],{"class":189,"line":312},[187,39379,39380],{},"    authors = models.ManyToManyField('authors.Author')\n",[11,39382,39383,39384,39387,39388,39390],{},"This dot notation with ",[33,39385,39386],{},"'authors.Author'"," is important. Previously I would have just imported the ",[33,39389,39360],{},"model and then referenced that in the ManyToManyField. However, this can lead to circular import errors which are hard to debug unless you know what a circular import is.",[11,39392,39393,39394,39397,39398,39400],{},"Now let's look at how to find all ",[33,39395,39396],{},"Books"," by a certain author, as well as all ",[33,39399,39354],{}," of a given book. Uing the Django shell, we can do the following:",[26,39402,39404],{"className":1383,"code":39403,"language":1125,"meta":35,"style":35},"from authors.models import Author\nfrom books.models import Book\na = Author.objects.all().first()\na.book_set.values()\n\u003CQuerySet [{'id': 583, 'title': 'None Or Other', 'lat': Decimal('-118.035345'), 'lon': Decimal('34.139729'), 'pages': 1881, 'publish_date': datetime.date(2018, 3, 18), 'website': 'https://www.fleming-mitchell.com/', 'synopsis': 'Option ever large throw dinner worker ahead realize clearly congress as smile size spend expert chair well. Brother item win follow hope coach garden later arrive who if ago voice analysis simply reflect. Amount under data drug kind book fish still information president minute stage dog. Focus full green society parent door according my management sell arrive only send international tonight. Player character financial detail oil check bring pressure possible former. Learn politics compare position large loss exactly probably approach physical international machine as model. ', 'slug': 'none-or-other', 'status': True}, {'id': 601, 'title': 'Teacher Them Step', 'lat': Decimal('-80.268357'), 'lon': Decimal('26.661763'), 'pages': 206, 'publish_date': datetime.date(2018, 2, 27), 'website': 'http://www.rodriguez.net/', 'synopsis': 'Drug fact behavior environment green try account where training brother building particular window even reach. Pressure lawyer dog world thought near institution we get force market can guy receive matter structure research foot of small. Professor production very this practice car wind wish relationship after follow professor news card concern media property have. According majority owner go mention reach store computer project kitchen group quality present several another time school and will. Before during central artist page only health region happen share traditional section well out human. Mention quite short race only education heavy book up recent official here spring oil buy which language information practice. Time mother better peace girl defense rock mr never feeling tax city stock bar tv others right conference skin. ', 'slug': 'teacher-them-step', 'status': True}]>\n",[33,39405,39406,39411,39416,39421,39426],{"__ignoreMap":35},[187,39407,39408],{"class":189,"line":190},[187,39409,39410],{},"from authors.models import Author\n",[187,39412,39413],{"class":189,"line":249},[187,39414,39415],{},"from books.models import Book\n",[187,39417,39418],{"class":189,"line":312},[187,39419,39420],{},"a = Author.objects.all().first()\n",[187,39422,39423],{"class":189,"line":319},[187,39424,39425],{},"a.book_set.values()\n",[187,39427,39428],{"class":189,"line":325},[187,39429,39430],{},"\u003CQuerySet [{'id': 583, 'title': 'None Or Other', 'lat': Decimal('-118.035345'), 'lon': Decimal('34.139729'), 'pages': 1881, 'publish_date': datetime.date(2018, 3, 18), 'website': 'https://www.fleming-mitchell.com/', 'synopsis': 'Option ever large throw dinner worker ahead realize clearly congress as smile size spend expert chair well. Brother item win follow hope coach garden later arrive who if ago voice analysis simply reflect. Amount under data drug kind book fish still information president minute stage dog. Focus full green society parent door according my management sell arrive only send international tonight. Player character financial detail oil check bring pressure possible former. Learn politics compare position large loss exactly probably approach physical international machine as model. ', 'slug': 'none-or-other', 'status': True}, {'id': 601, 'title': 'Teacher Them Step', 'lat': Decimal('-80.268357'), 'lon': Decimal('26.661763'), 'pages': 206, 'publish_date': datetime.date(2018, 2, 27), 'website': 'http://www.rodriguez.net/', 'synopsis': 'Drug fact behavior environment green try account where training brother building particular window even reach. Pressure lawyer dog world thought near institution we get force market can guy receive matter structure research foot of small. Professor production very this practice car wind wish relationship after follow professor news card concern media property have. According majority owner go mention reach store computer project kitchen group quality present several another time school and will. Before during central artist page only health region happen share traditional section well out human. Mention quite short race only education heavy book up recent official here spring oil buy which language information practice. Time mother better peace girl defense rock mr never feeling tax city stock bar tv others right conference skin. ', 'slug': 'teacher-them-step', 'status': True}]>\n",[11,39432,39433],{},"This gave us two books by an author.",[11,39435,39436,39437,39440],{},"Now, let's look at how to find all authors of a given book. For this example, we can get authors directly in the template from a book object. This example come from the ",[33,39438,39439],{},"book_detail"," template:",[11,39442,38063],{},[26,39444,39446],{"className":6715,"code":39445,"language":6717,"meta":35,"style":35},"\u003Ch3>\n  Authors: {% for author in book.authors.all %}\n  \u003Ca href=\"{% url 'authors:author_detail' id=author.id %}\"\n    >{{ author.full_name }}\u003C/a\n  >\n  {% endfor %}\n\u003C/h3>\n",[33,39447,39448,39456,39461,39474,39482,39487,39492],{"__ignoreMap":35},[187,39449,39450,39452,39454],{"class":189,"line":190},[187,39451,6724],{"class":577},[187,39453,911],{"class":2516},[187,39455,6730],{"class":577},[187,39457,39458],{"class":189,"line":249},[187,39459,39460],{"class":577},"  Authors: {% for author in book.authors.all %}\n",[187,39462,39463,39465,39467,39469,39471],{"class":189,"line":312},[187,39464,19179],{"class":577},[187,39466,15],{"class":2516},[187,39468,39286],{"class":193},[187,39470,595],{"class":577},[187,39472,39473],{"class":196},"\"{% url 'authors:author_detail' id=author.id %}\"\n",[187,39475,39476,39479],{"class":189,"line":319},[187,39477,39478],{"class":577},"    >{{ author.full_name }}\u003C/",[187,39480,39481],{"class":2516},"a\n",[187,39483,39484],{"class":189,"line":325},[187,39485,39486],{"class":577},"  >\n",[187,39488,39489],{"class":189,"line":686},[187,39490,39491],{"class":577},"  {% endfor %}\n",[187,39493,39494,39496,39498],{"class":189,"line":697},[187,39495,6856],{"class":577},[187,39497,911],{"class":2516},[187,39499,6730],{"class":577},[11,39501,38120],{},[11,39503,39504,39505,39508],{},"We could just as well access ",[33,39506,39507],{},"books.authors.all"," in the view and pass authors in a variable that we can iterate over in the template, but this keeps things simpler without much more code to write.",[168,39510,39512],{"id":39511},"filtering-data","Filtering Data",[11,39514,39515,39516,39518,39519,39521,39522,39525,39526,39528],{},"One other question I wanted to answer with this project is how to do relatively complex data filtering. To do this, I used Django forms to create a form that calls a ",[33,39517,28732],{}," request to the same view it came from, which is to say, a filter form on the template for the ",[33,39520,691],{}," books view has a form action with an ",[33,39523,39524],{},"action"," value of the url that returns the ",[33,39527,691],{}," view.",[11,39530,39531],{},"Here's a look at the form I put together for filtering data:",[26,39533,39535],{"className":1383,"code":39534,"language":1125,"meta":35,"style":35},"class QueryForm(forms.Form):\n\n    publish_date_before = forms.DateField(\n        label='',\n        required=False,\n        # initial = datetime.datetime.now(),\n        widget = forms.TextInput(\n            attrs={\n                'class': 'form-control',\n                'id':'datepicker1',\n                'placeholder':'published before'\n            }))\n\n    publish_date_after = forms.DateField(\n        label='',\n        required=False,\n        # initial = datetime.datetime.now(),\n        widget = forms.TextInput(\n            attrs={\n                'class': 'form-control',\n                'id':'datepicker2',\n                'placeholder':'published after'\n            }))\n\n    keywords = forms.CharField(\n        required=False,\n        label='',\n        widget=forms.TextInput(\n\n            attrs={\n                'class':'form-control',\n                'placeholder':'space-separated words matching title, synopsis, website or tags'\n            }))\n",[33,39536,39537,39542,39546,39551,39556,39561,39566,39571,39576,39581,39586,39591,39596,39600,39605,39609,39613,39617,39621,39625,39629,39634,39639,39643,39647,39652,39656,39660,39665,39669,39673,39678,39683],{"__ignoreMap":35},[187,39538,39539],{"class":189,"line":190},[187,39540,39541],{},"class QueryForm(forms.Form):\n",[187,39543,39544],{"class":189,"line":249},[187,39545,316],{"emptyLinePlaceholder":315},[187,39547,39548],{"class":189,"line":312},[187,39549,39550],{},"    publish_date_before = forms.DateField(\n",[187,39552,39553],{"class":189,"line":319},[187,39554,39555],{},"        label='',\n",[187,39557,39558],{"class":189,"line":325},[187,39559,39560],{},"        required=False,\n",[187,39562,39563],{"class":189,"line":686},[187,39564,39565],{},"        # initial = datetime.datetime.now(),\n",[187,39567,39568],{"class":189,"line":697},[187,39569,39570],{},"        widget = forms.TextInput(\n",[187,39572,39573],{"class":189,"line":1291},[187,39574,39575],{},"            attrs={\n",[187,39577,39578],{"class":189,"line":1306},[187,39579,39580],{},"                'class': 'form-control',\n",[187,39582,39583],{"class":189,"line":1434},[187,39584,39585],{},"                'id':'datepicker1',\n",[187,39587,39588],{"class":189,"line":2599},[187,39589,39590],{},"                'placeholder':'published before'\n",[187,39592,39593],{"class":189,"line":2607},[187,39594,39595],{},"            }))\n",[187,39597,39598],{"class":189,"line":2621},[187,39599,316],{"emptyLinePlaceholder":315},[187,39601,39602],{"class":189,"line":2631},[187,39603,39604],{},"    publish_date_after = forms.DateField(\n",[187,39606,39607],{"class":189,"line":2642},[187,39608,39555],{},[187,39610,39611],{"class":189,"line":2653},[187,39612,39560],{},[187,39614,39615],{"class":189,"line":2665},[187,39616,39565],{},[187,39618,39619],{"class":189,"line":2674},[187,39620,39570],{},[187,39622,39623],{"class":189,"line":2684},[187,39624,39575],{},[187,39626,39627],{"class":189,"line":2694},[187,39628,39580],{},[187,39630,39631],{"class":189,"line":2706},[187,39632,39633],{},"                'id':'datepicker2',\n",[187,39635,39636],{"class":189,"line":2715},[187,39637,39638],{},"                'placeholder':'published after'\n",[187,39640,39641],{"class":189,"line":2725},[187,39642,39595],{},[187,39644,39645],{"class":189,"line":2735},[187,39646,316],{"emptyLinePlaceholder":315},[187,39648,39649],{"class":189,"line":2743},[187,39650,39651],{},"    keywords = forms.CharField(\n",[187,39653,39654],{"class":189,"line":2754},[187,39655,39560],{},[187,39657,39658],{"class":189,"line":2762},[187,39659,39555],{},[187,39661,39662],{"class":189,"line":2770},[187,39663,39664],{},"        widget=forms.TextInput(\n",[187,39666,39667],{"class":189,"line":2781},[187,39668,316],{"emptyLinePlaceholder":315},[187,39670,39671],{"class":189,"line":2792},[187,39672,39575],{},[187,39674,39675],{"class":189,"line":2803},[187,39676,39677],{},"                'class':'form-control',\n",[187,39679,39680],{"class":189,"line":2808},[187,39681,39682],{},"                'placeholder':'space-separated words matching title, synopsis, website or tags'\n",[187,39684,39685],{"class":189,"line":2816},[187,39686,39595],{},[11,39688,39689,39690,39693,39694,39697,39698,27389],{},"Here we are using three fields to filter data, a ",[33,39691,39692],{},"published_before"," date, a ",[33,39695,39696],{},"published_after"," date and a ",[33,39699,39700],{},"keywords",[11,39702,39703,39704,39707,39708,39711,39712,39715,39716,39718],{},"Filtering this data can really clog up code in ",[33,39705,39706],{},"views.py",", and I know I was going to need this same code again for filtering data for CSV and XLS file downloads (more on this in a minute), so I decided to wrote a utility function that takes a ",[33,39709,39710],{},"Book"," queryset and parameter dictionary (",[33,39713,39714],{},"request.GET","), and returns a filtered ",[33,39717,39710],{}," queryset. Here's what that function looks like:",[26,39720,39722],{"className":1383,"code":39721,"language":1125,"meta":35,"style":35},"from django.db.models import Q\nfrom functools import reduce\nfrom ..models import Book\nimport datetime\n\ndef filter_books(books, paramDict):\n    # paramDict = request.GET\n    params = paramDict.keys()\n\n    # data filtering\n    if any(x!='' for x in paramDict.values()):\n        if paramDict['publish_date_after'] != '':\n            after_date = paramDict['publish_date_after']\n            _after_date = datetime.datetime.strptime(after_date, '%m/%d/%Y')\n\n            books = books.filter(publish_date__gte=_after_date)\n\n        if paramDict['publish_date_before'] != '':\n            before_date = paramDict['publish_date_before']\n            _before_date = datetime.datetime.strptime(before_date, '%m/%d/%Y')\n            books = books.filter(publish_date__lte=_before_date)\n\n        # filters records that contain any of the following keywords\n        if paramDict['keywords'] != '':\n            kws = paramDict['keywords'].split()\n            q_lookups = [Q(title__icontains=kw) for kw in kws] + \\\n                        [Q(synopsis__icontains=kw) for kw in kws] + \\\n                        [Q(website__icontains=kw) for kw in kws]\n            filters = Q()\n            filters |= reduce(lambda x, y: x | y, q_lookups)\n            books = books.filter(filters)\n\n    return books\n",[33,39723,39724,39729,39734,39739,39744,39748,39753,39758,39763,39767,39772,39777,39782,39787,39792,39796,39801,39805,39810,39815,39820,39825,39829,39834,39839,39844,39849,39854,39859,39864,39869,39874,39878],{"__ignoreMap":35},[187,39725,39726],{"class":189,"line":190},[187,39727,39728],{},"from django.db.models import Q\n",[187,39730,39731],{"class":189,"line":249},[187,39732,39733],{},"from functools import reduce\n",[187,39735,39736],{"class":189,"line":312},[187,39737,39738],{},"from ..models import Book\n",[187,39740,39741],{"class":189,"line":319},[187,39742,39743],{},"import datetime\n",[187,39745,39746],{"class":189,"line":325},[187,39747,316],{"emptyLinePlaceholder":315},[187,39749,39750],{"class":189,"line":686},[187,39751,39752],{},"def filter_books(books, paramDict):\n",[187,39754,39755],{"class":189,"line":697},[187,39756,39757],{},"    # paramDict = request.GET\n",[187,39759,39760],{"class":189,"line":1291},[187,39761,39762],{},"    params = paramDict.keys()\n",[187,39764,39765],{"class":189,"line":1306},[187,39766,316],{"emptyLinePlaceholder":315},[187,39768,39769],{"class":189,"line":1434},[187,39770,39771],{},"    # data filtering\n",[187,39773,39774],{"class":189,"line":2599},[187,39775,39776],{},"    if any(x!='' for x in paramDict.values()):\n",[187,39778,39779],{"class":189,"line":2607},[187,39780,39781],{},"        if paramDict['publish_date_after'] != '':\n",[187,39783,39784],{"class":189,"line":2621},[187,39785,39786],{},"            after_date = paramDict['publish_date_after']\n",[187,39788,39789],{"class":189,"line":2631},[187,39790,39791],{},"            _after_date = datetime.datetime.strptime(after_date, '%m/%d/%Y')\n",[187,39793,39794],{"class":189,"line":2642},[187,39795,316],{"emptyLinePlaceholder":315},[187,39797,39798],{"class":189,"line":2653},[187,39799,39800],{},"            books = books.filter(publish_date__gte=_after_date)\n",[187,39802,39803],{"class":189,"line":2665},[187,39804,316],{"emptyLinePlaceholder":315},[187,39806,39807],{"class":189,"line":2674},[187,39808,39809],{},"        if paramDict['publish_date_before'] != '':\n",[187,39811,39812],{"class":189,"line":2684},[187,39813,39814],{},"            before_date = paramDict['publish_date_before']\n",[187,39816,39817],{"class":189,"line":2694},[187,39818,39819],{},"            _before_date = datetime.datetime.strptime(before_date, '%m/%d/%Y')\n",[187,39821,39822],{"class":189,"line":2706},[187,39823,39824],{},"            books = books.filter(publish_date__lte=_before_date)\n",[187,39826,39827],{"class":189,"line":2715},[187,39828,316],{"emptyLinePlaceholder":315},[187,39830,39831],{"class":189,"line":2725},[187,39832,39833],{},"        # filters records that contain any of the following keywords\n",[187,39835,39836],{"class":189,"line":2735},[187,39837,39838],{},"        if paramDict['keywords'] != '':\n",[187,39840,39841],{"class":189,"line":2743},[187,39842,39843],{},"            kws = paramDict['keywords'].split()\n",[187,39845,39846],{"class":189,"line":2754},[187,39847,39848],{},"            q_lookups = [Q(title__icontains=kw) for kw in kws] + \\\n",[187,39850,39851],{"class":189,"line":2762},[187,39852,39853],{},"                        [Q(synopsis__icontains=kw) for kw in kws] + \\\n",[187,39855,39856],{"class":189,"line":2770},[187,39857,39858],{},"                        [Q(website__icontains=kw) for kw in kws]\n",[187,39860,39861],{"class":189,"line":2781},[187,39862,39863],{},"            filters = Q()\n",[187,39865,39866],{"class":189,"line":2792},[187,39867,39868],{},"            filters |= reduce(lambda x, y: x | y, q_lookups)\n",[187,39870,39871],{"class":189,"line":2803},[187,39872,39873],{},"            books = books.filter(filters)\n",[187,39875,39876],{"class":189,"line":2808},[187,39877,316],{"emptyLinePlaceholder":315},[187,39879,39880],{"class":189,"line":2816},[187,39881,39882],{},"    return books\n",[11,39884,39885,39886,39888],{},"This makes things much more simple in our views. Here's the code in the main ",[33,39887,38184],{}," view that uses this filter function, truncated for simplicity:",[26,39890,39892],{"className":1383,"code":39891,"language":1125,"meta":35,"style":35},"def all_books(request):\n    books = Book.objects.all()\n    form = QueryForm(request.GET or None)\n    paramDict = request.GET\n    books = filter_books(books, paramDict)\n    [...]\n    context = {\n        'books':books,\n        'form':form,\n        [...],}\n    return render(request, 'books/books.html', context)\n",[33,39893,39894,39898,39902,39906,39910,39914,39918,39922,39926,39931,39936],{"__ignoreMap":35},[187,39895,39896],{"class":189,"line":190},[187,39897,38198],{},[187,39899,39900],{"class":189,"line":249},[187,39901,38216],{},[187,39903,39904],{"class":189,"line":312},[187,39905,38221],{},[187,39907,39908],{"class":189,"line":319},[187,39909,38226],{},[187,39911,39912],{"class":189,"line":325},[187,39913,38235],{},[187,39915,39916],{"class":189,"line":686},[187,39917,38166],{},[187,39919,39920],{"class":189,"line":697},[187,39921,38278],{},[187,39923,39924],{"class":189,"line":1291},[187,39925,38283],{},[187,39927,39928],{"class":189,"line":1306},[187,39929,39930],{},"        'form':form,\n",[187,39932,39933],{"class":189,"line":1434},[187,39934,39935],{},"        [...],}\n",[187,39937,39938],{"class":189,"line":2599},[187,39939,38313],{},[168,39941,39943],{"id":39942},"exporting-data-as-csv-or-xls","Exporting data as CSV or XLS",[11,39945,39946],{},"One other requirement I gave myself for this project was giving users the option to export data in CSV or XLS file formats.",[11,39948,39949,39950,39955,39956,39960,39961,358],{},"With the filter data function, I was able to keep things DRY (Don't Repeart Yourself). This task taught me about a few aspects of HTML5 and forms that I wasn't aware of. First, let's take a look at the CSV export function that I learned about throught ",[15,39951,39954],{"href":39952,"rel":39953},"https://simpleisbetterthancomplex.com/tutorial/2016/07/29/how-to-export-to-excel.html",[19],"this blog post",", from a great Django blog called ",[15,39957,39959],{"href":39952,"rel":39958},[19],"Simple is Better than Complex"," by ",[15,39962,39965],{"href":39963,"rel":39964},"https://github.com/vitorfs",[19],"Vitor Freitas",[26,39967,39969],{"className":1383,"code":39968,"language":1125,"meta":35,"style":35},"def export_filtered_books_csv(request):\n    response = HttpResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment; filename=\"books.csv\"'\n\n    writer = csv.writer(response)\n    writer.writerow(['Title', 'Synopsis', 'Pages'])\n    books = Book.objects.all()\n    paramDict = request.GET\n    books = filter_books(books, paramDict)\n    books = books.values_list(\n        'title',\n        'synopsis',\n        'pages')\n\n    for book in books:\n        writer.writerow(book)\n\n    return response\n",[33,39970,39971,39976,39981,39986,39990,39995,40000,40004,40008,40012,40017,40022,40027,40032,40036,40041,40046,40050],{"__ignoreMap":35},[187,39972,39973],{"class":189,"line":190},[187,39974,39975],{},"def export_filtered_books_csv(request):\n",[187,39977,39978],{"class":189,"line":249},[187,39979,39980],{},"    response = HttpResponse(content_type='text/csv')\n",[187,39982,39983],{"class":189,"line":312},[187,39984,39985],{},"    response['Content-Disposition'] = 'attachment; filename=\"books.csv\"'\n",[187,39987,39988],{"class":189,"line":319},[187,39989,316],{"emptyLinePlaceholder":315},[187,39991,39992],{"class":189,"line":325},[187,39993,39994],{},"    writer = csv.writer(response)\n",[187,39996,39997],{"class":189,"line":686},[187,39998,39999],{},"    writer.writerow(['Title', 'Synopsis', 'Pages'])\n",[187,40001,40002],{"class":189,"line":697},[187,40003,38216],{},[187,40005,40006],{"class":189,"line":1291},[187,40007,38226],{},[187,40009,40010],{"class":189,"line":1306},[187,40011,38235],{},[187,40013,40014],{"class":189,"line":1434},[187,40015,40016],{},"    books = books.values_list(\n",[187,40018,40019],{"class":189,"line":2599},[187,40020,40021],{},"        'title',\n",[187,40023,40024],{"class":189,"line":2607},[187,40025,40026],{},"        'synopsis',\n",[187,40028,40029],{"class":189,"line":2621},[187,40030,40031],{},"        'pages')\n",[187,40033,40034],{"class":189,"line":2631},[187,40035,316],{"emptyLinePlaceholder":315},[187,40037,40038],{"class":189,"line":2642},[187,40039,40040],{},"    for book in books:\n",[187,40042,40043],{"class":189,"line":2653},[187,40044,40045],{},"        writer.writerow(book)\n",[187,40047,40048],{"class":189,"line":2665},[187,40049,316],{"emptyLinePlaceholder":315},[187,40051,40052],{"class":189,"line":2674},[187,40053,40054],{},"    return response\n",[11,40056,40057,40058,40060],{},"I wanted to put this button in the filter form on the main ",[33,40059,38184],{}," page, but I need to place the button outside of the form tag. To get around this, here is the HTML I used:",[11,40062,38063],{},[26,40064,40066],{"className":6715,"code":40065,"language":6717,"meta":35,"style":35},"\u003Cform action=\"{% url 'books:csv' %}\">\n  \u003Ca\n    class=\"btn btn-primary\"\n    data-toggle=\"collapse\"\n    href=\"#collapseExample\"\n    aria-expanded=\"false\"\n    aria-controls=\"collapseExample\"\n  >\n    Filter Books\n  \u003C/a>\n  \u003Cinput\n    type=\"submit\"\n    class=\"btn btn-info\"\n    form=\"id_query_form\"\n    formaction=\"{% url 'books:csv' %}\"\n    value=\"Export CSV\"\n  />\n  \u003Cinput\n    type=\"submit\"\n    class=\"btn btn-default\"\n    form=\"id_query_form\"\n    formaction=\"{% url 'books:xls' %}\"\n    value=\"Export XLS\"\n  />\n\u003C/form>\n\n\u003Cform method=\"get\" action=\".\" id=\"id_query_form\">\n  \u003Cp>{{ form.keywords }}\u003C/p>\n  \u003Cdiv class=\"row\">\n    \u003Cdiv class=\"col-md-6\">{{ form.publish_date_before }}\u003C/div>\n    \u003Cdiv class=\"col-md-6\">{{ form.publish_date_after }}\u003C/div>\n  \u003C/div>\n\n  \u003Cp>\u003C/p>\n  \u003Cp>\n    \u003Cinput class=\"btn btn-success\" type=\"submit\" />\n    \u003Cbutton type=\"reset\" class=\"btn btn-info\" value=\"Reset filters\">\n      Reset filters\n    \u003C/button>\n    \u003Cbutton\n      type=\"reset\"\n      class=\"btn btn-warning\"\n      id=\"id_clear_filters\"\n      onclick=\"return resetForm(this.form);\"\n    >\n      Clear Filters\n    \u003C/button>\n  \u003C/p>\n\u003C/form>\n",[33,40067,40068,40083,40089,40099,40109,40119,40129,40139,40143,40148,40156,40162,40171,40180,40190,40200,40210,40215,40221,40229,40238,40246,40255,40264,40268,40276,40280,40308,40321,40336,40356,40375,40383,40387,40400,40408,40429,40459,40464,40472,40479,40488,40498,40508,40534,40539,40544,40552,40560],{"__ignoreMap":35},[187,40069,40070,40072,40074,40076,40078,40081],{"class":189,"line":190},[187,40071,6724],{"class":577},[187,40073,29679],{"class":2516},[187,40075,39172],{"class":193},[187,40077,595],{"class":577},[187,40079,40080],{"class":196},"\"{% url 'books:csv' %}\"",[187,40082,6730],{"class":577},[187,40084,40085,40087],{"class":189,"line":249},[187,40086,19179],{"class":577},[187,40088,39481],{"class":2516},[187,40090,40091,40094,40096],{"class":189,"line":312},[187,40092,40093],{"class":193},"    class",[187,40095,595],{"class":577},[187,40097,40098],{"class":196},"\"btn btn-primary\"\n",[187,40100,40101,40104,40106],{"class":189,"line":319},[187,40102,40103],{"class":193},"    data-toggle",[187,40105,595],{"class":577},[187,40107,40108],{"class":196},"\"collapse\"\n",[187,40110,40111,40114,40116],{"class":189,"line":325},[187,40112,40113],{"class":193},"    href",[187,40115,595],{"class":577},[187,40117,40118],{"class":196},"\"#collapseExample\"\n",[187,40120,40121,40124,40126],{"class":189,"line":686},[187,40122,40123],{"class":193},"    aria-expanded",[187,40125,595],{"class":577},[187,40127,40128],{"class":196},"\"false\"\n",[187,40130,40131,40134,40136],{"class":189,"line":697},[187,40132,40133],{"class":193},"    aria-controls",[187,40135,595],{"class":577},[187,40137,40138],{"class":196},"\"collapseExample\"\n",[187,40140,40141],{"class":189,"line":1291},[187,40142,39486],{"class":577},[187,40144,40145],{"class":189,"line":1306},[187,40146,40147],{"class":577},"    Filter Books\n",[187,40149,40150,40152,40154],{"class":189,"line":1434},[187,40151,19938],{"class":577},[187,40153,15],{"class":2516},[187,40155,6730],{"class":577},[187,40157,40158,40160],{"class":189,"line":2599},[187,40159,19179],{"class":577},[187,40161,29316],{"class":2516},[187,40163,40164,40167,40169],{"class":189,"line":2607},[187,40165,40166],{"class":193},"    type",[187,40168,595],{"class":577},[187,40170,29615],{"class":196},[187,40172,40173,40175,40177],{"class":189,"line":2621},[187,40174,40093],{"class":193},[187,40176,595],{"class":577},[187,40178,40179],{"class":196},"\"btn btn-info\"\n",[187,40181,40182,40185,40187],{"class":189,"line":2631},[187,40183,40184],{"class":193},"    form",[187,40186,595],{"class":577},[187,40188,40189],{"class":196},"\"id_query_form\"\n",[187,40191,40192,40195,40197],{"class":189,"line":2642},[187,40193,40194],{"class":193},"    formaction",[187,40196,595],{"class":577},[187,40198,40199],{"class":196},"\"{% url 'books:csv' %}\"\n",[187,40201,40202,40205,40207],{"class":189,"line":2653},[187,40203,40204],{"class":193},"    value",[187,40206,595],{"class":577},[187,40208,40209],{"class":196},"\"Export CSV\"\n",[187,40211,40212],{"class":189,"line":2665},[187,40213,40214],{"class":577},"  />\n",[187,40216,40217,40219],{"class":189,"line":2674},[187,40218,19179],{"class":577},[187,40220,29316],{"class":2516},[187,40222,40223,40225,40227],{"class":189,"line":2684},[187,40224,40166],{"class":193},[187,40226,595],{"class":577},[187,40228,29615],{"class":196},[187,40230,40231,40233,40235],{"class":189,"line":2694},[187,40232,40093],{"class":193},[187,40234,595],{"class":577},[187,40236,40237],{"class":196},"\"btn btn-default\"\n",[187,40239,40240,40242,40244],{"class":189,"line":2706},[187,40241,40184],{"class":193},[187,40243,595],{"class":577},[187,40245,40189],{"class":196},[187,40247,40248,40250,40252],{"class":189,"line":2715},[187,40249,40194],{"class":193},[187,40251,595],{"class":577},[187,40253,40254],{"class":196},"\"{% url 'books:xls' %}\"\n",[187,40256,40257,40259,40261],{"class":189,"line":2725},[187,40258,40204],{"class":193},[187,40260,595],{"class":577},[187,40262,40263],{"class":196},"\"Export XLS\"\n",[187,40265,40266],{"class":189,"line":2735},[187,40267,40214],{"class":577},[187,40269,40270,40272,40274],{"class":189,"line":2743},[187,40271,6856],{"class":577},[187,40273,29679],{"class":2516},[187,40275,6730],{"class":577},[187,40277,40278],{"class":189,"line":2754},[187,40279,316],{"emptyLinePlaceholder":315},[187,40281,40282,40284,40286,40288,40290,40293,40295,40297,40299,40301,40303,40306],{"class":189,"line":2762},[187,40283,6724],{"class":577},[187,40285,29679],{"class":2516},[187,40287,39164],{"class":193},[187,40289,595],{"class":577},[187,40291,40292],{"class":196},"\"get\"",[187,40294,39172],{"class":193},[187,40296,595],{"class":577},[187,40298,39177],{"class":196},[187,40300,29179],{"class":193},[187,40302,595],{"class":577},[187,40304,40305],{"class":196},"\"id_query_form\"",[187,40307,6730],{"class":577},[187,40309,40310,40312,40314,40317,40319],{"class":189,"line":2770},[187,40311,19179],{"class":577},[187,40313,11],{"class":2516},[187,40315,40316],{"class":577},">{{ form.keywords }}\u003C/",[187,40318,11],{"class":2516},[187,40320,6730],{"class":577},[187,40322,40323,40325,40327,40329,40331,40334],{"class":189,"line":2781},[187,40324,19179],{"class":577},[187,40326,10229],{"class":2516},[187,40328,29292],{"class":193},[187,40330,595],{"class":577},[187,40332,40333],{"class":196},"\"row\"",[187,40335,6730],{"class":577},[187,40337,40338,40340,40342,40344,40346,40349,40352,40354],{"class":189,"line":2792},[187,40339,19865],{"class":577},[187,40341,10229],{"class":2516},[187,40343,29292],{"class":193},[187,40345,595],{"class":577},[187,40347,40348],{"class":196},"\"col-md-6\"",[187,40350,40351],{"class":577},">{{ form.publish_date_before }}\u003C/",[187,40353,10229],{"class":2516},[187,40355,6730],{"class":577},[187,40357,40358,40360,40362,40364,40366,40368,40371,40373],{"class":189,"line":2803},[187,40359,19865],{"class":577},[187,40361,10229],{"class":2516},[187,40363,29292],{"class":193},[187,40365,595],{"class":577},[187,40367,40348],{"class":196},[187,40369,40370],{"class":577},">{{ form.publish_date_after }}\u003C/",[187,40372,10229],{"class":2516},[187,40374,6730],{"class":577},[187,40376,40377,40379,40381],{"class":189,"line":2808},[187,40378,19938],{"class":577},[187,40380,10229],{"class":2516},[187,40382,6730],{"class":577},[187,40384,40385],{"class":189,"line":2816},[187,40386,316],{"emptyLinePlaceholder":315},[187,40388,40389,40391,40393,40396,40398],{"class":189,"line":2824},[187,40390,19179],{"class":577},[187,40392,11],{"class":2516},[187,40394,40395],{"class":577},">\u003C/",[187,40397,11],{"class":2516},[187,40399,6730],{"class":577},[187,40401,40402,40404,40406],{"class":189,"line":2834},[187,40403,19179],{"class":577},[187,40405,11],{"class":2516},[187,40407,6730],{"class":577},[187,40409,40410,40412,40414,40416,40418,40421,40423,40425,40427],{"class":189,"line":2845},[187,40411,19865],{"class":577},[187,40413,15028],{"class":2516},[187,40415,29292],{"class":193},[187,40417,595],{"class":577},[187,40419,40420],{"class":196},"\"btn btn-success\"",[187,40422,39193],{"class":193},[187,40424,595],{"class":577},[187,40426,39251],{"class":196},[187,40428,19871],{"class":577},[187,40430,40431,40433,40436,40438,40440,40443,40445,40447,40450,40452,40454,40457],{"class":189,"line":2856},[187,40432,19865],{"class":577},[187,40434,40435],{"class":2516},"button",[187,40437,39193],{"class":193},[187,40439,595],{"class":577},[187,40441,40442],{"class":196},"\"reset\"",[187,40444,29292],{"class":193},[187,40446,595],{"class":577},[187,40448,40449],{"class":196},"\"btn btn-info\"",[187,40451,39201],{"class":193},[187,40453,595],{"class":577},[187,40455,40456],{"class":196},"\"Reset filters\"",[187,40458,6730],{"class":577},[187,40460,40461],{"class":189,"line":2867},[187,40462,40463],{"class":577},"      Reset filters\n",[187,40465,40466,40468,40470],{"class":189,"line":2878},[187,40467,29686],{"class":577},[187,40469,40435],{"class":2516},[187,40471,6730],{"class":577},[187,40473,40474,40476],{"class":189,"line":2886},[187,40475,19865],{"class":577},[187,40477,40478],{"class":2516},"button\n",[187,40480,40481,40483,40485],{"class":189,"line":2900},[187,40482,24669],{"class":193},[187,40484,595],{"class":577},[187,40486,40487],{"class":196},"\"reset\"\n",[187,40489,40490,40493,40495],{"class":189,"line":2905},[187,40491,40492],{"class":193},"      class",[187,40494,595],{"class":577},[187,40496,40497],{"class":196},"\"btn btn-warning\"\n",[187,40499,40500,40503,40505],{"class":189,"line":2913},[187,40501,40502],{"class":193},"      id",[187,40504,595],{"class":577},[187,40506,40507],{"class":196},"\"id_clear_filters\"\n",[187,40509,40510,40513,40515,40517,40520,40523,40525,40527,40529,40531],{"class":189,"line":2921},[187,40511,40512],{"class":193},"      onclick",[187,40514,595],{"class":577},[187,40516,16508],{"class":196},[187,40518,40519],{"class":573},"return",[187,40521,40522],{"class":193}," resetForm",[187,40524,615],{"class":196},[187,40526,14293],{"class":588},[187,40528,752],{"class":196},[187,40530,29679],{"class":577},[187,40532,40533],{"class":196},");\"\n",[187,40535,40536],{"class":189,"line":2931},[187,40537,40538],{"class":577},"    >\n",[187,40540,40541],{"class":189,"line":2942},[187,40542,40543],{"class":577},"      Clear Filters\n",[187,40545,40546,40548,40550],{"class":189,"line":2953},[187,40547,29686],{"class":577},[187,40549,40435],{"class":2516},[187,40551,6730],{"class":577},[187,40553,40554,40556,40558],{"class":189,"line":2964},[187,40555,19938],{"class":577},[187,40557,11],{"class":2516},[187,40559,6730],{"class":577},[187,40561,40562,40564,40566],{"class":189,"line":2975},[187,40563,6856],{"class":577},[187,40565,29679],{"class":2516},[187,40567,6730],{"class":577},[11,40569,38120],{},[11,40571,40572,40573,40575,40576,40579,40580,40583,40584,40587],{},"There are two forms here, but the first form has an ",[33,40574,15028],{}," of ",[33,40577,40578],{},"type=submit"," that references another form with ",[33,40581,40582],{},"form=\"id_query_form\"",", and this button's action is the url for the ",[33,40585,40586],{},"export_filtered_books_csv"," function shown above.",[11,40589,40590,40591,40594,40595,40597],{},"For the filter function, I make a ",[33,40592,40593],{},"utils"," folder in the app I am working with, so the directory structure looks like this (for the ",[33,40596,38184],{}," app):",[26,40599,40602],{"className":40600,"code":40601,"language":31},[29],".\n├── admin.py\n├── apps.py\n├── forms.py\n├── __init__.py\n├── migrations\n│   ├── 0001_initial.py\n│   ├── 0002_book_authors.py\n│   └── __init__.py\n├── models.py\n├── templates\n│   └── books\n├── tests.py\n├── urls.py\n├── utils\n│   ├── filter.py\n│   ├── __init__.py\n│   └── nearby.py\n└── views.py\n",[33,40603,40601],{"__ignoreMap":35},[11,40605,40606,40607,637,40609,637,40611,1737],{},"I initially had a litte bit of confusion on how to write a simple search query. I wanted a user to be able to enter terms that would return a query set where the items in the query set contained at least on of the terms in at least on of a few different fields (",[33,40608,6537],{},[33,40610,8780],{},[33,40612,40613],{},"website",[11,40615,40616],{},"To make this filter, I found a nice idiomatic pattern:",[26,40618,40620],{"className":1383,"code":40619,"language":1125,"meta":35,"style":35},"if paramDict['keywords'] != '':\n    kws = paramDict['keywords'].split()\n    q_lookups = [Q(title__icontains=kw) for kw in kws] + \\\n                [Q(synopsis__icontains=kw) for kw in kws] + \\\n                [Q(website__icontains=kw) for kw in kws]\n    filters = Q()\n    filters |= reduce(lambda x, y: x | y, q_lookups)\n    books = books.filter(filters)\n",[33,40621,40622,40627,40632,40637,40642,40647,40652,40657],{"__ignoreMap":35},[187,40623,40624],{"class":189,"line":190},[187,40625,40626],{},"if paramDict['keywords'] != '':\n",[187,40628,40629],{"class":189,"line":249},[187,40630,40631],{},"    kws = paramDict['keywords'].split()\n",[187,40633,40634],{"class":189,"line":312},[187,40635,40636],{},"    q_lookups = [Q(title__icontains=kw) for kw in kws] + \\\n",[187,40638,40639],{"class":189,"line":319},[187,40640,40641],{},"                [Q(synopsis__icontains=kw) for kw in kws] + \\\n",[187,40643,40644],{"class":189,"line":325},[187,40645,40646],{},"                [Q(website__icontains=kw) for kw in kws]\n",[187,40648,40649],{"class":189,"line":686},[187,40650,40651],{},"    filters = Q()\n",[187,40653,40654],{"class":189,"line":697},[187,40655,40656],{},"    filters |= reduce(lambda x, y: x | y, q_lookups)\n",[187,40658,40659],{"class":189,"line":1291},[187,40660,40661],{},"    books = books.filter(filters)\n",[11,40663,40664],{},"This is good for a small database, but could end up having to do lots of operations, especially if I decided that I want to add more fields to be searched, such as category or tags.",[11,40666,40667,40668,40671,40672,40675,40676,40678],{},"One option that I have considered to simplify the search query is to make one field called something like ",[33,40669,40670],{},"search_string",". This field would be modified on ",[33,40673,40674],{},"save",", and it would simply appends the text from all of the fields I'm interested in searching. Then, instead of doing Q lookups for each word over many different fields, I could search each word in ",[33,40677,39700],{}," over one field. I haven't implemented this here, but I would like to test this in the future.",[168,40680,40682],{"id":40681},"finding-books-nearby","Finding \"Books Nearby\"",[11,40684,40685],{},"On the pages that show details for each book I wanted to add additional information. I thought it would be interesting to show the 5 books closest to the book we are currently showing. Since we are dealing with coordinate points on a globe, the best way to calculate distance between two points is to use the Halversine formula:",[107,40687,40688],{},[11,40689,40690],{},"The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Important in navigation, it is a special case of a more general formula in spherical trigonometry, the law of haversines, that relates the sides and angles of spherical triangles.",[11,40692,40693],{},[511,40694],{"alt":7255,"src":40695},"/static/great_circle.png",[11,40697,40698],{},"Here's the code I used for calculating distance (using the Halversine formula):",[26,40700,40702],{"className":1383,"code":40701,"language":1125,"meta":35,"style":35},"from math import radians, cos, sin, asin, sqrt\n\ndef distance(origin, destination):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n    \"\"\"\n\n    lon1, lat1 = origin\n    lon2, lat2 = destination\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a))\n    r = 3956 # miles\n    return c * r\n",[33,40703,40704,40709,40713,40718,40722,40727,40732,40736,40740,40745,40750,40755,40760,40765,40770,40775,40780,40785,40790],{"__ignoreMap":35},[187,40705,40706],{"class":189,"line":190},[187,40707,40708],{},"from math import radians, cos, sin, asin, sqrt\n",[187,40710,40711],{"class":189,"line":249},[187,40712,316],{"emptyLinePlaceholder":315},[187,40714,40715],{"class":189,"line":312},[187,40716,40717],{},"def distance(origin, destination):\n",[187,40719,40720],{"class":189,"line":319},[187,40721,4793],{},[187,40723,40724],{"class":189,"line":325},[187,40725,40726],{},"    Calculate the great circle distance between two points\n",[187,40728,40729],{"class":189,"line":686},[187,40730,40731],{},"    on the earth (specified in decimal degrees)\n",[187,40733,40734],{"class":189,"line":697},[187,40735,4793],{},[187,40737,40738],{"class":189,"line":1291},[187,40739,316],{"emptyLinePlaceholder":315},[187,40741,40742],{"class":189,"line":1306},[187,40743,40744],{},"    lon1, lat1 = origin\n",[187,40746,40747],{"class":189,"line":1434},[187,40748,40749],{},"    lon2, lat2 = destination\n",[187,40751,40752],{"class":189,"line":2599},[187,40753,40754],{},"    # convert decimal degrees to radians\n",[187,40756,40757],{"class":189,"line":2607},[187,40758,40759],{},"    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",[187,40761,40762],{"class":189,"line":2621},[187,40763,40764],{},"    # haversine formula\n",[187,40766,40767],{"class":189,"line":2631},[187,40768,40769],{},"    dlon = lon2 - lon1\n",[187,40771,40772],{"class":189,"line":2642},[187,40773,40774],{},"    dlat = lat2 - lat1\n",[187,40776,40777],{"class":189,"line":2653},[187,40778,40779],{},"    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",[187,40781,40782],{"class":189,"line":2665},[187,40783,40784],{},"    c = 2 * asin(sqrt(a))\n",[187,40786,40787],{"class":189,"line":2674},[187,40788,40789],{},"    r = 3956 # miles\n",[187,40791,40792],{"class":189,"line":2684},[187,40793,40794],{},"    return c * r\n",[11,40796,40797],{},"This formula assumes that the earth is a perfect sphere. Another approach would be to use the the Vincety Formula:",[107,40799,40800],{},[11,40801,40802],{},"Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid, developed by Thaddeus Vincenty (1975a). They are based on the assumption that the figure of the Earth is an oblate spheroid, and hence are more accurate than methods that assume a spherical Earth, such as great-circle distance.",[11,40804,40805,40806,40809],{},"This distance is packaged in ",[33,40807,40808],{},"geopy"," and can be calculated easily:",[26,40811,40813],{"className":1383,"code":40812,"language":1125,"meta":35,"style":35},"import geopy.distance\n\ncoords_1 = (52.2296756, 21.0122287)\ncoords_2 = (52.406374, 16.9251681)\n\nprint geopy.distance.vincenty(coords_1, coords_2).km\n",[33,40814,40815,40820,40824,40829,40834,40838],{"__ignoreMap":35},[187,40816,40817],{"class":189,"line":190},[187,40818,40819],{},"import geopy.distance\n",[187,40821,40822],{"class":189,"line":249},[187,40823,316],{"emptyLinePlaceholder":315},[187,40825,40826],{"class":189,"line":312},[187,40827,40828],{},"coords_1 = (52.2296756, 21.0122287)\n",[187,40830,40831],{"class":189,"line":319},[187,40832,40833],{},"coords_2 = (52.406374, 16.9251681)\n",[187,40835,40836],{"class":189,"line":325},[187,40837,316],{"emptyLinePlaceholder":315},[187,40839,40840],{"class":189,"line":686},[187,40841,40842],{},"print geopy.distance.vincenty(coords_1, coords_2).km\n",[11,40844,40845,40846,40848],{},"Here's a look at the function I wrote for the ",[33,40847,39439],{}," view. I have commented out parts that aren't related to finding nearby books:",[26,40850,40852],{"className":1383,"code":40851,"language":1125,"meta":35,"style":35},"def book_detail(request, id, slug):\n\n    book = Book.objects.get(id=id, slug=slug)\n    b_coords = (book.lat, book.lon)\n    all_books = Book.objects.all()\n    coords = [((b.lat, b.lon),b) for b in all_books]\n\n    distance_dict = {}\n    for c in coords:\n        if c[0] != b_coords:\n            distance_dict[c[0]]=(distance(c[0],b_coords),c)\n\n    sorted_nearby = sorted(distance_dict.items(), key=lambda x: x[1][0])[:5]\n\n    # map_book = [{'loc':[float(book.lon), float(book.lat)],\n    #              'title':book.title,\n    #              'url':book.get_absolute_url()}]\n    context = {\n        'book':book,\n        # 'map_book':mark_safe(escapejs(json.dumps(map_book))),\n        'sorted_nearby':sorted_nearby,\n    }\n    return render(request, 'books/book_detail.html', context)\n",[33,40853,40854,40859,40863,40868,40873,40878,40883,40887,40892,40897,40902,40907,40911,40916,40920,40925,40930,40935,40939,40944,40949,40954,40958],{"__ignoreMap":35},[187,40855,40856],{"class":189,"line":190},[187,40857,40858],{},"def book_detail(request, id, slug):\n",[187,40860,40861],{"class":189,"line":249},[187,40862,316],{"emptyLinePlaceholder":315},[187,40864,40865],{"class":189,"line":312},[187,40866,40867],{},"    book = Book.objects.get(id=id, slug=slug)\n",[187,40869,40870],{"class":189,"line":319},[187,40871,40872],{},"    b_coords = (book.lat, book.lon)\n",[187,40874,40875],{"class":189,"line":325},[187,40876,40877],{},"    all_books = Book.objects.all()\n",[187,40879,40880],{"class":189,"line":686},[187,40881,40882],{},"    coords = [((b.lat, b.lon),b) for b in all_books]\n",[187,40884,40885],{"class":189,"line":697},[187,40886,316],{"emptyLinePlaceholder":315},[187,40888,40889],{"class":189,"line":1291},[187,40890,40891],{},"    distance_dict = {}\n",[187,40893,40894],{"class":189,"line":1306},[187,40895,40896],{},"    for c in coords:\n",[187,40898,40899],{"class":189,"line":1434},[187,40900,40901],{},"        if c[0] != b_coords:\n",[187,40903,40904],{"class":189,"line":2599},[187,40905,40906],{},"            distance_dict[c[0]]=(distance(c[0],b_coords),c)\n",[187,40908,40909],{"class":189,"line":2607},[187,40910,316],{"emptyLinePlaceholder":315},[187,40912,40913],{"class":189,"line":2621},[187,40914,40915],{},"    sorted_nearby = sorted(distance_dict.items(), key=lambda x: x[1][0])[:5]\n",[187,40917,40918],{"class":189,"line":2631},[187,40919,316],{"emptyLinePlaceholder":315},[187,40921,40922],{"class":189,"line":2642},[187,40923,40924],{},"    # map_book = [{'loc':[float(book.lon), float(book.lat)],\n",[187,40926,40927],{"class":189,"line":2653},[187,40928,40929],{},"    #              'title':book.title,\n",[187,40931,40932],{"class":189,"line":2665},[187,40933,40934],{},"    #              'url':book.get_absolute_url()}]\n",[187,40936,40937],{"class":189,"line":2674},[187,40938,38278],{},[187,40940,40941],{"class":189,"line":2684},[187,40942,40943],{},"        'book':book,\n",[187,40945,40946],{"class":189,"line":2694},[187,40947,40948],{},"        # 'map_book':mark_safe(escapejs(json.dumps(map_book))),\n",[187,40950,40951],{"class":189,"line":2706},[187,40952,40953],{},"        'sorted_nearby':sorted_nearby,\n",[187,40955,40956],{"class":189,"line":2715},[187,40957,9799],{},[187,40959,40960],{"class":189,"line":2725},[187,40961,40962],{},"    return render(request, 'books/book_detail.html', context)\n",[11,40964,40965,40966,40969,40970,40973,40974,40977,40978,33917,40981,40984,40985,12282,40988,40991,40992,9420,40995,40998,40999,41002,41003,41006,41007,41010,41011,41014],{},"To find the 5 closest books I arrived at a solution that seems fairly convoluted and should be refactored, but works! I start with a queryset of all books, then use list comprehension to make a list of tuples containing ",[33,40967,40968],{},"((\u003C longitutde >, \u003C latitude >), \u003C Book Object >)",". Then I loop over this list and create a dictionary where the keys are ",[33,40971,40972],{},"(\u003C longitutde >, \u003C latitude >)"," and the values are tuples of the form: ",[33,40975,40976],{},"(\u003C distance in miles >, \u003C Book Object >)",". Finally, I use ",[33,40979,40980],{},"sorted",[33,40982,40983],{},"dictionary_dict.items()"," where the key is ",[33,40986,40987],{},"lambda x: x[1][0]",[33,40989,40990],{},"[1]"," accesses the ",[33,40993,40994],{},"value",[33,40996,40997],{},"(\u003C key >, \u003C value>)"," tuple returned by ",[33,41000,41001],{},"items()",", and the ",[33,41004,41005],{},"[0]"," access the first item, which is the ",[33,41008,41009],{},"distance"," value we calculated. Finally, I take the first ",[33,41012,41013],{},"[:5]"," items of this sorted list.",[11,41016,41017],{},"With this approach I think I avoided the possible issue of ambiguity if we have two books with the same coordinates. I'll need to add this scenario to my test suite later.",[168,41019,41021],{"id":41020},"testing-travis-ci","Testing & Travis CI",[11,41023,41024],{},"Speaking of testing, this applcation includes a simple testing suite (that I am currently working on expanding). I have also managed to setup Travis CI for this project. When I push code from my local repo to GitHub, Travis CI automatically runs test, and the commit message contains additional information about whether or not all of the tests were successful.",[11,41026,41027,41028,41031,41032,41034],{},"Setting up Travis CI is very simple. We need to grant Travis CI (.org) access to our GitHub account, enable Travis CI on the repository we are working with, and then add a ",[33,41029,41030],{},".travis.yml"," file to the top level of the project directory. Here's the ",[33,41033,41030],{}," file I have for this project:",[26,41036,41038],{"className":8656,"code":41037,"language":8658,"meta":35,"style":35},"language: python\n\npython:\n  - '3.5'\n\nservices:\n  - postgresql\n\nenv: -DJANGO=2.0 DB=postgresql\n\ninstall:\n  - pip install -r requirements.txt\n\nbefore_script:\n  - psql -c \"CREATE USER u_brian WITH PASSWORD 'password'; ALTER USER u_brian CREATEDB;\" -U postgres\n\nscript:\n  - python manage.py test books/\n",[33,41039,41040,41049,41053,41059,41066,41070,41076,41083,41087,41097,41101,41108,41115,41119,41125,41132,41136,41142],{"__ignoreMap":35},[187,41041,41042,41044,41046],{"class":189,"line":190},[187,41043,11619],{"class":2516},[187,41045,585],{"class":577},[187,41047,41048],{"class":196},"python\n",[187,41050,41051],{"class":189,"line":249},[187,41052,316],{"emptyLinePlaceholder":315},[187,41054,41055,41057],{"class":189,"line":312},[187,41056,1125],{"class":2516},[187,41058,2520],{"class":577},[187,41060,41061,41063],{"class":189,"line":319},[187,41062,6592],{"class":577},[187,41064,41065],{"class":196},"'3.5'\n",[187,41067,41068],{"class":189,"line":325},[187,41069,316],{"emptyLinePlaceholder":315},[187,41071,41072,41074],{"class":189,"line":686},[187,41073,3613],{"class":2516},[187,41075,2520],{"class":577},[187,41077,41078,41080],{"class":189,"line":697},[187,41079,6592],{"class":577},[187,41081,41082],{"class":196},"postgresql\n",[187,41084,41085],{"class":189,"line":1291},[187,41086,316],{"emptyLinePlaceholder":315},[187,41088,41089,41092,41094],{"class":189,"line":1306},[187,41090,41091],{"class":2516},"env",[187,41093,585],{"class":577},[187,41095,41096],{"class":196},"-DJANGO=2.0 DB=postgresql\n",[187,41098,41099],{"class":189,"line":1434},[187,41100,316],{"emptyLinePlaceholder":315},[187,41102,41103,41106],{"class":189,"line":2599},[187,41104,41105],{"class":2516},"install",[187,41107,2520],{"class":577},[187,41109,41110,41112],{"class":189,"line":2607},[187,41111,6592],{"class":577},[187,41113,41114],{"class":196},"pip install -r requirements.txt\n",[187,41116,41117],{"class":189,"line":2621},[187,41118,316],{"emptyLinePlaceholder":315},[187,41120,41121,41123],{"class":189,"line":2631},[187,41122,30784],{"class":2516},[187,41124,2520],{"class":577},[187,41126,41127,41129],{"class":189,"line":2642},[187,41128,6592],{"class":577},[187,41130,41131],{"class":196},"psql -c \"CREATE USER u_brian WITH PASSWORD 'password'; ALTER USER u_brian CREATEDB;\" -U postgres\n",[187,41133,41134],{"class":189,"line":2653},[187,41135,316],{"emptyLinePlaceholder":315},[187,41137,41138,41140],{"class":189,"line":2665},[187,41139,6727],{"class":2516},[187,41141,2520],{"class":577},[187,41143,41144,41146],{"class":189,"line":2674},[187,41145,6592],{"class":577},[187,41147,41148],{"class":196},"python manage.py test books/\n",[11,41150,41151],{},"We can generate a Travis CI badge that shows the status of the current Github deploy with the following code:",[26,41153,41155],{"className":6715,"code":41154,"language":6717,"meta":35,"style":35},"[![Build\nStatus](https://travis-ci.org/briancaffey/django-leaflet-demo.svg?branch=master)](https://travis-ci.org/briancaffey/django-leaflet-demo)\n",[33,41156,41157,41162],{"__ignoreMap":35},[187,41158,41159],{"class":189,"line":190},[187,41160,41161],{"class":577},"[![Build\n",[187,41163,41164],{"class":189,"line":249},[187,41165,41166],{"class":577},"Status](https://travis-ci.org/briancaffey/django-leaflet-demo.svg?branch=master)](https://travis-ci.org/briancaffey/django-leaflet-demo)\n",[11,41168,41169],{},[15,41170,41173],{"href":41171,"rel":41172},"https://travis-ci.org/briancaffey/django-leaflet-demo",[19],[511,41174],{"alt":41175,"src":41176},"Build Status","https://travis-ci.org/briancaffey/django-leaflet-demo.svg?branch=master",[168,41178,41180],{"id":41179},"deploying-to-digitalocean","Deploying to DigitalOcean",[11,41182,41183],{},"Finally, I used DigitalOcean to deploy this app. I have used Heroku for most of my previous projects, but I decided to use DigitalOcean for this one to learn something new and get more experience with using Ubuntu and related tools for running a website: nginx and gunicorn.",[11,41185,41186,41187,41191,41192,1172,41194,41196],{},"Again, for I turned to Vitor's blog for a very straightforward introduction to deploying on DigitalOcean with a simple Droplet. You can read more about the instructions for deployment ",[15,41188,1321],{"href":41189,"rel":41190},"https://simpleisbetterthancomplex.com/tutorial/2016/10/14/how-to-deploy-to-digital-ocean.html",[19],", and I can say that I had no problems following the instructions step by step. Here are the ",[33,41193,35961],{},[33,41195,30735],{}," scripts I have used to successfully deploy my project:",[11,41198,41199],{},[4339,41200,41201],{},"/home/brian/bin/gunicorn_start",[26,41203,41205],{"className":181,"code":41204,"language":183,"meta":35,"style":35},"#!/bin/bash\n\nNAME=\"django-leaflet-demo\"\nDIR=/home/brian/django-leaflet-demo\nUSER=brian\nGROUP=brian\nWORKERS=3\nBIND=unix:/home/brian/run/gunicorn.sock\nDJANGO_SETTINGS_MODULE=djangoapp.settings\nDJANGO_WSGI_MODULE=djangoapp.wsgi\nLOG_LEVEL=error\ncd $DIR\nsource ../bin/activate\n\nexport DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE\nexport PYTHONPATH=$DIR:$PYTHONPATH\n\nexec ../bin/gunicorn ${DJANGO_WSGI_MODULE}:application \\\n  --name=$NAME \\\n  --workers=$WORKERS \\\n  --user=$USER \\\n  --group=$GROUP \\\n  --bind=$BIND \\\n  --log-level=$LOG_LEVEL \\\n  --log-file=-\n",[33,41206,41207,41211,41215,41225,41235,41245,41254,41264,41274,41284,41294,41304,41312,41319,41323,41335,41347,41351,41367,41377,41387,41397,41407,41417,41427],{"__ignoreMap":35},[187,41208,41209],{"class":189,"line":190},[187,41210,16416],{"class":295},[187,41212,41213],{"class":189,"line":249},[187,41214,316],{"emptyLinePlaceholder":315},[187,41216,41217,41220,41222],{"class":189,"line":312},[187,41218,41219],{"class":577},"NAME",[187,41221,595],{"class":573},[187,41223,41224],{"class":196},"\"django-leaflet-demo\"\n",[187,41226,41227,41230,41232],{"class":189,"line":319},[187,41228,41229],{"class":577},"DIR",[187,41231,595],{"class":573},[187,41233,41234],{"class":196},"/home/brian/django-leaflet-demo\n",[187,41236,41237,41240,41242],{"class":189,"line":325},[187,41238,41239],{"class":577},"USER",[187,41241,595],{"class":573},[187,41243,41244],{"class":196},"brian\n",[187,41246,41247,41250,41252],{"class":189,"line":686},[187,41248,41249],{"class":577},"GROUP",[187,41251,595],{"class":573},[187,41253,41244],{"class":196},[187,41255,41256,41259,41261],{"class":189,"line":697},[187,41257,41258],{"class":577},"WORKERS",[187,41260,595],{"class":573},[187,41262,41263],{"class":196},"3\n",[187,41265,41266,41269,41271],{"class":189,"line":1291},[187,41267,41268],{"class":577},"BIND",[187,41270,595],{"class":573},[187,41272,41273],{"class":196},"unix:/home/brian/run/gunicorn.sock\n",[187,41275,41276,41279,41281],{"class":189,"line":1306},[187,41277,41278],{"class":577},"DJANGO_SETTINGS_MODULE",[187,41280,595],{"class":573},[187,41282,41283],{"class":196},"djangoapp.settings\n",[187,41285,41286,41289,41291],{"class":189,"line":1434},[187,41287,41288],{"class":577},"DJANGO_WSGI_MODULE",[187,41290,595],{"class":573},[187,41292,41293],{"class":196},"djangoapp.wsgi\n",[187,41295,41296,41299,41301],{"class":189,"line":2599},[187,41297,41298],{"class":577},"LOG_LEVEL",[187,41300,595],{"class":573},[187,41302,41303],{"class":196},"error\n",[187,41305,41306,41309],{"class":189,"line":2607},[187,41307,41308],{"class":588},"cd",[187,41310,41311],{"class":577}," $DIR\n",[187,41313,41314,41316],{"class":189,"line":2621},[187,41315,10184],{"class":588},[187,41317,41318],{"class":196}," ../bin/activate\n",[187,41320,41321],{"class":189,"line":2631},[187,41322,316],{"emptyLinePlaceholder":315},[187,41324,41325,41327,41330,41332],{"class":189,"line":2642},[187,41326,6371],{"class":573},[187,41328,41329],{"class":577}," DJANGO_SETTINGS_MODULE",[187,41331,595],{"class":573},[187,41333,41334],{"class":577},"$DJANGO_SETTINGS_MODULE\n",[187,41336,41337,41339,41342,41344],{"class":189,"line":2653},[187,41338,6371],{"class":573},[187,41340,41341],{"class":577}," PYTHONPATH",[187,41343,595],{"class":573},[187,41345,41346],{"class":577},"$DIR:$PYTHONPATH\n",[187,41348,41349],{"class":189,"line":2665},[187,41350,316],{"emptyLinePlaceholder":315},[187,41352,41353,41356,41359,41362,41365],{"class":189,"line":2674},[187,41354,41355],{"class":588},"exec",[187,41357,41358],{"class":196}," ../bin/gunicorn",[187,41360,41361],{"class":577}," ${DJANGO_WSGI_MODULE}",[187,41363,41364],{"class":196},":application",[187,41366,16644],{"class":588},[187,41368,41369,41372,41375],{"class":189,"line":2684},[187,41370,41371],{"class":588},"  --name=",[187,41373,41374],{"class":577},"$NAME",[187,41376,16644],{"class":588},[187,41378,41379,41382,41385],{"class":189,"line":2694},[187,41380,41381],{"class":588},"  --workers=",[187,41383,41384],{"class":577},"$WORKERS",[187,41386,16644],{"class":588},[187,41388,41389,41392,41395],{"class":189,"line":2706},[187,41390,41391],{"class":588},"  --user=",[187,41393,41394],{"class":577},"$USER",[187,41396,16644],{"class":588},[187,41398,41399,41402,41405],{"class":189,"line":2715},[187,41400,41401],{"class":588},"  --group=",[187,41403,41404],{"class":577},"$GROUP",[187,41406,16644],{"class":588},[187,41408,41409,41412,41415],{"class":189,"line":2725},[187,41410,41411],{"class":588},"  --bind=",[187,41413,41414],{"class":577},"$BIND",[187,41416,16644],{"class":588},[187,41418,41419,41422,41425],{"class":189,"line":2735},[187,41420,41421],{"class":588},"  --log-level=",[187,41423,41424],{"class":577},"$LOG_LEVEL",[187,41426,16644],{"class":588},[187,41428,41429],{"class":189,"line":2743},[187,41430,41431],{"class":588},"  --log-file=-\n",[11,41433,41434],{},[4339,41435,41436],{},"/etc/nginx/sites-available",[26,41438,41440],{"className":8656,"code":41439,"language":8658,"meta":35,"style":35},"upstream app_server {\nserver unix:/home/brian/run/gunicorn.sock fail_timeout=0;\n}\n\nserver {\nlisten 80;\nserver_name 159.89.235.193\nkeepalive_timeout 5;\nclient_max_body_size 4G;\naccess_log /home/brian/logs/nginx-access.log;\nerror_log /home/brian/logs/nginx-error.log;\n\nlocation /static/ {\nalias /home/brian/static/;\n}\n\nlocation / {\ntry_files $uri @proxy_to_app;\n}\n\nlocation @proxy_to_app {\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header Host $http_host;\nproxy_redirect off;\nproxy_pass http://app_server;\n}\n}\n",[33,41441,41442,41447,41452,41456,41460,41465,41470,41475,41480,41485,41490,41495,41499,41504,41509,41513,41517,41522,41527,41531,41535,41540,41545,41550,41555,41560,41564],{"__ignoreMap":35},[187,41443,41444],{"class":189,"line":190},[187,41445,41446],{"class":196},"upstream app_server {\n",[187,41448,41449],{"class":189,"line":249},[187,41450,41451],{"class":196},"server unix:/home/brian/run/gunicorn.sock fail_timeout=0;\n",[187,41453,41454],{"class":189,"line":312},[187,41455,1309],{"class":577},[187,41457,41458],{"class":189,"line":319},[187,41459,316],{"emptyLinePlaceholder":315},[187,41461,41462],{"class":189,"line":325},[187,41463,41464],{"class":196},"server {\n",[187,41466,41467],{"class":189,"line":686},[187,41468,41469],{"class":196},"listen 80;\n",[187,41471,41472],{"class":189,"line":697},[187,41473,41474],{"class":196},"server_name 159.89.235.193\n",[187,41476,41477],{"class":189,"line":1291},[187,41478,41479],{"class":196},"keepalive_timeout 5;\n",[187,41481,41482],{"class":189,"line":1306},[187,41483,41484],{"class":196},"client_max_body_size 4G;\n",[187,41486,41487],{"class":189,"line":1434},[187,41488,41489],{"class":196},"access_log /home/brian/logs/nginx-access.log;\n",[187,41491,41492],{"class":189,"line":2599},[187,41493,41494],{"class":196},"error_log /home/brian/logs/nginx-error.log;\n",[187,41496,41497],{"class":189,"line":2607},[187,41498,316],{"emptyLinePlaceholder":315},[187,41500,41501],{"class":189,"line":2621},[187,41502,41503],{"class":196},"location /static/ {\n",[187,41505,41506],{"class":189,"line":2631},[187,41507,41508],{"class":196},"alias /home/brian/static/;\n",[187,41510,41511],{"class":189,"line":2642},[187,41512,1309],{"class":577},[187,41514,41515],{"class":189,"line":2653},[187,41516,316],{"emptyLinePlaceholder":315},[187,41518,41519],{"class":189,"line":2665},[187,41520,41521],{"class":196},"location / {\n",[187,41523,41524],{"class":189,"line":2674},[187,41525,41526],{"class":196},"try_files $uri @proxy_to_app;\n",[187,41528,41529],{"class":189,"line":2684},[187,41530,1309],{"class":577},[187,41532,41533],{"class":189,"line":2694},[187,41534,316],{"emptyLinePlaceholder":315},[187,41536,41537],{"class":189,"line":2706},[187,41538,41539],{"class":196},"location @proxy_to_app {\n",[187,41541,41542],{"class":189,"line":2715},[187,41543,41544],{"class":196},"proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",[187,41546,41547],{"class":189,"line":2725},[187,41548,41549],{"class":196},"proxy_set_header Host $http_host;\n",[187,41551,41552],{"class":189,"line":2735},[187,41553,41554],{"class":196},"proxy_redirect off;\n",[187,41556,41557],{"class":189,"line":2743},[187,41558,41559],{"class":196},"proxy_pass http://app_server;\n",[187,41561,41562],{"class":189,"line":2754},[187,41563,1309],{"class":577},[187,41565,41566],{"class":189,"line":2762},[187,41567,1309],{"class":577},[11,41569,41570],{},"Finally, here is the Supervisor configuration file that runs the gunicorn server:",[26,41572,41575],{"className":41573,"code":41574,"language":31},[29],"[program:django-leaflet-demo]\ncommand=/home/brian/bin/gunicorn_start\nuser=brian\nautostart=true\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/home/brian/logs/gunicorn-error.log\n",[33,41576,41574],{"__ignoreMap":35},[11,41578,41579],{},"I would definitely like to dive into DigitalOcean deployment in more depth in my next post.",[11,41581,41582,41583,41586],{},"For now, you can view the live project on DigitalOcean here: ",[15,41584,38025],{"href":38025,"rel":41585},[19],". In the future I would like to use this Droplet to do more demo apps like this one as I continue to learn the ins and outs of using Django and more frontend tools. Thanks for reading to the end and let me know if you have any comments or critiques on how I went about this project, I would be happy to hear from you!",[855,41588,41589],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .s4XuR, html code.shiki .s4XuR{--shiki-default:#E36209;--shiki-dark:#FFAB70}",{"title":35,"searchDepth":249,"depth":249,"links":41591},[41592,41593,41594,41595,41596,41597,41598,41599,41600,41601],{"id":38022,"depth":312,"text":38027},{"id":38126,"depth":249,"text":38127},{"id":38172,"depth":249,"text":38173},{"id":39051,"depth":249,"text":39052},{"id":39353,"depth":249,"text":39354},{"id":39511,"depth":249,"text":39512},{"id":39942,"depth":249,"text":39943},{"id":40681,"depth":249,"text":40682},{"id":41020,"depth":249,"text":41021},{"id":41179,"depth":249,"text":41180},"2018-02-19","/static/map_homepage.png",{"layout":29014,"disqus_id":41605},"/2018/02/19/leaflet-maps-with-django.html","/2018/02/19/leaflet-maps-with-django",{"title":38012,"description":35},"2018/02/19/leaflet-maps-with-django",[41610,15290,41611,41612,41613,41614],"leaflet","mapbox","data-tables","bootstrap","travis-ci","MQcnGgBsUf_23j3UphEu3S8SRlmJCGDn2iAnG5HPulE",{"id":41617,"title":41618,"body":41619,"comments":315,"date":42197,"description":42198,"draft":872,"extension":873,"external":874,"image":42199,"meta":42200,"navigation":315,"path":42201,"seo":42202,"stem":42203,"tags":42204,"__hash__":42206},"blog/2018/01/30/reading-13f-sec-filings-with-python.md","Reading 13F SEC filings with python",{"type":8,"value":41620,"toc":42194},[41621,41629,41646,41656,41659,41668,41678,41684,41695,41698,41713,41719,41722,41821,41824,41874,41879,41883,41886,41977,41988,42083,42086,42092,42098,42101,42187,42192],[11,41622,41623,41625,41626,41628],{},[338,41624,8468],{},": This project has been updated, please see ",[15,41627,30306],{"href":28634}," to read about the most recent updates.",[916,41630,41631,41636,41641],{},[919,41632,41633,28515],{},[15,41634,28513],{"href":28513,"rel":41635},[19],[919,41637,41638,28521],{},[15,41639,27121],{"href":27121,"rel":41640},[19],[919,41642,41643,28528],{},[15,41644,28526],{"href":28526,"rel":41645},[19],[107,41647,41648],{},[11,41649,41650,41651],{},"The SEC Form 13F is a filing with the Securities and Exchange Commission (SEC) also known as the Information Required of Institutional Investment Managers Form. It is a quarterly filing required of institutional investment managers with over $100 million in qualifying assets. -",[15,41652,41655],{"href":41653,"rel":41654},"https://www.investopedia.com/terms/f/form-13f.asp",[19],"Investopedia",[11,41657,41658],{},"In this article I will show how to collect and parse 13F filing data from the SEC.",[11,41660,41661,41662,41667],{},"First, use ",[15,41663,41666],{"href":41664,"rel":41665},"https://www.sec.gov/edgar/searchedgar/companysearch.html",[19],"EDGAR"," to search the company of interest.",[107,41669,41670],{},[11,41671,41672,41673],{},"EDGAR, the Electronic Data Gathering, Analysis, and Retrieval system, performs automated collection, validation, indexing, acceptance, and forwarding of submissions by companies and others who are required by law to file forms with the U.S. Securities and Exchange Commission (the \"SEC\"). -",[15,41674,41677],{"href":41675,"rel":41676},"https://en.wikipedia.org/wiki/EDGAR",[19],"Wikipedia",[11,41679,41680,41681,752],{},"Click on the Central Index Key (CIK) of the company you are search for, and then click on ",[33,41682,41683],{},"Documents",[11,41685,41686,41687,41690,41691,41694],{},"You'll want to grab the HTML version of the ",[33,41688,41689],{},"Information Table",". I have saved them in a folder with their file names cooresponding to their dates (",[33,41692,41693],{},"YYYY-MM-DD"," format).",[11,41696,41697],{},"For this example, I have manually collected the files for a few years of data filed by a hedge fund. Here are the files I'll be working with:",[26,41699,41701],{"className":1383,"code":41700,"language":1125,"meta":35,"style":35},"files = os.listdir(\"13f/\")\nprint(*sorted(files), sep=\"\\n\")\n",[33,41702,41703,41708],{"__ignoreMap":35},[187,41704,41705],{"class":189,"line":190},[187,41706,41707],{},"files = os.listdir(\"13f/\")\n",[187,41709,41710],{"class":189,"line":249},[187,41711,41712],{},"print(*sorted(files), sep=\"\\n\")\n",[26,41714,41717],{"className":41715,"code":41716,"language":31},[29],"2014-02-14.html\n2014-05-15.html\n2014-08-14.html\n2014-11-14.html\n2015-02-17.html\n2015-05-14.html\n2015-08-14.html\n2015-11-12.html\n2016-02-16.html\n2016-05-16.html\n2016-08-12.html\n2016-11-14.html\n2017-02-14.html\n2017-05-15.html\n2017-08-10.html\n2017-10-30.html\n",[33,41718,41716],{"__ignoreMap":35},[11,41720,41721],{},"Here's a quick script we can use to parse information from each filing document:",[26,41723,41725],{"className":1383,"code":41724,"language":1125,"meta":35,"style":35},"def scrape_13f(file):\n    date = file\n    html = open(\"13f/\"+file).read()\n    soup = BeautifulSoup(html, 'lxml')\n    rows = soup.find_all('tr')[11:]\n    positions = []\n    for row in rows:\n        dic = {}\n        position = row.find_all('td')\n        dic[\"NAME_OF_ISSUER\"] = position[0].text\n        dic[\"TITLE_OF_CLASS\"] = position[1].text\n        dic[\"CUSIP\"] = position[2].text\n        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n        dic[\"DATE\"] = date.strip(\".html\")\n        positions.append(dic)\n\n    df = pd.DataFrame(positions)\n    return df\n",[33,41726,41727,41732,41737,41742,41747,41752,41757,41762,41767,41772,41777,41782,41787,41792,41797,41802,41807,41811,41816],{"__ignoreMap":35},[187,41728,41729],{"class":189,"line":190},[187,41730,41731],{},"def scrape_13f(file):\n",[187,41733,41734],{"class":189,"line":249},[187,41735,41736],{},"    date = file\n",[187,41738,41739],{"class":189,"line":312},[187,41740,41741],{},"    html = open(\"13f/\"+file).read()\n",[187,41743,41744],{"class":189,"line":319},[187,41745,41746],{},"    soup = BeautifulSoup(html, 'lxml')\n",[187,41748,41749],{"class":189,"line":325},[187,41750,41751],{},"    rows = soup.find_all('tr')[11:]\n",[187,41753,41754],{"class":189,"line":686},[187,41755,41756],{},"    positions = []\n",[187,41758,41759],{"class":189,"line":697},[187,41760,41761],{},"    for row in rows:\n",[187,41763,41764],{"class":189,"line":1291},[187,41765,41766],{},"        dic = {}\n",[187,41768,41769],{"class":189,"line":1306},[187,41770,41771],{},"        position = row.find_all('td')\n",[187,41773,41774],{"class":189,"line":1434},[187,41775,41776],{},"        dic[\"NAME_OF_ISSUER\"] = position[0].text\n",[187,41778,41779],{"class":189,"line":2599},[187,41780,41781],{},"        dic[\"TITLE_OF_CLASS\"] = position[1].text\n",[187,41783,41784],{"class":189,"line":2607},[187,41785,41786],{},"        dic[\"CUSIP\"] = position[2].text\n",[187,41788,41789],{"class":189,"line":2621},[187,41790,41791],{},"        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n",[187,41793,41794],{"class":189,"line":2631},[187,41795,41796],{},"        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n",[187,41798,41799],{"class":189,"line":2642},[187,41800,41801],{},"        dic[\"DATE\"] = date.strip(\".html\")\n",[187,41803,41804],{"class":189,"line":2653},[187,41805,41806],{},"        positions.append(dic)\n",[187,41808,41809],{"class":189,"line":2665},[187,41810,316],{"emptyLinePlaceholder":315},[187,41812,41813],{"class":189,"line":2674},[187,41814,41815],{},"    df = pd.DataFrame(positions)\n",[187,41817,41818],{"class":189,"line":2684},[187,41819,41820],{},"    return df\n",[11,41822,41823],{},"Using this function we can get a quick snapshot of this hedge fund by filing total over the last 4 years:",[26,41825,41827],{"className":1383,"code":41826,"language":1125,"meta":35,"style":35},"fund_growth = [sum(scrape_13f(file).VALUE) for file in sorted(files)]\ndates = [f.strip('.html') for f in sorted(files)]\nplt.figure(figsize=(10,5))\nplt.title('Total Fund Size')\nplt.xlabel('Filing Date')\nplt.ylabel('USD')\nplt.bar(dates, fund_growth)\nplt.yticks()\nplt.xticks(rotation='vertical')\n",[33,41828,41829,41834,41839,41844,41849,41854,41859,41864,41869],{"__ignoreMap":35},[187,41830,41831],{"class":189,"line":190},[187,41832,41833],{},"fund_growth = [sum(scrape_13f(file).VALUE) for file in sorted(files)]\n",[187,41835,41836],{"class":189,"line":249},[187,41837,41838],{},"dates = [f.strip('.html') for f in sorted(files)]\n",[187,41840,41841],{"class":189,"line":312},[187,41842,41843],{},"plt.figure(figsize=(10,5))\n",[187,41845,41846],{"class":189,"line":319},[187,41847,41848],{},"plt.title('Total Fund Size')\n",[187,41850,41851],{"class":189,"line":325},[187,41852,41853],{},"plt.xlabel('Filing Date')\n",[187,41855,41856],{"class":189,"line":686},[187,41857,41858],{},"plt.ylabel('USD')\n",[187,41860,41861],{"class":189,"line":697},[187,41862,41863],{},"plt.bar(dates, fund_growth)\n",[187,41865,41866],{"class":189,"line":1291},[187,41867,41868],{},"plt.yticks()\n",[187,41870,41871],{"class":189,"line":1306},[187,41872,41873],{},"plt.xticks(rotation='vertical')\n",[11,41875,41876],{},[511,41877],{"alt":7255,"src":41878},"/static/sec/fund_size.png",[168,41880,41882],{"id":41881},"fund-positions-with-bubble-chart","Fund Positions with Bubble Chart",[11,41884,41885],{},"Next, it would be great to get a snapshot of the stocks owned by this fund in a given year. Let's use a D3 bubble chart. The names for each stock are quite long, so first let's convert them to stock ticker values. Here's a quick script I hacked together using a Fidelity lookup service:",[26,41887,41889],{"className":1383,"code":41888,"language":1125,"meta":35,"style":35},"cusip_nums = set()\nfor file in files:\n    cusip_nums = cusip_nums | set(scrape_13f(file).CUSIP)\n\nticker_dic = {c:\"\" for c in cusip_nums}\nfor c in list(ticker_dic.keys()):\n    url = \"http://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=stock&by=cusip&criteria=\"+c+\"&submit=Search\"\n    html = requests.get(url).text\n    soup = BeautifulSoup(html, 'lxml')\n    ticker_elem = soup.find('tr', attrs={\"bgcolor\":\"#666666\"})\n    ticker = \"\"\n    try:\n        ticker = ticker_elem.next_sibling.next_sibling.find('a').text\n        ticker_dic[c] = ticker\n    except:\n        pass\n\n    time.sleep(1)\n",[33,41890,41891,41896,41901,41906,41910,41915,41920,41925,41930,41934,41939,41944,41948,41953,41958,41963,41968,41972],{"__ignoreMap":35},[187,41892,41893],{"class":189,"line":190},[187,41894,41895],{},"cusip_nums = set()\n",[187,41897,41898],{"class":189,"line":249},[187,41899,41900],{},"for file in files:\n",[187,41902,41903],{"class":189,"line":312},[187,41904,41905],{},"    cusip_nums = cusip_nums | set(scrape_13f(file).CUSIP)\n",[187,41907,41908],{"class":189,"line":319},[187,41909,316],{"emptyLinePlaceholder":315},[187,41911,41912],{"class":189,"line":325},[187,41913,41914],{},"ticker_dic = {c:\"\" for c in cusip_nums}\n",[187,41916,41917],{"class":189,"line":686},[187,41918,41919],{},"for c in list(ticker_dic.keys()):\n",[187,41921,41922],{"class":189,"line":697},[187,41923,41924],{},"    url = \"http://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=stock&by=cusip&criteria=\"+c+\"&submit=Search\"\n",[187,41926,41927],{"class":189,"line":1291},[187,41928,41929],{},"    html = requests.get(url).text\n",[187,41931,41932],{"class":189,"line":1306},[187,41933,41746],{},[187,41935,41936],{"class":189,"line":1434},[187,41937,41938],{},"    ticker_elem = soup.find('tr', attrs={\"bgcolor\":\"#666666\"})\n",[187,41940,41941],{"class":189,"line":2599},[187,41942,41943],{},"    ticker = \"\"\n",[187,41945,41946],{"class":189,"line":2607},[187,41947,5405],{},[187,41949,41950],{"class":189,"line":2621},[187,41951,41952],{},"        ticker = ticker_elem.next_sibling.next_sibling.find('a').text\n",[187,41954,41955],{"class":189,"line":2631},[187,41956,41957],{},"        ticker_dic[c] = ticker\n",[187,41959,41960],{"class":189,"line":2642},[187,41961,41962],{},"    except:\n",[187,41964,41965],{"class":189,"line":2653},[187,41966,41967],{},"        pass\n",[187,41969,41970],{"class":189,"line":2665},[187,41971,316],{"emptyLinePlaceholder":315},[187,41973,41974],{"class":189,"line":2674},[187,41975,41976],{},"    time.sleep(1)\n",[11,41978,41979,41980,41983,41984,41987],{},"I couldn't get all the CUSIP numbers, but I was able to get most of them. Some of the CUSIP numbers have changed for certain stocks and couldn't be looked up with this service. For now I won't fill these in. With the ",[33,41981,41982],{},"ticker_dic"," dictionary, we can make a quick edit to our ",[33,41985,41986],{},"scrape_13f"," function to populate ticker data for each holding:",[26,41989,41991],{"className":1383,"code":41990,"language":1125,"meta":35,"style":35},"ticker_dict = {'00206R102': 'T', '00507V109': 'ATVI', '00724F101': 'ADBE', ... }\n\ndef scrape_13f(file):\n    date = file\n    html = open(\"13f/\"+file).read()\n    soup = BeautifulSoup(html, 'lxml')\n    rows = soup.find_all('tr')[11:]\n    positions = []\n    for row in rows:\n        dic = {}\n        position = row.find_all('td')\n        dic[\"NAME_OF_ISSUER\"] = position[0].text\n        dic[\"TITLE_OF_CLASS\"] = position[1].text\n        dic[\"CUSIP\"] = position[2].text\n        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n        dic[\"DATE\"] = date.strip(\".html\")\n        dic[\"TICKER\"] = ticker_dict[position[2].text]\n        positions.append(dic)\n\n    df = pd.DataFrame(positions)\n    return df\n",[33,41992,41993,41998,42002,42006,42010,42014,42018,42022,42026,42030,42034,42038,42042,42046,42050,42054,42058,42062,42067,42071,42075,42079],{"__ignoreMap":35},[187,41994,41995],{"class":189,"line":190},[187,41996,41997],{},"ticker_dict = {'00206R102': 'T', '00507V109': 'ATVI', '00724F101': 'ADBE', ... }\n",[187,41999,42000],{"class":189,"line":249},[187,42001,316],{"emptyLinePlaceholder":315},[187,42003,42004],{"class":189,"line":312},[187,42005,41731],{},[187,42007,42008],{"class":189,"line":319},[187,42009,41736],{},[187,42011,42012],{"class":189,"line":325},[187,42013,41741],{},[187,42015,42016],{"class":189,"line":686},[187,42017,41746],{},[187,42019,42020],{"class":189,"line":697},[187,42021,41751],{},[187,42023,42024],{"class":189,"line":1291},[187,42025,41756],{},[187,42027,42028],{"class":189,"line":1306},[187,42029,41761],{},[187,42031,42032],{"class":189,"line":1434},[187,42033,41766],{},[187,42035,42036],{"class":189,"line":2599},[187,42037,41771],{},[187,42039,42040],{"class":189,"line":2607},[187,42041,41776],{},[187,42043,42044],{"class":189,"line":2621},[187,42045,41781],{},[187,42047,42048],{"class":189,"line":2631},[187,42049,41786],{},[187,42051,42052],{"class":189,"line":2642},[187,42053,41791],{},[187,42055,42056],{"class":189,"line":2653},[187,42057,41796],{},[187,42059,42060],{"class":189,"line":2665},[187,42061,41801],{},[187,42063,42064],{"class":189,"line":2674},[187,42065,42066],{},"        dic[\"TICKER\"] = ticker_dict[position[2].text]\n",[187,42068,42069],{"class":189,"line":2684},[187,42070,41806],{},[187,42072,42073],{"class":189,"line":2694},[187,42074,316],{"emptyLinePlaceholder":315},[187,42076,42077],{"class":189,"line":2706},[187,42078,41815],{},[187,42080,42081],{"class":189,"line":2715},[187,42082,41820],{},[11,42084,42085],{},"Let's check this:",[26,42087,42090],{"className":42088,"code":42089,"language":31},[29],"df = scrape_13f(files[2])\nprint(df[[\"CUSIP\", \"NAME_OF_ISSUER\", \"TICKER\"]].head())\n",[33,42091,42089],{"__ignoreMap":35},[26,42093,42096],{"className":42094,"code":42095,"language":31},[29],"       CUSIP         NAME_OF_ISSUER TICKER\n0  88579Y101                  3M CO    MMM\n1  G1151C101  ACCENTURE PLC IRELAND    ACN\n2  02209S103       ALTRIA GROUP INC     MO\n3  03076C106    AMERIPRISE FINL INC    AMP\n4  035710409    ANNALY CAP MGMT INC    NLY\n",[33,42097,42095],{"__ignoreMap":35},[11,42099,42100],{},"Let's take a look at the last filing, Q4 2017.",[26,42102,42104],{"className":1383,"code":42103,"language":1125,"meta":35,"style":35},"q4_2017 = sorted(files)[-1]\ndf_q4_2017 = scrape_13f(q4_2017)\n\ntop_20 = df_q4_2017.sort_values(by=\"VALUE\", ascending=False)[[\"TICKER\", \"VALUE\"]][:40]\na = top_20.TICKER\nb = top_20.VALUE\nc = range(len(b))\n\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111)\nax.bar(c, b)\n\nplt.xticks(c, a, rotation=90)\nplt.title('Top 40 Stock Holdings by Value')\nplt.xlabel('Stock Ticker')\nplt.ylabel('USD (10 MM))')\nplt.show()\n",[33,42105,42106,42111,42116,42120,42125,42130,42135,42140,42144,42149,42154,42159,42163,42168,42173,42178,42183],{"__ignoreMap":35},[187,42107,42108],{"class":189,"line":190},[187,42109,42110],{},"q4_2017 = sorted(files)[-1]\n",[187,42112,42113],{"class":189,"line":249},[187,42114,42115],{},"df_q4_2017 = scrape_13f(q4_2017)\n",[187,42117,42118],{"class":189,"line":312},[187,42119,316],{"emptyLinePlaceholder":315},[187,42121,42122],{"class":189,"line":319},[187,42123,42124],{},"top_20 = df_q4_2017.sort_values(by=\"VALUE\", ascending=False)[[\"TICKER\", \"VALUE\"]][:40]\n",[187,42126,42127],{"class":189,"line":325},[187,42128,42129],{},"a = top_20.TICKER\n",[187,42131,42132],{"class":189,"line":686},[187,42133,42134],{},"b = top_20.VALUE\n",[187,42136,42137],{"class":189,"line":697},[187,42138,42139],{},"c = range(len(b))\n",[187,42141,42142],{"class":189,"line":1291},[187,42143,316],{"emptyLinePlaceholder":315},[187,42145,42146],{"class":189,"line":1306},[187,42147,42148],{},"fig = plt.figure(figsize=(15,5))\n",[187,42150,42151],{"class":189,"line":1434},[187,42152,42153],{},"ax = fig.add_subplot(111)\n",[187,42155,42156],{"class":189,"line":2599},[187,42157,42158],{},"ax.bar(c, b)\n",[187,42160,42161],{"class":189,"line":2607},[187,42162,316],{"emptyLinePlaceholder":315},[187,42164,42165],{"class":189,"line":2621},[187,42166,42167],{},"plt.xticks(c, a, rotation=90)\n",[187,42169,42170],{"class":189,"line":2631},[187,42171,42172],{},"plt.title('Top 40 Stock Holdings by Value')\n",[187,42174,42175],{"class":189,"line":2642},[187,42176,42177],{},"plt.xlabel('Stock Ticker')\n",[187,42179,42180],{"class":189,"line":2653},[187,42181,42182],{},"plt.ylabel('USD (10 MM))')\n",[187,42184,42185],{"class":189,"line":2665},[187,42186,26147],{},[11,42188,42189],{},[511,42190],{"alt":7255,"src":42191},"/static/sec/2017_filing.png",[855,42193,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":42195},[42196],{"id":41881,"depth":249,"text":41882},"2018-01-30","How to read SEC filing data with Python","/static/sec/sec.jpg",{"layout":29014},"/2018/01/30/reading-13f-sec-filings-with-python",{"title":41618,"description":42198},"2018/01/30/reading-13f-sec-filings-with-python",[42205,1125,582,27051],"sec","D6TNxFV67Xh4BsDCS5kZQv22BbqytCEaC26V9sAMfks",{"id":42208,"title":42209,"body":42210,"comments":315,"date":42581,"description":42214,"draft":872,"extension":873,"external":874,"image":42582,"meta":42583,"navigation":315,"path":42585,"seo":42586,"stem":42587,"tags":42588,"__hash__":42591},"blog/2018/01/28/simple-sql-tasks-with-mariadb.md","How to do simple SQL tasks with MariaDB",{"type":8,"value":42211,"toc":42572},[42212,42215,42218,42226,42230,42233,42236,42247,42252,42263,42267,42275,42279,42286,42292,42300,42303,42309,42315,42321,42336,42340,42347,42353,42356,42362,42368,42374,42377,42380,42382,42419,42425,42428,42430,42492,42498,42501,42503,42553,42559,42562,42567,42570],[11,42213,42214],{},"In this article I'll look at another interview takehome assignment. This assignment included SQL, python and some logic puzzles. If you read to the end I'll share a fun \"trick question\" that was also included.",[11,42216,42217],{},"I mostly want to talk about the SQL portion of the test. Since I mostly work with postgresql through the Django ORM, this was a good refresher. I'll show how I approached some simple SQL tasks, and how to use MariaDB.",[107,42219,42220],{},[11,42221,42222,42225],{},[338,42223,42224],{},"MariaDB"," is the default implementation of MySQL in Arch Linux, provided with the mariadb package.",[168,42227,42229],{"id":42228},"the-task","The Task",[11,42231,42232],{},"Here are the three questions I was asked:",[11,42234,42235],{},"Please reference the 3 tables below.",[2276,42237,42238,42241,42244],{},[919,42239,42240],{},"Using SQL please write the code to generate a table that includes all individuals on the file and contains the following fields: ID, Congressional District, and Gender",[919,42242,42243],{},"Using SQL please write the code to generate a table that includes only individuals with a gender on file that have a DistrictID of 3. Also please convert the values for Gender from “M” and “F” to “Male” and “Female” respectively. Your final table should include only ID and the converted Gender.",[919,42245,42246],{},"Using SQL please generate the code to run a count of gender by Congressional District.",[11,42248,42249],{},[511,42250],{"alt":7255,"src":42251},"/static/sql.png",[916,42253,42254,42257,42260],{},[919,42255,42256],{},"ID is a unique ID that is applied to each individual on file",[919,42258,42259],{},"DistrictID is a unique ID that is applied to each district on file",[919,42261,42262],{},"Gender is a value that is recorded on some records on file",[911,42264,42266],{"id":42265},"install-mysqlmariadb","Install MySQL/MariaDB",[11,42268,42269,42270,752],{},"First, we need to install MariaDB. As usual, just ",[15,42271,42274],{"href":42272,"rel":42273},"https://wiki.archlinux.org/index.php/MySQL",[19],"follow the Arch Wiki",[911,42276,42278],{"id":42277},"run-mysql","Run mysql",[11,42280,42281,42282,42285],{},"Next, we can run ",[33,42283,42284],{},"mysql"," in the terminal:",[26,42287,42290],{"className":42288,"code":42289,"language":31},[29]," $ mysql\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 18\nServer version: 10.1.30-MariaDB MariaDB Server\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n",[33,42291,42289],{"__ignoreMap":35},[911,42293,42295,42296,42299],{"id":42294},"create-and-set-a-database-to-use","Create ",[4339,42297,42298],{},"and set"," a database to use",[11,42301,42302],{},"The next step is to create a database. We can do the following:",[26,42304,42307],{"className":42305,"code":42306,"language":31},[29],"MariaDB [(none)]> create database sample_db default character set utf8 default collate utf8_bin;\nERROR 1044 (42000): Access denied for user ''@'localhost' to database 'sample_db'\nMariaDB [(none)]> Ctrl-C -- exit!\nAborted\n",[33,42308,42306],{"__ignoreMap":35},[11,42310,42311,42312,42314],{},"If you see this error, it is because we aren't using the correct user for SQL. We can use the default root user with no password instead. So here we will rerun the ",[33,42313,42284],{}," command with additional arguments:",[26,42316,42319],{"className":42317,"code":42318,"language":31},[29]," $ mysql -u root\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 19\nServer version: 10.1.30-MariaDB MariaDB Server\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]> create database sample_db default character set utf8 default collate utf8_bin;\nQuery OK, 1 row affected (0.01 sec)\n\nMariaDB [(none)]> use sample_db\nDatabase changed\nMariaDB [sample_db]>\n",[33,42320,42318],{"__ignoreMap":35},[11,42322,42323,42324,42327,42328,42331,42332,42335],{},"Great, no we are using the blank database called ",[33,42325,42326],{},"sample_db"," and we are using it, notice that ",[33,42329,42330],{},"[sample_db]>"," has repaced ",[33,42333,42334],{},"[(none)]>"," in the MariaDB prompt.",[911,42337,42339],{"id":42338},"create-tables-and-insert-data-into-the-tables","Create tables and insert data into the tables",[11,42341,42342,42343,42346],{},"Now we are read to get going with our questions. Before we write our queries, we need to get the data from the question into our database. To do this I wrote ",[33,42344,42345],{},"insert.sql"," and ran it in MariaDB. Here is the script:",[26,42348,42351],{"className":42349,"code":42350,"language":31},[29],"CREATE TABLE table_A (\n  ID int,\n  District_ID int\n  );\n\nCREATE TABLE table_B (\n  District_ID int,\n  Congressional_District int\n  );\n\nCREATE TABLE table_C (\n  ID int,\n  Gender CHAR(1)\n  );\n\nINSERT INTO table_A (ID, District_ID) VALUES ('1', '3');\nINSERT INTO table_A (ID, District_ID) VALUES ('2', '3');\nINSERT INTO table_A (ID, District_ID) VALUES ('3', '4');\nINSERT INTO table_A (ID, District_ID) VALUES ('4', '4');\nINSERT INTO table_A (ID, District_ID) VALUES ('5', '3');\nINSERT INTO table_A (ID, District_ID) VALUES ('6', '4');\nINSERT INTO table_A (ID, District_ID) VALUES ('7', '4');\nINSERT INTO table_A (ID, District_ID) VALUES ('8', '3');\nINSERT INTO table_A (ID, District_ID) VALUES ('9', '4');\nINSERT INTO table_A (ID, District_ID) VALUES ('10', '3');\n\nINSERT INTO table_B (District_ID, Congressional_District) VALUES ('1', '8');\nINSERT INTO table_B (District_ID, Congressional_District) VALUES ('2', '2');\nINSERT INTO table_B (District_ID, Congressional_District) VALUES ('3', '14');\nINSERT INTO table_B (District_ID, Congressional_District) VALUES ('4', '7');\nINSERT INTO table_B (District_ID, Congressional_District) VALUES ('5', '11');\n\nINSERT INTO table_C (ID, Gender) VALUES ('1', 'M');\nINSERT INTO table_C (ID, Gender) VALUES ('3', 'F');\nINSERT INTO table_C (ID, Gender) VALUES ('4', 'M');\nINSERT INTO table_C (ID, Gender) VALUES ('5', 'F');\nINSERT INTO table_C (ID, Gender) VALUES ('6', 'F');\nINSERT INTO table_C (ID, Gender) VALUES ('7', 'F');\nINSERT INTO table_C (ID, Gender) VALUES ('8', 'M');\nINSERT INTO table_C (ID, Gender) VALUES ('9', 'F');\nINSERT INTO table_C (ID, Gender) VALUES ('10', 'M');\n",[33,42352,42350],{"__ignoreMap":35},[11,42354,42355],{},"Now we can run the script with the following command:",[26,42357,42360],{"className":42358,"code":42359,"language":31},[29],"MariaDB [sample_db]> source my_sql_script.sql\nQuery OK, 0 rows affected (0.08 sec)\n\nQuery OK, 0 rows affected (0.07 sec)\n\nQuery OK, 0 rows affected (0.06 sec)\n\nQuery OK, 1 row affected (0.02 sec)\n\nQuery OK, 1 row affected (0.00 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.03 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.00 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.02 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nQuery OK, 1 row affected (0.01 sec)\n\nMariaDB [sample_db]>\n",[33,42361,42359],{"__ignoreMap":35},[11,42363,42364,42365,18410],{},"Great, now let's test it out with a simple ",[33,42366,42367],{},"select * from table_name",[26,42369,42372],{"className":42370,"code":42371,"language":31},[29],"MariaDB [sample_db]> select * from table_A;\n+------+-------------+\n| ID   | District_ID |\n+------+-------------+\n|    1 |           3 |\n|    2 |           3 |\n|    3 |           4 |\n|    4 |           4 |\n|    5 |           3 |\n|    6 |           4 |\n|    7 |           4 |\n|    8 |           3 |\n|    9 |           4 |\n|   10 |           3 |\n+------+-------------+\n10 rows in set (0.00 sec)\n",[33,42373,42371],{"__ignoreMap":35},[11,42375,42376],{},"This matches what we were given, so now let's move on to the first task, creating one table from the three we are given.",[34786,42378,15625],{"id":42379},"_1",[11,42381,42240],{},[26,42383,42387],{"className":42384,"code":42385,"language":42386,"meta":35,"style":35},"language-sql shiki shiki-themes github-light github-dark","select table_A.ID, Congressional_District, Gender\nfrom table_A\n  left join table_B\n    on table_A.District_ID = table_B.District_ID\n  left join table_C\n    on table_A.ID = table_C.ID order by ID;\n","sql",[33,42388,42389,42394,42399,42404,42409,42414],{"__ignoreMap":35},[187,42390,42391],{"class":189,"line":190},[187,42392,42393],{},"select table_A.ID, Congressional_District, Gender\n",[187,42395,42396],{"class":189,"line":249},[187,42397,42398],{},"from table_A\n",[187,42400,42401],{"class":189,"line":312},[187,42402,42403],{},"  left join table_B\n",[187,42405,42406],{"class":189,"line":319},[187,42407,42408],{},"    on table_A.District_ID = table_B.District_ID\n",[187,42410,42411],{"class":189,"line":325},[187,42412,42413],{},"  left join table_C\n",[187,42415,42416],{"class":189,"line":686},[187,42417,42418],{},"    on table_A.ID = table_C.ID order by ID;\n",[26,42420,42423],{"className":42421,"code":42422,"language":31},[29],"+------+------------------------+--------+\n| ID   | Congressional_District | Gender |\n+------+------------------------+--------+\n|    1 |                     14 | M      |\n|    2 |                     14 | NULL   |\n|    3 |                      7 | F      |\n|    4 |                      7 | M      |\n|    5 |                     14 | F      |\n|    6 |                      7 | F      |\n|    7 |                      7 | F      |\n|    8 |                     14 | M      |\n|    9 |                      7 | F      |\n|   10 |                     14 | M      |\n+------+------------------------+--------+\n10 rows in set (0.00 sec)\n",[33,42424,42422],{"__ignoreMap":35},[34786,42426,22129],{"id":42427},"_2",[11,42429,42243],{},[26,42431,42433],{"className":42384,"code":42432,"language":42386,"meta":35,"style":35},"select\n  table_A.ID,\n  case\n    when Gender = \"M\" then \"Male\"\n    when Gender = \"F\" then \"Female\"\n  end as Gender\nfrom table_A\n  left join table_B\n    on table_A.District_ID = table_B.District_ID\n  left join table_C on table_A.ID = table_C.ID\nwhere table_A.District_ID = 3 and table_C.Gender is not null\norder by ID;\n",[33,42434,42435,42440,42445,42450,42455,42460,42465,42469,42473,42477,42482,42487],{"__ignoreMap":35},[187,42436,42437],{"class":189,"line":190},[187,42438,42439],{},"select\n",[187,42441,42442],{"class":189,"line":249},[187,42443,42444],{},"  table_A.ID,\n",[187,42446,42447],{"class":189,"line":312},[187,42448,42449],{},"  case\n",[187,42451,42452],{"class":189,"line":319},[187,42453,42454],{},"    when Gender = \"M\" then \"Male\"\n",[187,42456,42457],{"class":189,"line":325},[187,42458,42459],{},"    when Gender = \"F\" then \"Female\"\n",[187,42461,42462],{"class":189,"line":686},[187,42463,42464],{},"  end as Gender\n",[187,42466,42467],{"class":189,"line":697},[187,42468,42398],{},[187,42470,42471],{"class":189,"line":1291},[187,42472,42403],{},[187,42474,42475],{"class":189,"line":1306},[187,42476,42408],{},[187,42478,42479],{"class":189,"line":1434},[187,42480,42481],{},"  left join table_C on table_A.ID = table_C.ID\n",[187,42483,42484],{"class":189,"line":2599},[187,42485,42486],{},"where table_A.District_ID = 3 and table_C.Gender is not null\n",[187,42488,42489],{"class":189,"line":2607},[187,42490,42491],{},"order by ID;\n",[26,42493,42496],{"className":42494,"code":42495,"language":31},[29],"+------+--------+\n| ID   | Gender |\n+------+--------+\n|    1 | Male   |\n|    5 | Female |\n|    8 | Male   |\n|   10 | Male   |\n+------+--------+\n4 rows in set (0.00 sec)\n",[33,42497,42495],{"__ignoreMap":35},[34786,42499,34913],{"id":42500},"_3",[11,42502,42246],{},[26,42504,42506],{"className":42384,"code":42505,"language":42386,"meta":35,"style":35},"select\n  count(table_C.Gender) Count,\n  Congressional_District,\n  Gender\nfrom table_A\n  left join table_B\n    on table_A.District_ID = table_B.District_ID\n  left join table_C\n    on table_A.ID = table_C.ID\ngroup by Congressional_District, Gender;\n",[33,42507,42508,42512,42517,42522,42527,42531,42535,42539,42543,42548],{"__ignoreMap":35},[187,42509,42510],{"class":189,"line":190},[187,42511,42439],{},[187,42513,42514],{"class":189,"line":249},[187,42515,42516],{},"  count(table_C.Gender) Count,\n",[187,42518,42519],{"class":189,"line":312},[187,42520,42521],{},"  Congressional_District,\n",[187,42523,42524],{"class":189,"line":319},[187,42525,42526],{},"  Gender\n",[187,42528,42529],{"class":189,"line":325},[187,42530,42398],{},[187,42532,42533],{"class":189,"line":686},[187,42534,42403],{},[187,42536,42537],{"class":189,"line":697},[187,42538,42408],{},[187,42540,42541],{"class":189,"line":1291},[187,42542,42413],{},[187,42544,42545],{"class":189,"line":1306},[187,42546,42547],{},"    on table_A.ID = table_C.ID\n",[187,42549,42550],{"class":189,"line":1434},[187,42551,42552],{},"group by Congressional_District, Gender;\n",[26,42554,42557],{"className":42555,"code":42556,"language":31},[29],"+-------+------------------------+--------+\n| Count | Congressional_District | Gender |\n+-------+------------------------+--------+\n|     4 |                      7 | F      |\n|     1 |                      7 | M      |\n|     0 |                     14 | NULL   |\n|     1 |                     14 | F      |\n|     3 |                     14 | M      |\n+-------+------------------------+--------+\n5 rows in set (0.00 sec)\n",[33,42558,42556],{"__ignoreMap":35},[11,42560,42561],{},"That's it for the three SQL tasks. Here's the bonus question:",[107,42563,42564],{},[11,42565,42566],{},"Assume there are 6,000,000,000 (6 billion) people on Earth. What would you estimate to be the result, if you multiply together the number of fingers on every person's left-hands? (For the purposes of this exercise, thumbs count as fingers.)",[11,42568,42569],{},"Think about it for a minute. I have hidden my answer at the end of the URL for this article.",[855,42571,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":42573},[42574],{"id":42228,"depth":249,"text":42229,"children":42575},[42576,42577,42578,42580],{"id":42265,"depth":312,"text":42266},{"id":42277,"depth":312,"text":42278},{"id":42294,"depth":312,"text":42579},"Create and set a database to use",{"id":42338,"depth":312,"text":42339},"2018-01-28","/static/mariadb.png",{"layout":29014,"permalink":42584},"/2018/01/28/how-to-do-simple-sql-tasks-with-mariadb-0","/2018/01/28/simple-sql-tasks-with-mariadb",{"title":42209,"description":42214},"2018/01/28/simple-sql-tasks-with-mariadb",[42589,42386,42590],"maria-db","databases","dmmqMTlVioFfCNikBHkA9lAeJ5bsC5llxzjlPI4g-2g",{"id":42593,"title":42594,"body":42595,"comments":315,"date":43504,"description":35,"draft":872,"extension":873,"external":874,"image":42601,"meta":43505,"navigation":315,"path":43506,"seo":43507,"stem":43508,"tags":43509,"__hash__":43511},"blog/2018/01/02/checking-poker-hands-with-python.md","Finding the best poker hand in five-card draw with python",{"type":8,"value":42596,"toc":43498},[42597,42602,42605,42609,42628,42632,42638,42643,42646,42651,42654,42660,42664,42667,42752,42756,42759,42762,42771,42780,42783,42787,42821,42824,42831,42838,42901,42908,42912,42915,43258,43263,43267,43270,43284,43376,43380,43383,43411,43414,43420,43423,43429,43433,43447,43450,43453,43459,43468,43493,43496],[11,42598,42599],{},[511,42600],{"alt":7255,"src":42601},"/static/poker.jpg",[11,42603,42604],{},"I recently took a Hackerrank challenge for a job application that involved poker. I'm not a poker player, so I had a brief moment of panic as I read over the problem the description. In this article I want to do some reflection on how I approached the problem.",[2215,42606,42608],{"id":42607},"the-problem","The Problem",[11,42610,42611,42612,42617,42618,42620,42621,42624,42625,42627],{},"The hackerrank question asked me to write a program that would determine the best poker hand possible in ",[15,42613,42616],{"href":42614,"rel":42615},"https://en.wikipedia.org/wiki/Five-card_draw",[19],"five-card draw"," poker. We are given 10 cards, the first 5 are the current hand, and the second 5 are the next five cards in the deck. We assume that we can see the next five cards (they are not hidden). We want to exchange any ",[33,42619,10979],{}," number of cards (where ",[33,42622,42623],{},"n \u003C= 5",") in our hand for the next ",[33,42626,10979],{}," cards in the deck. For example, we can take out any combination of 2 cards from the hand we are given, but we must replace these two cards with the next two cards from the deck (we can't pick any two cards from the deck).",[2215,42629,42631],{"id":42630},"evaluating-hands","Evaluating hands",[11,42633,42634,42635,752],{},"Suit and value make up the value of playing cards. For example, you can have a 3 of clubs. 3 is the value, clubs is the suit. We can represent this as ",[33,42636,42637],{},"3C",[11,42639,42640],{},[338,42641,42642],{},"Suits",[11,42644,42645],{},"Clubs C\nSpades S\nHeart H\nDiamonds D",[11,42647,42648],{},[338,42649,42650],{},"Value (Rank)",[11,42652,42653],{},"2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace",[26,42655,42658],{"className":42656,"code":42657,"language":31},[29],"values = {\"2\":2, \"3\":3, \"4\":4, \"5\":5, \"6\":6, \"7\":7, \"8\":8, \"9\":9, \"10\":10, \"J\":11, \"Q\":12, \"K\":13, \"A\":14}\n",[33,42659,42657],{"__ignoreMap":35},[168,42661,42663],{"id":42662},"hands","Hands",[11,42665,42666],{},"Here are the hands of poker",[2276,42668,42669,42677,42688,42696,42704,42712,42720,42728,42736,42744],{},[919,42670,42671,42672],{},"Royal flush (the problem didn't ask me to consider Royal Flush)",[107,42673,42674],{},[11,42675,42676],{},"A, K, Q, J, 10, all the same suit.",[919,42678,42679,42680],{},"Straight flush",[107,42681,42682],{},[11,42683,42684,42685],{},"Five cards in a sequence, all in the same suit. ",[338,42686,42687],{},"Ace can either come before 2 or come after King.",[919,42689,42690,42691],{},"Four of a kind",[107,42692,42693],{},[11,42694,42695],{},"All four cards of the same rank.",[919,42697,42698,42699],{},"Full house",[107,42700,42701],{},[11,42702,42703],{},"Three of a kind with a pair.",[919,42705,42706,42707],{},"Flush",[107,42708,42709],{},[11,42710,42711],{},"Any five cards of the same suit, but not in a sequence.",[919,42713,42714,42715],{},"Straight",[107,42716,42717],{},[11,42718,42719],{},"Five cards in a sequence, but not of the same suit.",[919,42721,42722,42723],{},"Three of a kind",[107,42724,42725],{},[11,42726,42727],{},"Three cards of the same rank.",[919,42729,42730,42731],{},"Two pair",[107,42732,42733],{},[11,42734,42735],{},"Two different pairs.",[919,42737,42738,42739],{},"Pair",[107,42740,42741],{},[11,42742,42743],{},"Two cards of the same rank.",[919,42745,42746,42747],{},"High Card",[107,42748,42749],{},[11,42750,42751],{},"When you haven't made any of the hands above, the highest card plays.\nIn the example below, the jack plays as the highest card.",[168,42753,42755],{"id":42754},"evaluating-a-hand-of-cards","Evaluating a hand of cards",[11,42757,42758],{},"A hand is five cards. The first thing I did was write out functions to evaluate if a group of 5 cards satisfies the conditions of one of the ten hands.",[11,42760,42761],{},"Here's a sample hand:",[26,42763,42765],{"className":1383,"code":42764,"language":1125,"meta":35,"style":35},"hand = [\"3S\", \"JC\", \"QD\", \"5D\", \"AH\"]\n",[33,42766,42767],{"__ignoreMap":35},[187,42768,42769],{"class":189,"line":190},[187,42770,42764],{},[11,42772,42773,42774,1172,42777,752],{},"To write functions, I reached for using 2 important python features: ",[33,42775,42776],{},"set",[33,42778,42779],{},"defaultdict",[11,42781,42782],{},"Here's an example of a simple function to detect a flush, a hand with cards of all the same suit:",[168,42784,42786],{"id":42785},"checking-a-flush","Checking a flush",[26,42788,42790],{"className":1383,"code":42789,"language":1125,"meta":35,"style":35},"def check_flush(hand):\n    suits = [h[1] for h in hand]\n    if len(set(suits)) == 1:\n      return True\n    else:\n      return False\n",[33,42791,42792,42797,42802,42807,42812,42816],{"__ignoreMap":35},[187,42793,42794],{"class":189,"line":190},[187,42795,42796],{},"def check_flush(hand):\n",[187,42798,42799],{"class":189,"line":249},[187,42800,42801],{},"    suits = [h[1] for h in hand]\n",[187,42803,42804],{"class":189,"line":312},[187,42805,42806],{},"    if len(set(suits)) == 1:\n",[187,42808,42809],{"class":189,"line":319},[187,42810,42811],{},"      return True\n",[187,42813,42814],{"class":189,"line":325},[187,42815,23313],{},[187,42817,42818],{"class":189,"line":686},[187,42819,42820],{},"      return False\n",[11,42822,42823],{},"This function creates a list of the suits in our hand, and then counts the unique elements in that list by making it a set. If the length of the set is 1, then all the cards in the hand must be of the same suit.",[11,42825,42826,42827,42830],{},"But wait, what if we have a straight flush? Also, a hand that satisfies a flush could also be described as a two pair hand. The problem asked me to find the highest possible hand for a given set of cards, so I tried to keep things simple by writing a ",[33,42828,42829],{},"check_hand()"," function that checks each hand starting from straight flush down to high card. As soon as a condition for a hand was satisfied, I returned a number that corresponded to the strength of the hand (1 for high card up to 10 for straight flush). The problem didn't include Royal flush, so I will not include that here.",[11,42832,42833,42834,42837],{},"Here's the ",[33,42835,42836],{},"check_hand"," function:",[26,42839,42841],{"className":1383,"code":42840,"language":1125,"meta":35,"style":35},"def check_hand(hand):\n    if check_straight_flush(hand):\n        return 9\n    if check_four_of_a_kind(hand):\n        return 8\n\n    [...]\n    if check_two_pair(hand):\n        return 3\n    if check_pair(hand):\n        return 2\n    return 1\n",[33,42842,42843,42848,42853,42858,42863,42868,42872,42876,42881,42886,42891,42896],{"__ignoreMap":35},[187,42844,42845],{"class":189,"line":190},[187,42846,42847],{},"def check_hand(hand):\n",[187,42849,42850],{"class":189,"line":249},[187,42851,42852],{},"    if check_straight_flush(hand):\n",[187,42854,42855],{"class":189,"line":312},[187,42856,42857],{},"        return 9\n",[187,42859,42860],{"class":189,"line":319},[187,42861,42862],{},"    if check_four_of_a_kind(hand):\n",[187,42864,42865],{"class":189,"line":325},[187,42866,42867],{},"        return 8\n",[187,42869,42870],{"class":189,"line":686},[187,42871,316],{"emptyLinePlaceholder":315},[187,42873,42874],{"class":189,"line":697},[187,42875,38166],{},[187,42877,42878],{"class":189,"line":1291},[187,42879,42880],{},"    if check_two_pair(hand):\n",[187,42882,42883],{"class":189,"line":1306},[187,42884,42885],{},"        return 3\n",[187,42887,42888],{"class":189,"line":1434},[187,42889,42890],{},"    if check_pair(hand):\n",[187,42892,42893],{"class":189,"line":2599},[187,42894,42895],{},"        return 2\n",[187,42897,42898],{"class":189,"line":2607},[187,42899,42900],{},"    return 1\n",[11,42902,42903,42904,42907],{},"This function starts checking the most valuable hands. After it checks the second to lowest hand (pair), it returns a value of 1. This value of 1 corresponds to the \"highest card\" hand. Since I'm not comparing the relative value of hands, it doesn't matter what the highest card is, so the number just represents the ",[4339,42905,42906],{},"type"," of hand that is the strongest.",[168,42909,42911],{"id":42910},"other-hands","Other hands",[11,42913,42914],{},"Here are the all of the functions I used to detect hands:",[26,42916,42918],{"className":1383,"code":42917,"language":1125,"meta":35,"style":35},"card_order_dict = {\"2\":2, \"3\":3, \"4\":4, \"5\":5, \"6\":6, \"7\":7, \"8\":8, \"9\":9, \"T\":10,\"J\":11, \"Q\":12, \"K\":13, \"A\":14}\n\ndef check_straight_flush(hand):\n    if check_flush(hand) and check_straight(hand):\n        return True\n    else:\n        return False\n\ndef check_four_of_a_kind(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v]+=1\n    if sorted(value_counts.values()) == [1,4]:\n        return True\n    return False\n\ndef check_full_house(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v]+=1\n    if sorted(value_counts.values()) == [2,3]:\n        return True\n    return False\n\ndef check_flush(hand):\n    suits = [i[1] for i in hand]\n    if len(set(suits))==1:\n        return True\n    else:\n        return False\n\ndef check_straight(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v] += 1\n    rank_values = [card_order_dict[i] for i in values]\n    value_range = max(rank_values) - min(rank_values)\n    if len(set(value_counts.values())) == 1 and (value_range==4):\n        return True\n    else:\n        #check straight with low Ace\n        if set(values) == set([\"A\", \"2\", \"3\", \"4\", \"5\"]):\n            return True\n        return False\n\ndef check_three_of_a_kind(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v]+=1\n    if set(value_counts.values()) == set([3,1]):\n        return True\n    else:\n        return False\n\ndef check_two_pairs(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v]+=1\n    if sorted(value_counts.values())==[1,2,2]:\n        return True\n    else:\n        return False\n\ndef check_one_pairs(hand):\n    values = [i[0] for i in hand]\n    value_counts = defaultdict(lambda:0)\n    for v in values:\n        value_counts[v]+=1\n    if 2 in value_counts.values():\n        return True\n    else:\n        return False\n",[33,42919,42920,42925,42929,42934,42939,42944,42948,42953,42957,42962,42967,42972,42977,42982,42987,42991,42996,43000,43005,43009,43013,43017,43021,43026,43030,43034,43038,43042,43047,43052,43056,43060,43064,43068,43073,43077,43081,43085,43090,43095,43100,43105,43109,43113,43118,43123,43128,43132,43136,43141,43145,43149,43153,43157,43162,43166,43170,43174,43178,43183,43187,43191,43195,43199,43204,43208,43212,43216,43220,43225,43229,43233,43237,43241,43246,43250,43254],{"__ignoreMap":35},[187,42921,42922],{"class":189,"line":190},[187,42923,42924],{},"card_order_dict = {\"2\":2, \"3\":3, \"4\":4, \"5\":5, \"6\":6, \"7\":7, \"8\":8, \"9\":9, \"T\":10,\"J\":11, \"Q\":12, \"K\":13, \"A\":14}\n",[187,42926,42927],{"class":189,"line":249},[187,42928,316],{"emptyLinePlaceholder":315},[187,42930,42931],{"class":189,"line":312},[187,42932,42933],{},"def check_straight_flush(hand):\n",[187,42935,42936],{"class":189,"line":319},[187,42937,42938],{},"    if check_flush(hand) and check_straight(hand):\n",[187,42940,42941],{"class":189,"line":325},[187,42942,42943],{},"        return True\n",[187,42945,42946],{"class":189,"line":686},[187,42947,23313],{},[187,42949,42950],{"class":189,"line":697},[187,42951,42952],{},"        return False\n",[187,42954,42955],{"class":189,"line":1291},[187,42956,316],{"emptyLinePlaceholder":315},[187,42958,42959],{"class":189,"line":1306},[187,42960,42961],{},"def check_four_of_a_kind(hand):\n",[187,42963,42964],{"class":189,"line":1434},[187,42965,42966],{},"    values = [i[0] for i in hand]\n",[187,42968,42969],{"class":189,"line":2599},[187,42970,42971],{},"    value_counts = defaultdict(lambda:0)\n",[187,42973,42974],{"class":189,"line":2607},[187,42975,42976],{},"    for v in values:\n",[187,42978,42979],{"class":189,"line":2621},[187,42980,42981],{},"        value_counts[v]+=1\n",[187,42983,42984],{"class":189,"line":2631},[187,42985,42986],{},"    if sorted(value_counts.values()) == [1,4]:\n",[187,42988,42989],{"class":189,"line":2642},[187,42990,42943],{},[187,42992,42993],{"class":189,"line":2653},[187,42994,42995],{},"    return False\n",[187,42997,42998],{"class":189,"line":2665},[187,42999,316],{"emptyLinePlaceholder":315},[187,43001,43002],{"class":189,"line":2674},[187,43003,43004],{},"def check_full_house(hand):\n",[187,43006,43007],{"class":189,"line":2684},[187,43008,42966],{},[187,43010,43011],{"class":189,"line":2694},[187,43012,42971],{},[187,43014,43015],{"class":189,"line":2706},[187,43016,42976],{},[187,43018,43019],{"class":189,"line":2715},[187,43020,42981],{},[187,43022,43023],{"class":189,"line":2725},[187,43024,43025],{},"    if sorted(value_counts.values()) == [2,3]:\n",[187,43027,43028],{"class":189,"line":2735},[187,43029,42943],{},[187,43031,43032],{"class":189,"line":2743},[187,43033,42995],{},[187,43035,43036],{"class":189,"line":2754},[187,43037,316],{"emptyLinePlaceholder":315},[187,43039,43040],{"class":189,"line":2762},[187,43041,42796],{},[187,43043,43044],{"class":189,"line":2770},[187,43045,43046],{},"    suits = [i[1] for i in hand]\n",[187,43048,43049],{"class":189,"line":2781},[187,43050,43051],{},"    if len(set(suits))==1:\n",[187,43053,43054],{"class":189,"line":2792},[187,43055,42943],{},[187,43057,43058],{"class":189,"line":2803},[187,43059,23313],{},[187,43061,43062],{"class":189,"line":2808},[187,43063,42952],{},[187,43065,43066],{"class":189,"line":2816},[187,43067,316],{"emptyLinePlaceholder":315},[187,43069,43070],{"class":189,"line":2824},[187,43071,43072],{},"def check_straight(hand):\n",[187,43074,43075],{"class":189,"line":2834},[187,43076,42966],{},[187,43078,43079],{"class":189,"line":2845},[187,43080,42971],{},[187,43082,43083],{"class":189,"line":2856},[187,43084,42976],{},[187,43086,43087],{"class":189,"line":2867},[187,43088,43089],{},"        value_counts[v] += 1\n",[187,43091,43092],{"class":189,"line":2878},[187,43093,43094],{},"    rank_values = [card_order_dict[i] for i in values]\n",[187,43096,43097],{"class":189,"line":2886},[187,43098,43099],{},"    value_range = max(rank_values) - min(rank_values)\n",[187,43101,43102],{"class":189,"line":2900},[187,43103,43104],{},"    if len(set(value_counts.values())) == 1 and (value_range==4):\n",[187,43106,43107],{"class":189,"line":2905},[187,43108,42943],{},[187,43110,43111],{"class":189,"line":2913},[187,43112,23313],{},[187,43114,43115],{"class":189,"line":2921},[187,43116,43117],{},"        #check straight with low Ace\n",[187,43119,43120],{"class":189,"line":2931},[187,43121,43122],{},"        if set(values) == set([\"A\", \"2\", \"3\", \"4\", \"5\"]):\n",[187,43124,43125],{"class":189,"line":2942},[187,43126,43127],{},"            return True\n",[187,43129,43130],{"class":189,"line":2953},[187,43131,42952],{},[187,43133,43134],{"class":189,"line":2964},[187,43135,316],{"emptyLinePlaceholder":315},[187,43137,43138],{"class":189,"line":2975},[187,43139,43140],{},"def check_three_of_a_kind(hand):\n",[187,43142,43143],{"class":189,"line":2983},[187,43144,42966],{},[187,43146,43147],{"class":189,"line":2992},[187,43148,42971],{},[187,43150,43151],{"class":189,"line":3001},[187,43152,42976],{},[187,43154,43155],{"class":189,"line":3010},[187,43156,42981],{},[187,43158,43159],{"class":189,"line":3019},[187,43160,43161],{},"    if set(value_counts.values()) == set([3,1]):\n",[187,43163,43164],{"class":189,"line":3028},[187,43165,42943],{},[187,43167,43168],{"class":189,"line":3033},[187,43169,23313],{},[187,43171,43172],{"class":189,"line":3041},[187,43173,42952],{},[187,43175,43176],{"class":189,"line":3049},[187,43177,316],{"emptyLinePlaceholder":315},[187,43179,43180],{"class":189,"line":3059},[187,43181,43182],{},"def check_two_pairs(hand):\n",[187,43184,43185],{"class":189,"line":3070},[187,43186,42966],{},[187,43188,43189],{"class":189,"line":3075},[187,43190,42971],{},[187,43192,43193],{"class":189,"line":3083},[187,43194,42976],{},[187,43196,43197],{"class":189,"line":3091},[187,43198,42981],{},[187,43200,43201],{"class":189,"line":3101},[187,43202,43203],{},"    if sorted(value_counts.values())==[1,2,2]:\n",[187,43205,43206],{"class":189,"line":3111},[187,43207,42943],{},[187,43209,43210],{"class":189,"line":3122},[187,43211,23313],{},[187,43213,43214],{"class":189,"line":3132},[187,43215,42952],{},[187,43217,43218],{"class":189,"line":3143},[187,43219,316],{"emptyLinePlaceholder":315},[187,43221,43222],{"class":189,"line":3151},[187,43223,43224],{},"def check_one_pairs(hand):\n",[187,43226,43227],{"class":189,"line":3161},[187,43228,42966],{},[187,43230,43231],{"class":189,"line":3170},[187,43232,42971],{},[187,43234,43235],{"class":189,"line":3178},[187,43236,42976],{},[187,43238,43239],{"class":189,"line":3185},[187,43240,42981],{},[187,43242,43243],{"class":189,"line":3195},[187,43244,43245],{},"    if 2 in value_counts.values():\n",[187,43247,43248],{"class":189,"line":3205},[187,43249,42943],{},[187,43251,43252],{"class":189,"line":3210},[187,43253,23313],{},[187,43255,43256],{"class":189,"line":3216},[187,43257,42952],{},[11,43259,43260,43262],{},[33,43261,42779],{}," is a great built-in that is good to use when you don't know what elements will be in your dictionary, but you know what the initial values of any key that could be added should be. We don't need it here, but the alternative would be to write a very long dictionary where keys are the possible card values and the values of each key is 0.",[2215,43264,43266],{"id":43265},"finding-the-best-hand","Finding the best hand",[11,43268,43269],{},"It would certainly be cleaner and more efficient to write out the above functions into one large function, but I wanted to keep things simple as I was under time constraints.",[11,43271,43272,43273,43276,43277,43280,43281,43283],{},"The next step in the problem is to determine the best possible hand we can get given the hand we are dealt and the 5 cards on top of the deck. I decided to first solve this problem with brute force. Here was my logic for this part: use ",[33,43274,43275],{},"itertools"," to get all combinations of groups of 0, 1, 2, 3, 4 and 5 cards from my hand and add the first ",[33,43278,43279],{},"5 - n"," cards from the deck so we get a five card deck. For each combination of cards we can run ",[33,43282,42829],{}," and keep track of the highest rank hand, and then return that hand as the best hand. Here's the code I wrote for this part of the problem:",[26,43285,43287],{"className":1383,"code":43286,"language":1125,"meta":35,"style":35},"from itertools import combinations\n\nhand_dict = {9:\"straight-flush\", 8:\"four-of-a-kind\", 7:\"full-house\", 6:\"flush\", 5:\"straight\", 4:\"three-of-a-kind\", 3:\"two-pairs\", 2:\"one-pair\", 1:\"highest-card\"}\n\n#exhaustive search using itertools.combinations\ndef play(cards):\n    hand = cards[:5]\n    deck = cards[5:]\n    best_hand = 0\n    for i in range(6):\n        possible_combos = combinations(hand, 5-i)\n        for c in possible_combos:\n            current_hand = list(c) + deck[:i]\n            hand_value = check_hand(current_hand)\n            if hand_value > best_hand:\n                best_hand = hand_value\n\n    return hand_dict[best_hand]\n",[33,43288,43289,43294,43298,43303,43307,43312,43317,43322,43327,43332,43337,43342,43347,43352,43357,43362,43367,43371],{"__ignoreMap":35},[187,43290,43291],{"class":189,"line":190},[187,43292,43293],{},"from itertools import combinations\n",[187,43295,43296],{"class":189,"line":249},[187,43297,316],{"emptyLinePlaceholder":315},[187,43299,43300],{"class":189,"line":312},[187,43301,43302],{},"hand_dict = {9:\"straight-flush\", 8:\"four-of-a-kind\", 7:\"full-house\", 6:\"flush\", 5:\"straight\", 4:\"three-of-a-kind\", 3:\"two-pairs\", 2:\"one-pair\", 1:\"highest-card\"}\n",[187,43304,43305],{"class":189,"line":319},[187,43306,316],{"emptyLinePlaceholder":315},[187,43308,43309],{"class":189,"line":325},[187,43310,43311],{},"#exhaustive search using itertools.combinations\n",[187,43313,43314],{"class":189,"line":686},[187,43315,43316],{},"def play(cards):\n",[187,43318,43319],{"class":189,"line":697},[187,43320,43321],{},"    hand = cards[:5]\n",[187,43323,43324],{"class":189,"line":1291},[187,43325,43326],{},"    deck = cards[5:]\n",[187,43328,43329],{"class":189,"line":1306},[187,43330,43331],{},"    best_hand = 0\n",[187,43333,43334],{"class":189,"line":1434},[187,43335,43336],{},"    for i in range(6):\n",[187,43338,43339],{"class":189,"line":2599},[187,43340,43341],{},"        possible_combos = combinations(hand, 5-i)\n",[187,43343,43344],{"class":189,"line":2607},[187,43345,43346],{},"        for c in possible_combos:\n",[187,43348,43349],{"class":189,"line":2621},[187,43350,43351],{},"            current_hand = list(c) + deck[:i]\n",[187,43353,43354],{"class":189,"line":2631},[187,43355,43356],{},"            hand_value = check_hand(current_hand)\n",[187,43358,43359],{"class":189,"line":2642},[187,43360,43361],{},"            if hand_value > best_hand:\n",[187,43363,43364],{"class":189,"line":2653},[187,43365,43366],{},"                best_hand = hand_value\n",[187,43368,43369],{"class":189,"line":2665},[187,43370,316],{"emptyLinePlaceholder":315},[187,43372,43373],{"class":189,"line":2674},[187,43374,43375],{},"    return hand_dict[best_hand]\n",[2215,43377,43379],{"id":43378},"checking-test-cases","Checking test cases",[11,43381,43382],{},"Lastly, I need to check each hand and print out the best hand possible. Here's the loop I wrote to do this:",[26,43384,43386],{"className":1383,"code":43385,"language":1125,"meta":35,"style":35},"for i in sys.stdin.readlines():\n    cards = list(map(lambda x:x, i.split()))\n    hand = cards[:5]\n    deck = cards[5:]\n    print(\"Hand:\", \" \".join(hand), \"Deck:\", \" \".join(deck), \"Best hand:\", play(cards))\n",[33,43387,43388,43393,43398,43402,43406],{"__ignoreMap":35},[187,43389,43390],{"class":189,"line":190},[187,43391,43392],{},"for i in sys.stdin.readlines():\n",[187,43394,43395],{"class":189,"line":249},[187,43396,43397],{},"    cards = list(map(lambda x:x, i.split()))\n",[187,43399,43400],{"class":189,"line":312},[187,43401,43321],{},[187,43403,43404],{"class":189,"line":319},[187,43405,43326],{},[187,43407,43408],{"class":189,"line":325},[187,43409,43410],{},"    print(\"Hand:\", \" \".join(hand), \"Deck:\", \" \".join(deck), \"Best hand:\", play(cards))\n",[11,43412,43413],{},"This will accept one round of cards per line:",[26,43415,43418],{"className":43416,"code":43417,"language":31},[29],"2C 3D 4S 5D 7H KD QH 6C JH 2D\n",[33,43419,43417],{"__ignoreMap":35},[11,43421,43422],{},"and it will output the following:",[26,43424,43427],{"className":43425,"code":43426,"language":31},[29],"Hand: 2C 3D 4S 5D 7H Deck: KD QH 6C JH 2D Best hand: straight\n",[33,43428,43426],{"__ignoreMap":35},[2215,43430,43432],{"id":43431},"optimization","Optimization",[11,43434,43435,43436,43439,43440,43442,43443,43446],{},"This was an interesting problem to deal with as the solution contained several parts that worked together. While solving the problem I aimed worked through to the end leaving some parts to come back to that I felt confident in solving. Instead of writing each function to check differnt hands at the beginning, I filled most of these functions with ",[33,43437,43438],{},"pass"," and moved on to write the next part that involves checking each different combination of cards. Recently having worked through python's ",[33,43441,43275],{}," exercises on Hackerrank, the ",[33,43444,43445],{},"combinations"," functions was fresh in my mind.",[11,43448,43449],{},"While I was able to arrive at a solution that satisfied the test cases, I did not have time to think about the efficiency or Big O analysis of the problem.",[11,43451,43452],{},"There is obviously some refactoring that I could do to make things cleaner. With more time I would take an object oriented approach by making classes for cards and hands, and adding class methods to evaluate the hands.",[11,43454,43455,43456,43458],{},"For each round, we have to run ",[33,43457,42829],{}," on each hand combination. Let's think about how many hands we have to evaluate:",[11,43460,43461,43462,43464,43465,752],{},"We have to consider combinations of cards formed by taking out groups of 0, 1, 2, 3, 4 and 5 cards and adding the next number of cards in the deck that bring the total card count to 5, which means we have to do 5C0 + 5C1 + 5C2 + 5C3 + 5C4 + 5C5 calls to ",[33,43463,42829],{},". So the sum of total calls is 1 + 5 + 10 + 10 + 5 + 1 = ",[338,43466,43467],{},"32",[11,43469,43470,43471,637,43474,43477,43478,43481,43482,43484,43485,43488,43489,43492],{},"For each of these 32 calls that happen when we run ",[33,43472,43473],{},"play()",[33,43475,43476],{},"check_hands()"," runs through each of the ",[33,43479,43480],{},"check_"," functions starting with the highest value hand. As soon as it finds a \"match\", ",[33,43483,43476],{}," returns a number value (",[33,43486,43487],{},"hand_value",") corresponding to straight flush, four of a kind, etc. This value is then compared with the highest value that has been previously found (",[33,43490,43491],{},"best_hand",") and replaces that value if the current hand's hand rank has a higher value.",[11,43494,43495],{},"I'm not sure if there is faster way to find the best hand than the brute force method I implemented.",[855,43497,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":43499},[43500,43501,43502,43503],{"id":42662,"depth":249,"text":42663},{"id":42754,"depth":249,"text":42755},{"id":42785,"depth":249,"text":42786},{"id":42910,"depth":249,"text":42911},"2018-01-02",{"layout":29014},"/2018/01/02/checking-poker-hands-with-python",{"title":42594,"description":35},"2018/01/02/checking-poker-hands-with-python",[1125,43510],"poker","Ret7WGssQiJ36CiZjuavTj9rr9qE6e4m6qfH8l7sSks",{"id":43513,"title":43514,"body":43515,"comments":315,"date":43790,"description":35,"draft":872,"extension":873,"external":874,"image":43521,"meta":43791,"navigation":315,"path":43792,"seo":43793,"stem":43794,"tags":43795,"__hash__":43797},"blog/2017/12/22/getting-started-with-twisted.md","Getting started with Python's Twisted Framework",{"type":8,"value":43516,"toc":43786},[43517,43522,43529,43533,43539,43542,43547,43551,43554,43589,43598,43602,43720,43724,43784],[11,43518,43519],{},[511,43520],{"alt":7255,"src":43521},"/static/twisted-snakes.png",[11,43523,43524,43525,43528],{},"In this article I'm going to be exploring python's twisted framework. I'm working through the ",[4339,43526,43527],{},"Twisted Network Programming Essentials"," book from O'Reilly.",[168,43530,43532],{"id":43531},"installation","Installation",[26,43534,43537],{"className":43535,"code":43536,"language":31},[29]," $ pip install twisted\n",[33,43538,43536],{"__ignoreMap":35},[11,43540,43541],{},"The main idea behind Twisted is that it gives us the parallelism of multithreading programming with the ease of reasoning of single threaded programming.",[11,43543,43544],{},[511,43545],{"alt":7255,"src":43546},"/static/event-driven.jpg",[168,43548,43550],{"id":43549},"the-reactor","The Reactor",[11,43552,43553],{},"This is the core of Twisted. Here is a simple explanation of what the reactor does with psuedo-code:",[26,43555,43557],{"className":1383,"code":43556,"language":1125,"meta":35,"style":35},"while True:\n    timeout = timeout_until_next_timed_event()\n    events = wait_for_events(timeout)\n    events += timed_events_until(now())\n    for event in events:\n        event.process()\n",[33,43558,43559,43564,43569,43574,43579,43584],{"__ignoreMap":35},[187,43560,43561],{"class":189,"line":190},[187,43562,43563],{},"while True:\n",[187,43565,43566],{"class":189,"line":249},[187,43567,43568],{},"    timeout = timeout_until_next_timed_event()\n",[187,43570,43571],{"class":189,"line":312},[187,43572,43573],{},"    events = wait_for_events(timeout)\n",[187,43575,43576],{"class":189,"line":319},[187,43577,43578],{},"    events += timed_events_until(now())\n",[187,43580,43581],{"class":189,"line":325},[187,43582,43583],{},"    for event in events:\n",[187,43585,43586],{"class":189,"line":686},[187,43587,43588],{},"        event.process()\n",[11,43590,43591,43592,1172,43595,358],{},"Here's a simple echo server/client example that illustrates how the reactor works. It is composed of ",[33,43593,43594],{},"echoclient.py",[33,43596,43597],{},"echoserver.py",[11,43599,43600],{},[4339,43601,43594],{},[26,43603,43605],{"className":1383,"code":43604,"language":1125,"meta":35,"style":35},"from twisted.internet import reactor, protocol\n\nclass EchoClient(protocol.Protocol):\n    def connectionMade(self):\n        self.transport.write(u\"Hello, world!\".encode('utf-8'))\n\n    def dataReceived(self, data):\n        print(\"Server said:\", data)\n        self.transport.loseConnection()\n\nclass EchoFactory(protocol.ClientFactory):\n    def buildProtocol(self, addr):\n        return EchoClient()\n\n    def clientConnectionFailed(self, connector, reason):\n        print(\"Connection failed.\")\n        reactor.stop()\n\n    def clientConnectionLost(self, connector, reason):\n        print(\"Connection lost.\")\n        reactor.stop()\n\nreactor.connectTCP(\"localhost\", 8000, EchoFactory())\nreactor.run()\n",[33,43606,43607,43612,43616,43621,43626,43631,43635,43640,43645,43650,43654,43659,43664,43669,43673,43678,43683,43688,43692,43697,43702,43706,43710,43715],{"__ignoreMap":35},[187,43608,43609],{"class":189,"line":190},[187,43610,43611],{},"from twisted.internet import reactor, protocol\n",[187,43613,43614],{"class":189,"line":249},[187,43615,316],{"emptyLinePlaceholder":315},[187,43617,43618],{"class":189,"line":312},[187,43619,43620],{},"class EchoClient(protocol.Protocol):\n",[187,43622,43623],{"class":189,"line":319},[187,43624,43625],{},"    def connectionMade(self):\n",[187,43627,43628],{"class":189,"line":325},[187,43629,43630],{},"        self.transport.write(u\"Hello, world!\".encode('utf-8'))\n",[187,43632,43633],{"class":189,"line":686},[187,43634,316],{"emptyLinePlaceholder":315},[187,43636,43637],{"class":189,"line":697},[187,43638,43639],{},"    def dataReceived(self, data):\n",[187,43641,43642],{"class":189,"line":1291},[187,43643,43644],{},"        print(\"Server said:\", data)\n",[187,43646,43647],{"class":189,"line":1306},[187,43648,43649],{},"        self.transport.loseConnection()\n",[187,43651,43652],{"class":189,"line":1434},[187,43653,316],{"emptyLinePlaceholder":315},[187,43655,43656],{"class":189,"line":2599},[187,43657,43658],{},"class EchoFactory(protocol.ClientFactory):\n",[187,43660,43661],{"class":189,"line":2607},[187,43662,43663],{},"    def buildProtocol(self, addr):\n",[187,43665,43666],{"class":189,"line":2621},[187,43667,43668],{},"        return EchoClient()\n",[187,43670,43671],{"class":189,"line":2631},[187,43672,316],{"emptyLinePlaceholder":315},[187,43674,43675],{"class":189,"line":2642},[187,43676,43677],{},"    def clientConnectionFailed(self, connector, reason):\n",[187,43679,43680],{"class":189,"line":2653},[187,43681,43682],{},"        print(\"Connection failed.\")\n",[187,43684,43685],{"class":189,"line":2665},[187,43686,43687],{},"        reactor.stop()\n",[187,43689,43690],{"class":189,"line":2674},[187,43691,316],{"emptyLinePlaceholder":315},[187,43693,43694],{"class":189,"line":2684},[187,43695,43696],{},"    def clientConnectionLost(self, connector, reason):\n",[187,43698,43699],{"class":189,"line":2694},[187,43700,43701],{},"        print(\"Connection lost.\")\n",[187,43703,43704],{"class":189,"line":2706},[187,43705,43687],{},[187,43707,43708],{"class":189,"line":2715},[187,43709,316],{"emptyLinePlaceholder":315},[187,43711,43712],{"class":189,"line":2725},[187,43713,43714],{},"reactor.connectTCP(\"localhost\", 8000, EchoFactory())\n",[187,43716,43717],{"class":189,"line":2735},[187,43718,43719],{},"reactor.run()\n",[11,43721,43722],{},[4339,43723,43597],{},[26,43725,43727],{"className":1383,"code":43726,"language":1125,"meta":35,"style":35},"from twisted.internet import protocol, reactor\n\nclass Echo(protocol.Protocol):\n    def dataReceived(self,data):\n        self.transport.write(data)\n\nclass EchoFactory(protocol.Factory):\n    def buildProtocol(self, addr):\n        return Echo()\n\nreactor.listenTCP(8000, EchoFactory())\nreactor.run()\n",[33,43728,43729,43734,43738,43743,43748,43753,43757,43762,43766,43771,43775,43780],{"__ignoreMap":35},[187,43730,43731],{"class":189,"line":190},[187,43732,43733],{},"from twisted.internet import protocol, reactor\n",[187,43735,43736],{"class":189,"line":249},[187,43737,316],{"emptyLinePlaceholder":315},[187,43739,43740],{"class":189,"line":312},[187,43741,43742],{},"class Echo(protocol.Protocol):\n",[187,43744,43745],{"class":189,"line":319},[187,43746,43747],{},"    def dataReceived(self,data):\n",[187,43749,43750],{"class":189,"line":325},[187,43751,43752],{},"        self.transport.write(data)\n",[187,43754,43755],{"class":189,"line":686},[187,43756,316],{"emptyLinePlaceholder":315},[187,43758,43759],{"class":189,"line":697},[187,43760,43761],{},"class EchoFactory(protocol.Factory):\n",[187,43763,43764],{"class":189,"line":1291},[187,43765,43663],{},[187,43767,43768],{"class":189,"line":1306},[187,43769,43770],{},"        return Echo()\n",[187,43772,43773],{"class":189,"line":1434},[187,43774,316],{"emptyLinePlaceholder":315},[187,43776,43777],{"class":189,"line":2599},[187,43778,43779],{},"reactor.listenTCP(8000, EchoFactory())\n",[187,43781,43782],{"class":189,"line":2607},[187,43783,43719],{},[855,43785,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":43787},[43788,43789],{"id":43531,"depth":249,"text":43532},{"id":43549,"depth":249,"text":43550},"2017-12-22",{"layout":29014},"/2017/12/22/getting-started-with-twisted",{"title":43514,"description":35},"2017/12/22/getting-started-with-twisted",[1125,43796],"twisted","0bbDBKU8PLdboKlUP7UMQOO0FrqQijbGteo3aI2H7W8",{"id":43799,"title":43800,"body":43801,"comments":315,"date":44504,"description":35,"draft":872,"extension":873,"external":874,"image":43807,"meta":44505,"navigation":315,"path":44506,"seo":44507,"stem":44508,"tags":44509,"__hash__":44510},"blog/2017/12/09/setting-up-flask-cli-with-docker.md","Setting up a Flask project with Flask CLI and Docker",{"type":8,"value":43802,"toc":44497},[43803,43808,43811,43832,43837,43840,43844,43850,43854,43860,43864,43870,43877,43881,43887,43890,43893,43905,43916,43920,43971,43974,43980,43991,43997,44004,44009,44082,44088,44097,44103,44109,44119,44122,44128,44131,44137,44149,44183,44186,44192,44198,44204,44211,44214,44219,44225,44231,44237,44243,44255,44258,44264,44269,44272,44280,44284,44290,44296,44300,44306,44309,44312,44318,44321,44327,44337,44343,44350,44356,44363,44366,44372,44375,44381,44384,44390,44400,44406,44423,44433,44439,44447,44455,44461,44464,44470,44481,44487,44492,44495],[11,43804,43805],{},[511,43806],{"alt":7255,"src":43807},"/static/flask-docker.png",[11,43809,43810],{},"⚠️ This article is under contruction ⚠️",[11,43812,43813,43814,43819,43820,5857,43823,43825,43826,43829,43830,752],{},"I've recently been working on an awesome tutorial from ",[15,43815,43818],{"href":43816,"rel":43817},"https://testdriven.io",[19],"testdriven.io"," that covers flask, react and docker. The beginning of the project covers how to setup a basic flask app using ",[33,43821,43822],{},"flask-scripts",[33,43824,43822],{}," is a deprecated tool and the tutorial recommends using ",[33,43827,43828],{},"Flask CLI",". I have fumbled with this the first time I tried to set it up and while I was able to get it working, I couldn't get it working inside of docker. In this article I'll detail the setup of my flask project with ",[33,43831,43828],{},[107,43833,43834],{},[11,43835,43836],{},"One of the nice new features in Flask 0.11 is the built-in integration of the click command line interface. This enables a wide range of new features for the Flask ecosystem and your own applications.",[11,43838,43839],{},"OK. Let's set up a basic flask app:",[911,43841,43843],{"id":43842},"directories","Directories",[26,43845,43848],{"className":43846,"code":43847,"language":31},[29]," $ mkdir test-flask && cd test-flask\n $ mkdir users-service && cd users-service\n $ mkdir project\n\n",[33,43849,43847],{"__ignoreMap":35},[911,43851,43853],{"id":43852},"virtual-environment","Virtual Environment",[26,43855,43858],{"className":43856,"code":43857,"language":31},[29]," $ virtualenv -p python3 env\nRunning virtualenv with interpreter /home/brian/anaconda3/bin/python3\nUsing base prefix '/home/brian/anaconda3'\nNew python executable in /home/brian/Documents/flask/test-flask/users-service/env/bin/python3\nAlso creating executable in /home/brian/Documents/flask/test-flask/users-service/env/bin/python\nInstalling setuptools, pip, wheel...done.\n",[33,43859,43857],{"__ignoreMap":35},[911,43861,43863],{"id":43862},"activate-virtual-environment","Activate Virtual Environment",[26,43865,43868],{"className":43866,"code":43867,"language":31},[29]," $ source env/bin/activate\n(env) $\n",[33,43869,43867],{"__ignoreMap":35},[11,43871,43872,43873,43876],{},"We will come back to the ",[33,43874,43875],{},"activate"," script and add some environment variables to the bottom if it so we have them accessible when we activate the virtual environment.",[911,43878,43880],{"id":43879},"install-flask-in-the-virtual-environment","Install flask in the virtual environment",[26,43882,43885],{"className":43883,"code":43884,"language":31},[29]," $ pip install flask==0.12.2\nCollecting flask==0.12.2\n  Using cached Flask-0.12.2-py2.py3-none-any.whl\nCollecting itsdangerous>=0.21 (from flask==0.12.2)\nCollecting Werkzeug>=0.7 (from flask==0.12.2)\n  Using cached Werkzeug-0.13-py2.py3-none-any.whl\nCollecting click>=2.0 (from flask==0.12.2)\n  Using cached click-6.7-py2.py3-none-any.whl\nCollecting Jinja2>=2.4 (from flask==0.12.2)\n  Using cached Jinja2-2.10-py2.py3-none-any.whl\nCollecting MarkupSafe>=0.23 (from Jinja2>=2.4->flask==0.12.2)\nInstalling collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask\nSuccessfully installed Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.13 click-6.7 flask-0.12.2 itsdangerous-0.24\n(env) $\n",[33,43886,43884],{"__ignoreMap":35},[11,43888,43889],{},"At this point we are ready to create our flask app.",[11,43891,43892],{},"Here's a note about the CLI:",[107,43894,43895],{},[11,43896,43897,43898,43900,43901,43904],{},"For the ",[338,43899,24507],{}," script to work, an application needs to be discovered. This is achieved by exporting the ",[33,43902,43903],{},"FLASK_APP"," environment variable. It can be either set to an import path or to a filename of a Python module that contains a Flask application.",[11,43906,43907,43908,43911,43912,43915],{},"Let's add an ",[33,43909,43910],{},"app.py"," file inside the ",[33,43913,43914],{},"project"," folder:",[11,43917,43918],{},[4339,43919,43910],{},[26,43921,43923],{"className":1383,"code":43922,"language":1125,"meta":35,"style":35},"from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/users/ping', methods=['GET'])\ndef ping_pong():\n    return jsonify({\n        'message':'pong!',\n        'status':'success'\n        })\n",[33,43924,43925,43930,43934,43938,43942,43947,43952,43957,43962,43967],{"__ignoreMap":35},[187,43926,43927],{"class":189,"line":190},[187,43928,43929],{},"from flask import Flask, jsonify\n",[187,43931,43932],{"class":189,"line":249},[187,43933,316],{"emptyLinePlaceholder":315},[187,43935,43936],{"class":189,"line":312},[187,43937,23275],{},[187,43939,43940],{"class":189,"line":319},[187,43941,316],{"emptyLinePlaceholder":315},[187,43943,43944],{"class":189,"line":325},[187,43945,43946],{},"@app.route('/users/ping', methods=['GET'])\n",[187,43948,43949],{"class":189,"line":686},[187,43950,43951],{},"def ping_pong():\n",[187,43953,43954],{"class":189,"line":697},[187,43955,43956],{},"    return jsonify({\n",[187,43958,43959],{"class":189,"line":1291},[187,43960,43961],{},"        'message':'pong!',\n",[187,43963,43964],{"class":189,"line":1306},[187,43965,43966],{},"        'status':'success'\n",[187,43968,43969],{"class":189,"line":1434},[187,43970,23614],{},[11,43972,43973],{},"And now let's add an environment variable to tell the Flask CLI where our app is located:",[26,43975,43978],{"className":43976,"code":43977,"language":31},[29]," $ export FLASK_APP=/home/brian/Documents/flask/test-flask/users-service/project/app.py\n",[33,43979,43977],{"__ignoreMap":35},[11,43981,43982,43983,43986,43987,43990],{},"OK, now let's try to run ",[33,43984,43985],{},"flask run"," and navigate to ",[33,43988,43989],{},"/users/ping"," and see what happens:",[26,43992,43995],{"className":43993,"code":43994,"language":31},[29]," $ flask run\n * Serving Flask app \"app\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n127.0.0.1 - - [09/Dec/2017 19:20:14] \"GET /users/ping HTTP/1.1\" 200 -\n",[33,43996,43994],{"__ignoreMap":35},[11,43998,43999,44000,44003],{},"Great! We see our ",[33,44001,44002],{},"pong!"," message returned in the browser. Next, let's configure our settings:",[11,44005,44006],{},[4339,44007,44008],{},"project/config.py",[26,44010,44012],{"className":1383,"code":44011,"language":1125,"meta":35,"style":35},"class BaseConfig:\n    \"\"\"Base configuration\"\"\"\n    DEBUG = False\n    TESTING = False\nclass DevelopmentConfig(BaseConfig):\n    \"\"\"Development configuration\"\"\"\n    DEBUG = True\nclass TestingConfig(BaseConfig):\n    \"\"\"Testing configuration\"\"\"\n    DEBUG = True\n    TESTING = True\nclass ProductionConfig(BaseConfig):\n    \"\"\"Production configuration\"\"\"\n    DEBUG = False\n",[33,44013,44014,44019,44024,44029,44034,44039,44044,44049,44054,44059,44063,44068,44073,44078],{"__ignoreMap":35},[187,44015,44016],{"class":189,"line":190},[187,44017,44018],{},"class BaseConfig:\n",[187,44020,44021],{"class":189,"line":249},[187,44022,44023],{},"    \"\"\"Base configuration\"\"\"\n",[187,44025,44026],{"class":189,"line":312},[187,44027,44028],{},"    DEBUG = False\n",[187,44030,44031],{"class":189,"line":319},[187,44032,44033],{},"    TESTING = False\n",[187,44035,44036],{"class":189,"line":325},[187,44037,44038],{},"class DevelopmentConfig(BaseConfig):\n",[187,44040,44041],{"class":189,"line":686},[187,44042,44043],{},"    \"\"\"Development configuration\"\"\"\n",[187,44045,44046],{"class":189,"line":697},[187,44047,44048],{},"    DEBUG = True\n",[187,44050,44051],{"class":189,"line":1291},[187,44052,44053],{},"class TestingConfig(BaseConfig):\n",[187,44055,44056],{"class":189,"line":1306},[187,44057,44058],{},"    \"\"\"Testing configuration\"\"\"\n",[187,44060,44061],{"class":189,"line":1434},[187,44062,44048],{},[187,44064,44065],{"class":189,"line":2599},[187,44066,44067],{},"    TESTING = True\n",[187,44069,44070],{"class":189,"line":2607},[187,44071,44072],{},"class ProductionConfig(BaseConfig):\n",[187,44074,44075],{"class":189,"line":2621},[187,44076,44077],{},"    \"\"\"Production configuration\"\"\"\n",[187,44079,44080],{"class":189,"line":2631},[187,44081,44028],{},[11,44083,44084,44085,358],{},"And now we can add the following line right below where we define ",[33,44086,44087],{},"app = Flask(__name__)",[26,44089,44091],{"className":1383,"code":44090,"language":1125,"meta":35,"style":35},"app.config.from_object('project.config.DevelopmentConfig')\n",[33,44092,44093],{"__ignoreMap":35},[187,44094,44095],{"class":189,"line":190},[187,44096,44090],{},[11,44098,44099,44100,44102],{},"When we run ",[33,44101,43985],{},", we get a long error message including:",[26,44104,44107],{"className":44105,"code":44106,"language":31},[29],"Debugged import:\n\n- 'project' not found.\n\nOriginal exception:\n\nImportStringError: import_string() failed for 'project.config'. Possible reasons are:\n\n- missing __init__.py in a package;\n- package or module path not included in sys.path;\n- duplicated package or module name taking precedence in sys.path;\n- missing module, class, function or variable;\n",[33,44108,44106],{"__ignoreMap":35},[11,44110,44111,44112,44115,44116,44118],{},"Let's try to add ",[33,44113,44114],{},"__init__.py"," to our ",[33,44117,43914],{}," folder.",[11,44120,44121],{},"Once we do this, we are able to run the app successfully, but we don't see any special message about Debug mode being on:",[26,44123,44126],{"className":44124,"code":44125,"language":31},[29]," $ flask run\n * Serving Flask app \"project.app\"\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",[33,44127,44125],{"__ignoreMap":35},[11,44129,44130],{},"The documentation mentions that we can turn on Debug mode with:",[26,44132,44135],{"className":44133,"code":44134,"language":31},[29],"export FLASK_DEBUG=1\n",[33,44136,44134],{"__ignoreMap":35},[11,44138,44139,44140,9054,44143,44146,44147,358],{},"We can check the debug mode by printing ",[33,44141,44142],{},"app.config",[33,44144,44145],{},"ping_pong()"," function that is returned when we hit ",[33,44148,43989],{},[26,44150,44152],{"className":1383,"code":44151,"language":1125,"meta":35,"style":35},"@app.route('/users/ping', methods=['GET'])\ndef ping_pong():\n    print(app.config)\n    return jsonify({\n        'message':'pong!',\n        'status':'success'\n        })\n",[33,44153,44154,44158,44162,44167,44171,44175,44179],{"__ignoreMap":35},[187,44155,44156],{"class":189,"line":190},[187,44157,43946],{},[187,44159,44160],{"class":189,"line":249},[187,44161,43951],{},[187,44163,44164],{"class":189,"line":312},[187,44165,44166],{},"    print(app.config)\n",[187,44168,44169],{"class":189,"line":319},[187,44170,43956],{},[187,44172,44173],{"class":189,"line":325},[187,44174,43961],{},[187,44176,44177],{"class":189,"line":686},[187,44178,43966],{},[187,44180,44181],{"class":189,"line":697},[187,44182,23614],{},[11,44184,44185],{},"Here's what we see in the terminal:",[26,44187,44190],{"className":44188,"code":44189,"language":31},[29],"\u003CConfig {'DEBUG': True, 'TESTING': False, 'PROPAGATE_EXCEPTIONS': None, 'PRESERVE_CONTEXT_ON_EXCEPTION': None, 'SECRET_KEY': None, 'PERMANENT_SESSION_LIFETIME': datetime.timedelta(31), 'USE_X_SENDFILE': False, 'LOGGER_NAME': 'project.app', 'LOGGER_HANDLER_POLICY': 'always', 'SERVER_NAME': None, 'APPLICATION_ROOT': None, 'SESSION_COOKIE_NAME': 'session', 'SESSION_COOKIE_DOMAIN': None, 'SESSION_COOKIE_PATH': None, 'SESSION_COOKIE_HTTPONLY': True, 'SESSION_COOKIE_SECURE': False, 'SESSION_REFRESH_EACH_REQUEST': True, 'MAX_CONTENT_LENGTH': None, 'SEND_FILE_MAX_AGE_DEFAULT': datetime.timedelta(0, 43200), 'TRAP_BAD_REQUEST_ERRORS': False, 'TRAP_HTTP_EXCEPTIONS': False, 'EXPLAIN_TEMPLATE_LOADING': False, 'PREFERRED_URL_SCHEME': 'http', 'JSON_AS_ASCII': True, 'JSON_SORT_KEYS': True, 'JSONIFY_PRETTYPRINT_REGULAR': True, 'JSONIFY_MIMETYPE': 'application/json', 'TEMPLATES_AUTO_RELOAD': None}>\n127.0.0.1 - - [09/Dec/2017 19:42:46] \"GET /users/ping HTTP/1.1\" 200 -\n",[33,44191,44189],{"__ignoreMap":35},[11,44193,44194,44195,358],{},"Just to be sure this is working correctly, let's try another config setting, ",[33,44196,44197],{},"ProductionConfig",[26,44199,44202],{"className":44200,"code":44201,"language":31},[29],"\u003CConfig {'DEBUG': False, 'TESTING': False, 'PROPAGATE_EXCEPTIONS': None, 'PRESERVE_CONTEXT_ON_EXCEPTION': None, 'SECRET_KEY': None, 'PERMANENT_SESSION_LIFETIME': datetime.timedelta(31), 'USE_X_SENDFILE': False, 'LOGGER_NAME': 'project.app', 'LOGGER_HANDLER_POLICY': 'always', 'SERVER_NAME': None, 'APPLICATION_ROOT': None, 'SESSION_COOKIE_NAME': 'session', 'SESSION_COOKIE_DOMAIN': None, 'SESSION_COOKIE_PATH': None, 'SESSION_COOKIE_HTTPONLY': True, 'SESSION_COOKIE_SECURE': False, 'SESSION_REFRESH_EACH_REQUEST': True, 'MAX_CONTENT_LENGTH': None, 'SEND_FILE_MAX_AGE_DEFAULT': datetime.timedelta(0, 43200), 'TRAP_BAD_REQUEST_ERRORS': False, 'TRAP_HTTP_EXCEPTIONS': False, 'EXPLAIN_TEMPLATE_LOADING': False, 'PREFERRED_URL_SCHEME': 'http', 'JSON_AS_ASCII': True, 'JSON_SORT_KEYS': True, 'JSONIFY_PRETTYPRINT_REGULAR': True, 'JSONIFY_MIMETYPE': 'application/json', 'TEMPLATES_AUTO_RELOAD': None}>\n127.0.0.1 - - [09/Dec/2017 19:45:29] \"GET /users/ping HTTP/1.1\" 200 -\n",[33,44203,44201],{"__ignoreMap":35},[11,44205,44206,44207,44210],{},"OK, so far so good! I think that ",[33,44208,44209],{},"FLASK_DEBUG"," may give us some additional information in the terminal.",[11,44212,44213],{},"Let's run:",[26,44215,44217],{"className":44216,"code":44134,"language":31},[29],[33,44218,44134],{"__ignoreMap":35},[11,44220,44221,44222,44224],{},"and run our app in ",[33,44223,44197],{}," mode:",[26,44226,44229],{"className":44227,"code":44228,"language":31},[29]," $ flask run\n * Serving Flask app \"project.app\"\n * Forcing debug mode on\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n * Restarting with stat\n * Debugger is active!\n * Debugger PIN: 775-946-486\n",[33,44230,44228],{"__ignoreMap":35},[11,44232,44233,44234,44236],{},"and when we hit ",[33,44235,43989],{}," we get:",[26,44238,44241],{"className":44239,"code":44240,"language":31},[29],"\u003CConfig {'DEBUG': True, 'TESTING': False, 'PROPAGATE_EXCEPTIONS': None, 'PRESERVE_CONTEXT_ON_EXCEPTION': None, 'SECRET_KEY': None, 'PERMANENT_SESSION_LIFETIME': datetime.timedelta(31), 'USE_X_SENDFILE': False, 'LOGGER_NAME': 'project.app', 'LOGGER_HANDLER_POLICY': 'always', 'SERVER_NAME': None, 'APPLICATION_ROOT': None, 'SESSION_COOKIE_NAME': 'session', 'SESSION_COOKIE_DOMAIN': None, 'SESSION_COOKIE_PATH': None, 'SESSION_COOKIE_HTTPONLY': True, 'SESSION_COOKIE_SECURE': False, 'SESSION_REFRESH_EACH_REQUEST': True, 'MAX_CONTENT_LENGTH': None, 'SEND_FILE_MAX_AGE_DEFAULT': datetime.timedelta(0, 43200), 'TRAP_BAD_REQUEST_ERRORS': False, 'TRAP_HTTP_EXCEPTIONS': False, 'EXPLAIN_TEMPLATE_LOADING': False, 'PREFERRED_URL_SCHEME': 'http', 'JSON_AS_ASCII': True, 'JSON_SORT_KEYS': True, 'JSONIFY_PRETTYPRINT_REGULAR': True, 'JSONIFY_MIMETYPE': 'application/json', 'TEMPLATES_AUTO_RELOAD': None}>\n127.0.0.1 - - [09/Dec/2017 19:51:17] \"GET /users/ping HTTP/1.1\" 200 -\n",[33,44242,44240],{"__ignoreMap":35},[11,44244,44245,44246,30741,44248,44251,44252,752],{},"Here we can see that ",[33,44247,30387],{},[33,44249,44250],{},"True",", which was forced when we set ",[33,44253,44254],{},"FLASK_DEBUG=1",[11,44256,44257],{},"For clarity, let's review the directory structure of our project:",[26,44259,44262],{"className":44260,"code":44261,"language":31},[29]," $ tree project/\nproject/\n├── app.py\n├── config.py\n└── __init__.py\n",[33,44263,44261],{"__ignoreMap":35},[11,44265,44266,44268],{},[33,44267,44114],{}," is just an empty file at this point, but in the testdriven.io tutorial it is the file that contains our app.",[11,44270,44271],{},"OK, we have a very simple flask app that we can control with the flask cli. Let's get ready to dockerize this simple project.",[11,44273,44274,44275,44277,44278,43915],{},"We need a ",[33,44276,23123],{}," file. So far we just have flask. We want to place this file on the same level as our ",[33,44279,43914],{},[11,44281,44282],{},[4339,44283,23123],{},[26,44285,44288],{"className":44286,"code":44287,"language":31},[29],"Flask==0.12.1\n",[33,44289,44287],{"__ignoreMap":35},[11,44291,44292,44293,44295],{},"And we can add a ",[33,44294,783],{}," file as well at the same level:",[11,44297,44298],{},[4339,44299,783],{},[26,44301,44304],{"className":44302,"code":44303,"language":31},[29],"__pycache__\nenv\n",[33,44305,44303],{"__ignoreMap":35},[168,44307,44308],{"id":15298},"Docker",[11,44310,44311],{},"Here are the versions of docker applications I have installed:",[26,44313,44316],{"className":44314,"code":44315,"language":31},[29]," $ docker -v && docker-compose -v && docker-machine -v\nDocker version 17.10.0-ce, build f4ffd2511c\ndocker-compose version 1.17.1, build unknown\ndocker-machine version 0.13.0, build HEAD\n",[33,44317,44315],{"__ignoreMap":35},[11,44319,44320],{},"Currently I don't have any docker machines, images or containers. Here is the status of the docker service:",[26,44322,44325],{"className":44323,"code":44324,"language":31},[29]," $ systemctl status docker\n● docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)\n   Active: active (running) since Fri 2017-12-08 19:13:09 EST; 24h ago\n     Docs: https://docs.docker.com\n Main PID: 5486 (dockerd)\n    Tasks: 26 (limit: 4915)\n   CGroup: /system.slice/docker.service\n           ├─5486 /usr/bin/dockerd -g /home/brian/docker -H fd://\n           └─5492 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc\n\nDec 08 19:13:08 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:08.126834430-05:00\" level=warning msg=\"Your kernel does not support cgroup rt period\"\nDec 08 19:13:08 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:08.126850093-05:00\" level=warning msg=\"Your kernel does not support cgroup rt runtime\"\nDec 08 19:13:08 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:08.127216425-05:00\" level=info msg=\"Loading containers: start.\"\nDec 08 19:13:08 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:08.776371797-05:00\" level=info msg=\"Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address\"\nDec 08 19:13:09 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:09.013651222-05:00\" level=info msg=\"Loading containers: done.\"\nDec 08 19:13:09 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:09.041318847-05:00\" level=warning msg=\"Not using native diff for overlay2, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled\"\nDec 08 19:13:09 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:09.075633462-05:00\" level=info msg=\"Docker daemon\" commit=f4ffd2511c graphdriver(s)=overlay2 version=17.10.0-ce\nDec 08 19:13:09 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:09.076162900-05:00\" level=info msg=\"Daemon has completed initialization\"\nDec 08 19:13:09 archthinkpad dockerd[5486]: time=\"2017-12-08T19:13:09.082056339-05:00\" level=info msg=\"API listen on /var/run/docker.sock\"\nDec 08 19:13:09 archthinkpad systemd[1]: Started Docker Application Container Engine.\n",[33,44326,44324],{"__ignoreMap":35},[11,44328,44329,44330,44333,44334,18410],{},"OK, docker seems to be working fine. Now we need to create a Docker host and point the docker client at it. What does this mean? From what I understand, we will be running docker containers not on our local machine but in an instance of ",[33,44331,44332],{},"virtualbox"," on our local machine. To do this, we will use the ",[33,44335,44336],{},"docker-machine",[26,44338,44341],{"className":44339,"code":44340,"language":31},[29]," $ docker-machine create -d virtualbox testdriven-dev\nRunning pre-create checks...\nCreating machine...\n(testdriven-dev) Copying /home/brian/.docker/machine/cache/boot2docker.iso to /home/brian/.docker/machine/machines/testdriven-dev/boot2docker.iso...\n(testdriven-dev) Creating VirtualBox VM...\n(testdriven-dev) Creating SSH key...\n(testdriven-dev) Starting the VM...\n(testdriven-dev) Check network to re-create if needed...\n(testdriven-dev) Waiting for an IP...\nWaiting for machine to be running, this may take a few minutes...\nDetecting operating system of created instance...\nWaiting for SSH to be available...\nDetecting the provisioner...\nProvisioning with boot2docker...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\n\nThis machine has been allocated an IP address, but Docker Machine could not reach it successfully.\n\nSSH for the machine should still work, but connecting to exposed ports, such as the Docker daemon port (usually \u003Cip>:2376), may not work properly.\n\nYou may need to add the route manually, or use another related workaround.\n\nThis could be due to a VPN, proxy, or host file configuration issue.\n\nYou also might want to clear any VirtualBox host only interfaces you are not using.\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env testdriven-dev\n",[33,44342,44340],{"__ignoreMap":35},[11,44344,44345,44346,44349],{},"We see that ",[33,44347,44348],{},"Docker is up and running!",", but notice this line:",[26,44351,44354],{"className":44352,"code":44353,"language":31},[29],"This machine has been allocated an IP address, but Docker Machine could not reach it successfully.\n",[33,44355,44353],{"__ignoreMap":35},[11,44357,44358,44359,44362],{},"This might be a problem. I think that the ",[33,44360,44361],{},"docker-machine env \u003Cmachine-name>"," command fixes this:",[11,44364,44365],{},"When you run this command you get the following:",[26,44367,44370],{"className":44368,"code":44369,"language":31},[29]," $ docker-machine env testdriven-dev\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.100:2376\"\nexport DOCKER_CERT_PATH=\"/home/brian/.docker/machine/machines/testdriven-dev\"\nexport DOCKER_MACHINE_NAME=\"testdriven-dev\"\n# Run this command to configure your shell:\n# eval $(docker-machine env testdriven-dev)\n",[33,44371,44369],{"__ignoreMap":35},[11,44373,44374],{},"The result of this command tells us to run `eval $(docker-machine env testdriven-dev):",[26,44376,44379],{"className":44377,"code":44378,"language":31},[29],"eval \"$(docker-machine env testdriven-dev)\"\n",[33,44380,44378],{"__ignoreMap":35},[11,44382,44383],{},"To clarify, running the above commands puts adds some environment variables:",[26,44385,44388],{"className":44386,"code":44387,"language":31},[29]," $ env | grep DOCKER\nDOCKER_MACHINE_NAME=testdriven-dev\nDOCKER_CERT_PATH=/home/brian/.docker/machine/machines/testdriven-dev\nDOCKER_TLS_VERIFY=1\nDOCKER_HOST=tcp://192.168.99.100:2376\n",[33,44389,44387],{"__ignoreMap":35},[11,44391,44392,44393,44396,44397,44399],{},"Next we need to add a Dockerfile. We will call it ",[33,44394,44395],{},"Dockerfile-dev",". Let's look at ",[33,44398,44395],{}," from the tutorial and see how we may need to modify it for the way we set up our project:",[26,44401,44404],{"className":44402,"code":44403,"language":31},[29],"FROM python:3.6.3\n\n# set working directory\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\n# add requirements\nADD ./requirements.txt /usr/src/app/requirements.txt\n\n# install requirements\nRUN pip install -r requirements.txt\n\n# add app\nADD . /usr/src/app\n\n# run server\nCMD python manage.py runserver -h 0.0.0.0\n",[33,44405,44403],{"__ignoreMap":35},[11,44407,44408,44409,44412,44413,44415,44416,44419,44420,752],{},"We start by defining a base image with the ",[33,44410,44411],{},"FROM"," line which will give us the correct version of python. We then set folders and the current working directory in docker. Next we install flask add the ",[33,44414,23123],{}," file and install flask with the ",[33,44417,44418],{},"RUN"," line. We then add the directory (on our local machine) with ",[33,44421,44422],{},"ADD . /usr/src/app",[11,44424,44425,44426,44429,44430,44432],{},"This should all be fine up until the last line where we see a ",[33,44427,44428],{},"manage.py"," file. We never created this file since we wish to use the Flask CLI. We could try replicating the process did locally inside our Dockerfile. We need to add the the ",[33,44431,43903],{}," environment variable, and its value should be the script that has just been added to the docker image. Let's try:",[26,44434,44437],{"className":44435,"code":44436,"language":31},[29],"[...]\n\nENV FLASK_APP /usr/src/app/project/app.py\n\nCMD flask run\n",[33,44438,44436],{"__ignoreMap":35},[11,44440,44441,44442,5857,44444,44446],{},"Next we need a script for ",[33,44443,30873],{},[33,44445,30873],{}," is a tool for defining and running multi-container Docker applications. Again, let's look at what was included in the tutorial and then see if we need to make any adjustments:",[11,44448,44449,44452,44453,343],{},[4339,44450,44451],{},"docker-compose-dev.yml"," (this file goes in the root directory, one level up from where ",[33,44454,44395],{},[26,44456,44459],{"className":44457,"code":44458,"language":31},[29],"version: '3.3'\n\nservices:\n\n  users-service:\n    container_name: users-service\n      build:\n        context: ./users-service\n        dockerfile: Dockerfile-dev\n      volumes:\n        - './users-service:/usr/src/app'\n      ports:\n        - 5001:5000\n",[33,44460,44458],{"__ignoreMap":35},[11,44462,44463],{},"OK, this looks good! Let's give it a try. We can run the following command:",[26,44465,44468],{"className":44466,"code":44467,"language":31},[29]," $ docker-compose -f docker-compose-dev.yml build\nBuilding users-service\n85b1f47fba49: Pull complete\nba6bd283713a: Pull complete\n817c8cd48a09: Pull complete\n47cc0ed96dc3: Pull complete\n4a36819a59dc: Pull complete\ndb9a0221399f: Pull complete\n7a511a7689b6: Pull complete\n1223757f6914: Pull complete\nDigest: sha256:db9d8546f3ff74e96702abe0a78a0e0454df6ea898de8f124feba81deea416d7\nStatus: Downloaded newer image for python:3.6.3\n ---> 79e1dc9af1c1\nStep 2/8 : RUN mkdir -p /usr/src/app\n ---> Running in 808f6c0497d3\n ---> 8873e8e0d526\nRemoving intermediate container 808f6c0497d3\nStep 3/8 : WORKDIR /usr/src/app\n ---> 76ef8912b4d5\nRemoving intermediate container a3ca419fe9c1\nStep 4/8 : ADD ./requirements.txt /usr/src/app/requirements.txt\n ---> eb513314527a\nStep 5/8 : RUN pip install -r requirements.txt\n ---> Running in 1a708ec3b565\nCollecting Flask==0.12.1 (from -r requirements.txt (line 1))\n  Downloading Flask-0.12.1-py2.py3-none-any.whl (82kB)\nCollecting Jinja2>=2.4 (from Flask==0.12.1->-r requirements.txt (line 1))\n  Downloading Jinja2-2.10-py2.py3-none-any.whl (126kB)\nCollecting click>=2.0 (from Flask==0.12.1->-r requirements.txt (line 1))\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\nCollecting Werkzeug>=0.7 (from Flask==0.12.1->-r requirements.txt (line 1))\n  Downloading Werkzeug-0.13-py2.py3-none-any.whl (311kB)\nCollecting itsdangerous>=0.21 (from Flask==0.12.1->-r requirements.txt (line 1))\n  Downloading itsdangerous-0.24.tar.gz (46kB)\nCollecting MarkupSafe>=0.23 (from Jinja2>=2.4->Flask==0.12.1->-r requirements.txt (line 1))\n  Downloading MarkupSafe-1.0.tar.gz\nBuilding wheels for collected packages: itsdangerous, MarkupSafe\n  Running setup.py bdist_wheel for itsdangerous: started\n  Running setup.py bdist_wheel for itsdangerous: finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/fc/a8/66/24d655233c757e178d45dea2de22a04c6d92766abfb741129a\n  Running setup.py bdist_wheel for MarkupSafe: started\n  Running setup.py bdist_wheel for MarkupSafe: finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/88/a7/30/e39a54a87bcbe25308fa3ca64e8ddc75d9b3e5afa21ee32d57\nSuccessfully built itsdangerous MarkupSafe\nInstalling collected packages: MarkupSafe, Jinja2, click, Werkzeug, itsdangerous, Flask\nSuccessfully installed Flask-0.12.1 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.13 click-6.7 itsdangerous-0.24\n ---> d828a0518114\nRemoving intermediate container 1a708ec3b565\nStep 6/8 : ADD . /usr/src/app\n ---> 8e0efae73a47\nStep 7/8 : ENV FLASK_APP /usr/src/app/project/app.py\n ---> Running in 959581952d05\n ---> 20aaec61b615\nRemoving intermediate container 959581952d05\nStep 8/8 : CMD flask run\n ---> Running in 4f2fc701ba14\n ---> 1d5b59f3cef2\nRemoving intermediate container 4f2fc701ba14\n\nSuccessfully built 1d5b59f3cef2\nSuccessfully tagged testflask_users-service:latest\n\n",[33,44469,44467],{"__ignoreMap":35},[11,44471,44472,44473,44476,44477,44480],{},"That seemed to work. Next the tutorial says to run ",[33,44474,44475],{},"docker-compose -f docker-compose-dev.yml up -d",". I'll run this without the ",[33,44478,44479],{},"-d"," flag so we can see if there are any errors. Moment of truth!",[26,44482,44485],{"className":44483,"code":44484,"language":31},[29]," $ docker-compose -f docker-compose-dev.yml up\nCreating network \"testflask_default\" with the default driver\nCreating users-service ...\nCreating users-service ... done\nAttaching to users-service\nusers-service    | Usage: flask run [OPTIONS]\nusers-service    |\nusers-service    | Error: The file/path provided (/usr/src/app/project/app.py) does not appear to exist.  Please verify the path is correct.  If app is not on PYTHONPATH, ensure the extension is .py\nusers-service exited with code 2\n",[33,44486,44484],{"__ignoreMap":35},[11,44488,44489,44490,30978],{},"OK, we have an error that seems to have come from our ",[33,44491,43985],{},[11,44493,44494],{},"To bo continued...",[855,44496,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":44498},[44499,44500,44501,44502,44503],{"id":43842,"depth":312,"text":43843},{"id":43852,"depth":312,"text":43853},{"id":43862,"depth":312,"text":43863},{"id":43879,"depth":312,"text":43880},{"id":15298,"depth":249,"text":44308},"2017-12-09",{"layout":29014},"/2017/12/09/setting-up-flask-cli-with-docker",{"title":43800,"description":35},"2017/12/09/setting-up-flask-cli-with-docker",[24507,15298],"fbuiSF29xymI-3hKEWCZRvtX3gCtkDIYuVCn0uPUyh4",{"id":44512,"title":44513,"body":44514,"comments":315,"date":45118,"description":35,"draft":872,"extension":873,"external":874,"image":44520,"meta":45119,"navigation":315,"path":45120,"seo":45121,"stem":45122,"tags":45123,"__hash__":45125},"blog/2017/12/03/the-twelve-factor-app-and-my-experience-developing-web-apps.md","Reflecting on my web-app development process after reading The Twelve-Factor App",{"type":8,"value":44515,"toc":45104},[44516,44521,44533,44537,44542,44560,44566,44609,44613,44618,44625,44631,44634,44644,44660,44663,44668,44681,44685,44690,44693,44699,44706,44712,44716,44721,44724,44729,44738,44784,44797,44803,44819,44822,44851,44855,44860,44863,44869,44873,44878,44881,44887,44890,44903,44908,44915,44951,44955,44960,44963,44969,44974,44977,44983,44987,44992,44995,45001,45004,45009,45016,45020,45025,45030,45042,45047,45051,45056,45059,45062,45066,45071,45076,45082,45085,45089,45094,45099,45102],[11,44517,44518],{},[511,44519],{"alt":7255,"src":44520},"/static/the-12-factor-app.png",[11,44522,44523,44524,44527,44528,44532],{},"I recently had a conversation with a developer on the topic of bridging application development and production. From this conversation I was recommneded to have a look at ",[4339,44525,44526],{},"The Twelve-Factor App",", a high level guide for building modern, production-ready web applications. In this article I thought it would be interesting to go through each of the twelve sections and reflect on my current development process and how it follows and/or deviates from these factors. I also want to talk about the new technologies and techniques I have been learning from ",[15,44529,43818],{"href":44530,"rel":44531},"https://testdriven.io/",[19]," and how I hope they can benefit me on the next stage of my learning.",[168,44534,44536],{"id":44535},"i-codebase","I. Codebase",[11,44538,44539],{},[338,44540,44541],{},"One codebase tracked in revision control, many deploys",[11,44543,44544,44545,44550,44551,677,44554,677,44556,44559],{},"I have been using git to track my projects since starting ",[15,44546,44549],{"href":44547,"rel":44548},"https://www.obeythetestinggoat.com/",[19],"Obey the Testing Goat!"," and have been gradually exploring many of the different features beyond a linear ",[33,44552,44553],{},"add",[33,44555,19283],{},[33,44557,44558],{},"push"," loop. Using Visual Studio Code makes resolving merge conflicts very easy. On this blog I have accepted at least one pull requests from a helpful readers to correct outdated information.",[11,44561,44562,44563,44565],{},"The pattern I have been following for Django uses ",[33,44564,783],{}," to keep local application settings out of the production codebase using the following logic:",[26,44567,44569],{"className":1383,"code":44568,"language":1125,"meta":35,"style":35},"from .base import *\n\nfrom .production import *\n\ntry:\n    from .local import *\nexcept:\n    pass\n",[33,44570,44571,44576,44580,44585,44589,44594,44599,44604],{"__ignoreMap":35},[187,44572,44573],{"class":189,"line":190},[187,44574,44575],{},"from .base import *\n",[187,44577,44578],{"class":189,"line":249},[187,44579,316],{"emptyLinePlaceholder":315},[187,44581,44582],{"class":189,"line":312},[187,44583,44584],{},"from .production import *\n",[187,44586,44587],{"class":189,"line":319},[187,44588,316],{"emptyLinePlaceholder":315},[187,44590,44591],{"class":189,"line":325},[187,44592,44593],{},"try:\n",[187,44595,44596],{"class":189,"line":686},[187,44597,44598],{},"    from .local import *\n",[187,44600,44601],{"class":189,"line":697},[187,44602,44603],{},"except:\n",[187,44605,44606],{"class":189,"line":1291},[187,44607,44608],{},"    pass\n",[168,44610,44612],{"id":44611},"ii-dependencies","II. Dependencies",[11,44614,44615],{},[338,44616,44617],{},"Explicitly declare and isolate dependencies",[11,44619,44620,44621,44624],{},"Using python makes this easy and I have had success running ",[33,44622,44623],{},"pip install -r requirements.txt",", or:",[26,44626,44629],{"className":44627,"code":44628,"language":31},[29],"ADD ./requirements.txt /usr/src/app/requirements.txt\nRUN pip install -r requirements.txt\n",[33,44630,44628],{"__ignoreMap":35},[11,44632,44633],{},"when running Docker.",[11,44635,44636,44637,44640,44641,44643],{},"I have ran into minor dependency issues in experimenting with my personal website. I shouldn't have been doing it this way, but I made a virtual environment with ",[33,44638,44639],{},"conda create"," and included all of the packages in the environment in my production ",[33,44642,23123],{},". The Heroku build process gave me a tricky error that I was able to trace to a dependency that was failing to install on the Heroku instance.",[11,44645,44646,44647,637,44650,1172,44653,44656,44657,44659],{},"With Python 3 there are a few different options for creating virtual environments: ",[33,44648,44649],{},"virtualenv",[33,44651,44652],{},"venv",[33,44654,44655],{},"conda"," are three that I have used, and ",[33,44658,44649],{}," is a reliable tool for building webapps that I have seen used widely.",[11,44661,44662],{},"Another interesting tip from this section relates to common system tools:",[107,44664,44665],{},[11,44666,44667],{},"Twelve-factor apps also do not rely on the implicit existence of any system tools. Examples include shelling out to ImageMagick or curl.",[11,44669,44670,44671,44674,44675,44677,44678,752],{},"This is something that I have been grappling with in Docker. Tools like ",[33,44672,44673],{},"wget"," aren't part of \"base images\" and need to be intalled in the ",[33,44676,30713],{}," or in scripts called from ",[33,44679,44680],{},"docker-compose.yml",[168,44682,44684],{"id":44683},"iii-config","III. Config",[11,44686,44687],{},[338,44688,44689],{},"Store config in the environment",[11,44691,44692],{},"Heroku's command line utilities make setting production environments very easy. I think I can improve the way I organize environment variables locally since I often have many different projects it would be easy for projects to accidentally share the same variable name. One idea I have thought about would be to have an untracked bash script that sets environment variables that I run when starting the development environment, something like:",[26,44694,44697],{"className":44695,"code":44696,"language":31},[29],"export SECRET_KEY=\"my_secret_key\"\nexport DB_URL=\"postgres://my_db_url\"\n",[33,44698,44696],{"__ignoreMap":35},[11,44700,44701,44702,44705],{},"Docker wins points again on this factor because environemnt variables can simple be defined in a development and production ",[33,44703,44704],{},"docker-compose-*.yml"," files:",[26,44707,44710],{"className":44708,"code":44709,"language":31},[29],"    environment:\n      - APP_SETTINGS=project.config.DevelopmentConfig\n      - DATABASE_URL=postgres://postgres:postgres@users-db:5432/users_dev\n      - DATABASE_TEST_URL=postgres://postgres:postgres@users-db:5432/users_test\n",[33,44711,44709],{"__ignoreMap":35},[168,44713,44715],{"id":44714},"iv-backing-services","IV. Backing services",[11,44717,44718],{},[338,44719,44720],{},"Treat backing services as attached resources",[11,44722,44723],{},"The key takeaway for backing services (databases, message/queuing systems, etc.) is:",[107,44725,44726],{},[11,44727,44728],{},"The code for a twelve-factor app makes no distinction between local and third party services.",[11,44730,44731,44732,44737],{},"This factor made me think of an interesting syntax that I saw for the first time when learning the microservices architecture with docker in ",[15,44733,44736],{"href":44734,"rel":44735},"https://github.com/jakewright/tutorials/tree/master/docker/02-docker-compose",[19],"this Docker example"," that uses two Flask apps to provide 1) front-end templates and 2) API backend. When the front end calls the API backend, it does so by referencing the service name in the URL. Here is an example with PHP, but it would work similarly in any other framework:",[26,44739,44743],{"className":44740,"code":44741,"language":44742,"meta":35,"style":35},"language-php shiki shiki-themes github-light github-dark","\u003C?php\n    $json = file_get_contents('http://product-service/');\n    $obj = json_decode($json);\n    $products = $obj->products;\n    foreach ($products as $product) {\n        echo \"\u003Cli>$product\u003C/li>\";\n    }\n?>\n","php",[33,44744,44745,44750,44755,44760,44765,44770,44775,44779],{"__ignoreMap":35},[187,44746,44747],{"class":189,"line":190},[187,44748,44749],{},"\u003C?php\n",[187,44751,44752],{"class":189,"line":249},[187,44753,44754],{},"    $json = file_get_contents('http://product-service/');\n",[187,44756,44757],{"class":189,"line":312},[187,44758,44759],{},"    $obj = json_decode($json);\n",[187,44761,44762],{"class":189,"line":319},[187,44763,44764],{},"    $products = $obj->products;\n",[187,44766,44767],{"class":189,"line":325},[187,44768,44769],{},"    foreach ($products as $product) {\n",[187,44771,44772],{"class":189,"line":686},[187,44773,44774],{},"        echo \"\u003Cli>$product\u003C/li>\";\n",[187,44776,44777],{"class":189,"line":697},[187,44778,9799],{},[187,44780,44781],{"class":189,"line":1291},[187,44782,44783],{},"?>\n",[11,44785,44786,44787,44790,44791,44794,44795,358],{},"This front-end code hits the backend API endpoint ",[33,44788,44789],{},"http://product-service/",", where ",[33,44792,44793],{},"product-service"," is the name of a service included in ",[33,44796,44680],{},[26,44798,44801],{"className":44799,"code":44800,"language":31},[29],"version: '3'\n\nservices:\n  product-service:\n    build: ./product\n    volumes:\n      - ./product:/usr/src/app\n    ports:\n      - 5001:80\n\n  website:\n    image: php:apache\n    volumes:\n      - ./website:/var/www/html\n    ports:\n      - 5000:80\n    depends_on:\n      - product-service\n",[33,44802,44800],{"__ignoreMap":35},[11,44804,44805,44806,1172,44808,44810,44811,44814,44815,44818],{},"This is docker-compose file creates a network that includes both ",[33,44807,40613],{},[33,44809,44793],{}," that can be accessed by simply creating a URL with the name of the service in the domain. Coming back to the fourth factor, ",[338,44812,44813],{},"The code for a twelve-factor app makes no distinction between local and third party services",", multiple docker containers can be thought of as ",[4339,44816,44817],{},"separate services"," even though they may be running on the same virtual environment in either development or production, and the unique domain cooresponding to the service name seems to reinforce this concept.",[11,44820,44821],{},"This URL could also be referenced with environment variables:",[26,44823,44825],{"className":6362,"code":44824,"language":6364,"meta":35,"style":35},"getUsers() {\n  axios.get(`${process.env.REACT_APP_USERS_SERVICE_URL}/users`)\n  .then((res) => { console.log(res); })\n  .catch((err) => { console.log(err); })\n}\n",[33,44826,44827,44832,44837,44842,44847],{"__ignoreMap":35},[187,44828,44829],{"class":189,"line":190},[187,44830,44831],{},"getUsers() {\n",[187,44833,44834],{"class":189,"line":249},[187,44835,44836],{},"  axios.get(`${process.env.REACT_APP_USERS_SERVICE_URL}/users`)\n",[187,44838,44839],{"class":189,"line":312},[187,44840,44841],{},"  .then((res) => { console.log(res); })\n",[187,44843,44844],{"class":189,"line":319},[187,44845,44846],{},"  .catch((err) => { console.log(err); })\n",[187,44848,44849],{"class":189,"line":325},[187,44850,1309],{},[168,44852,44854],{"id":44853},"v-build-release-run","V. Build, release, run",[11,44856,44857],{},[338,44858,44859],{},"Strictly separate build and run stages",[11,44861,44862],{},"This is an important stage in getting from development to production. It is the handoff from a local repo to a live, running web application. Here's the process live in action:",[26,44864,44867],{"className":44865,"code":44866,"language":31},[29]," $ git push heroku master\nCounting objects: 3, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 303 bytes | 303.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0)\nremote: Compressing source files... done.\nremote: Building source:\nremote:\nremote: -----> Python app detected\nremote: -----> Installing requirements with pip\nremote:\nremote: -----> Discovering process types\nremote:        Procfile declares types -> web\nremote:\nremote: -----> Compressing...\nremote:        Done: 193.7M\nremote: -----> Launching...\nremote:        Released v410\nremote:        https://briancaffey.herokuapp.com/ deployed to Heroku\nremote:\nremote: Verifying deploy... done.\nTo https://git.heroku.com/briancaffey.git\n   cd6ce76..d8981a3  master -> master\n(briancaffey) brian@archthinkpad ~/Documents/github/briancaffey/src\n",[33,44868,44866],{"__ignoreMap":35},[168,44870,44872],{"id":44871},"vi-processes","VI. Processes",[11,44874,44875],{},[338,44876,44877],{},"Execute the app as one or more stateless processes",[11,44879,44880],{},"This is handled in Heroku by the Procfile. For simple Django apps on Heroku this is usually always the same one line:",[26,44882,44885],{"className":44883,"code":44884,"language":31},[29],"web: gunicorn projectname.wsgi --log-file -\n",[33,44886,44884],{"__ignoreMap":35},[11,44888,44889],{},"Here's what the Django project says about using gunicorn:",[107,44891,44892,44895,44898],{},[11,44893,44894],{},"When Gunicorn is installed, a gunicorn command is available which starts the Gunicorn server process. At its simplest, gunicorn just needs to be called with the location of a module containing a WSGI application object named application.",[11,44896,44897],{},"So for a typical Django project, invoking gunicorn would look like:",[11,44899,44900],{},[33,44901,44902],{},"gunicorn myproject.wsgi",[107,44904,44905],{},[11,44906,44907],{},"This will start one process running one thread listening on 127.0.0.1:8000. It requires that your project be on the Python path; the simplest way to ensure that is to run this command from the same directory as your manage.py file.",[11,44909,44910,44911,44914],{},"In a Django project, the ",[33,44912,44913],{},"wsgi.py"," file in the main folder of the project root directory has the following contents:",[26,44916,44918],{"className":1383,"code":44917,"language":1125,"meta":35,"style":35},"import os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"brianblog.settings\")\n\napplication = get_wsgi_application()\n",[33,44919,44920,44924,44928,44933,44937,44942,44946],{"__ignoreMap":35},[187,44921,44922],{"class":189,"line":190},[187,44923,10345],{},[187,44925,44926],{"class":189,"line":249},[187,44927,316],{"emptyLinePlaceholder":315},[187,44929,44930],{"class":189,"line":312},[187,44931,44932],{},"from django.core.wsgi import get_wsgi_application\n",[187,44934,44935],{"class":189,"line":319},[187,44936,316],{"emptyLinePlaceholder":315},[187,44938,44939],{"class":189,"line":325},[187,44940,44941],{},"os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"brianblog.settings\")\n",[187,44943,44944],{"class":189,"line":686},[187,44945,316],{"emptyLinePlaceholder":315},[187,44947,44948],{"class":189,"line":697},[187,44949,44950],{},"application = get_wsgi_application()\n",[168,44952,44954],{"id":44953},"vi-port-binding","VI. Port binding",[11,44956,44957],{},[338,44958,44959],{},"Export services via port binding",[11,44961,44962],{},"This is an area that I'm trying to learn more about. I feel like I have a pretty good grasp of what is going on regarding port binding in the microservice architecture with docker I have seen.",[11,44964,44965,44966,358],{},"From the docker-compose docs, the \"short syntax\" for mapping ports between hosts and containers is ",[33,44967,44968],{},"HOST:CONTAINER",[107,44970,44971],{},[11,44972,44973],{},"Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen).",[11,44975,44976],{},"The following are examples of how this could work:",[26,44978,44981],{"className":44979,"code":44980,"language":31},[29],"ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n - \"6060:6060/udp\"\n",[33,44982,44980],{"__ignoreMap":35},[168,44984,44986],{"id":44985},"viii-concurrency","VIII. Concurrency",[11,44988,44989],{},[338,44990,44991],{},"Scale out via the process model",[11,44993,44994],{},"This is an important area, but it is something I haven't had to be aware of since the apps I have developed don't require scaling processes. I belive that Heroku makes this fairly simple by allowing you to increase the number of web or worker processes through the CLI:",[26,44996,44999],{"className":44997,"code":44998,"language":31},[29],"heroku ps:scale web=1 worker=5\n",[33,45000,44998],{"__ignoreMap":35},[11,45002,45003],{},"I haven't covered Part 5 of testdriven.io yet, but it has a section on Elastic Load Balancing with EC2 which should cover this area.",[107,45005,45006],{},[11,45007,45008],{},"Twelve-factor app processes should never daemonize or write PID files. Instead, rely on the operating system’s process manager to manage output streams, respond to crashed processes, and handle user-initiated restarts and shutdowns.",[11,45010,45011,45012,45015],{},"I have been learning more about ",[33,45013,45014],{},"systemd"," and customizing",[168,45017,45019],{"id":45018},"ix-disposability","IX. Disposability",[11,45021,45022],{},[338,45023,45024],{},"Maximize robustness with fast startup and graceful shutdown",[107,45026,45027],{},[11,45028,45029],{},"The twelve-factor app’s processes are disposable, meaning they can be started or stopped at a moment’s notice. This facilitates fast elastic scaling, rapid deployment of code or config changes, and robustness of production deploys.",[11,45031,45032,45033,45038,45039,18410],{},"I have used some Heroku tools to start and stop web workers, and docker commands make this factor fairly easy to do correctly. Here's an excerpt from ",[15,45034,45037],{"href":45035,"rel":45036},"https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/",[19],"Century Link"," about the ",[33,45040,45041],{},"docker stop",[107,45043,45044],{},[11,45045,45046],{},"The docker stop command attempts to stop a running container first by sending a SIGTERM signal to the root process (PID 1) in the container. If the process hasn't exited within the timeout period a SIGKILL signal will be sent.",[168,45048,45050],{"id":45049},"x-devprod-parity","X. Dev/prod parity",[107,45052,45053],{},[11,45054,45055],{},"Keep development, staging, and production as similar as possible",[11,45057,45058],{},"This is exactly why I'm so interested in using Docker.",[11,45060,45061],{},"In one of my personal projects I did with Heroku I was relying on a feature of Postgres that is not available in sqlite3, the default database that comes with Django. This produced friction that I wouldn't have had to deal with if I was using Docker. I could have set up a local postgres server, but it would have been much easier to run a docker container that ran the server.",[168,45063,45065],{"id":45064},"xi-logs","XI. Logs",[11,45067,45068],{},[338,45069,45070],{},"Treat logs as event streams",[107,45072,45073],{},[11,45074,45075],{},"A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior.",[11,45077,30971,45078,45081],{},[33,45079,45080],{},"heroku log"," has been helpful in debugging deployment issues.",[11,45083,45084],{},"Docker also produces helpful logs for all the containers currently running.",[168,45086,45088],{"id":45087},"xii-admin-processes","XII. Admin processes",[11,45090,45091],{},[338,45092,45093],{},"Run admin/management tasks as one-off processes",[107,45095,45096],{},[11,45097,45098],{},"One-off admin processes should be run in an identical environment as the regular long-running processes of the app. They run against a release, using the same codebase and config as any process run against that release. Admin code must ship with application code to avoid synchronization issues.",[11,45100,45101],{},"This seems to be true about the way I run admin processes on my Django apps.",[855,45103,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":45105},[45106,45107,45108,45109,45110,45111,45112,45113,45114,45115,45116,45117],{"id":44535,"depth":249,"text":44536},{"id":44611,"depth":249,"text":44612},{"id":44683,"depth":249,"text":44684},{"id":44714,"depth":249,"text":44715},{"id":44853,"depth":249,"text":44854},{"id":44871,"depth":249,"text":44872},{"id":44953,"depth":249,"text":44954},{"id":44985,"depth":249,"text":44986},{"id":45018,"depth":249,"text":45019},{"id":45049,"depth":249,"text":45050},{"id":45064,"depth":249,"text":45065},{"id":45087,"depth":249,"text":45088},"2017-12-03",{"layout":29014},"/2017/12/03/the-twelve-factor-app-and-my-experience-developing-web-apps",{"title":44513,"description":35},"2017/12/03/the-twelve-factor-app-and-my-experience-developing-web-apps",[45124,15298],"development","Yxnwmq7RszHrbMNugtpEvMTRljc16Aoy0-hAP56Fwh0",{"id":45127,"title":45128,"body":45129,"comments":315,"date":45890,"description":45891,"draft":872,"extension":873,"external":874,"image":45232,"meta":45892,"navigation":315,"path":45893,"seo":45894,"stem":45895,"tags":45896,"__hash__":45898},"blog/2017/12/02/arch-linux-package-data-analysis.md","Analysis of AUR and Official Arch Repository data",{"type":8,"value":45130,"toc":45883},[45131,45138,45147,45151,45158,45163,45218,45221,45225,45228,45233,45236,45244,45261,45281,45284,45292,45295,45298,45323,45326,45331,45351,45356,45365,45370,45385,45390,45405,45410,45425,45445,45470,45473,45477,45492,45502,45505,45508,45668,45672,45675,45680,45683,45803,45807,45810,45815,45874,45881],[11,45132,45133,45134,45137],{},"Arch Linux provides packages through the official Arch Linux repositories and the Arch User Repository (AUR). I recently gathered data on ~50,000 packages from these repositories on ",[15,45135,45136],{"href":45136},"archlinux.org"," to better understand the makeup of the packages. In this article I will share some visualizations I made as well as some key takeaways about the data set I gathered.",[11,45139,45140,45141,45146],{},"The repo with all of the data I collected as well as the code I used to do so is available in ",[15,45142,45145],{"href":45143,"rel":45144},"https://github.com/briancaffey/AUR-data",[19],"this repository"," on my Github account.",[168,45148,45150],{"id":45149},"growth-of-the-aur","Growth of the AUR",[11,45152,45153,45154,45157],{},"The first questions I had about the dataset were about visualizing the growth of the AUR over time. Each package in the AUR has a ",[33,45155,45156],{},"First Submitted"," date, so I was able to put this together easily:",[11,45159,45160],{},[511,45161],{"alt":7255,"src":45162},"/static/aur/aur_packages.png",[26,45164,45166],{"className":1383,"code":45165,"language":1125,"meta":35,"style":35},"sns.set()\ndf = df[df['First Submitted'].notnull()]\ndf[\"First Submitted\"] = pd.to_datetime(df['First Submitted'])\nlist_of_dates = df[\"First Submitted\"].sort_values()\ncounts = np.arange(0, len(list_of_dates))\nplt.figure(figsize=(10, 5))\n_ = plt.plot(list_of_dates, counts)\n_ = plt.title('AUR Packages over time')\n_ = plt.xlabel('Date')\n_ = plt.ylabel('Packages')\n",[33,45167,45168,45173,45178,45183,45188,45193,45198,45203,45208,45213],{"__ignoreMap":35},[187,45169,45170],{"class":189,"line":190},[187,45171,45172],{},"sns.set()\n",[187,45174,45175],{"class":189,"line":249},[187,45176,45177],{},"df = df[df['First Submitted'].notnull()]\n",[187,45179,45180],{"class":189,"line":312},[187,45181,45182],{},"df[\"First Submitted\"] = pd.to_datetime(df['First Submitted'])\n",[187,45184,45185],{"class":189,"line":319},[187,45186,45187],{},"list_of_dates = df[\"First Submitted\"].sort_values()\n",[187,45189,45190],{"class":189,"line":325},[187,45191,45192],{},"counts = np.arange(0, len(list_of_dates))\n",[187,45194,45195],{"class":189,"line":686},[187,45196,45197],{},"plt.figure(figsize=(10, 5))\n",[187,45199,45200],{"class":189,"line":697},[187,45201,45202],{},"_ = plt.plot(list_of_dates, counts)\n",[187,45204,45205],{"class":189,"line":1291},[187,45206,45207],{},"_ = plt.title('AUR Packages over time')\n",[187,45209,45210],{"class":189,"line":1306},[187,45211,45212],{},"_ = plt.xlabel('Date')\n",[187,45214,45215],{"class":189,"line":1434},[187,45216,45217],{},"_ = plt.ylabel('Packages')\n",[11,45219,45220],{},"It looks like there was a big boost in the number of packages submitted in mid-2015 and that number has been growing consistantly since then.",[168,45222,45224],{"id":45223},"official-repositories","Official Repositories",[11,45226,45227],{},"The official repositories contain just under 10,000 packages. Here is a force-directed graph (undirected graph) in D3.js that shows these packages as nodes and package dependencies as edges. This image shows a sample of about 1,000 packages and it is somewhat representative of the whole graph show further below.",[11,45229,45230],{},[511,45231],{"alt":7255,"src":45232},"/static/aur/official_repos.png",[11,45234,45235],{},"There are four main repo in the official repositories:",[916,45237,45238],{},[919,45239,45240,45243],{},[338,45241,45242],{},"core"," contains packages for:",[916,45245,45246,45249,45252,45255,45258],{},[919,45247,45248],{},"booting Arch Linux",[919,45250,45251],{},"connecting to the Internet",[919,45253,45254],{},"building packages",[919,45256,45257],{},"management and repair of supported file systems",[919,45259,45260],{},"the system setup process (e.g. openssh)",[916,45262,45263,45269,45275],{},[919,45264,45265,45268],{},[338,45266,45267],{},"extra"," contains all packages that do not fit in core. Example: Xorg, window managers, web browsers, media players, tools for working with languages such as Python and Ruby, and a lot more.",[919,45270,45271,45274],{},[338,45272,45273],{},"community"," contains packages that have been adopted by Trusted Users from the Arch User Repository. Some of these packages may eventually make the transition to the core or extra repositories as the developers consider them crucial to the distribution.",[919,45276,45277,45280],{},[338,45278,45279],{},"multilib"," contains 32 bit software and libraries that can be used to run and build 32 bit applications on 64 bit installs (e.g. wine, steam, etc).",[11,45282,45283],{},"Here is an SVG showing all packages in the official repository. Click on the image to explore the SVG in more detail, and you can hover over nodes to see which packages they represent.",[11,45285,45286],{},[15,45287,45289],{"href":45288},"/static/aur/graph.svg",[511,45290],{"alt":45291,"src":45288},"svg",[11,45293,45294],{},"Turn off the lights and you can see a ring of packages orbiting in a circle!",[45296,45297],"color-mode-picker",{},[11,45299,45300,45301,5857,45304,637,45306,23120,45309,45312,45313,45316,45317,1172,45320,752],{},"If you look at this file in detail you can find some interesting clusters of packages. The \"island\" in the top left includes mostly Haskell packages and ",[33,45302,45303],{},"pandoc",[33,45305,1125],{},[33,45307,45308],{},"python2",[33,45310,45311],{},"git"," are three of the main central hubs in the middle cluster. ",[33,45314,45315],{},"perl"," and other related packages make of most of the bottom right cluster and you can see \"flowers\" of package dependencies mostly for internationalization for popular programs like ",[33,45318,45319],{},"firefox",[33,45321,45322],{},"thunderbird",[11,45324,45325],{},"To make this interactive graph and SVG images I did the following:",[916,45327,45328],{},[919,45329,45330],{},"Create a dictionary from my data base with packages keys and a list of dependencies as values:",[26,45332,45334],{"className":1383,"code":45333,"language":1125,"meta":35,"style":35},"graph_dict = {}\nfor _, i in df.iterrows():\n    graph_dict[i[\"package_name\"]] = i[\"pkgdeps\"]\n",[33,45335,45336,45341,45346],{"__ignoreMap":35},[187,45337,45338],{"class":189,"line":190},[187,45339,45340],{},"graph_dict = {}\n",[187,45342,45343],{"class":189,"line":249},[187,45344,45345],{},"for _, i in df.iterrows():\n",[187,45347,45348],{"class":189,"line":312},[187,45349,45350],{},"    graph_dict[i[\"package_name\"]] = i[\"pkgdeps\"]\n",[916,45352,45353],{},[919,45354,45355],{},"Create a NetworkX graph with the dictionary created in the previous step:",[26,45357,45359],{"className":1383,"code":45358,"language":1125,"meta":35,"style":35},"G = nx.Graph(graph_dict)\n",[33,45360,45361],{"__ignoreMap":35},[187,45362,45363],{"class":189,"line":190},[187,45364,45358],{},[916,45366,45367],{},[919,45368,45369],{},"Export the NetworkX graph to JSON using a built-in NetworkX function:",[26,45371,45373],{"className":1383,"code":45372,"language":1125,"meta":35,"style":35},"from networkx.readwrite import json_graph\ndata = json_graph.node_link_data(G)\n",[33,45374,45375,45380],{"__ignoreMap":35},[187,45376,45377],{"class":189,"line":190},[187,45378,45379],{},"from networkx.readwrite import json_graph\n",[187,45381,45382],{"class":189,"line":249},[187,45383,45384],{},"data = json_graph.node_link_data(G)\n",[916,45386,45387],{},[919,45388,45389],{},"Add a group number to each node element cooresponding to the repository it belongs to.",[26,45391,45393],{"className":1383,"code":45392,"language":1125,"meta":35,"style":35},"for n in data['nodes']:\n    n['group'] = int(df.loc[(df.package_name == n[\"id\"]), \"repo_number\"].iloc[0])\n",[33,45394,45395,45400],{"__ignoreMap":35},[187,45396,45397],{"class":189,"line":190},[187,45398,45399],{},"for n in data['nodes']:\n",[187,45401,45402],{"class":189,"line":249},[187,45403,45404],{},"    n['group'] = int(df.loc[(df.package_name == n[\"id\"]), \"repo_number\"].iloc[0])\n",[916,45406,45407],{},[919,45408,45409],{},"Save the JSON to a file:",[26,45411,45413],{"className":1383,"code":45412,"language":1125,"meta":35,"style":35},"with open('/home/brian/Documents/github/briancaffey.github.io/aur/data.json', 'w') as outfile:\n    json.dump(data, outfile)\n",[33,45414,45415,45420],{"__ignoreMap":35},[187,45416,45417],{"class":189,"line":190},[187,45418,45419],{},"with open('/home/brian/Documents/github/briancaffey.github.io/aur/data.json', 'w') as outfile:\n",[187,45421,45422],{"class":189,"line":249},[187,45423,45424],{},"    json.dump(data, outfile)\n",[916,45426,45427,45436],{},[919,45428,45429,45430,45435],{},"Feed the JSON file into ",[15,45431,45434],{"href":45432,"rel":45433},"https://bl.ocks.org/mbostock/4062045",[19],"this template"," which renders a D3.js force-directed graph.",[919,45437,45438,45439,45444],{},"To save the graph as a SVG file, I ran the ",[15,45440,45443],{"href":45441,"rel":45442},"https://graphicdesign.stackexchange.com/questions/55123/how-do-i-save-an-svg-thats-on-a-website-to-my-computer",[19],"NYT crowbar script"," in the browser console:",[26,45446,45448],{"className":6362,"code":45447,"language":6364,"meta":35,"style":35},"var e = document.createElement('script')\ne.setAttribute('src', 'https://nytimes.github.io/svg-crowbar/svg-crowbar.js')\ne.setAttribute('class', 'svg-crowbar')\ndocument.body.appendChild(e)\n",[33,45449,45450,45455,45460,45465],{"__ignoreMap":35},[187,45451,45452],{"class":189,"line":190},[187,45453,45454],{},"var e = document.createElement('script')\n",[187,45456,45457],{"class":189,"line":249},[187,45458,45459],{},"e.setAttribute('src', 'https://nytimes.github.io/svg-crowbar/svg-crowbar.js')\n",[187,45461,45462],{"class":189,"line":312},[187,45463,45464],{},"e.setAttribute('class', 'svg-crowbar')\n",[187,45466,45467],{"class":189,"line":319},[187,45468,45469],{},"document.body.appendChild(e)\n",[11,45471,45472],{},"Let's take one more look at how tightly",[168,45474,45476],{"id":45475},"official-repository-package-sizes","Official Repository Package Sizes",[11,45478,45479,45480,1172,45483,45486,45487,45489,45490,358],{},"Official Packages include both a ",[33,45481,45482],{},"Package Size",[33,45484,45485],{},"Installed Size",". Here is a Bokeh plot showing ",[33,45488,45482],{}," vs. ",[33,45491,45485],{},[11,45493,45494,45497,45498,45501],{},[338,45495,45496],{},"Warning",": Don't hover directly over the cluster of plotted points near the origin of the graph. ",[338,45499,45500],{},"DOING SO WILL CRASH YOUR BROWSER",". This is because the hover tool will attempt to display all packages that you are hovered over and it may be far too many for the browser to handle. Carefully zoom in using the scroll tool and you can find some interesting trends in the types of packages and how much they are able to be compressed.",[11,45503,45504],{},"{% include package_sizes.html %}",[11,45506,45507],{},"Here's the setup for this bokeh graph:",[26,45509,45511],{"className":1383,"code":45510,"language":1125,"meta":35,"style":35},"from bokeh.plotting import figure, output_file, show, ColumnDataSource\nfrom bokeh.models import HoverTool\nfrom bokeh.io import output_notebook\noutput_notebook()\n\noutput_file(\"/home/brian/Documents/github/briancaffey.github.io/_includes/package_sizes.html\")\n\nsource = ColumnDataSource(\n        data=dict(\n            x=df.package_size,\n            y=df.installed_size,\n            desc=df.Description,\n            name=df.package_name\n        )\n    )\n\nhover = HoverTool(\n        tooltips=[\n            (\"Name\", \"@name\"),\n            (\"Package Size\", \"@x MB\"),\n            (\"Installed Size\", \"@y MB\"),\n            (\"Description\", \"@desc\"),\n        ]\n    )\n\nTOOLS = 'box_zoom,box_select,reset,pan,wheel_zoom'\n\np = figure(plot_width=400, plot_height=400, tools=[TOOLS, hover],\n           title=\"Packages Size vs. Installed Size\", sizing_mode='scale_width')\n\np.circle('x', 'y', size=5, source=source, alpha=0.2)\np.toolbar.logo = None\nshow(p)\n",[33,45512,45513,45518,45523,45528,45533,45537,45542,45546,45551,45556,45561,45566,45571,45576,45580,45584,45588,45593,45598,45603,45608,45613,45618,45622,45626,45630,45635,45639,45644,45649,45653,45658,45663],{"__ignoreMap":35},[187,45514,45515],{"class":189,"line":190},[187,45516,45517],{},"from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",[187,45519,45520],{"class":189,"line":249},[187,45521,45522],{},"from bokeh.models import HoverTool\n",[187,45524,45525],{"class":189,"line":312},[187,45526,45527],{},"from bokeh.io import output_notebook\n",[187,45529,45530],{"class":189,"line":319},[187,45531,45532],{},"output_notebook()\n",[187,45534,45535],{"class":189,"line":325},[187,45536,316],{"emptyLinePlaceholder":315},[187,45538,45539],{"class":189,"line":686},[187,45540,45541],{},"output_file(\"/home/brian/Documents/github/briancaffey.github.io/_includes/package_sizes.html\")\n",[187,45543,45544],{"class":189,"line":697},[187,45545,316],{"emptyLinePlaceholder":315},[187,45547,45548],{"class":189,"line":1291},[187,45549,45550],{},"source = ColumnDataSource(\n",[187,45552,45553],{"class":189,"line":1306},[187,45554,45555],{},"        data=dict(\n",[187,45557,45558],{"class":189,"line":1434},[187,45559,45560],{},"            x=df.package_size,\n",[187,45562,45563],{"class":189,"line":2599},[187,45564,45565],{},"            y=df.installed_size,\n",[187,45567,45568],{"class":189,"line":2607},[187,45569,45570],{},"            desc=df.Description,\n",[187,45572,45573],{"class":189,"line":2621},[187,45574,45575],{},"            name=df.package_name\n",[187,45577,45578],{"class":189,"line":2631},[187,45579,4531],{},[187,45581,45582],{"class":189,"line":2642},[187,45583,23653],{},[187,45585,45586],{"class":189,"line":2653},[187,45587,316],{"emptyLinePlaceholder":315},[187,45589,45590],{"class":189,"line":2665},[187,45591,45592],{},"hover = HoverTool(\n",[187,45594,45595],{"class":189,"line":2674},[187,45596,45597],{},"        tooltips=[\n",[187,45599,45600],{"class":189,"line":2684},[187,45601,45602],{},"            (\"Name\", \"@name\"),\n",[187,45604,45605],{"class":189,"line":2694},[187,45606,45607],{},"            (\"Package Size\", \"@x MB\"),\n",[187,45609,45610],{"class":189,"line":2706},[187,45611,45612],{},"            (\"Installed Size\", \"@y MB\"),\n",[187,45614,45615],{"class":189,"line":2715},[187,45616,45617],{},"            (\"Description\", \"@desc\"),\n",[187,45619,45620],{"class":189,"line":2725},[187,45621,25086],{},[187,45623,45624],{"class":189,"line":2735},[187,45625,23653],{},[187,45627,45628],{"class":189,"line":2743},[187,45629,316],{"emptyLinePlaceholder":315},[187,45631,45632],{"class":189,"line":2754},[187,45633,45634],{},"TOOLS = 'box_zoom,box_select,reset,pan,wheel_zoom'\n",[187,45636,45637],{"class":189,"line":2762},[187,45638,316],{"emptyLinePlaceholder":315},[187,45640,45641],{"class":189,"line":2770},[187,45642,45643],{},"p = figure(plot_width=400, plot_height=400, tools=[TOOLS, hover],\n",[187,45645,45646],{"class":189,"line":2781},[187,45647,45648],{},"           title=\"Packages Size vs. Installed Size\", sizing_mode='scale_width')\n",[187,45650,45651],{"class":189,"line":2792},[187,45652,316],{"emptyLinePlaceholder":315},[187,45654,45655],{"class":189,"line":2803},[187,45656,45657],{},"p.circle('x', 'y', size=5, source=source, alpha=0.2)\n",[187,45659,45660],{"class":189,"line":2808},[187,45661,45662],{},"p.toolbar.logo = None\n",[187,45664,45665],{"class":189,"line":2816},[187,45666,45667],{},"show(p)\n",[168,45669,45671],{"id":45670},"aur-word-cloud","AUR Word Cloud",[11,45673,45674],{},"Let's make a word cloud out of text descriptions for packages in the AUR.",[11,45676,45677],{},[511,45678],{"alt":7255,"src":45679},"/static/aur/word_cloud.png",[11,45681,45682],{},"We can use a popular python package for making word clouds. Here's the code:",[26,45684,45686],{"className":1383,"code":45685,"language":1125,"meta":35,"style":35},"import numpy as np\nfrom PIL import Image\nfrom os import path\nimport matplotlib.pyplot as plt\nimport random\n\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef grey_color_func(word, font_size, position, orientation, random_state=None,\n                    **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n\nmask = np.array(Image.open(\"/home/brian/Documents/aur/images/arch_logo.png\"))\n\ntext = open(\"/home/brian/Documents/aur/ipynb/package_descriptions.txt\").read()\n\nwc = WordCloud(max_words=1000, mask=mask, stopwords=stopwords, margin=10,\n               random_state=1).generate(text)\n\ndefault_colors = wc.to_array()\nplt.figure(figsize=(20, 20))\nplt.imshow(wc.recolor(color_func=grey_color_func, random_state=3),\n           interpolation=\"bilinear\")\nwc.to_file(\"arch_word_cloud.png\")\nplt.axis(\"off\")\nplt.show()\n",[33,45687,45688,45692,45696,45700,45704,45708,45712,45716,45720,45725,45730,45735,45739,45744,45748,45753,45757,45762,45767,45771,45775,45780,45785,45790,45795,45799],{"__ignoreMap":35},[187,45689,45690],{"class":189,"line":190},[187,45691,10563],{},[187,45693,45694],{"class":189,"line":249},[187,45695,26048],{},[187,45697,45698],{"class":189,"line":312},[187,45699,26026],{},[187,45701,45702],{"class":189,"line":319},[187,45703,26035],{},[187,45705,45706],{"class":189,"line":325},[187,45707,26012],{},[187,45709,45710],{"class":189,"line":686},[187,45711,316],{"emptyLinePlaceholder":315},[187,45713,45714],{"class":189,"line":697},[187,45715,26053],{},[187,45717,45718],{"class":189,"line":1291},[187,45719,316],{"emptyLinePlaceholder":315},[187,45721,45722],{"class":189,"line":1306},[187,45723,45724],{},"def grey_color_func(word, font_size, position, orientation, random_state=None,\n",[187,45726,45727],{"class":189,"line":1434},[187,45728,45729],{},"                    **kwargs):\n",[187,45731,45732],{"class":189,"line":2599},[187,45733,45734],{},"    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",[187,45736,45737],{"class":189,"line":2607},[187,45738,316],{"emptyLinePlaceholder":315},[187,45740,45741],{"class":189,"line":2621},[187,45742,45743],{},"mask = np.array(Image.open(\"/home/brian/Documents/aur/images/arch_logo.png\"))\n",[187,45745,45746],{"class":189,"line":2631},[187,45747,316],{"emptyLinePlaceholder":315},[187,45749,45750],{"class":189,"line":2642},[187,45751,45752],{},"text = open(\"/home/brian/Documents/aur/ipynb/package_descriptions.txt\").read()\n",[187,45754,45755],{"class":189,"line":2653},[187,45756,316],{"emptyLinePlaceholder":315},[187,45758,45759],{"class":189,"line":2665},[187,45760,45761],{},"wc = WordCloud(max_words=1000, mask=mask, stopwords=stopwords, margin=10,\n",[187,45763,45764],{"class":189,"line":2674},[187,45765,45766],{},"               random_state=1).generate(text)\n",[187,45768,45769],{"class":189,"line":2684},[187,45770,316],{"emptyLinePlaceholder":315},[187,45772,45773],{"class":189,"line":2694},[187,45774,26114],{},[187,45776,45777],{"class":189,"line":2706},[187,45778,45779],{},"plt.figure(figsize=(20, 20))\n",[187,45781,45782],{"class":189,"line":2715},[187,45783,45784],{},"plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3),\n",[187,45786,45787],{"class":189,"line":2725},[187,45788,45789],{},"           interpolation=\"bilinear\")\n",[187,45791,45792],{"class":189,"line":2735},[187,45793,45794],{},"wc.to_file(\"arch_word_cloud.png\")\n",[187,45796,45797],{"class":189,"line":2743},[187,45798,26137],{},[187,45800,45801],{"class":189,"line":2754},[187,45802,26147],{},[168,45804,45806],{"id":45805},"arch-wiki-members","Arch Wiki Members",[11,45808,45809],{},"The Arch wiki is the first place I go for troubleshooting any issue with Arch. Users of other Linux distributions have also said how useful it can be even if you don't user Arch Linux. Here's a look at the number of registered users on the Arch Wiki over time:",[11,45811,45812],{},[511,45813],{"alt":7255,"src":45814},"/static/aur/wiki_users.png",[26,45816,45818],{"className":1383,"code":45817,"language":1125,"meta":35,"style":35},"sns.set()\ndf = df[df['registered'].notnull()]\ndf[\"registered\"] = pd.to_datetime(df['registered'])\nlist_of_dates = df[\"registered\"].sort_values()\ncounts = np.arange(0, len(list_of_dates))\nplt.figure(figsize=(10, 5))\n_ = plt.plot(list_of_dates, counts)\n_ = plt.title('Registered Arch Wiki members over time')\n_ = plt.xlabel('Date')\n_ = plt.ylabel('Members')\nplt.show()\nplt.savefig('/home/brian/Documents/github/briancaffey.github.io/aur/wiki_users.png')\n",[33,45819,45820,45824,45829,45834,45839,45843,45847,45851,45856,45860,45865,45869],{"__ignoreMap":35},[187,45821,45822],{"class":189,"line":190},[187,45823,45172],{},[187,45825,45826],{"class":189,"line":249},[187,45827,45828],{},"df = df[df['registered'].notnull()]\n",[187,45830,45831],{"class":189,"line":312},[187,45832,45833],{},"df[\"registered\"] = pd.to_datetime(df['registered'])\n",[187,45835,45836],{"class":189,"line":319},[187,45837,45838],{},"list_of_dates = df[\"registered\"].sort_values()\n",[187,45840,45841],{"class":189,"line":325},[187,45842,45192],{},[187,45844,45845],{"class":189,"line":686},[187,45846,45197],{},[187,45848,45849],{"class":189,"line":697},[187,45850,45202],{},[187,45852,45853],{"class":189,"line":1291},[187,45854,45855],{},"_ = plt.title('Registered Arch Wiki members over time')\n",[187,45857,45858],{"class":189,"line":1306},[187,45859,45212],{},[187,45861,45862],{"class":189,"line":1434},[187,45863,45864],{},"_ = plt.ylabel('Members')\n",[187,45866,45867],{"class":189,"line":2599},[187,45868,26147],{},[187,45870,45871],{"class":189,"line":2607},[187,45872,45873],{},"plt.savefig('/home/brian/Documents/github/briancaffey.github.io/aur/wiki_users.png')\n",[11,45875,45876,45877,752],{},"There is a massive amount of data in the Wiki that I haven't obtained for this article. You can also find some interesting statistics on the Arch Wiki site ",[15,45878,1321],{"href":45879,"rel":45880},"https://wiki.archlinux.org/index.php/ArchWiki:Statistics#Histograms",[19],[855,45882,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":45884},[45885,45886,45887,45888,45889],{"id":45149,"depth":249,"text":45150},{"id":45223,"depth":249,"text":45224},{"id":45475,"depth":249,"text":45476},{"id":45670,"depth":249,"text":45671},{"id":45805,"depth":249,"text":45806},"2017-12-02","Arch Linux provides packages through the official Arch Linux repositories and the Arch User Repository (AUR). I recently gathered data on ~50,000 packages from these repositories on archlinux.org to better understand the makeup of the packages. In this article I will share some visualizations I made as well as some key takeaways about the data set I gathered.",{"layout":29014},"/2017/12/02/arch-linux-package-data-analysis",{"title":45128,"description":45891},"2017/12/02/arch-linux-package-data-analysis",[45897,582,1125],"arch-linux","3gZAPx4MlD7kjWro9EVetO9m0CCZIioqzIVSIxaa-EE",{"id":45900,"title":45901,"body":45902,"comments":315,"date":46081,"description":45906,"draft":872,"extension":873,"external":874,"image":46008,"meta":46082,"navigation":315,"path":46083,"seo":46084,"stem":46085,"tags":46086,"__hash__":46088},"blog/2017/11/28/remove-root-partition-bloat-from-docker.md","Removing root partition bloat caused by docker",{"type":8,"value":45903,"toc":46079},[45904,45907,45918,45925,45930,45941,45947,45950,45956,45959,45964,45972,45975,45983,45986,45989,45995,45998,46004,46009,46012,46018,46024,46030,46033,46048,46056,46073],[11,45905,45906],{},"Recently I've been having storage issues in the root partitions of both my desktop and laptop computers. These issues came up soon after I started playing around with docker. In this article I'll talk briefly about how I fixed this problem, the resources and tools I picked up along the way, and anything else I have learned along the way.",[11,45908,45909,45910,45913,45914,45917],{},"I first learned how bad this issue was when I went to install anaconda on my desktop PC. I quiclky ran ",[33,45911,45912],{},"df -h"," and saw that my 20G root partition had less than 1G of available space. To look further into this I ran ",[33,45915,45916],{},"baobab",". In the baobab home screen the root partition had slightly more space, but it was still close to being full. The expanded view was only showing me informatoin for around 8G of storage, leaving almost 11G of space not accounted for.",[11,45919,45920,45921,45924],{},"I started reaching for different tools and packages to slim down disk usage. ",[33,45922,45923],{},"pacgraph"," is a pretty neat way to visualize the relative size of packages. Here's an example:",[11,45926,45927],{},[511,45928],{"alt":7255,"src":45929},"/static/pacgraph.png",[11,45931,45932,45933,1172,45935,45937,45938,45940],{},"This helps you quickly find packages that you can do without. After removing some large packages like Libre Office I realized that this was barely moving the needle on my storage problem. Running ",[33,45934,45912],{},[33,45936,45916],{}," again with root priviledges gave me slightly different results. At this point I turned to docker and deleted all of the images with `docker rmi ",[511,45939],{"id":35}," -f. This didn't help either. Here are the images that I removed from my desktop:",[26,45942,45945],{"className":45943,"code":45944,"language":31},[29],"[brian@a1arch ~]$ docker images\nREPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE\nflaskmicroservicesusers_users-service   latest              1e59fa4d2af5        5 days ago          739MB\n\u003Cnone>                                  \u003Cnone>              18f9191b4d9a        5 days ago          739MB\nflaskmicroservicesusers_users-db        latest              f1de1c3ef3f2        5 days ago          287MB\n\u003Cnone>                                  \u003Cnone>              11188ac6f36a        5 days ago          712MB\npostgres                                latest              599272bf538f        12 days ago         287MB\ntensorflow/tensorflow                   latest-gpu          2f243a16ff63        3 weeks ago         3.36GB\npython                                  3.6.2               26acbad26a2c        2 months ago        690MB\n",[33,45946,45944],{"__ignoreMap":35},[11,45948,45949],{},"Here's the storage profile before I started remove docker-related files:",[26,45951,45954],{"className":45952,"code":45953,"language":31},[29]," $ df -h | grep /dev/sda1\n/dev/sda1        20G   18G  737M  97% /\n",[33,45955,45953],{"__ignoreMap":35},[11,45957,45958],{},"After I removed the docker images, here is the same command:",[26,45960,45962],{"className":45961,"code":45953,"language":31},[29],[33,45963,45953],{"__ignoreMap":35},[11,45965,45966,45967,752],{},"I found a helpful serverfault question from 6 years ago that address the issue I was having titled ",[15,45968,45971],{"href":45969,"rel":45970},"https://serverfault.com/questions/275206/disk-full-du-tells-different-how-to-further-investigate",[19],"Disk full, du tells different. How to further investigate?",[11,45973,45974],{},"I saw a helpful comment related to docker:",[107,45976,45977],{},[11,45978,45979,45980],{},"Thanks - this showed that docker was filling up my hard drive with diffs in ",[33,45981,45982],{},"/var/lib/docker/aufs/diff/",[11,45984,45985],{},"Could this be my issue?",[11,45987,45988],{},"Here's the folder in question on my laptop:",[26,45990,45993],{"className":45991,"code":45992,"language":31},[29]," $ cd /var/lib/docker\n $ sudo du -s -h .\n2.6G    .\n",[33,45994,45992],{"__ignoreMap":35},[11,45996,45997],{},"On my desktop this was taking up about 10G!",[11,45999,46000,46001,358],{},"Wow! I didn't even see this when I ran ",[33,46002,46003],{},"sudo baobab",[11,46005,46006],{},[511,46007],{"alt":7255,"src":46008},"/static/baobab.png",[11,46010,46011],{},"I stopped the docker service and deleted the overlay2 file:",[26,46013,46016],{"className":46014,"code":46015,"language":31},[29]," $ sudo systemctl stop docker\n $ cd /var/lib/docker\n $ sudo rm -rf layover2\n",[33,46017,46015],{"__ignoreMap":35},[11,46019,46020,46021,46023],{},"With ",[33,46022,46003],{}," I was also able to delete 3.6G of trash with this command:",[26,46025,46028],{"className":46026,"code":46027,"language":31},[29]," $ sudo -i\n # rm -rf /root/.local/share/Trash\n",[33,46029,46027],{"__ignoreMap":35},[11,46031,46032],{},"I think this may be related to having previously emptied the Trash in nautilus file browser with files that I might not have owned.",[11,46034,46035,46036,46041,46042,46047],{},"I think it would be a good idea to change the docker image installation directory. ",[15,46037,46040],{"href":46038,"rel":46039},"https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169",[19],"Here is a link"," from a docker forum talking about how to do that. ",[15,46043,46046],{"href":46044,"rel":46045},"https://forums.docker.com/t/some-way-to-clean-up-identify-contents-of-var-lib-docker-overlay/30604",[19],"Here is another docker forum post"," that talks about the overlay and storage issues that docker has.",[11,46049,46050,46051,358],{},"Here is a helpful snippet from the ",[15,46052,46055],{"href":46053,"rel":46054},"https://wiki.archlinux.org/index.php/Docker",[19],"Arch Wiki Docker article",[107,46057,46058,46061,46064,46067],{},[11,46059,46060],{},"Images location\nBy default, docker images are located at /var/lib/docker. They can be moved to other partitions. First, stop the docker.service.",[11,46062,46063],{},"If you have run the docker images, you need to make sure the images are unmounted totally. Once that is completed, you may move the images from /var/lib/docker to the target destination.",[11,46065,46066],{},"Then add a Drop-in snippet for the docker.service, adding the --data-root parameter to the ExecStart:",[26,46068,46071],{"className":46069,"code":46070,"language":31},[29],"/etc/systemd/system/docker.service.d/docker-storage.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd --data-root=/path/to/new/location/docker -H fd://\n",[33,46072,46070],{"__ignoreMap":35},[26,46074,46077],{"className":46075,"code":46076,"language":31},[29],"\nUpdate: I did this on my desktop with a `--data-rogettingot` path in my home folder.\n\nI followed the directions form [this article](https://linuxconfig.org/how-to-move-docker-s-default-var-lib-docker-to-another-directory-on-ubuntu-debian-linux) and was able to set up docker on my home partition.\n",[33,46078,46076],{"__ignoreMap":35},{"title":35,"searchDepth":249,"depth":249,"links":46080},[],"2017-11-28",{"layout":29014},"/2017/11/28/remove-root-partition-bloat-from-docker",{"title":45901,"description":45906},"2017/11/28/remove-root-partition-bloat-from-docker",[15298,46087],"linux","ybCRaGwh7aK22BZ7xY38Z4Unh5nTJMR90l6y7AYTwxg",{"id":46090,"title":46091,"body":46092,"comments":315,"date":46814,"description":46096,"draft":872,"extension":873,"external":874,"image":46806,"meta":46815,"navigation":315,"path":46816,"seo":46817,"stem":46818,"tags":46819,"__hash__":46821},"blog/2017/11/20/using-tensorflow-and-tensor-board-with-docker.md","Using Tensorflow and Tensorboard with Docker",{"type":8,"value":46093,"toc":46812},[46094,46097,46100,46106,46116,46122,46134,46137,46143,46146,46786,46789,46795,46802,46807,46810],[11,46095,46096],{},"In my last article we set up Tensorflow with Docker. Next I want to try to get Tensorboard running.",[11,46098,46099],{},"When we opened the Jupyter notebook, our command included port mapping. Here is that command:",[26,46101,46104],{"className":46102,"code":46103,"language":31},[29],"$ sudo nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu\n",[33,46105,46103],{"__ignoreMap":35},[11,46107,46108,46109,46112,46113,18410],{},"Tensorboard will be served in our browser on port ",[33,46110,46111],{},"6006",", so we will want to do that port mapping in our ",[33,46114,46115],{},"nvidia-docker",[26,46117,46120],{"className":46118,"code":46119,"language":31},[29],"sudo nvidia-docker run -p 0.0.0.0:6006:6006 -it tensorflow/tensorflow:latest-gpu bash\n",[33,46121,46119],{"__ignoreMap":35},[11,46123,46124,46125,46130,46131,752],{},"I want to run ",[15,46126,46129],{"href":46127,"rel":46128},"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py",[19],"this script"," from the Tensorflow github repo. It is an example of MNIST with summaries. Summaries are logs that are captured from script and they provide the data that runs Tensorboard. In this case they are recorded in ",[33,46132,46133],{},"/tmp/tensorflow/mnist/logs/",[11,46135,46136],{},"To start with this script let's just copy and paste it into a file. We will need to add vim to our docker container for that:",[26,46138,46141],{"className":46139,"code":46140,"language":31},[29],"# apt-get update\n# apt-get install vim\n",[33,46142,46140],{"__ignoreMap":35},[11,46144,46145],{},"Now we can copy and paste the script and run it:",[26,46147,46149],{"className":1383,"code":46148,"language":1125,"meta":35,"style":35},"root@eb9e069064d7:~# python tb.py\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n2017-11-20 03:52:53.792141: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-20 03:52:53.878640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-20 03:52:53.878892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.43GiB\n2017-11-20 03:52:53.878904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\nAccuracy at step 0: 0.1235\nAccuracy at step 10: 0.7297\nAccuracy at step 20: 0.8414\nAccuracy at step 30: 0.8717\nAccuracy at step 40: 0.886\nAccuracy at step 50: 0.896\nAccuracy at step 60: 0.9027\nAccuracy at step 70: 0.9068\nAccuracy at step 80: 0.9101\nAccuracy at step 90: 0.9121\n2017-11-20 03:52:57.583676: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\nAdding run metadata for 99\nAccuracy at step 100: 0.9124\nAccuracy at step 110: 0.9164\nAccuracy at step 120: 0.9198\nAccuracy at step 130: 0.9205\nAccuracy at step 140: 0.9142\nAccuracy at step 150: 0.9224\nAccuracy at step 160: 0.9294\nAccuracy at step 170: 0.928\nAccuracy at step 180: 0.9312\nAccuracy at step 190: 0.9301\nAdding run metadata for 199\nAccuracy at step 200: 0.9346\nAccuracy at step 210: 0.9381\nAccuracy at step 220: 0.9396\nAccuracy at step 230: 0.9406\nAccuracy at step 240: 0.9273\nAccuracy at step 250: 0.941\nAccuracy at step 260: 0.9369\nAccuracy at step 270: 0.9329\nAccuracy at step 280: 0.9404\nAccuracy at step 290: 0.9444\nAdding run metadata for 299\nAccuracy at step 300: 0.9438\nAccuracy at step 310: 0.9426\nAccuracy at step 320: 0.9462\nAccuracy at step 330: 0.9449\nAccuracy at step 340: 0.9478\nAccuracy at step 350: 0.9458\nAccuracy at step 360: 0.9464\nAccuracy at step 370: 0.9474\nAccuracy at step 380: 0.9528\nAccuracy at step 390: 0.9499\nAdding run metadata for 399\nAccuracy at step 400: 0.9507\nAccuracy at step 410: 0.9501\nAccuracy at step 420: 0.9513\nAccuracy at step 430: 0.9483\nAccuracy at step 440: 0.9518\nAccuracy at step 450: 0.949\nAccuracy at step 460: 0.9543\nAccuracy at step 470: 0.9552\nAccuracy at step 480: 0.9515\nAccuracy at step 490: 0.9544\nAdding run metadata for 499\nAccuracy at step 500: 0.9586\nAccuracy at step 510: 0.9567\nAccuracy at step 520: 0.9572\nAccuracy at step 530: 0.9574\nAccuracy at step 540: 0.9584\nAccuracy at step 550: 0.9593\nAccuracy at step 560: 0.958\nAccuracy at step 570: 0.9575\nAccuracy at step 580: 0.9582\nAccuracy at step 590: 0.9609\nAdding run metadata for 599\nAccuracy at step 600: 0.9618\nAccuracy at step 610: 0.9605\nAccuracy at step 620: 0.9606\nAccuracy at step 630: 0.961\nAccuracy at step 640: 0.963\nAccuracy at step 650: 0.9614\nAccuracy at step 660: 0.9622\nAccuracy at step 670: 0.9634\nAccuracy at step 680: 0.9641\nAccuracy at step 690: 0.9627\nAdding run metadata for 699\nAccuracy at step 700: 0.9623\nAccuracy at step 710: 0.9612\nAccuracy at step 720: 0.9628\nAccuracy at step 730: 0.965\nAccuracy at step 740: 0.9635\nAccuracy at step 750: 0.9635\nAccuracy at step 760: 0.9648\nAccuracy at step 770: 0.9637\nAccuracy at step 780: 0.9658\nAccuracy at step 790: 0.9649\nAdding run metadata for 799\nAccuracy at step 800: 0.9681\nAccuracy at step 810: 0.9661\nAccuracy at step 820: 0.9657\nAccuracy at step 830: 0.9646\nAccuracy at step 840: 0.9647\nAccuracy at step 850: 0.965\nAccuracy at step 860: 0.9677\nAccuracy at step 870: 0.9649\nAccuracy at step 880: 0.9675\nAccuracy at step 890: 0.969\nAdding run metadata for 899\nAccuracy at step 900: 0.9689\nAccuracy at step 910: 0.967\nAccuracy at step 920: 0.9672\nAccuracy at step 930: 0.9645\nAccuracy at step 940: 0.9657\nAccuracy at step 950: 0.9699\nAccuracy at step 960: 0.968\nAccuracy at step 970: 0.9679\nAccuracy at step 980: 0.9651\nAccuracy at step 990: 0.9683\nAdding run metadata for 999\n",[33,46150,46151,46156,46161,46166,46171,46176,46181,46186,46191,46196,46201,46206,46211,46216,46221,46226,46231,46236,46241,46246,46251,46256,46261,46266,46271,46276,46281,46286,46291,46296,46301,46306,46311,46316,46321,46326,46331,46336,46341,46346,46351,46356,46361,46366,46371,46376,46381,46386,46391,46396,46401,46406,46411,46416,46421,46426,46431,46436,46441,46446,46451,46456,46461,46466,46471,46476,46481,46486,46491,46496,46501,46506,46511,46516,46521,46526,46531,46536,46541,46546,46551,46556,46561,46566,46571,46576,46581,46586,46591,46596,46601,46606,46611,46616,46621,46626,46631,46636,46641,46646,46651,46656,46661,46666,46671,46676,46681,46686,46691,46696,46701,46706,46711,46716,46721,46726,46731,46736,46741,46746,46751,46756,46761,46766,46771,46776,46781],{"__ignoreMap":35},[187,46152,46153],{"class":189,"line":190},[187,46154,46155],{},"root@eb9e069064d7:~# python tb.py\n",[187,46157,46158],{"class":189,"line":249},[187,46159,46160],{},"Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",[187,46162,46163],{"class":189,"line":312},[187,46164,46165],{},"Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",[187,46167,46168],{"class":189,"line":319},[187,46169,46170],{},"Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",[187,46172,46173],{"class":189,"line":325},[187,46174,46175],{},"Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",[187,46177,46178],{"class":189,"line":686},[187,46179,46180],{},"Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",[187,46182,46183],{"class":189,"line":697},[187,46184,46185],{},"Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",[187,46187,46188],{"class":189,"line":1291},[187,46189,46190],{},"Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",[187,46192,46193],{"class":189,"line":1306},[187,46194,46195],{},"Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",[187,46197,46198],{"class":189,"line":1434},[187,46199,46200],{},"2017-11-20 03:52:53.792141: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",[187,46202,46203],{"class":189,"line":2599},[187,46204,46205],{},"2017-11-20 03:52:53.878640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",[187,46207,46208],{"class":189,"line":2607},[187,46209,46210],{},"2017-11-20 03:52:53.878892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\n",[187,46212,46213],{"class":189,"line":2621},[187,46214,46215],{},"name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\n",[187,46217,46218],{"class":189,"line":2631},[187,46219,46220],{},"pciBusID: 0000:01:00.0\n",[187,46222,46223],{"class":189,"line":2642},[187,46224,46225],{},"totalMemory: 7.92GiB freeMemory: 7.43GiB\n",[187,46227,46228],{"class":189,"line":2653},[187,46229,46230],{},"2017-11-20 03:52:53.878904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",[187,46232,46233],{"class":189,"line":2665},[187,46234,46235],{},"Accuracy at step 0: 0.1235\n",[187,46237,46238],{"class":189,"line":2674},[187,46239,46240],{},"Accuracy at step 10: 0.7297\n",[187,46242,46243],{"class":189,"line":2684},[187,46244,46245],{},"Accuracy at step 20: 0.8414\n",[187,46247,46248],{"class":189,"line":2694},[187,46249,46250],{},"Accuracy at step 30: 0.8717\n",[187,46252,46253],{"class":189,"line":2706},[187,46254,46255],{},"Accuracy at step 40: 0.886\n",[187,46257,46258],{"class":189,"line":2715},[187,46259,46260],{},"Accuracy at step 50: 0.896\n",[187,46262,46263],{"class":189,"line":2725},[187,46264,46265],{},"Accuracy at step 60: 0.9027\n",[187,46267,46268],{"class":189,"line":2735},[187,46269,46270],{},"Accuracy at step 70: 0.9068\n",[187,46272,46273],{"class":189,"line":2743},[187,46274,46275],{},"Accuracy at step 80: 0.9101\n",[187,46277,46278],{"class":189,"line":2754},[187,46279,46280],{},"Accuracy at step 90: 0.9121\n",[187,46282,46283],{"class":189,"line":2762},[187,46284,46285],{},"2017-11-20 03:52:57.583676: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\n",[187,46287,46288],{"class":189,"line":2770},[187,46289,46290],{},"Adding run metadata for 99\n",[187,46292,46293],{"class":189,"line":2781},[187,46294,46295],{},"Accuracy at step 100: 0.9124\n",[187,46297,46298],{"class":189,"line":2792},[187,46299,46300],{},"Accuracy at step 110: 0.9164\n",[187,46302,46303],{"class":189,"line":2803},[187,46304,46305],{},"Accuracy at step 120: 0.9198\n",[187,46307,46308],{"class":189,"line":2808},[187,46309,46310],{},"Accuracy at step 130: 0.9205\n",[187,46312,46313],{"class":189,"line":2816},[187,46314,46315],{},"Accuracy at step 140: 0.9142\n",[187,46317,46318],{"class":189,"line":2824},[187,46319,46320],{},"Accuracy at step 150: 0.9224\n",[187,46322,46323],{"class":189,"line":2834},[187,46324,46325],{},"Accuracy at step 160: 0.9294\n",[187,46327,46328],{"class":189,"line":2845},[187,46329,46330],{},"Accuracy at step 170: 0.928\n",[187,46332,46333],{"class":189,"line":2856},[187,46334,46335],{},"Accuracy at step 180: 0.9312\n",[187,46337,46338],{"class":189,"line":2867},[187,46339,46340],{},"Accuracy at step 190: 0.9301\n",[187,46342,46343],{"class":189,"line":2878},[187,46344,46345],{},"Adding run metadata for 199\n",[187,46347,46348],{"class":189,"line":2886},[187,46349,46350],{},"Accuracy at step 200: 0.9346\n",[187,46352,46353],{"class":189,"line":2900},[187,46354,46355],{},"Accuracy at step 210: 0.9381\n",[187,46357,46358],{"class":189,"line":2905},[187,46359,46360],{},"Accuracy at step 220: 0.9396\n",[187,46362,46363],{"class":189,"line":2913},[187,46364,46365],{},"Accuracy at step 230: 0.9406\n",[187,46367,46368],{"class":189,"line":2921},[187,46369,46370],{},"Accuracy at step 240: 0.9273\n",[187,46372,46373],{"class":189,"line":2931},[187,46374,46375],{},"Accuracy at step 250: 0.941\n",[187,46377,46378],{"class":189,"line":2942},[187,46379,46380],{},"Accuracy at step 260: 0.9369\n",[187,46382,46383],{"class":189,"line":2953},[187,46384,46385],{},"Accuracy at step 270: 0.9329\n",[187,46387,46388],{"class":189,"line":2964},[187,46389,46390],{},"Accuracy at step 280: 0.9404\n",[187,46392,46393],{"class":189,"line":2975},[187,46394,46395],{},"Accuracy at step 290: 0.9444\n",[187,46397,46398],{"class":189,"line":2983},[187,46399,46400],{},"Adding run metadata for 299\n",[187,46402,46403],{"class":189,"line":2992},[187,46404,46405],{},"Accuracy at step 300: 0.9438\n",[187,46407,46408],{"class":189,"line":3001},[187,46409,46410],{},"Accuracy at step 310: 0.9426\n",[187,46412,46413],{"class":189,"line":3010},[187,46414,46415],{},"Accuracy at step 320: 0.9462\n",[187,46417,46418],{"class":189,"line":3019},[187,46419,46420],{},"Accuracy at step 330: 0.9449\n",[187,46422,46423],{"class":189,"line":3028},[187,46424,46425],{},"Accuracy at step 340: 0.9478\n",[187,46427,46428],{"class":189,"line":3033},[187,46429,46430],{},"Accuracy at step 350: 0.9458\n",[187,46432,46433],{"class":189,"line":3041},[187,46434,46435],{},"Accuracy at step 360: 0.9464\n",[187,46437,46438],{"class":189,"line":3049},[187,46439,46440],{},"Accuracy at step 370: 0.9474\n",[187,46442,46443],{"class":189,"line":3059},[187,46444,46445],{},"Accuracy at step 380: 0.9528\n",[187,46447,46448],{"class":189,"line":3070},[187,46449,46450],{},"Accuracy at step 390: 0.9499\n",[187,46452,46453],{"class":189,"line":3075},[187,46454,46455],{},"Adding run metadata for 399\n",[187,46457,46458],{"class":189,"line":3083},[187,46459,46460],{},"Accuracy at step 400: 0.9507\n",[187,46462,46463],{"class":189,"line":3091},[187,46464,46465],{},"Accuracy at step 410: 0.9501\n",[187,46467,46468],{"class":189,"line":3101},[187,46469,46470],{},"Accuracy at step 420: 0.9513\n",[187,46472,46473],{"class":189,"line":3111},[187,46474,46475],{},"Accuracy at step 430: 0.9483\n",[187,46477,46478],{"class":189,"line":3122},[187,46479,46480],{},"Accuracy at step 440: 0.9518\n",[187,46482,46483],{"class":189,"line":3132},[187,46484,46485],{},"Accuracy at step 450: 0.949\n",[187,46487,46488],{"class":189,"line":3143},[187,46489,46490],{},"Accuracy at step 460: 0.9543\n",[187,46492,46493],{"class":189,"line":3151},[187,46494,46495],{},"Accuracy at step 470: 0.9552\n",[187,46497,46498],{"class":189,"line":3161},[187,46499,46500],{},"Accuracy at step 480: 0.9515\n",[187,46502,46503],{"class":189,"line":3170},[187,46504,46505],{},"Accuracy at step 490: 0.9544\n",[187,46507,46508],{"class":189,"line":3178},[187,46509,46510],{},"Adding run metadata for 499\n",[187,46512,46513],{"class":189,"line":3185},[187,46514,46515],{},"Accuracy at step 500: 0.9586\n",[187,46517,46518],{"class":189,"line":3195},[187,46519,46520],{},"Accuracy at step 510: 0.9567\n",[187,46522,46523],{"class":189,"line":3205},[187,46524,46525],{},"Accuracy at step 520: 0.9572\n",[187,46527,46528],{"class":189,"line":3210},[187,46529,46530],{},"Accuracy at step 530: 0.9574\n",[187,46532,46533],{"class":189,"line":3216},[187,46534,46535],{},"Accuracy at step 540: 0.9584\n",[187,46537,46538],{"class":189,"line":3224},[187,46539,46540],{},"Accuracy at step 550: 0.9593\n",[187,46542,46543],{"class":189,"line":3234},[187,46544,46545],{},"Accuracy at step 560: 0.958\n",[187,46547,46548],{"class":189,"line":3242},[187,46549,46550],{},"Accuracy at step 570: 0.9575\n",[187,46552,46553],{"class":189,"line":3252},[187,46554,46555],{},"Accuracy at step 580: 0.9582\n",[187,46557,46558],{"class":189,"line":3260},[187,46559,46560],{},"Accuracy at step 590: 0.9609\n",[187,46562,46563],{"class":189,"line":3270},[187,46564,46565],{},"Adding run metadata for 599\n",[187,46567,46568],{"class":189,"line":3275},[187,46569,46570],{},"Accuracy at step 600: 0.9618\n",[187,46572,46573],{"class":189,"line":3283},[187,46574,46575],{},"Accuracy at step 610: 0.9605\n",[187,46577,46578],{"class":189,"line":3291},[187,46579,46580],{},"Accuracy at step 620: 0.9606\n",[187,46582,46583],{"class":189,"line":3300},[187,46584,46585],{},"Accuracy at step 630: 0.961\n",[187,46587,46588],{"class":189,"line":3310},[187,46589,46590],{},"Accuracy at step 640: 0.963\n",[187,46592,46593],{"class":189,"line":3320},[187,46594,46595],{},"Accuracy at step 650: 0.9614\n",[187,46597,46598],{"class":189,"line":3325},[187,46599,46600],{},"Accuracy at step 660: 0.9622\n",[187,46602,46603],{"class":189,"line":3333},[187,46604,46605],{},"Accuracy at step 670: 0.9634\n",[187,46607,46608],{"class":189,"line":3343},[187,46609,46610],{},"Accuracy at step 680: 0.9641\n",[187,46612,46613],{"class":189,"line":3354},[187,46614,46615],{},"Accuracy at step 690: 0.9627\n",[187,46617,46618],{"class":189,"line":17135},[187,46619,46620],{},"Adding run metadata for 699\n",[187,46622,46623],{"class":189,"line":17141},[187,46624,46625],{},"Accuracy at step 700: 0.9623\n",[187,46627,46628],{"class":189,"line":17146},[187,46629,46630],{},"Accuracy at step 710: 0.9612\n",[187,46632,46633],{"class":189,"line":17152},[187,46634,46635],{},"Accuracy at step 720: 0.9628\n",[187,46637,46638],{"class":189,"line":17164},[187,46639,46640],{},"Accuracy at step 730: 0.965\n",[187,46642,46643],{"class":189,"line":17175},[187,46644,46645],{},"Accuracy at step 740: 0.9635\n",[187,46647,46648],{"class":189,"line":17188},[187,46649,46650],{},"Accuracy at step 750: 0.9635\n",[187,46652,46653],{"class":189,"line":17199},[187,46654,46655],{},"Accuracy at step 760: 0.9648\n",[187,46657,46658],{"class":189,"line":17207},[187,46659,46660],{},"Accuracy at step 770: 0.9637\n",[187,46662,46663],{"class":189,"line":17212},[187,46664,46665],{},"Accuracy at step 780: 0.9658\n",[187,46667,46668],{"class":189,"line":17217},[187,46669,46670],{},"Accuracy at step 790: 0.9649\n",[187,46672,46673],{"class":189,"line":17223},[187,46674,46675],{},"Adding run metadata for 799\n",[187,46677,46678],{"class":189,"line":17235},[187,46679,46680],{},"Accuracy at step 800: 0.9681\n",[187,46682,46683],{"class":189,"line":17248},[187,46684,46685],{},"Accuracy at step 810: 0.9661\n",[187,46687,46688],{"class":189,"line":17267},[187,46689,46690],{},"Accuracy at step 820: 0.9657\n",[187,46692,46693],{"class":189,"line":17278},[187,46694,46695],{},"Accuracy at step 830: 0.9646\n",[187,46697,46698],{"class":189,"line":11081},[187,46699,46700],{},"Accuracy at step 840: 0.9647\n",[187,46702,46703],{"class":189,"line":17293},[187,46704,46705],{},"Accuracy at step 850: 0.965\n",[187,46707,46708],{"class":189,"line":17298},[187,46709,46710],{},"Accuracy at step 860: 0.9677\n",[187,46712,46713],{"class":189,"line":17304},[187,46714,46715],{},"Accuracy at step 870: 0.9649\n",[187,46717,46718],{"class":189,"line":17332},[187,46719,46720],{},"Accuracy at step 880: 0.9675\n",[187,46722,46723],{"class":189,"line":17337},[187,46724,46725],{},"Accuracy at step 890: 0.969\n",[187,46727,46728],{"class":189,"line":17343},[187,46729,46730],{},"Adding run metadata for 899\n",[187,46732,46733],{"class":189,"line":17349},[187,46734,46735],{},"Accuracy at step 900: 0.9689\n",[187,46737,46738],{"class":189,"line":17361},[187,46739,46740],{},"Accuracy at step 910: 0.967\n",[187,46742,46743],{"class":189,"line":17373},[187,46744,46745],{},"Accuracy at step 920: 0.9672\n",[187,46747,46748],{"class":189,"line":17388},[187,46749,46750],{},"Accuracy at step 930: 0.9645\n",[187,46752,46753],{"class":189,"line":17398},[187,46754,46755],{},"Accuracy at step 940: 0.9657\n",[187,46757,46758],{"class":189,"line":17407},[187,46759,46760],{},"Accuracy at step 950: 0.9699\n",[187,46762,46763],{"class":189,"line":17412},[187,46764,46765],{},"Accuracy at step 960: 0.968\n",[187,46767,46768],{"class":189,"line":17417},[187,46769,46770],{},"Accuracy at step 970: 0.9679\n",[187,46772,46773],{"class":189,"line":17425},[187,46774,46775],{},"Accuracy at step 980: 0.9651\n",[187,46777,46778],{"class":189,"line":17430},[187,46779,46780],{},"Accuracy at step 990: 0.9683\n",[187,46782,46783],{"class":189,"line":17436},[187,46784,46785],{},"Adding run metadata for 999\n",[11,46787,46788],{},"The script completed successfully! Now we can can take a look at what happened during the training. Launch Tensorboard with the following command:",[26,46790,46793],{"className":46791,"code":46792,"language":31},[29],"root@eb9e069064d7:~# tensorboard --logdir=/tmp/tensorflow/mnist/logs/\nTensorBoard 0.4.0rc2 at http://eb9e069064d7:6006 (Press CTRL+C to quit)\n",[33,46794,46792],{"__ignoreMap":35},[11,46796,46797,46798,46801],{},"Now we can simply navigate to ",[33,46799,46800],{},"localhost:6006"," in our browser to start using Tensorboard. Here's a screenshot of Tensorboard showing accuracy:",[11,46803,46804],{},[511,46805],{"alt":7255,"src":46806},"/static/tf.png",[11,46808,46809],{},"This wasn't too bad. The MNIST example included a very nice script with everything set up properly. My next big challenge is to implement some type of learning model with a data set of my own and visualize it with TensorBoard, but I'll have to go through several examples before then.",[855,46811,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":46813},[],"2017-11-20",{"layout":29014},"/2017/11/20/using-tensorflow-and-tensor-board-with-docker",{"title":46091,"description":46096},"2017/11/20/using-tensorflow-and-tensor-board-with-docker",[46820,15298],"tensorflow","ybgqIS1EgjBhIqVmwzjnwYy2cWf5ZmNs5DhhTPngjPM",{"id":46823,"title":46824,"body":46825,"comments":315,"date":46814,"description":46887,"draft":872,"extension":873,"external":874,"image":27082,"meta":46888,"navigation":315,"path":46889,"seo":46890,"stem":46891,"tags":46892,"__hash__":46894},"blog/2017/11/24/how-to-get-color-emoji-in-arch-linux.md","How to enable color emoji on Arch Linux with Emoji One Font",{"type":8,"value":46826,"toc":46885},[46827,46836,46843,46850,46853,46859,46862,46882],[11,46828,46829,46830,46835],{},"This is a short article about how to enable color emoji on Arch Linux. I have searched for a working solution for this a few times but never found something that worked. I stumbled upon ",[15,46831,46834],{"href":46832,"rel":46833},"https://gist.github.com/himalay/5c404a5f6653cb35154ceb3a6c606211",[19],"this Gist"," that I used to hack together a solution that enables color emoji from Emoji One on Arch Linux.",[11,46837,46838,46839,46842],{},"This setup doesn't show all emoji in ",[33,46840,46841],{},"urxvt",", and I have heard that this is simply not possible, but most other programs and UIs display the emoji just fine. Here's what I did:",[11,46844,46845,46846,46849],{},"First, install ",[33,46847,46848],{},"tff-emojione",", a package that seems to have been added to the AUR just a week ago.",[11,46851,46852],{},"Next, run the script from the gist mentioned above:",[26,46854,46857],{"className":46855,"code":46856,"language":31},[29],"# create folders if does not exist\nmkdir -p ~/.fonts\nmkdir -p ~/.config/fontconfig/\n\n# download noto color emoji font from https://www.google.com/get/noto/#emoji-zsye-color\n# extract NotoColorEmoji.ttf file into ~/.fonts/\n\n# create font config file\ncat \u003C\u003C 'EOF' > ~/.config/fontconfig/fonts.conf\n\u003C?xml version=\"1.0\" encoding=\"UTF-8\"?>\u003C!DOCTYPE fontconfig SYSTEM \"fonts.dtd\">\n\u003Cmatch>\n \u003Ctest name=\"family\">\u003Cstring>sans-serif\u003C/string>\u003C/test>\n \u003Cedit name=\"family\" mode=\"prepend\" binding=\"strong\">\n \u003Cstring>Noto Color Emoji\u003C/string>\n \u003C/edit>\n \u003C/match>\n\u003Cmatch>\n \u003Ctest name=\"family\">\u003Cstring>serif\u003C/string>\u003C/test>\n \u003Cedit name=\"family\" mode=\"prepend\" binding=\"strong\">\n \u003Cstring>Noto Color Emoji\u003C/string>\n \u003C/edit>\n \u003C/match>\n\u003Cmatch>\n \u003Ctest name=\"family\">\u003Cstring>Apple Color Emoji\u003C/string>\u003C/test>\n \u003Cedit name=\"family\" mode=\"prepend\" binding=\"strong\">\n \u003Cstring>Noto Color Emoji\u003C/string>\n \u003C/edit>\n \u003C/match>\nEOF\n# build font information cache files\nfc-cache -f -v\n",[33,46858,46856],{"__ignoreMap":35},[11,46860,46861],{},"As recommended in the comments of the gist, you will notice that numbers in most applications are represented with emoji numbers.",[11,46863,46864,46865,33096,46868,46871,46872,46875,46876,46878,46879,46881],{},"To fix this, remove all three instances of ",[33,46866,46867],{},"mode=\"prepend\" binding=\"strong\"",[33,46869,46870],{},"~/.config/fontconfig/fonts.conf"," and then run ",[33,46873,46874],{},"fc-cache -f -v ",". This should fix some issues, but you may notice that spaces between words are not displayed in some applications and some of the instances of numbers displayed as emoji should be have fixed, but not all. Once I removed all of the text from ",[33,46877,46870],{}," and ran ",[33,46880,46874],{}," one more time, I seemed to get the space and number issues to go away while the emoji still work!",[11,46883,46884],{},"I'm really not sure how or why this works, but it has solved the issue I've been having of emojis not displaying. Hopefully this helps if you are trying to add color emoji to your Arch Linux installation. Apparently the next version of Ubuntu will include support for color emoji out of the box, but for now you will have to hack together your own solution for Arch Linux.",{"title":35,"searchDepth":249,"depth":249,"links":46886},[],"This is a short article about how to enable color emoji on Arch Linux. I have searched for a working solution for this a few times but never found something that worked. I stumbled upon this Gist that I used to hack together a solution that enables color emoji from Emoji One on Arch Linux.",{"layout":29014},"/2017/11/24/how-to-get-color-emoji-in-arch-linux",{"title":46824,"description":46887},"2017/11/24/how-to-get-color-emoji-in-arch-linux",[45897,46893],"emoji","WeGruEu1CuX5gLYYbZyzgtW37DWVTYo8LW2xL71jp9k",{"id":46896,"title":46897,"body":46898,"comments":315,"date":47243,"description":47244,"draft":872,"extension":873,"external":874,"image":47226,"meta":47245,"navigation":315,"path":47246,"seo":47247,"stem":47248,"tags":47249,"__hash__":47250},"blog/2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux.md","Installing the GPU version of Tensorflow with Docker on Arch Linux",{"type":8,"value":46899,"toc":47237},[46900,46903,46917,46921,46924,46928,46935,46938,46944,46947,46953,46956,46962,46976,46982,46985,46988,46994,46998,47007,47010,47015,47021,47028,47035,47041,47044,47050,47053,47059,47064,47070,47073,47079,47082,47088,47091,47097,47100,47152,47164,47175,47178,47184,47187,47193,47199,47202,47205,47211,47214,47222,47227,47230,47235],[11,46901,46902],{},"I've tried installing the GPU version of Tensorflow a few times before and failed. There seems to be lots of confusion about the build process, of which there are many. Also, over the last few years there have been many new versions of the software needed to support the GPU version of Tensorflow as well as the first official release of Tensorflow itself (which is now on version 1.4), such as CUDA and cudnn, and different version of python. This is one more attempt at installing the GPU version of Tensor Flow on my Desktop PC that is currently dual booting with Arch Linux and Windows 10. I've decided to try going the docker route because it should eliminate some of the headache of missing depedencies. Here are the specs for my computer:",[916,46904,46905,46908,46911,46914],{},[919,46906,46907],{},"i7-6700K",[919,46909,46910],{},"NVIDIA GTX 1080",[919,46912,46913],{},"Asus Hero VIII motherboard",[919,46915,46916],{},"Arch Linux on a 128 GB SSD (Windows 10 is installed on a separate SSD)",[168,46918,46920],{"id":46919},"installing-cuda-and-cudnn","Installing CUDA and cudnn",[11,46922,46923],{},"We don't need to install these when installing Tensorflow with Docker. Read to the bottom for more info.",[168,46925,46927],{"id":46926},"installing-docker","Installing Docker",[11,46929,46930,46931,752],{},"To install docker on our machine, let's start with the ",[15,46932,46934],{"href":46053,"rel":46933},[19],"Arch Wiki article on docker",[11,46936,46937],{},"We need to add the Loopback module to the Linux Kernel, so we run:",[26,46939,46942],{"className":46940,"code":46941,"language":31},[29],"# tee /etc/modules-load.d/loop.conf \u003C\u003C\u003C \"loop\"\n# modprobe loop\n$ reboot\n",[33,46943,46941],{"__ignoreMap":35},[11,46945,46946],{},"Ater rebooting we can install docker:",[26,46948,46951],{"className":46949,"code":46950,"language":31},[29],"yaourt -S docker\n",[33,46952,46950],{"__ignoreMap":35},[11,46954,46955],{},"Now we want to add ourself to the docker group with the following command:",[26,46957,46960],{"className":46958,"code":46959,"language":31},[29],"$ sudo gpasswd -a brian docker\n[sudo] password for brian:\nAdding user brian to group docker\n",[33,46961,46959],{"__ignoreMap":35},[11,46963,46964,46965,46968,46969,46972,46973,46975],{},"If you run ",[33,46966,46967],{},"groups",", you won't see docker listed in the groups you (brian) belong to. Run ",[33,46970,46971],{},"newgrp docker"," and then re-run docker and you should see ",[33,46974,15298],{}," listed with any other groups you belong to:",[26,46977,46980],{"className":46978,"code":46979,"language":31},[29],"[brian@a1arch ~]$ groups\nwheel storage power users\n[brian@a1arch ~]$ newgrp docker\n                   -`                    brian@a1arch\n                  .o+`                   ------------\n                 `ooo/                   OS: Arch Linux x86_64\n                `+oooo:                  Kernel: 4.12.8-2-ARCH\n               `+oooooo:                 Uptime: 6 mins\n               -+oooooo+:                Packages: 1127\n             `/:-:++oooo+:               Shell: bash 4.4.12\n            `/++++/+++++++:              Resolution: 1920x1080\n           `/++++++++++++++:             WM: i3\n          `/+++ooooooooooooo/`           Theme: Adwaita [GTK2]\n         ./ooosssso++osssssso+`          Icons: Adwaita [GTK2]\n        .oossssso-````/ossssss+`         Terminal: urxvt\n       -osssssso.      :ssssssso.        Terminal Font: Inconsolata-12\n      :osssssss/        osssso+++.       CPU: Intel i7-6700K (8) @ 4.200GHz\n     /ossssssss/        +ssssooo/-       GPU: NVIDIA GeForce GTX 1080\n   `/ossssso+/:-        -:/+osssso+-     Memory: 3289MiB / 15975MiB\n  `+sso+:-`                 `.-/+oso:\n `++:.                           `-/+/\n .`                                 `/\n\n[brian@a1arch ~]$ groups\ndocker wheel storage power users\n",[33,46981,46979],{"__ignoreMap":35},[11,46983,46984],{},"Doing this prevents us from having to write sudo each time we run docker.",[11,46986,46987],{},"Next we need to start the docker daemon.",[26,46989,46992],{"className":46990,"code":46991,"language":31},[29],"$ systemctl start docker\n==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ====\nAuthentication is required to start 'docker.service'.\nAuthenticating as: brian\nPassword:\n==== AUTHENTICATION COMPLETE ====\n$\n",[33,46993,46991],{"__ignoreMap":35},[911,46995,46997],{"id":46996},"side-note","Side note",[11,46999,47000,47001,47006],{},"There seems to be an ",[15,47002,47005],{"href":47003,"rel":47004},"https://github.com/moby/moby/issues/23289",[19],"Arch Linux-specific bug"," which prevents us from enabling docker (and nvidia-docker which we will get next). There is a solution to downgrade to an older version of docker, or you can just start the docker service and the nvidia-docker service when you want to use them. I have found it faster to first start nvidia-docker and then start docker services.",[11,47008,47009],{},"So far so good. Next let's look at the Tensorflow documentation for installing Tensorflow with docker.",[11,47011,47012,47013,358],{},"We need to install ",[33,47014,46115],{},[26,47016,47019],{"className":47017,"code":47018,"language":31},[29],"$ yaourt -S nvidia-docker\n[...]\n[sudo] password for brian:\nloading packages...\nresolving dependencies...\nlooking for conflicting packages...\n\nPackages (1) nvidia-docker-1.0.1-1\n\nTotal Installed Size:  13.34 MiB\n\n:: Proceed with installation? [Y/n]\n(1/1) checking keys in keyring                                 [##################################] 100%\n(1/1) checking package integrity                               [##################################] 100%\n(1/1) loading package files                                    [##################################] 100%\n(1/1) checking for file conflicts                              [##################################] 100%\n(1/1) checking available disk space                            [##################################] 100%\n:: Processing package changes...\n(1/1) installing nvidia-docker                                 [##################################] 100%\n=> Prior to running 'CUDA'-containers, ensure that the nvidia-docker-plugin\n   is loaded. -> https://github.com/NVIDIA/nvidia-docker#other-distributions\n\n*) manually; sudo -b nohup nvidia-docker-plugin > /tmp/nvidia-docker.log\n\n*) automatically at startup; systemctl enable nvidia-docker.service\nOptional dependencies for nvidia-docker\n    cuda [installed]\n    nvidia [installed]\n    opencl-nvidia [installed]\n:: Running post-transaction hooks...\n(1/1) Arming ConditionNeedsUpdate...\n",[33,47020,47018],{"__ignoreMap":35},[11,47022,47023,47024,752],{},"Next it says: Launch a Docker container that contains one of the TensorFlow binary images. Those images are available ",[15,47025,1321],{"href":47026,"rel":47027},"https://hub.docker.com/r/tensorflow/tensorflow/tags/",[19],[11,47029,47030,47031,47034],{},"Next I pulled the container with the ",[33,47032,47033],{},"gpu-latest"," tag and it started to download the container:",[26,47036,47039],{"className":47037,"code":47038,"language":31},[29],"$ docker pull tensorflow/tensorflow:gpu-latest\n[sudo] password for brian:\nlatest-gpu: Pulling from tensorflow/tensorflow\nae79f2514705: Pull complete\nc59d01a7e4ca: Pull complete\n41ba73a9054d: Pull complete\nf1bbfd495cc1: Pull complete\n0c346f7223e2: Pull complete\n5dcd01667896: Pull complete\nca677f607487: Downloading  180.7MB/453MB\nb4637619a887: Download complete\n8c644ff287da: Downloading    224MB/465.6MB\n119c5f576e79: Download complete\n009f82e71a7c: Download complete\ndbc0fb5872c7: Downloading  17.83MB/66.54MB\n5ef01389c5b2: Waiting\n04f824004b76: Waiting\n5861b82f52e5: Waiting\na495a3b4e6e1: Waiting\n3a0a25b1bbaf: Pulling fs layer\nb76a0afeb1e1: Waiting\n",[33,47040,47038],{"__ignoreMap":35},[11,47042,47043],{},"It finished after several minutes:",[26,47045,47048],{"className":47046,"code":47047,"language":31},[29],"ca677f607487: Pull complete\nb4637619a887: Pull complete\n8c644ff287da: Pull complete\n119c5f576e79: Pull complete\n009f82e71a7c: Pull complete\ndbc0fb5872c7: Pull complete\n5ef01389c5b2: Pull complete\n04f824004b76: Pull complete\n5861b82f52e5: Pull complete\na495a3b4e6e1: Pull complete\n3a0a25b1bbaf: Pull complete\nb76a0afeb1e1: Pull complete\nDigest: sha256:90e27448121b321c5ec66069fb2c718301df2ddaf25ba916b6f53719141572b0\nStatus: Downloaded newer image for tensorflow/tensorflow:latest-gpu\n$\n",[33,47049,47047],{"__ignoreMap":35},[11,47051,47052],{},"Let's verify that it has the image:",[26,47054,47057],{"className":47055,"code":47056,"language":31},[29],"$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\ntensorflow/tensorflow   latest-gpu          2f243a16ff63        13 days ago         3.36GB\n",[33,47058,47056],{"__ignoreMap":35},[11,47060,47061,47062,31279],{},"Next let's start the ",[33,47063,46115],{},[26,47065,47068],{"className":47066,"code":47067,"language":31},[29],"$ systemctl start nvidia-docker\n==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ====\nAuthentication is required to start 'nvidia-docker.service'.\nAuthenticating as: brian\nPassword:\n==== AUTHENTICATION COMPLETE ====\n$\n",[33,47069,47067],{"__ignoreMap":35},[11,47071,47072],{},"OK, we should be ready to launch the image:",[26,47074,47077],{"className":47075,"code":47076,"language":31},[29],"$ nvidia-docker run -it tensorflow/tensorflow:latest-gpu bash\nroot@761a62c1cff1:/notebooks#\n",[33,47078,47076],{"__ignoreMap":35},[11,47080,47081],{},"This is looking good. Let's try to start python:",[26,47083,47086],{"className":47084,"code":47085,"language":31},[29],"root@761a62c1cff1:/notebooks# python\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>>\n",[33,47087,47085],{"__ignoreMap":35},[11,47089,47090],{},"That works! Let's try out the classic MNIST hand-written digit classification problem that comes packaged as a notebook with the container image:",[26,47092,47095],{"className":47093,"code":47094,"language":31},[29],"$ nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu\n[sudo] password for brian:\n[I 21:54:26.671 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n[W 21:54:26.689 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\n[I 21:54:26.693 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 21:54:26.693 NotebookApp] 0 active kernels\n[I 21:54:26.693 NotebookApp] The Jupyter Notebook is running at:\n[I 21:54:26.693 NotebookApp] http://[all ip addresses on your system]:8888/?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd\n[I 21:54:26.693 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 21:54:26.693 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd\n[I 21:54:34.489 NotebookApp] 302 GET /?token=cda89aff96a3d4a9741cc755aac07f65f3aa372f60a198bd (172.17.0.1) 0.32ms\n[I 21:54:59.019 NotebookApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\n[W 21:54:59.023 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n[W 21:54:59.049 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20171119215426 (172.17.0.1) 4.38ms referer=http://localhost:8888/notebooks/3_mnist_from_scratch.ipynb\n[I 21:54:59.813 NotebookApp] Kernel started: 00027a3e-59ae-47ce-90a5-752a9d1fe075\n[I 21:55:00.199 NotebookApp] Adapting to protocol v5.1 for kernel 00027a3e-59ae-47ce-90a5-752a9d1fe075\n[I 21:56:59.815 NotebookApp] Saving file at /3_mnist_from_scratch.ipynb\n[W 21:56:59.816 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n2017-11-19 21:57:03.988627: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 21:57:04.070873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 21:57:04.071129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.44GiB\n2017-11-19 21:57:04.071143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",[33,47096,47094],{"__ignoreMap":35},[11,47098,47099],{},"I was only able to get the entire notebook to run after making a few small configuration tweaks to the tensorflow Interactive Session to fix some memory issues:",[26,47101,47103],{"className":1383,"code":47102,"language":1125,"meta":35,"style":35},"gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.75)\n\ns = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n\n# Use our newly created session as the default for\n# subsequent operations.\ns.as_default()\n\n# Initialize all the variables we defined above.\ntf.global_variables_initializer().run()\n",[33,47104,47105,47110,47114,47119,47123,47128,47133,47138,47142,47147],{"__ignoreMap":35},[187,47106,47107],{"class":189,"line":190},[187,47108,47109],{},"gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.75)\n",[187,47111,47112],{"class":189,"line":249},[187,47113,316],{"emptyLinePlaceholder":315},[187,47115,47116],{"class":189,"line":312},[187,47117,47118],{},"s = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",[187,47120,47121],{"class":189,"line":319},[187,47122,316],{"emptyLinePlaceholder":315},[187,47124,47125],{"class":189,"line":325},[187,47126,47127],{},"# Use our newly created session as the default for\n",[187,47129,47130],{"class":189,"line":686},[187,47131,47132],{},"# subsequent operations.\n",[187,47134,47135],{"class":189,"line":697},[187,47136,47137],{},"s.as_default()\n",[187,47139,47140],{"class":189,"line":1291},[187,47141,316],{"emptyLinePlaceholder":315},[187,47143,47144],{"class":189,"line":1306},[187,47145,47146],{},"# Initialize all the variables we defined above.\n",[187,47148,47149],{"class":189,"line":1434},[187,47150,47151],{},"tf.global_variables_initializer().run()\n",[11,47153,47154,47155,47158,47159,1737],{},"Without setting ",[33,47156,47157],{},"gpu_options",", Tensorflow allocates 95% of available GPU memory (according to ",[15,47160,47163],{"href":47161,"rel":47162},"https://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu",[19],"this SO question",[11,47165,47166,47167,47170,47171,47174],{},"Setting it to ",[33,47168,47169],{},"0.333"," was too low and didn't allow for training to complete, but setting it to ",[33,47172,47173],{},"0.75"," seemed to work just fine.",[11,47176,47177],{},"You can monitor GPU memory usage on NVIDIA cards with the following command:",[26,47179,47182],{"className":47180,"code":47181,"language":31},[29],"$ nvidia-smi\nSun Nov 19 17:03:03 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\n| 27%   32C    P8    10W / 180W |   6707MiB /  8105MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0       350    C   /usr/bin/python                               6365MiB |\n|    0       554    G   /usr/lib/xorg-server/Xorg                       19MiB |\n|    0       588    G   /usr/bin/gnome-shell                            28MiB |\n|    0       853    G   /usr/lib/xorg-server/Xorg                      186MiB |\n|    0       873    G   compton                                          2MiB |\n|    0      1114    G   ...el-token=A50C2F183DB4F79482A2D8768ED1B285    64MiB |\n|    0      2190    G   ...el-token=1AC796A35DBDCDBE07AEC2FC1E8026C4    35MiB |\n+-----------------------------------------------------------------------------+\n",[33,47183,47181],{"__ignoreMap":35},[11,47185,47186],{},"I think this was a success! I'm fairly certain that we were leveraging the GPU to run the MNIST hand-written digit notebook. I didn't see messages that CUDNN loaded, but I can find versions of both CUDNN and CUDA in the docker image:",[26,47188,47191],{"className":47189,"code":47190,"language":31},[29],"root@80f65a971e9a:/# ls /usr/include/x86_64-linux-gnu/\na.out.h  bits  cudnn_v6.h      fpu_control.h  gnu        python2.7\nasm      c++   expat_config.h  freetype2      ieee754.h  sys\n",[33,47192,47190],{"__ignoreMap":35},[26,47194,47197],{"className":47195,"code":47196,"language":31},[29],"root@80f65a971e9a:/# nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Tue_Jan_10_13:22:03_CST_2017\nCuda compilation tools, release 8.0, V8.0.61\n",[33,47198,47196],{"__ignoreMap":35},[11,47200,47201],{},"In previous attempts I had to register for an NVIDIA developer account and install these packages, but they seem to be packaged with the container.",[11,47203,47204],{},"Finally, we can check the installed python packages:",[26,47206,47209],{"className":47207,"code":47208,"language":31},[29],"root@80f65a971e9a:~# pip freeze | grep tensorflow\ntensorflow-gpu==1.4.0\ntensorflow-tensorboard==0.4.0rc2\nroot@80f65a971e9a:~#\n",[33,47210,47208],{"__ignoreMap":35},[11,47212,47213],{},"This looks good, but I'm still not 100% sure that everything was done properly. I would like to learn more about Tensorflow and also play around with some examples using Tensorboard. Let me know if you have any questions or comments about this setup, I'm still learning! Thanks for reading.",[11,47215,47216,47217,358],{},"Just for fun, here's a DeepDream rendering of a famous Donald Trump picture using Google's pre-trained ",[15,47218,47221],{"href":47219,"rel":47220},"https://github.com/google/inception",[19],"Inception model",[11,47223,47224],{},[511,47225],{"alt":7255,"src":47226},"/static/trump.png",[11,47228,47229],{},"For comparison, here is the original image:",[11,47231,47232],{},[511,47233],{"alt":7255,"src":47234},"/static/trump_original.jpg",[855,47236,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":47238},[47239,47240],{"id":46919,"depth":249,"text":46920},{"id":46926,"depth":249,"text":46927,"children":47241},[47242],{"id":46996,"depth":312,"text":46997},"2017-11-19","A walkthrough of Tensorflow setup and usage on Arch Linux with docker",{"layout":29014},"/2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux",{"title":46897,"description":47244},"2017/11/19/tensorflow-gpu-setup-with-docker-on-arch-linux",[45897,46820,15298,2203,1125],"Xeqj18Gemo2MIxIJy9yzMuHurRFxfkQuh4diJlOIdkM",{"id":47252,"title":47253,"body":47254,"comments":315,"date":48225,"description":48226,"draft":872,"extension":873,"external":874,"image":47260,"meta":48227,"navigation":315,"path":48228,"seo":48229,"stem":48230,"tags":48231,"__hash__":48232},"blog/2017/10/31/a-binary-clock-written-in-bash.md","A binary clock written in bash",{"type":8,"value":47255,"toc":48223},[47256,47261,47270,47279,47285,47288,47291,47753,47756,47761,47786,47826,47835,47840,47992,47995,48062,48071,48083,48092,48098,48125,48133,48167,48197,48200,48204,48207,48213,48220],[11,47257,47258],{},[511,47259],{"alt":7255,"src":47260},"/static/binaryclock.png",[11,47262,47263,47264,47269],{},"Configuring the i3 window manager on my laptop has got me interested in learning more about bash scripting. As an exercise for getting more familiar with bash, I set out to write a simple ",[15,47265,47268],{"href":47266,"rel":47267},"https://en.wikipedia.org/wiki/Binary_clock",[19],"binary clock"," application that runs in the terminal.",[11,47271,47272,47273,47278],{},"To simplifiy my clock, I decided to display Unix time as a binary number with ones and zeros represented as the unicode symbols ● and ○, respectively. ",[15,47274,47277],{"href":47275,"rel":47276},"https://en.wikipedia.org/wiki/Unix_time",[19],"Unix time"," is the number of second that have passed since January 1, 1970. Here's what I had in mind when I started out:",[26,47280,47283],{"className":47281,"code":47282,"language":31},[29]," ○ ○ ● ● ○\n ○ ● ● ● ●\n ○ ● ● ○ ●\n ● ● ○ ● ○\n ● ○ ○ ● ●\n ● ● ○ ● ○\n",[33,47284,47282],{"__ignoreMap":35},[11,47286,47287],{},"In this representation, the lower right cell represents the one's place, the next cell to the left represents the two's place, the next over the four's place, the next the eight's, and so on.",[11,47289,47290],{},"Here's the code that I ended up using for my clock program:",[26,47292,47294],{"className":181,"code":47293,"language":183,"meta":35,"style":35},"#!/bin/bash\nresize -s 8 19\n\nfunction decToBin { echo \"ibase=10; obase=2; $1\" | bc; };\n\ndraw() {\n  binstring=$(decToBin {$(date '+%s')})\n\n  for i in {31..6..-5}\n    do\n      echo $binstring | tail -c $i | head -c 5\n      printf \"\\n\"\n    done\n}\n\nprintf '\\e[?25l'\nclear\n\nwhile true ; do\n  printf '\\033[;H'\n  offset_v=$(( $(( $(tput lines)  / 2  ))  - 3  ))\n  v=$(( $offset_v > 0 ? $offset_v : 0 ));\n  for i in `seq 1 $v`;\n    do\n        printf \"\\n\"\n    done\n  offset_h=$(( $(( $(tput cols)  / 2  ))  - 7  ))\n  h=$(( $offset_h > 0 ? $offset_h : 0 ));\n  $(echo draw) | sed \"s/1/ $(tput setaf 6)● /g\" |\n                 sed \"s/0/ $(tput setaf 6)○ /g\" |\n                 sed \"s/^/$(head -c $h \u003C /dev/zero | tr '\\0' '\\ ';)/\"\n  sleep 1\ndone\n",[33,47295,47296,47300,47314,47318,47346,47350,47358,47387,47391,47406,47411,47440,47448,47453,47457,47461,47469,47474,47478,47490,47498,47539,47567,47590,47594,47601,47605,47638,47663,47695,47714,47741,47749],{"__ignoreMap":35},[187,47297,47298],{"class":189,"line":190},[187,47299,16416],{"class":295},[187,47301,47302,47305,47308,47311],{"class":189,"line":249},[187,47303,47304],{"class":193},"resize",[187,47306,47307],{"class":588}," -s",[187,47309,47310],{"class":588}," 8",[187,47312,47313],{"class":588}," 19\n",[187,47315,47316],{"class":189,"line":312},[187,47317,316],{"emptyLinePlaceholder":315},[187,47319,47320,47323,47326,47328,47330,47333,47336,47338,47340,47343],{"class":189,"line":319},[187,47321,47322],{"class":573},"function",[187,47324,47325],{"class":193}," decToBin",[187,47327,578],{"class":577},[187,47329,16486],{"class":588},[187,47331,47332],{"class":196}," \"ibase=10; obase=2; ",[187,47334,47335],{"class":588},"$1",[187,47337,16508],{"class":196},[187,47339,18453],{"class":573},[187,47341,47342],{"class":193}," bc",[187,47344,47345],{"class":577},"; };\n",[187,47347,47348],{"class":189,"line":325},[187,47349,316],{"emptyLinePlaceholder":315},[187,47351,47352,47355],{"class":189,"line":686},[187,47353,47354],{"class":193},"draw",[187,47356,47357],{"class":577},"() {\n",[187,47359,47360,47363,47365,47367,47370,47373,47375,47377,47380,47382,47385],{"class":189,"line":697},[187,47361,47362],{"class":577},"  binstring",[187,47364,595],{"class":573},[187,47366,16633],{"class":577},[187,47368,47369],{"class":193},"decToBin",[187,47371,47372],{"class":196}," {",[187,47374,16633],{"class":577},[187,47376,6550],{"class":193},[187,47378,47379],{"class":196}," '+%s'",[187,47381,343],{"class":577},[187,47383,47384],{"class":196},"}",[187,47386,621],{"class":577},[187,47388,47389],{"class":189,"line":1291},[187,47390,316],{"emptyLinePlaceholder":315},[187,47392,47393,47396,47399,47401,47403],{"class":189,"line":1306},[187,47394,47395],{"class":573},"  for",[187,47397,47398],{"class":577}," i ",[187,47400,16549],{"class":573},[187,47402,47372],{"class":577},[187,47404,47405],{"class":193},"31..6..-5}\n",[187,47407,47408],{"class":189,"line":1434},[187,47409,47410],{"class":573},"    do\n",[187,47412,47413,47416,47419,47421,47424,47427,47430,47432,47435,47437],{"class":189,"line":2599},[187,47414,47415],{"class":588},"      echo",[187,47417,47418],{"class":577}," $binstring ",[187,47420,16679],{"class":573},[187,47422,47423],{"class":193}," tail",[187,47425,47426],{"class":588}," -c",[187,47428,47429],{"class":577}," $i ",[187,47431,16679],{"class":573},[187,47433,47434],{"class":193}," head",[187,47436,47426],{"class":588},[187,47438,47439],{"class":588}," 5\n",[187,47441,47442,47445],{"class":189,"line":2607},[187,47443,47444],{"class":588},"      printf",[187,47446,47447],{"class":196}," \"\\n\"\n",[187,47449,47450],{"class":189,"line":2621},[187,47451,47452],{"class":573},"    done\n",[187,47454,47455],{"class":189,"line":2631},[187,47456,1309],{"class":577},[187,47458,47459],{"class":189,"line":2642},[187,47460,316],{"emptyLinePlaceholder":315},[187,47462,47463,47466],{"class":189,"line":2653},[187,47464,47465],{"class":588},"printf",[187,47467,47468],{"class":196}," '\\e[?25l'\n",[187,47470,47471],{"class":189,"line":2665},[187,47472,47473],{"class":193},"clear\n",[187,47475,47476],{"class":189,"line":2674},[187,47477,316],{"emptyLinePlaceholder":315},[187,47479,47480,47483,47485,47488],{"class":189,"line":2684},[187,47481,47482],{"class":573},"while",[187,47484,19269],{"class":588},[187,47486,47487],{"class":577}," ; ",[187,47489,16566],{"class":573},[187,47491,47492,47495],{"class":189,"line":2694},[187,47493,47494],{"class":588},"  printf",[187,47496,47497],{"class":196}," '\\033[;H'\n",[187,47499,47500,47503,47505,47508,47511,47514,47517,47520,47523,47525,47528,47531,47533,47536],{"class":189,"line":2706},[187,47501,47502],{"class":577},"  offset_v",[187,47504,595],{"class":573},[187,47506,47507],{"class":577},"$(( ",[187,47509,47510],{"class":193},"$((",[187,47512,47513],{"class":577}," $(",[187,47515,47516],{"class":193},"tput",[187,47518,47519],{"class":196}," lines",[187,47521,47522],{"class":577},")  ",[187,47524,20174],{"class":196},[187,47526,47527],{"class":588}," 2",[187,47529,47530],{"class":577},"  ))  ",[187,47532,677],{"class":193},[187,47534,47535],{"class":588}," 3",[187,47537,47538],{"class":577},"  ))\n",[187,47540,47541,47544,47546,47549,47551,47554,47557,47560,47562,47564],{"class":189,"line":2715},[187,47542,47543],{"class":577},"  v",[187,47545,595],{"class":573},[187,47547,47548],{"class":577},"$(( $offset_v ",[187,47550,37205],{"class":573},[187,47552,47553],{"class":588}," 0",[187,47555,47556],{"class":196}," ?",[187,47558,47559],{"class":577}," $offset_v ",[187,47561,358],{"class":196},[187,47563,47553],{"class":588},[187,47565,47566],{"class":577}," ));\n",[187,47568,47569,47571,47573,47575,47578,47581,47583,47586,47588],{"class":189,"line":2725},[187,47570,47395],{"class":573},[187,47572,47398],{"class":577},[187,47574,16549],{"class":573},[187,47576,47577],{"class":196}," `",[187,47579,47580],{"class":193},"seq",[187,47582,680],{"class":588},[187,47584,47585],{"class":577}," $v",[187,47587,13291],{"class":196},[187,47589,6961],{"class":577},[187,47591,47592],{"class":189,"line":2735},[187,47593,47410],{"class":573},[187,47595,47596,47599],{"class":189,"line":2743},[187,47597,47598],{"class":588},"        printf",[187,47600,47447],{"class":196},[187,47602,47603],{"class":189,"line":2754},[187,47604,47452],{"class":573},[187,47606,47607,47610,47612,47614,47616,47618,47620,47623,47625,47627,47629,47631,47633,47636],{"class":189,"line":2762},[187,47608,47609],{"class":577},"  offset_h",[187,47611,595],{"class":573},[187,47613,47507],{"class":577},[187,47615,47510],{"class":193},[187,47617,47513],{"class":577},[187,47619,47516],{"class":193},[187,47621,47622],{"class":196}," cols",[187,47624,47522],{"class":577},[187,47626,20174],{"class":196},[187,47628,47527],{"class":588},[187,47630,47530],{"class":577},[187,47632,677],{"class":193},[187,47634,47635],{"class":588}," 7",[187,47637,47538],{"class":577},[187,47639,47640,47643,47645,47648,47650,47652,47654,47657,47659,47661],{"class":189,"line":2770},[187,47641,47642],{"class":577},"  h",[187,47644,595],{"class":573},[187,47646,47647],{"class":577},"$(( $offset_h ",[187,47649,37205],{"class":573},[187,47651,47553],{"class":588},[187,47653,47556],{"class":196},[187,47655,47656],{"class":577}," $offset_h ",[187,47658,358],{"class":196},[187,47660,47553],{"class":588},[187,47662,47566],{"class":577},[187,47664,47665,47668,47671,47673,47675,47678,47681,47683,47686,47689,47692],{"class":189,"line":2781},[187,47666,47667],{"class":193},"  $(echo",[187,47669,47670],{"class":196}," draw",[187,47672,13780],{"class":577},[187,47674,16679],{"class":573},[187,47676,47677],{"class":193}," sed",[187,47679,47680],{"class":196}," \"s/1/ $(",[187,47682,47516],{"class":193},[187,47684,47685],{"class":196}," setaf ",[187,47687,47688],{"class":588},"6",[187,47690,47691],{"class":196},")● /g\"",[187,47693,47694],{"class":573}," |\n",[187,47696,47697,47700,47703,47705,47707,47709,47712],{"class":189,"line":2792},[187,47698,47699],{"class":193},"                 sed",[187,47701,47702],{"class":196}," \"s/0/ $(",[187,47704,47516],{"class":193},[187,47706,47685],{"class":196},[187,47708,47688],{"class":588},[187,47710,47711],{"class":196},")○ /g\"",[187,47713,47694],{"class":573},[187,47715,47716,47718,47721,47723,47725,47728,47731,47734,47736,47738],{"class":189,"line":2803},[187,47717,47699],{"class":193},[187,47719,47720],{"class":196}," \"s/^/$(",[187,47722,7848],{"class":193},[187,47724,47426],{"class":588},[187,47726,47727],{"class":577}," $h",[187,47729,47730],{"class":573}," \u003C",[187,47732,47733],{"class":196}," /dev/zero ",[187,47735,16679],{"class":573},[187,47737,17321],{"class":193},[187,47739,47740],{"class":196}," '\\0' '\\ ';)/\"\n",[187,47742,47743,47746],{"class":189,"line":2808},[187,47744,47745],{"class":193},"  sleep",[187,47747,47748],{"class":588}," 1\n",[187,47750,47751],{"class":189,"line":2816},[187,47752,17114],{"class":573},[11,47754,47755],{},"The program uses two function and one while loop to display the time.",[11,47757,47758,47760],{},[33,47759,47369],{}," is a simple helper function to convert decimal numbers to binary representations.",[11,47762,47763,47765,47766,1172,47768,47770,47771,47773,47774,47777,47778,1172,47780,47782,47783,358],{},[33,47764,47354],{}," structures the the string of ones and zeros into 6 rows and five columns of ones and zeros. This function uses ",[33,47767,7848],{},[33,47769,35579],{}," in combination with a ",[33,47772,16543],{}," loop to iterate over a string. Notice the ",[33,47775,47776],{},"-c"," flag on ",[33,47779,35579],{},[33,47781,7848],{},". The following is from the ",[33,47784,47785],{},"man head",[26,47787,47789],{"className":181,"code":47788,"language":183,"meta":35,"style":35},"       -c, --bytes=[-]NUM\n              print the first NUM bytes of each file;\n",[33,47790,47791,47799],{"__ignoreMap":35},[187,47792,47793,47796],{"class":189,"line":190},[187,47794,47795],{"class":193},"       -c,",[187,47797,47798],{"class":588}," --bytes=[-]NUM\n",[187,47800,47801,47804,47807,47810,47813,47816,47819,47822,47824],{"class":189,"line":249},[187,47802,47803],{"class":588},"              print",[187,47805,47806],{"class":196}," the",[187,47808,47809],{"class":196}," first",[187,47811,47812],{"class":196}," NUM",[187,47814,47815],{"class":196}," bytes",[187,47817,47818],{"class":196}," of",[187,47820,47821],{"class":196}," each",[187,47823,1907],{"class":196},[187,47825,6961],{"class":577},[11,47827,47828,47829,47832,47833,752],{},"This gets ",[33,47830,47831],{},"NUM"," number of ones and zeros (each being one byte) from the string of ones and zeros that results from ",[33,47834,47369],{},[11,47836,12674,47837,47839],{},[33,47838,47482],{}," loop, I measure the length and width of the terminal window to center the position of the clock in case it has been changed with the following lines of code:",[26,47841,47843],{"className":181,"code":47842,"language":183,"meta":35,"style":35},"[...]\n  offset_v=$(( $(( $(tput lines)  / 2  ))  - 3  ))\n  v=$(( $offset_v > 0 ? $offset_v : 0 ));\n  for i in `seq 1 $v`;\n    do\n        printf \"\\n\"\n    done\n  offset_h=$(( $(( $(tput cols)  / 2  ))  - 7  ))\n  h=$(( $offset_h > 0 ? $offset_h : 0 ));\n[...]\n",[33,47844,47845,47850,47880,47902,47922,47926,47932,47936,47966,47988],{"__ignoreMap":35},[187,47846,47847],{"class":189,"line":190},[187,47848,47849],{"class":577},"[...]\n",[187,47851,47852,47854,47856,47858,47860,47862,47864,47866,47868,47870,47872,47874,47876,47878],{"class":189,"line":249},[187,47853,47502],{"class":577},[187,47855,595],{"class":573},[187,47857,47507],{"class":577},[187,47859,47510],{"class":193},[187,47861,47513],{"class":577},[187,47863,47516],{"class":193},[187,47865,47519],{"class":196},[187,47867,47522],{"class":577},[187,47869,20174],{"class":196},[187,47871,47527],{"class":588},[187,47873,47530],{"class":577},[187,47875,677],{"class":193},[187,47877,47535],{"class":588},[187,47879,47538],{"class":577},[187,47881,47882,47884,47886,47888,47890,47892,47894,47896,47898,47900],{"class":189,"line":312},[187,47883,47543],{"class":577},[187,47885,595],{"class":573},[187,47887,47548],{"class":577},[187,47889,37205],{"class":573},[187,47891,47553],{"class":588},[187,47893,47556],{"class":196},[187,47895,47559],{"class":577},[187,47897,358],{"class":196},[187,47899,47553],{"class":588},[187,47901,47566],{"class":577},[187,47903,47904,47906,47908,47910,47912,47914,47916,47918,47920],{"class":189,"line":319},[187,47905,47395],{"class":573},[187,47907,47398],{"class":577},[187,47909,16549],{"class":573},[187,47911,47577],{"class":196},[187,47913,47580],{"class":193},[187,47915,680],{"class":588},[187,47917,47585],{"class":577},[187,47919,13291],{"class":196},[187,47921,6961],{"class":577},[187,47923,47924],{"class":189,"line":325},[187,47925,47410],{"class":573},[187,47927,47928,47930],{"class":189,"line":686},[187,47929,47598],{"class":588},[187,47931,47447],{"class":196},[187,47933,47934],{"class":189,"line":697},[187,47935,47452],{"class":573},[187,47937,47938,47940,47942,47944,47946,47948,47950,47952,47954,47956,47958,47960,47962,47964],{"class":189,"line":1291},[187,47939,47609],{"class":577},[187,47941,595],{"class":573},[187,47943,47507],{"class":577},[187,47945,47510],{"class":193},[187,47947,47513],{"class":577},[187,47949,47516],{"class":193},[187,47951,47622],{"class":196},[187,47953,47522],{"class":577},[187,47955,20174],{"class":196},[187,47957,47527],{"class":588},[187,47959,47530],{"class":577},[187,47961,677],{"class":193},[187,47963,47635],{"class":588},[187,47965,47538],{"class":577},[187,47967,47968,47970,47972,47974,47976,47978,47980,47982,47984,47986],{"class":189,"line":1306},[187,47969,47642],{"class":577},[187,47971,595],{"class":573},[187,47973,47647],{"class":577},[187,47975,37205],{"class":573},[187,47977,47553],{"class":588},[187,47979,47556],{"class":196},[187,47981,47656],{"class":577},[187,47983,358],{"class":196},[187,47985,47553],{"class":588},[187,47987,47566],{"class":577},[187,47989,47990],{"class":189,"line":1434},[187,47991,47849],{"class":577},[11,47993,47994],{},"Finally, I convert the ones and zeros to the colored unicode circles with the following lines of code:",[26,47996,47998],{"className":181,"code":47997,"language":183,"meta":35,"style":35},"  $(echo draw) | sed \"s/1/ $(tput setaf 6)● /g\" |\n                 sed \"s/0/ $(tput setaf 6)○ /g\" |\n                 sed \"s/^/$(head -c $h \u003C /dev/zero | tr '\\0' '\\ ';)/\"\n",[33,47999,48000,48024,48040],{"__ignoreMap":35},[187,48001,48002,48004,48006,48008,48010,48012,48014,48016,48018,48020,48022],{"class":189,"line":190},[187,48003,47667],{"class":193},[187,48005,47670],{"class":196},[187,48007,13780],{"class":577},[187,48009,16679],{"class":573},[187,48011,47677],{"class":193},[187,48013,47680],{"class":196},[187,48015,47516],{"class":193},[187,48017,47685],{"class":196},[187,48019,47688],{"class":588},[187,48021,47691],{"class":196},[187,48023,47694],{"class":573},[187,48025,48026,48028,48030,48032,48034,48036,48038],{"class":189,"line":249},[187,48027,47699],{"class":193},[187,48029,47702],{"class":196},[187,48031,47516],{"class":193},[187,48033,47685],{"class":196},[187,48035,47688],{"class":588},[187,48037,47711],{"class":196},[187,48039,47694],{"class":573},[187,48041,48042,48044,48046,48048,48050,48052,48054,48056,48058,48060],{"class":189,"line":312},[187,48043,47699],{"class":193},[187,48045,47720],{"class":196},[187,48047,7848],{"class":193},[187,48049,47426],{"class":588},[187,48051,47727],{"class":577},[187,48053,47730],{"class":573},[187,48055,47733],{"class":196},[187,48057,16679],{"class":573},[187,48059,17321],{"class":193},[187,48061,47740],{"class":196},[11,48063,48064,48065,765,48067,48070],{},"Piping the output of ",[33,48066,47354],{},[33,48068,48069],{},"sed"," lets us do some simple substition using the pattern:",[26,48072,48074],{"className":181,"code":48073,"language":183,"meta":35,"style":35},"sed \"s/\u003Cwhat you want to swap out>/\u003Cwhat you want to swap in>/g\"\n",[33,48075,48076],{"__ignoreMap":35},[187,48077,48078,48080],{"class":189,"line":190},[187,48079,48069],{"class":193},[187,48081,48082],{"class":196}," \"s/\u003Cwhat you want to swap out>/\u003Cwhat you want to swap in>/g\"\n",[11,48084,6131,48085,48088,48089,48091],{},[33,48086,48087],{},"\"../g\""," at the end of the ",[33,48090,48069],{}," argument specifies that we want to make the substition globally.",[11,48093,48094,48095,48097],{},"The last ",[33,48096,48069],{}," command inserts spaces to the right of each row for the horizontal offset (in order to center the clock on our terminal window). This uses another interesting pattern that I came across on StackOverflow:",[26,48099,48101],{"className":181,"code":48100,"language":183,"meta":35,"style":35},"sed \"s/^/$(head -c $h \u003C /dev/zero | tr '\\0' '\\ ';)/\"\n",[33,48102,48103],{"__ignoreMap":35},[187,48104,48105,48107,48109,48111,48113,48115,48117,48119,48121,48123],{"class":189,"line":190},[187,48106,48069],{"class":193},[187,48108,47720],{"class":196},[187,48110,7848],{"class":193},[187,48112,47426],{"class":588},[187,48114,47727],{"class":577},[187,48116,47730],{"class":573},[187,48118,47733],{"class":196},[187,48120,16679],{"class":573},[187,48122,17321],{"class":193},[187,48124,47740],{"class":196},[11,48126,6131,48127,48129,48130,48132],{},[33,48128,37176],{}," is a regular expression that represents the beginning of a line. So with this ",[33,48131,48069],{}," substitution we will be adding to the beginning of each line. What we are adding is the following:",[26,48134,48136],{"className":181,"code":48135,"language":183,"meta":35,"style":35},"$(head -c $h \u003C /dev/zero | tr '\\0' '\\ ';)/\n",[33,48137,48138],{"__ignoreMap":35},[187,48139,48140,48142,48144,48146,48149,48151,48154,48156,48158,48161,48164],{"class":189,"line":190},[187,48141,16633],{"class":577},[187,48143,7848],{"class":193},[187,48145,47426],{"class":588},[187,48147,48148],{"class":577}," $h ",[187,48150,6724],{"class":573},[187,48152,48153],{"class":196}," /dev/zero",[187,48155,18453],{"class":573},[187,48157,17321],{"class":193},[187,48159,48160],{"class":196}," '\\0'",[187,48162,48163],{"class":196}," '\\ '",[187,48165,48166],{"class":577},";)/\n",[11,48168,48169,48170,48173,48174,48176,48177,5857,48180,48182,48183,48185,48186,48189,48190,48192,48193,48196],{},"This takes the number of columns that we want to shift our clock as ",[33,48171,48172],{},"$h"," and reads the first ",[33,48175,48172],{}," bytes from ",[33,48178,48179],{},"/dev/zero",[33,48181,48179],{}," produces a continuous stream of NULL (zero value) bytes, so the first ",[33,48184,48172],{}," bytes will be something like ",[33,48187,48188],{},"\\0, \\0, \\0, \\0, \\0",". We then pipe this output to ",[33,48191,1531],{}," which translates the null bytes into spaces (",[33,48194,48195],{},"'\\ '",") which help us pad our clock.",[11,48198,48199],{},"Here's a screenshot of the clock in action:",[11,48201,48202],{},[511,48203],{"alt":7255,"src":47260},[11,48205,48206],{},"Here's the script on my github account:",[11,48208,48209],{},[15,48210,48211],{"href":48211,"rel":48212},"https://github.com/briancaffey/binaryclock/blob/master/binaryclock",[19],[11,48214,48215,48216,48219],{},"The clock works well on ",[33,48217,48218],{},"rxvt-unicode",", but I need to make some small changes to make it work on other terminal emulators.",[855,48221,48222],{},"html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":35,"searchDepth":249,"depth":249,"links":48224},[],"2017-10-31","A program that displays the current time as a binary representation",{"layout":29014},"/2017/10/31/a-binary-clock-written-in-bash",{"title":47253,"description":48226},"2017/10/31/a-binary-clock-written-in-bash",[183],"5OzG9vdXWN4bjmp5JB58rR_hxqTkgRzQUB3WM7TVgIM",{"id":48234,"title":48235,"body":48236,"comments":315,"date":49116,"description":35,"draft":872,"extension":873,"external":874,"image":49117,"meta":49118,"navigation":315,"path":49119,"seo":49120,"stem":49121,"tags":49122,"__hash__":49124},"blog/2017/10/17/moving-from-gnome-to-i3-on-arch-linux.md","Moving from Gnome Desktop to i3 window manager on Arch Linux",{"type":8,"value":48237,"toc":49099},[48238,48243,48252,48255,48258,48296,48302,48308,48314,48323,48334,48351,48354,48362,48365,48371,48377,48383,48392,48395,48399,48411,48417,48425,48432,48442,48452,48458,48461,48471,48477,48483,48493,48508,48512,48523,48531,48534,48540,48545,48548,48556,48559,48571,48585,48591,48597,48600,48603,48609,48612,48618,48624,48630,48643,48649,48655,48661,48667,48672,48675,48681,48684,48690,48696,48701,48707,48710,48712,48718,48724,48730,48735,48741,48744,48748,48751,48754,48762,48768,48775,48778,48784,48787,48793,48801,48808,48814,48817,48824,48827,48833,48843,48849,48856,48859,48864,48870,48873,48877,48883,48886,48892,48896,48901,48907,48910,48916,48922,48925,48962,48968,48972,48980,48986,48989,48995,49001,49004,49008,49012,49018,49021,49027,49032,49038,49042,49045,49093],[11,48239,48240],{},[511,48241],{"alt":7255,"src":48242},"/static/gnome.png",[11,48244,48245,48246,48251],{},"I recently tried out ",[15,48247,48250],{"href":48248,"rel":48249},"http://i3wm.org",[19],"i3"," on my laptop and I'm really liking it so far. I am going to try to recreate the same i3 configuration on my desktop installation of Arch Linux. I'll try to faithfully cover each step of the process in this article.",[11,48253,48254],{},"i3 sounded like a nice idea at first, but there were a lot of aspects of my Gnome desktop that I didn't think I could do without. I'm still new to i3, but I have found it very interesting to see how everything can be configured. If you are thinking about switching to i3, hopefully this can help you out.",[11,48256,48257],{},"Here's a list of everything I want to go through:",[916,48259,48260,48263,48266,48269,48275,48278,48281,48284,48287,48290,48293],{},[919,48261,48262],{},"Installing i3",[919,48264,48265],{},"Basic commands",[919,48267,48268],{},"Workflow",[919,48270,48271,48272],{},"Setting a background with ",[33,48273,48274],{},"feh",[919,48276,48277],{},"The i3 config file",[919,48279,48280],{},"Setting uxrvt as a default terminal",[919,48282,48283],{},"Customizing uxrvt",[919,48285,48286],{},"Customizing workspaces",[919,48288,48289],{},"Setting up blocks with i3blocks",[919,48291,48292],{},"Custom lock screen with animation",[919,48294,48295],{},"i3-gaps",[11,48297,48298,48299,48301],{},"Start out by installing ",[33,48300,48250],{}," with pacman as follows",[26,48303,48306],{"className":48304,"code":48305,"language":31},[29],"[brian@archthinkpad ~]$ sudo pacman -S i3\n[sudo] password for brian:\n:: There are 4 members in group i3:\n:: Repository community\n   1) i3-wm  2) i3blocks  3) i3lock  4) i3status\n\nEnter a selection (default=all):\n",[33,48307,48305],{"__ignoreMap":35},[11,48309,48310,48311,48313],{},"You won't find ",[33,48312,48250],{}," with a regular search in the AUR because it is a package group, containing a number of packages that will help us do things with i3.",[11,48315,48316,48317,48319,48320,48322],{},"Once you have installed ",[33,48318,48250],{},", logout of your current Gnome session, and then go back to login and select ",[33,48321,48250],{}," from the login menu.",[11,48324,48325,48326,48329,48330,48333],{},"You will be greated with a black screen and a dialogue box that says ",[33,48327,48328],{},"i3: first confuguration",". I recommend that you press ",[33,48331,48332],{},"Enter"," and have i3 generate a config file for you as it says in the prompt.",[11,48335,48336,48337,15754,48340,48343,48344,48346,48347,48350],{},"Next, choose either ",[33,48338,48339],{},"Win",[33,48341,48342],{},"Alt"," as the key that will help you launch most commands in i3. I use ",[33,48345,48342],{},", but it won't make a difference in this tutorial since we will be refering to whichever key you select as ",[33,48348,48349],{},"Mod1"," from here on out.",[11,48352,48353],{},"Once you make this selection, the prompt will go away and you are met with a black screen and and a status bar on the bottom, as well as workspace indicator on the bottom left. Welcome to i3!",[11,48355,48356,48357,48361],{},"At this point, you should have a read through the very well-written i3 User Guide linked ",[15,48358,1321],{"href":48359,"rel":48360},"https://i3wm.org/docs/userguide.html",[19],". Learn how to move windows around, close windows and stack windows.",[11,48363,48364],{},"Here's the one command you really need to get going:",[11,48366,48367,48370],{},[33,48368,48369],{},"Mod1+Enter",": open a new terminal",[11,48372,48373,48374,358],{},"You probably want to change this terminal right away. There's a handy little program launcher that we can use for now called ",[33,48375,48376],{},"dmenu",[26,48378,48381],{"className":48379,"code":48380,"language":31},[29],"[brian@archthinkpad ~]$ sudo pacman -Ss dmenu\n[sudo] password for brian:\ncommunity/dmenu 4.7-1 [installed]\n    A generic menu for X\ncommunity/pdmenu 1.3.2-2\n    simple full screen menu program\n[brian@archthinkpad ~]$\n",[33,48382,48380],{"__ignoreMap":35},[11,48384,48385,48386,48388,48389,752],{},"Once you install ",[33,48387,48376],{},", you can easily launch programs with ",[33,48390,48391],{},"Mod1+d",[11,48393,48394],{},"In a minute we will customize dmenu to look better, but for now we need to get into the meat of i3: customization.",[168,48396,48398],{"id":48397},"customization","Customization",[11,48400,48401,48402,48404,48405,765,48408,22465],{},"Most of the work you do in customizing i3 involves editing ",[33,48403,48250],{},"'s config file. To change the configuration of i3, copy ",[33,48406,48407],{},"/etc/i3/config",[33,48409,48410],{},"~/.i3/config",[26,48412,48415],{"className":48413,"code":48414,"language":31},[29],"cp /etc/i3/config ~/.i3/config\n",[33,48416,48414],{"__ignoreMap":35},[11,48418,48419,48420,15754,48422,752],{},"At this point I will reference my \"Dotfiles\" on github. Dotfiles is used to refer to hidden configuration folders and files prepended with a \".\", such as ",[33,48421,48410],{},[33,48423,48424],{},"~/.Xresources",[11,48426,48427,48428,752],{},"A public repo with my dotfiles is available ",[15,48429,1321],{"href":48430,"rel":48431},"https://github.com/briancaffey/.i3",[19],[11,48433,48434,48435,48438,48439,48441],{},"Let's start by adding a background image. Find an image you like and add it to ",[33,48436,48437],{},"~/Pictures",". Then install ",[33,48440,48274],{}," from the AUR if you don't alread have it.",[11,48443,48444,48446,48447,48449,48450,358],{},[33,48445,48274],{}," will let us set a background image from the command line, and we can do so each time we launch ",[33,48448,48250],{}," by adding the following line to ",[33,48451,48410],{},[26,48453,48456],{"className":48454,"code":48455,"language":31},[29],"exec_always feh --bg-scale ~/Pictures/image.jpg\n",[33,48457,48455],{"__ignoreMap":35},[11,48459,48460],{},"Now we can run the following command to restart i3 in place (without having to logout):",[11,48462,48463,9585,48465,9585,48468],{},[33,48464,48349],{},[33,48466,48467],{},"shift",[33,48469,48470],{},"r",[11,48472,48473,48474,48476],{},"You will see this shortcut in ",[33,48475,48410],{}," in the following line:",[26,48478,48481],{"className":48479,"code":48480,"language":31},[29],"bindsym Mod1+Shift+r restart\n",[33,48482,48480],{"__ignoreMap":35},[11,48484,48485,48486,9585,48488,9585,48490,48492],{},"This binds ",[33,48487,48349],{},[33,48489,48467],{},[33,48491,48470],{}," to instructions that restart i3.",[11,48494,48495,48496,48499,48500,48503,48504,48507],{},"If you write ",[33,48497,48498],{},"bindsym Some+key+combo exec reboot",", your system will reboot when you press ",[33,48501,48502],{},"Some+key+combo",". You can call any command this way, allowing for a high level of customization. It is helpful to see what others have done by browsing ",[33,48505,48506],{},".dotfiles"," online. You can mix and match commands to your liking. If you make an error, i3 will warn you when you start or restart i3.",[168,48509,48511],{"id":48510},"customizing-the-terminal-urxvt","Customizing the Terminal (urxvt)",[11,48513,48514,48515,48518,48519,48522],{},"Next let's take care of our terminal. You will notice that the command to launch a new terminal actually launches another program called ",[33,48516,48517],{},"i3-sensible-terminal",". Run ",[33,48520,48521],{},"man i3-sensible-terminal"," to see how this works in detail. It basically picks a terminal program for you based on what you have installed on your system.",[11,48524,48525,48526,48528,48529,752],{},"For my terminal, I use a program called ",[33,48527,46841],{},". This is a popular terminal program in the i3 community because of the fact that it is highly customizable. There are a lot of options for terminals, so feel free to use whatever you like. I will go into depth here about how I customize ",[33,48530,46841],{},[11,48532,48533],{},"First, install it with:",[26,48535,48538],{"className":48536,"code":48537,"language":31},[29],"sudo pacman -S rxvt-unicode\n",[33,48539,48537],{"__ignoreMap":35},[11,48541,48542,48543],{},"And then launch it with ",[33,48544,46841],{},[11,48546,48547],{},"It probably looks equally bad to whatever default you were using, but we are about to fix it up so it looks and works great.",[11,48549,48550,48551,752],{},"First, have a look at the ",[15,48552,48555],{"href":48553,"rel":48554},"https://wiki.archlinux.org/index.php/rxvt-unicode",[19],"Arch Wikie article on rxvt-unicode",[911,48557,48424],{"id":48558},"xresources",[11,48560,48561,48562,48564,48565,48567,48568,48570],{},"Just like how the behavior of i3 is read from ",[33,48563,48410],{},", the behavior of ",[33,48566,46841],{}," is read from a file in your home directory called ",[33,48569,48424],{},". This file won't be here, so we need to create it.",[11,48572,48573,48574,48579,48580,48582,48583,358],{},"For simplicity, I recommend that you copy the contents of ",[15,48575,48578],{"href":48576,"rel":48577},"https://raw.githubusercontent.com/briancaffey/.i3/master/.Xresources",[19],"this link"," into you newly created ",[33,48581,48424],{}," file and then run the following command to refresh the settings for ",[33,48584,46841],{},[26,48586,48589],{"className":48587,"code":48588,"language":31},[29],"xrdb ~/.Xresources\n",[33,48590,48588],{"__ignoreMap":35},[11,48592,48593,48594,48596],{},"Now restart ",[33,48595,46841],{}," and you should see that it looks very different.",[11,48598,48599],{},"Look over the arch wiki article mentioned above for more information on how to customize urxvt.",[168,48601,48602],{"id":48602},"pywal",[11,48604,48605,48606,48608],{},"Next lets work on terminal colors. There is a great program called ",[33,48607,48602],{}," which reads one or several image file and then applies a color scheme to your system based on the colors found.",[11,48610,48611],{},"Install it from the AUR:",[26,48613,48616],{"className":48614,"code":48615,"language":31},[29],"sudo pacman -S python-pywal\n",[33,48617,48615],{"__ignoreMap":35},[11,48619,48620,48621,358],{},"Now we just need to add the following line to ",[33,48622,48623],{},"~/.bashrc",[26,48625,48628],{"className":48626,"code":48627,"language":31},[29],"# pywal\nsetsid wal -i ~/Pictures/image.jpg\n",[33,48629,48627],{"__ignoreMap":35},[11,48631,1164,48632,48635,48636,48638,48639,48642],{},[33,48633,48634],{},"source ~/.bashrc"," and reopen ",[33,48637,46841],{}," and it should have new color scheme that goes well with you background image. You can install another program called ",[33,48640,48641],{},"neofetch"," to print out the current colorscheme along with system information:",[26,48644,48647],{"className":48645,"code":48646,"language":31},[29],"yaourt -S neofetch\n",[33,48648,48646],{"__ignoreMap":35},[11,48650,48651,48652,48654],{},"You can add the following line to the bottom of your ",[33,48653,48623],{}," file and have neofetch run whenever you open a new terminal:",[26,48656,48659],{"className":48657,"code":48658,"language":31},[29],"setsid wal -r\n",[33,48660,48658],{"__ignoreMap":35},[26,48662,48665],{"className":48663,"code":48664,"language":31},[29],"neofetch\n",[33,48666,48664],{"__ignoreMap":35},[11,48668,48669,48670,22038],{},"Here are a few other customizations I have in my ",[33,48671,48410],{},[11,48673,48674],{},"Remove window boarders:",[26,48676,48679],{"className":48677,"code":48678,"language":31},[29],"for_window [class=\"^.*\"] border pixel 0\n",[33,48680,48678],{"__ignoreMap":35},[11,48682,48683],{},"Enable smooth transitions with compton:",[11,48685,48686,48689],{},[33,48687,48688],{},"compton"," is a package that enables for nice transitions when navigating i3. First, install the compton package:",[26,48691,48694],{"className":48692,"code":48693,"language":31},[29],"yaourt -S compton\n",[33,48695,48693],{"__ignoreMap":35},[11,48697,48698,48699,358],{},"Next we need to add the following line to ",[33,48700,48410],{},[26,48702,48705],{"className":48703,"code":48704,"language":31},[29],"exec compton -f\n",[33,48706,48704],{"__ignoreMap":35},[11,48708,48709],{},"You might need to reboot to see how the effect that compton has.",[168,48711,48295],{"id":48295},[11,48713,48714,48715,48717],{},"We can now add a neat feature to our i3 setup by installing a popular fork of i3 callde ",[33,48716,48295],{},". It adds some additional functionality to i3, including the ability to add gaps in between our windows.",[11,48719,48720,48721,48723],{},"Install ",[33,48722,48295],{}," from the AUR:",[26,48725,48728],{"className":48726,"code":48727,"language":31},[29],"yaourt -S i3-gaps\n",[33,48729,48727],{"__ignoreMap":35},[11,48731,48732,48733,358],{},"Remove the packages in conflict and then add the following lines to ",[33,48734,48410],{},[26,48736,48739],{"className":48737,"code":48738,"language":31},[29],"# i3-gaps\ngaps inner 10\ngaps outer 0\n",[33,48740,48738],{"__ignoreMap":35},[11,48742,48743],{},"Refresh i3 and you should now see gaps in between your windows.",[168,48745,48747],{"id":48746},"the-bar","The Bar",[11,48749,48750],{},"Things should be looking pretty good, but we still need to do some work on the bar at the bottom of the screen.",[11,48752,48753],{},"Let's install FontAwesome so we can use some nice icons in our bar:",[11,48755,48756,48757,48761],{},"Find the most recent release of FontAwesome ",[15,48758,1321],{"href":48759,"rel":48760},"https://github.com/FortAwesome/Font-Awesome/releases",[19],", click on the zip download link and then run:",[26,48763,48766],{"className":48764,"code":48765,"language":31},[29],"unzip ~/Downloads/Font-Awesome-4.7.0\n",[33,48767,48765],{"__ignoreMap":35},[11,48769,48770,48771,48774],{},"The release number may be different for you. Once you have unzipped the file, we want to move all of the files ending with ",[33,48772,48773],{},".ttf"," to a folder that may or may not exist on your machine.",[11,48776,48777],{},"First, run:",[26,48779,48782],{"className":48780,"code":48781,"language":31},[29],"mkdir ~/.fonts\n",[33,48783,48781],{"__ignoreMap":35},[11,48785,48786],{},"and then run:",[26,48788,48791],{"className":48789,"code":48790,"language":31},[29],"cp ~/Downloads/Font-Awesome-4.7.0/fonts/*.ttf ~/.fonts\n",[33,48792,48790],{"__ignoreMap":35},[11,48794,48795,48796,48800],{},"Now that you have these fonts installed, go over to ",[15,48797,48798],{"href":48798,"rel":48799},"http://fontawesome.io/cheatsheet/",[19]," and you should see lots of icons that you might not have been able to see before.",[11,48802,48803,48804,48807],{},"We will come back to the fonts in just a minute. First let's change the bar by replacing ",[33,48805,48806],{},"bar {...}"," with the following:",[26,48809,48812],{"className":48810,"code":48811,"language":31},[29],"bar {\n    position top\n        status_command i3blocks -c /home/brian/.i3/i3blocks.conf\n    colors {\n        background $bg-color\n            separator #757575\n        #                  border             background         text\n        focused_workspace  $bg-color          $bg-color          $text-color\n        inactive_workspace $inactive-bg-color $inactive-bg-color $inactive-text-color\n        urgent_workspace   $urgent-bg-color   $urgent-bg-color   $text-color\n    }\n}\n",[33,48813,48811],{"__ignoreMap":35},[11,48815,48816],{},"Don't refresh i3 just yet. Let's go through this block first.",[11,48818,48819,48820,48823],{},"We tell i3 that our bar will appear at the top of the screen, and that the contents of the bar come from the command: ",[33,48821,48822],{},"status_command i3blocks -c /home/brian/.i3/i3blocks.conf",". Finally, we define some background colors.",[11,48825,48826],{},"First, pull in the colors from my dotfiles before the bar block:",[26,48828,48831],{"className":48829,"code":48830,"language":31},[29],"set $bg-color            #2f343f\nset $inactive-bg-color   #2f343f\nset $text-color          #f3f4f5\nset $inactive-text-color #676E7D\nset $urgent-bg-color     #E53935\n\n# window colors\n#                       border              background         text                 indicator\nclient.focused          $bg-color           $bg-color          $text-color          #00ff00\nclient.unfocused        $inactive-bg-color $inactive-bg-color $inactive-text-color #00ff00\nclient.focused_inactive $inactive-bg-color $inactive-bg-color $inactive-text-color #00ff00\nclient.urgent           $urgent-bg-color    $urgent-bg-color   $text-color          #00ff00\n",[33,48832,48830],{"__ignoreMap":35},[11,48834,48835,48836,48839,48840,48842],{},"Next let's take care of the i3blocks command. The ",[33,48837,48838],{},"i3blocks"," package was installed when we instaled i3, so we just need to provide an absolute path to a file as the argument for this command, as well as the ",[33,48841,47776],{}," flag. In this file we simply define what shows up in the bar. Here's a simple version that shows date, time, CPU temperature and wifi:",[26,48844,48847],{"className":48845,"code":48846,"language":31},[29],"[wifi]\nlabel=\ncommand=iwgetid -r\nseparator=true\ninterval=3\n\n#[volume]\n#label=\n#interval=1\n#separator=true\n#command=amixer get Master | egrep -o \"[0-9]+%\" | sed -n '2 p'\n\n#[cpu]\n#label=\n#interval=10\n#separator=true\n\n[temperature]\ncommand=T=$(cat /sys/class/thermal/thermal_zone0/temp); echo $(( $T / 1000 ))°C\nlabel=\ninterval=10\nseparator=true\n\n[time]\ncommand= date '+%H:%M:%S'\ninterval=2\nlabel=\nseparator=true\n\n[day]\ncommand= date '+%a %b %e, %Y'\ninterval=2\nlabel=\nseparator=true\n",[33,48848,48846],{"__ignoreMap":35},[11,48850,48851,48852,48855],{},"This is why it is called i3",[4339,48853,48854],{},"blocks",", because each part of the bar is defined in a block that has a command, an interval in seconds that determines how often the command is run, and a label. You can choose any text or icon for the labels.",[11,48857,48858],{},"Next we can add some labels to the workspaces on the left side of the bar. I like to divide my workspaces into groups to keep things organized.",[11,48860,48861,48862,22038],{},"Add the following to your ",[33,48863,48410],{},[26,48865,48868],{"className":48866,"code":48867,"language":31},[29],"set $workspace10 \"H0me \"\nset $workspace1 \"F1rst \"\nset $workspace4 \"Edi4or \"\nset $workspace3 \"Brows3r \"\nset $workspace2 \"2erminal \"\nset $workspace5 \"Mu5ic \"\nset $workspace8 \"O8S \"\n\n# switch to workspace\nbindsym Mod1+1 workspace $workspace1\nbindsym Mod1+2 workspace $workspace2\nbindsym Mod1+3 workspace $workspace3\nbindsym Mod1+4 workspace $workspace4\nbindsym Mod1+5 workspace $workspace5\nbindsym Mod1+6 workspace 6\nbindsym Mod1+7 workspace 7\nbindsym Mod1+8 workspace $workspace8\nbindsym Mod1+9 workspace 9\nbindsym Mod1+0 workspace $workspace10\n\n# move focused container to workspace\nbindsym Mod1+Shift+1 move container to workspace $workspace1\nbindsym Mod1+Shift+2 move container to workspace $workspace2\nbindsym Mod1+Shift+3 move container to workspace $workspace3\nbindsym Mod1+Shift+4 move container to workspace $workspace4\nbindsym Mod1+Shift+5 move container to workspace $workspace5\nbindsym Mod1+Shift+6 move container to workspace 6\nbindsym Mod1+Shift+7 move container to workspace 7\nbindsym Mod1+Shift+8 move container to workspace $workspace8\nbindsym Mod1+Shift+9 move container to workspace 9\nbindsym Mod1+Shift+0 move container to workspace $workspace10\n",[33,48869,48867],{"__ignoreMap":35},[11,48871,48872],{},"Things are looking pretty good at this point. Here are some other things that I have found to be very helpful:",[911,48874,48876],{"id":48875},"brightness-controller","Brightness Controller",[26,48878,48881],{"className":48879,"code":48880,"language":31},[29],"yaourt -S brightness-controller\n",[33,48882,48880],{"__ignoreMap":35},[11,48884,48885],{},"Next we can add a shortcut for this package:",[26,48887,48890],{"className":48888,"code":48889,"language":31},[29],"bindsym Mod1+Ctrl+b exec brightness-controller\n",[33,48891,48889],{"__ignoreMap":35},[911,48893,48895],{"id":48894},"ranger","Ranger",[11,48897,48898,48900],{},[33,48899,48894],{}," is a terminal-based file browser that is nice to use.",[26,48902,48905],{"className":48903,"code":48904,"language":31},[29],"yaourt -S ranger\n",[33,48906,48904],{"__ignoreMap":35},[11,48908,48909],{},"There is a lot we can do to customize ranger, here are some important things that I do. First, run the following command:",[26,48911,48914],{"className":48912,"code":48913,"language":31},[29],"ranger --copy-config=all\n",[33,48915,48913],{"__ignoreMap":35},[11,48917,48918,48919,752],{},"This will create a file located here: ",[33,48920,48921],{},"~/.config/ranger/rc.conf",[11,48923,48924],{},"We need to make the following adjustments:",[916,48926,48927,48936,48944,48950,48956],{},[919,48928,48929,48930,765,48933],{},"change ",[33,48931,48932],{},"set preview_images false",[33,48934,48935],{},"set preview_images true",[919,48937,48929,48938,765,48941],{},[33,48939,48940],{},"set draw_borders false",[33,48942,48943],{},"set draw_borders true",[919,48945,48946,48947],{},"install a package called ",[33,48948,48949],{},"w3m",[919,48951,48952,48953],{},"add (overwrite) this line: ",[33,48954,48955],{},"set preview_images_method w3m",[919,48957,15504,48958,48961],{},[33,48959,48960],{},"source ~/.config/ranger/rc.conf"," and then launch ranger in a new terminal",[11,48963,48964,48965,48967],{},"These changes will allow you to preview images right inside of the ranger file browser. There may be another combination of settings that get this to work, but these worked for me. Be careful that things you want to change in ",[33,48966,48921],{}," are not overwritten by other lines.",[168,48969,48971],{"id":48970},"rofi","Rofi",[11,48973,48974,48976,48977,48979],{},[33,48975,48970],{}," is a launcher similar to ",[33,48978,48376],{}," that we used in the beginnings, but it is better. Here's how to install it:",[26,48981,48984],{"className":48982,"code":48983,"language":31},[29],"sudo pacman -S rofi\n",[33,48985,48983],{"__ignoreMap":35},[11,48987,48988],{},"Next we can add a key binding so we can quickly launch any program with rofi:",[26,48990,48993],{"className":48991,"code":48992,"language":31},[29],"bindsym Mod1+space exec rofi -show run\n",[33,48994,48992],{"__ignoreMap":35},[11,48996,48997,48998,752],{},"Be careful when you add new bindings! i3 will give you an error if you assign one binding to more than one command. I had to shuffle some of the default bindings around to use ",[33,48999,49000],{},"Mod1+space",[11,49002,49003],{},"This is a good place to stop for now. There is a lot you can do with i3, so it is good to take things one step at a time and also try to do things in different ways to see what you like.",[168,49005,49007],{"id":49006},"extras","Extras",[911,49009,49011],{"id":49010},"chinese-input-support","Chinese Input Support",[11,49013,48720,49014,49017],{},[33,49015,49016],{},"ibus-pinyin"," from AUR",[11,49019,49020],{},"Run the ibus daemon:",[26,49022,49025],{"className":49023,"code":49024,"language":31},[29],"ibus-daemon -xim&\n",[33,49026,49024],{"__ignoreMap":35},[11,49028,49029,49030],{},"Install another Chinese font and set in ",[33,49031,48424],{},[11,49033,49034],{},[15,49035,49036],{"href":49036,"rel":49037},"https://askubuntu.com/questions/826577/switch-keyboard-layouts-with-i3/826578",[19],[168,49039,49041],{"id":49040},"dotfiles","Dotfiles",[11,49043,49044],{},"Here are the configuration files I use for i3:",[916,49046,49047,49052,49058,49066,49073,49080,49088],{},[919,49048,49049,49051],{},[33,49050,48410],{},": general configurations for i3",[919,49053,49054,49057],{},[33,49055,49056],{},"~/.i3/i3blocks.conf",": configuration for status bar in i3",[919,49059,49060,49063,49064],{},[33,49061,49062],{},"~/.i3/blocks/scripts/",": scripts that run for ",[33,49065,48838],{},[919,49067,49068,49070,49071],{},[33,49069,48424],{},": configuration for ",[33,49072,48218],{},[919,49074,49075,49077,49078],{},[33,49076,48921],{},": configuration files for ",[33,49079,48894],{},[919,49081,49082,49070,49085],{},[33,49083,49084],{},"~/.vimrc",[33,49086,49087],{},"vim",[919,49089,49090,49092],{},[33,49091,48623],{},": environment variables, functions and aliases",[11,49094,49095,49096,752],{},"I'm constantly changing things around in these files but I try to keep them up to date in a repo on my Github account which you can find ",[15,49097,1321],{"href":48430,"rel":49098},[19],{"title":35,"searchDepth":249,"depth":249,"links":49100},[49101,49102,49105,49106,49107,49111,49112,49115],{"id":48397,"depth":249,"text":48398},{"id":48510,"depth":249,"text":48511,"children":49103},[49104],{"id":48558,"depth":312,"text":48424},{"id":48602,"depth":249,"text":48602},{"id":48295,"depth":249,"text":48295},{"id":48746,"depth":249,"text":48747,"children":49108},[49109,49110],{"id":48875,"depth":312,"text":48876},{"id":48894,"depth":312,"text":48895},{"id":48970,"depth":249,"text":48971},{"id":49006,"depth":249,"text":49007,"children":49113},[49114],{"id":49010,"depth":312,"text":49011},{"id":49040,"depth":249,"text":49041},"2017-10-17","/static/gnome-i3.png",{"layout":29014},"/2017/10/17/moving-from-gnome-to-i3-on-arch-linux",{"title":48235,"description":35},"2017/10/17/moving-from-gnome-to-i3-on-arch-linux",[45897,48250,49123],"gnome","iihtujHj81LaR4jH3gZCG0z8XCZ1wE09c_CmI4hE7ag",{"id":49126,"title":49127,"body":49128,"comments":315,"date":49116,"description":49345,"draft":872,"extension":873,"external":874,"image":49346,"meta":49347,"navigation":315,"path":49348,"seo":49349,"stem":49350,"tags":49351,"__hash__":49353},"blog/2017/10/17/reading-rss-with-python.md","Reading RSS with Python",{"type":8,"value":49129,"toc":49343},[49130,49147,49159,49161,49170,49180,49186,49194,49253,49256,49338,49341],[11,49131,49132,49133,49136,49137,49139,49140,49142,49143,49146],{},"On my other personal website, ",[15,49134,49135],{"href":49135},"briancaffey.com",", I have a blog. The content on that blog has mostly mirrored what I put on this github pages site, ",[15,49138,743],{"href":743},". I want to display my most recent blog posts from briancaffey.github.io on briancaffey.com, and to do this I will be using the RSS feed that comes with a Jekyll site. This should be pretty simple, we are going to use the ",[33,49141,34422],{}," librrary, as well as the ",[33,49144,49145],{},"feedparser"," library.",[11,49148,49149,49150,49155,49156,49158],{},"Here are some ",[15,49151,49154],{"href":49152,"rel":49153},"https://wiki.python.org/moin/RssLibraries",[19],"docs"," on how to use ",[33,49157,49145],{},", it is very simple.",[11,49160,48533],{},[26,49162,49164],{"className":1383,"code":49163,"language":1125,"meta":35,"style":35},"pip install feedparser\n",[33,49165,49166],{"__ignoreMap":35},[187,49167,49168],{"class":189,"line":190},[187,49169,49163],{},[11,49171,49172,49173,49175,49176,49179],{},"Here's the setup that I will be using in utility a function that will be imported to ",[33,49174,39706],{}," and called in the ",[33,49177,49178],{},"home()"," function that renders the homepage for briancaffey.com:",[26,49181,49184],{"className":49182,"code":49183,"language":31},[29],"import feedparser\n\ndef get_blog_posts(number_of_posts):\n    url = \"http://briancaffey.github.io/feed\"\n    feed = feedparser.parse(url)\n    posts = feed['items'][:number_of_posts]\n    return posts\n\n",[33,49185,49183],{"__ignoreMap":35},[11,49187,49188,49189,49191,49192,358],{},"Next, in ",[33,49190,39706],{},", we just need to import the function, call it with the number of articles we want to show, save the returned value to a variable and then pass that to ",[33,49193,27106],{},[26,49195,49197],{"className":1383,"code":49196,"language":1125,"meta":35,"style":35},"from utils import get_blog_posts\ndef home(request):\n    ...\n    posts = get_blog_posts(4)\n\n    context = {\n        ...\n        'recent_posts': posts,\n        ...\n    }\n\n    return render(request, 'home.html', context)\n",[33,49198,49199,49204,49209,49214,49219,49223,49227,49231,49236,49240,49244,49248],{"__ignoreMap":35},[187,49200,49201],{"class":189,"line":190},[187,49202,49203],{},"from utils import get_blog_posts\n",[187,49205,49206],{"class":189,"line":249},[187,49207,49208],{},"def home(request):\n",[187,49210,49211],{"class":189,"line":312},[187,49212,49213],{},"    ...\n",[187,49215,49216],{"class":189,"line":319},[187,49217,49218],{},"    posts = get_blog_posts(4)\n",[187,49220,49221],{"class":189,"line":325},[187,49222,316],{"emptyLinePlaceholder":315},[187,49224,49225],{"class":189,"line":686},[187,49226,38278],{},[187,49228,49229],{"class":189,"line":697},[187,49230,3587],{},[187,49232,49233],{"class":189,"line":1291},[187,49234,49235],{},"        'recent_posts': posts,\n",[187,49237,49238],{"class":189,"line":1306},[187,49239,3587],{},[187,49241,49242],{"class":189,"line":1434},[187,49243,9799],{},[187,49245,49246],{"class":189,"line":2599},[187,49247,316],{"emptyLinePlaceholder":315},[187,49249,49250],{"class":189,"line":2607},[187,49251,49252],{},"    return render(request, 'home.html', context)\n",[11,49254,49255],{},"In the context, we can access the following data for each item:",[1525,49257,49258,49266],{},[1528,49259,49260],{},[1531,49261,49262,49264],{},[1534,49263,30164],{},[1534,49265,6560],{},[1544,49267,49268,49278,49288,49298,49308,49318,49328],{},[1531,49269,49270,49275],{},[1549,49271,7948,49272],{},[187,49273,49274],{}," \"date\"",[1549,49276,49277],{},"\"2004-02-13T22:28:23+08:00\" - ISO 8601 date",[1531,49279,49280,49285],{},[1549,49281,7948,49282],{},[187,49283,49284],{}," \"date_parsed\"",[1549,49286,49287],{},"(2004,02,13,14,28,23,4,44,0)",[1531,49289,49290,49295],{},[1549,49291,7948,49292],{},[187,49293,49294],{}," \"title\"",[1549,49296,49297],{},"title for item",[1531,49299,49300,49305],{},[1549,49301,7948,49302],{},[187,49303,49304],{}," \"summary\"",[1549,49306,49307],{},"change summary",[1531,49309,49310,49315],{},[1549,49311,7948,49312],{},[187,49313,49314],{}," \"link\"",[1549,49316,49317],{},"URL to the page",[1531,49319,49320,49325],{},[1549,49321,7948,49322],{},[187,49323,49324],{}," \"wiki_diff\"",[1549,49326,49327],{},"for wiki, a link to the diff for the page",[1531,49329,49330,49335],{},[1549,49331,7948,49332],{},[187,49333,49334],{}," \"wiki_history\"",[1549,49336,49337],{},"for wiki, a link to the page history",[11,49339,49340],{},"That's it!",[855,49342,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":49344},[],"On my other personal website, briancaffey.com, I have a blog. The content on that blog has mostly mirrored what I put on this github pages site, briancaffey.github.io. I want to display my most recent blog posts from briancaffey.github.io on briancaffey.com, and to do this I will be using the RSS feed that comes with a Jekyll site. This should be pretty simple, we are going to use the requests librrary, as well as the feedparser library.","/static/python_rss.png",{"layout":29014},"/2017/10/17/reading-rss-with-python",{"title":49127,"description":49345},"2017/10/17/reading-rss-with-python",[1125,49352],"rss","1jmRY0FSIOEINuNy2ZNrTWoINQR4d1m7uXLhWBM4Y54",{"id":49355,"title":49356,"body":49357,"comments":315,"date":50809,"description":50810,"draft":872,"extension":873,"external":874,"image":50026,"meta":50811,"navigation":315,"path":50812,"seo":50813,"stem":50814,"tags":50815,"__hash__":50817},"blog/2017/10/03/simple-games-in-react.md","Simple Board Games in ReactJS",{"type":8,"value":49358,"toc":50807},[49359,49368,49371,49376,49382,49385,50016,50019,50022,50027,50032,50035,50646,50649,50796,50799,50802,50805],[11,49360,49361,49362,49367],{},"To ease into learning ReactJS, I took a shot at implementing a simple tic-tac-toe game with React. This is covered in the official ",[15,49363,49366],{"href":49364,"rel":49365},"https://reactjs.org/tutorial/tutorial.html",[19],"Facebook React tutorial",", but I haven't actually looked at how they did this yet. Instead, I wanted to see how far I could get on my own, and then fallback to the tutorial if I needed help. I heard that there are many different ways that React components can be organized and structured in a React project, so I wanted to see how my results compared to what the official tutorial recommends.",[11,49369,49370],{},"Here's the final result:",[11,49372,49373],{},[511,49374],{"alt":7255,"src":49375},"/static/react/ttt.png",[11,49377,49378,49379],{},"You can play this game ",[15,49380,1321],{"href":49381,"target":37968},"/static/react/tic-tac-react/tic-tac-react.html",[11,49383,49384],{},"I wanted to give the game some additional features, so I let the player set the dimensions of the board to be any integer greater than 1. Here's a look at the main component called \"Board\" which contains most of the business logic:",[26,49386,49388],{"className":6362,"code":49387,"language":6364,"meta":35,"style":35},"{% raw %}import React from 'react';\nimport { grid } from '../data/grid.js';\nimport { Square } from './Square';\n\nexport class Board extends React.Component {\n  constructor(props){\n    super(props);\n    this.state = {\n      dim:3,\n      grid:Array(3).fill(0).map(x=>Array(3).fill(\"+\")),\n      player:'X',\n      winner:null,\n      active:true,\n    };\n    this.handleOnClick = this.handleOnClick.bind(this);\n    this.checkWins = this.checkWins.bind(this);\n    this.handleReset = this.handleReset.bind(this);\n    this.dims = [parseFloat(500/this.state.grid.length), parseFloat(500/this.state.grid[0].length)]\n  }\n\n  handleReset(){\n    const newGrid = Array(this.state.dim).fill(0).map(x=>Array(this.state.dim).fill(\"+\"))\n    this.setState({'grid':newGrid, 'player':'X'});\n  }\n\n  checkWins(x, y){\n    const g = this.state.grid\n\n    function checkDiagonal1(){\n      if (x == y){\n        const result = new Set(g.map((_, i)=>g[i][i]));\n        announceWin(result);\n      }\n    }\n\n    function checkDiagonal2(){\n      if (x+y+1 == g.length){\n        const result = new Set(g.map((_, i)=>g[i][g.length-1-i]))\n        announceWin(result);\n      }\n    }\n\n    function checkHorizontal(x){\n      const result = new Set(g[x]);\n      announceWin(result);\n    }\n\n    function checkVertical(y){\n      const result = new Set(g.map((x)=>x[y]));\n      announceWin(result);\n    }\n\n    function announceWin(l){\n      if (l.size == 1){\n        if (l.has(\"X\")){\n          setTimeout(()=>{alert(\"X wins\")}, 10);\n          return;\n        } else {\n          setTimeout(()=>{alert(\"O wins\")}, 10);\n          return;\n        }\n      }\n    }\n\n    checkDiagonal1();\n    checkDiagonal2();\n    checkHorizontal(x);\n    checkVertical(y);\n  }\n\n  handleOnClick(x, y){\n    const g = this.state.grid\n    if (this.state.active){\n      if (g[x][y] == '+'){\n        g[x][y] = this.state.player;\n        this.setState({'grid':g});\n        this.state.player = this.state.player == 'X' ? 'O':'X';\n        this.checkWins(x, y);\n    } else {\n      alert('Please select an empty square!');\n      }\n    }\n  }\n\n  render(){\n    const style = {\n      margin:'auto',\n      width: \"auto\",\n      height:\"auto\",\n      backgroundColor:'darkorange',\n      color:'white',\n      fontSize:\"3em\",\n      tableLayout:'fixed',\n    }\n    const rows = this.state.grid.map((r, i) => {return (\n      \u003Ctr key={\"row_\"+i}>\n        {r.map((d, j) => {console.log('building'); return(\n          \u003CSquare\n            key={i+\"_\"+j}\n            dims={this.dims}\n            onClick={()=>{this.handleOnClick(i,j)}}\n            contents={d==\"+\"?\" \":d} />\n              )\n            }\n          )\n        }\n        \u003C/tr>)\n        }\n      );\n    return (\n      \u003Cdiv style={{textAlign:\"center\"}}>\n        \u003Ch1>Tic-Tac-React!\u003C/h1>\n        \u003Csmall>tic-tac-toe, written with \u003Cb>ReactJS\u003C/b>. Enjoy!\u003C/small>\n        \u003Cp>Current Player: {this.state.player}\u003C/p>\n        \u003Ctable cellSpacing=\"0\" id=\"table\" style={style}>\n          \u003Ctbody>\n            {rows}\n          \u003C/tbody>\n        \u003C/table>\n        \u003Cbr />\n        \u003Cbutton style={{margin:\"auto\"}} onClick={this.handleReset}>reset\u003C/button>\n        \u003Cbr />\u003Cbr />\n        \u003Cbutton onClick={()=>{this.state.dim==1?1:this.state.dim-=1;this.setState({dim:this.state.dim})}}>-\u003C/button>\n\n            &nbsp;&nbsp;&nbsp;\u003Cspan style={{color:'white'}}>{this.state.dim}\u003C/span>&nbsp;&nbsp;&nbsp;\n\n        \u003Cbutton onClick={()=>{this.state.dim+=1;this.setState({dim:this.state.dim})}}>+\u003C/button>\n        \u003Cbr />\u003Cbr/>\u003Cbr/>\n      \u003C/div>\n  )\n  }\n}\n{% endraw %}\n",[33,49389,49390,49395,49400,49405,49409,49414,49419,49424,49429,49434,49439,49444,49449,49454,49459,49464,49469,49474,49479,49483,49487,49492,49497,49502,49506,49510,49515,49520,49524,49529,49534,49539,49544,49549,49553,49557,49562,49567,49572,49576,49580,49584,49588,49593,49598,49603,49607,49611,49616,49621,49625,49629,49633,49638,49643,49648,49653,49658,49663,49668,49672,49676,49680,49684,49688,49693,49698,49703,49708,49712,49716,49721,49725,49730,49735,49740,49745,49750,49755,49760,49765,49769,49773,49777,49781,49786,49791,49796,49801,49806,49811,49816,49821,49826,49830,49835,49840,49845,49850,49855,49860,49865,49870,49875,49879,49883,49887,49892,49896,49901,49906,49911,49916,49921,49926,49931,49936,49941,49946,49951,49956,49961,49966,49971,49975,49980,49984,49989,49994,49999,50003,50007,50011],{"__ignoreMap":35},[187,49391,49392],{"class":189,"line":190},[187,49393,49394],{},"{% raw %}import React from 'react';\n",[187,49396,49397],{"class":189,"line":249},[187,49398,49399],{},"import { grid } from '../data/grid.js';\n",[187,49401,49402],{"class":189,"line":312},[187,49403,49404],{},"import { Square } from './Square';\n",[187,49406,49407],{"class":189,"line":319},[187,49408,316],{"emptyLinePlaceholder":315},[187,49410,49411],{"class":189,"line":325},[187,49412,49413],{},"export class Board extends React.Component {\n",[187,49415,49416],{"class":189,"line":686},[187,49417,49418],{},"  constructor(props){\n",[187,49420,49421],{"class":189,"line":697},[187,49422,49423],{},"    super(props);\n",[187,49425,49426],{"class":189,"line":1291},[187,49427,49428],{},"    this.state = {\n",[187,49430,49431],{"class":189,"line":1306},[187,49432,49433],{},"      dim:3,\n",[187,49435,49436],{"class":189,"line":1434},[187,49437,49438],{},"      grid:Array(3).fill(0).map(x=>Array(3).fill(\"+\")),\n",[187,49440,49441],{"class":189,"line":2599},[187,49442,49443],{},"      player:'X',\n",[187,49445,49446],{"class":189,"line":2607},[187,49447,49448],{},"      winner:null,\n",[187,49450,49451],{"class":189,"line":2621},[187,49452,49453],{},"      active:true,\n",[187,49455,49456],{"class":189,"line":2631},[187,49457,49458],{},"    };\n",[187,49460,49461],{"class":189,"line":2642},[187,49462,49463],{},"    this.handleOnClick = this.handleOnClick.bind(this);\n",[187,49465,49466],{"class":189,"line":2653},[187,49467,49468],{},"    this.checkWins = this.checkWins.bind(this);\n",[187,49470,49471],{"class":189,"line":2665},[187,49472,49473],{},"    this.handleReset = this.handleReset.bind(this);\n",[187,49475,49476],{"class":189,"line":2674},[187,49477,49478],{},"    this.dims = [parseFloat(500/this.state.grid.length), parseFloat(500/this.state.grid[0].length)]\n",[187,49480,49481],{"class":189,"line":2684},[187,49482,6847],{},[187,49484,49485],{"class":189,"line":2694},[187,49486,316],{"emptyLinePlaceholder":315},[187,49488,49489],{"class":189,"line":2706},[187,49490,49491],{},"  handleReset(){\n",[187,49493,49494],{"class":189,"line":2715},[187,49495,49496],{},"    const newGrid = Array(this.state.dim).fill(0).map(x=>Array(this.state.dim).fill(\"+\"))\n",[187,49498,49499],{"class":189,"line":2725},[187,49500,49501],{},"    this.setState({'grid':newGrid, 'player':'X'});\n",[187,49503,49504],{"class":189,"line":2735},[187,49505,6847],{},[187,49507,49508],{"class":189,"line":2743},[187,49509,316],{"emptyLinePlaceholder":315},[187,49511,49512],{"class":189,"line":2754},[187,49513,49514],{},"  checkWins(x, y){\n",[187,49516,49517],{"class":189,"line":2762},[187,49518,49519],{},"    const g = this.state.grid\n",[187,49521,49522],{"class":189,"line":2770},[187,49523,316],{"emptyLinePlaceholder":315},[187,49525,49526],{"class":189,"line":2781},[187,49527,49528],{},"    function checkDiagonal1(){\n",[187,49530,49531],{"class":189,"line":2792},[187,49532,49533],{},"      if (x == y){\n",[187,49535,49536],{"class":189,"line":2803},[187,49537,49538],{},"        const result = new Set(g.map((_, i)=>g[i][i]));\n",[187,49540,49541],{"class":189,"line":2808},[187,49542,49543],{},"        announceWin(result);\n",[187,49545,49546],{"class":189,"line":2816},[187,49547,49548],{},"      }\n",[187,49550,49551],{"class":189,"line":2824},[187,49552,9799],{},[187,49554,49555],{"class":189,"line":2834},[187,49556,316],{"emptyLinePlaceholder":315},[187,49558,49559],{"class":189,"line":2845},[187,49560,49561],{},"    function checkDiagonal2(){\n",[187,49563,49564],{"class":189,"line":2856},[187,49565,49566],{},"      if (x+y+1 == g.length){\n",[187,49568,49569],{"class":189,"line":2867},[187,49570,49571],{},"        const result = new Set(g.map((_, i)=>g[i][g.length-1-i]))\n",[187,49573,49574],{"class":189,"line":2878},[187,49575,49543],{},[187,49577,49578],{"class":189,"line":2886},[187,49579,49548],{},[187,49581,49582],{"class":189,"line":2900},[187,49583,9799],{},[187,49585,49586],{"class":189,"line":2905},[187,49587,316],{"emptyLinePlaceholder":315},[187,49589,49590],{"class":189,"line":2913},[187,49591,49592],{},"    function checkHorizontal(x){\n",[187,49594,49595],{"class":189,"line":2921},[187,49596,49597],{},"      const result = new Set(g[x]);\n",[187,49599,49600],{"class":189,"line":2931},[187,49601,49602],{},"      announceWin(result);\n",[187,49604,49605],{"class":189,"line":2942},[187,49606,9799],{},[187,49608,49609],{"class":189,"line":2953},[187,49610,316],{"emptyLinePlaceholder":315},[187,49612,49613],{"class":189,"line":2964},[187,49614,49615],{},"    function checkVertical(y){\n",[187,49617,49618],{"class":189,"line":2975},[187,49619,49620],{},"      const result = new Set(g.map((x)=>x[y]));\n",[187,49622,49623],{"class":189,"line":2983},[187,49624,49602],{},[187,49626,49627],{"class":189,"line":2992},[187,49628,9799],{},[187,49630,49631],{"class":189,"line":3001},[187,49632,316],{"emptyLinePlaceholder":315},[187,49634,49635],{"class":189,"line":3010},[187,49636,49637],{},"    function announceWin(l){\n",[187,49639,49640],{"class":189,"line":3019},[187,49641,49642],{},"      if (l.size == 1){\n",[187,49644,49645],{"class":189,"line":3028},[187,49646,49647],{},"        if (l.has(\"X\")){\n",[187,49649,49650],{"class":189,"line":3033},[187,49651,49652],{},"          setTimeout(()=>{alert(\"X wins\")}, 10);\n",[187,49654,49655],{"class":189,"line":3041},[187,49656,49657],{},"          return;\n",[187,49659,49660],{"class":189,"line":3049},[187,49661,49662],{},"        } else {\n",[187,49664,49665],{"class":189,"line":3059},[187,49666,49667],{},"          setTimeout(()=>{alert(\"O wins\")}, 10);\n",[187,49669,49670],{"class":189,"line":3070},[187,49671,49657],{},[187,49673,49674],{"class":189,"line":3075},[187,49675,9780],{},[187,49677,49678],{"class":189,"line":3083},[187,49679,49548],{},[187,49681,49682],{"class":189,"line":3091},[187,49683,9799],{},[187,49685,49686],{"class":189,"line":3101},[187,49687,316],{"emptyLinePlaceholder":315},[187,49689,49690],{"class":189,"line":3111},[187,49691,49692],{},"    checkDiagonal1();\n",[187,49694,49695],{"class":189,"line":3122},[187,49696,49697],{},"    checkDiagonal2();\n",[187,49699,49700],{"class":189,"line":3132},[187,49701,49702],{},"    checkHorizontal(x);\n",[187,49704,49705],{"class":189,"line":3143},[187,49706,49707],{},"    checkVertical(y);\n",[187,49709,49710],{"class":189,"line":3151},[187,49711,6847],{},[187,49713,49714],{"class":189,"line":3161},[187,49715,316],{"emptyLinePlaceholder":315},[187,49717,49718],{"class":189,"line":3170},[187,49719,49720],{},"  handleOnClick(x, y){\n",[187,49722,49723],{"class":189,"line":3178},[187,49724,49519],{},[187,49726,49727],{"class":189,"line":3185},[187,49728,49729],{},"    if (this.state.active){\n",[187,49731,49732],{"class":189,"line":3195},[187,49733,49734],{},"      if (g[x][y] == '+'){\n",[187,49736,49737],{"class":189,"line":3205},[187,49738,49739],{},"        g[x][y] = this.state.player;\n",[187,49741,49742],{"class":189,"line":3210},[187,49743,49744],{},"        this.setState({'grid':g});\n",[187,49746,49747],{"class":189,"line":3216},[187,49748,49749],{},"        this.state.player = this.state.player == 'X' ? 'O':'X';\n",[187,49751,49752],{"class":189,"line":3224},[187,49753,49754],{},"        this.checkWins(x, y);\n",[187,49756,49757],{"class":189,"line":3234},[187,49758,49759],{},"    } else {\n",[187,49761,49762],{"class":189,"line":3242},[187,49763,49764],{},"      alert('Please select an empty square!');\n",[187,49766,49767],{"class":189,"line":3252},[187,49768,49548],{},[187,49770,49771],{"class":189,"line":3260},[187,49772,9799],{},[187,49774,49775],{"class":189,"line":3270},[187,49776,6847],{},[187,49778,49779],{"class":189,"line":3275},[187,49780,316],{"emptyLinePlaceholder":315},[187,49782,49783],{"class":189,"line":3283},[187,49784,49785],{},"  render(){\n",[187,49787,49788],{"class":189,"line":3291},[187,49789,49790],{},"    const style = {\n",[187,49792,49793],{"class":189,"line":3300},[187,49794,49795],{},"      margin:'auto',\n",[187,49797,49798],{"class":189,"line":3310},[187,49799,49800],{},"      width: \"auto\",\n",[187,49802,49803],{"class":189,"line":3320},[187,49804,49805],{},"      height:\"auto\",\n",[187,49807,49808],{"class":189,"line":3325},[187,49809,49810],{},"      backgroundColor:'darkorange',\n",[187,49812,49813],{"class":189,"line":3333},[187,49814,49815],{},"      color:'white',\n",[187,49817,49818],{"class":189,"line":3343},[187,49819,49820],{},"      fontSize:\"3em\",\n",[187,49822,49823],{"class":189,"line":3354},[187,49824,49825],{},"      tableLayout:'fixed',\n",[187,49827,49828],{"class":189,"line":17135},[187,49829,9799],{},[187,49831,49832],{"class":189,"line":17141},[187,49833,49834],{},"    const rows = this.state.grid.map((r, i) => {return (\n",[187,49836,49837],{"class":189,"line":17146},[187,49838,49839],{},"      \u003Ctr key={\"row_\"+i}>\n",[187,49841,49842],{"class":189,"line":17152},[187,49843,49844],{},"        {r.map((d, j) => {console.log('building'); return(\n",[187,49846,49847],{"class":189,"line":17164},[187,49848,49849],{},"          \u003CSquare\n",[187,49851,49852],{"class":189,"line":17175},[187,49853,49854],{},"            key={i+\"_\"+j}\n",[187,49856,49857],{"class":189,"line":17188},[187,49858,49859],{},"            dims={this.dims}\n",[187,49861,49862],{"class":189,"line":17199},[187,49863,49864],{},"            onClick={()=>{this.handleOnClick(i,j)}}\n",[187,49866,49867],{"class":189,"line":17207},[187,49868,49869],{},"            contents={d==\"+\"?\" \":d} />\n",[187,49871,49872],{"class":189,"line":17212},[187,49873,49874],{},"              )\n",[187,49876,49877],{"class":189,"line":17217},[187,49878,23609],{},[187,49880,49881],{"class":189,"line":17223},[187,49882,13369],{},[187,49884,49885],{"class":189,"line":17235},[187,49886,9780],{},[187,49888,49889],{"class":189,"line":17248},[187,49890,49891],{},"        \u003C/tr>)\n",[187,49893,49894],{"class":189,"line":17267},[187,49895,9780],{},[187,49897,49898],{"class":189,"line":17278},[187,49899,49900],{},"      );\n",[187,49902,49903],{"class":189,"line":11081},[187,49904,49905],{},"    return (\n",[187,49907,49908],{"class":189,"line":17293},[187,49909,49910],{},"      \u003Cdiv style={{textAlign:\"center\"}}>\n",[187,49912,49913],{"class":189,"line":17298},[187,49914,49915],{},"        \u003Ch1>Tic-Tac-React!\u003C/h1>\n",[187,49917,49918],{"class":189,"line":17304},[187,49919,49920],{},"        \u003Csmall>tic-tac-toe, written with \u003Cb>ReactJS\u003C/b>. Enjoy!\u003C/small>\n",[187,49922,49923],{"class":189,"line":17332},[187,49924,49925],{},"        \u003Cp>Current Player: {this.state.player}\u003C/p>\n",[187,49927,49928],{"class":189,"line":17337},[187,49929,49930],{},"        \u003Ctable cellSpacing=\"0\" id=\"table\" style={style}>\n",[187,49932,49933],{"class":189,"line":17343},[187,49934,49935],{},"          \u003Ctbody>\n",[187,49937,49938],{"class":189,"line":17349},[187,49939,49940],{},"            {rows}\n",[187,49942,49943],{"class":189,"line":17361},[187,49944,49945],{},"          \u003C/tbody>\n",[187,49947,49948],{"class":189,"line":17373},[187,49949,49950],{},"        \u003C/table>\n",[187,49952,49953],{"class":189,"line":17388},[187,49954,49955],{},"        \u003Cbr />\n",[187,49957,49958],{"class":189,"line":17398},[187,49959,49960],{},"        \u003Cbutton style={{margin:\"auto\"}} onClick={this.handleReset}>reset\u003C/button>\n",[187,49962,49963],{"class":189,"line":17407},[187,49964,49965],{},"        \u003Cbr />\u003Cbr />\n",[187,49967,49968],{"class":189,"line":17412},[187,49969,49970],{},"        \u003Cbutton onClick={()=>{this.state.dim==1?1:this.state.dim-=1;this.setState({dim:this.state.dim})}}>-\u003C/button>\n",[187,49972,49973],{"class":189,"line":17417},[187,49974,316],{"emptyLinePlaceholder":315},[187,49976,49977],{"class":189,"line":17425},[187,49978,49979],{},"            &nbsp;&nbsp;&nbsp;\u003Cspan style={{color:'white'}}>{this.state.dim}\u003C/span>&nbsp;&nbsp;&nbsp;\n",[187,49981,49982],{"class":189,"line":17430},[187,49983,316],{"emptyLinePlaceholder":315},[187,49985,49986],{"class":189,"line":17436},[187,49987,49988],{},"        \u003Cbutton onClick={()=>{this.state.dim+=1;this.setState({dim:this.state.dim})}}>+\u003C/button>\n",[187,49990,49991],{"class":189,"line":17453},[187,49992,49993],{},"        \u003Cbr />\u003Cbr/>\u003Cbr/>\n",[187,49995,49996],{"class":189,"line":17458},[187,49997,49998],{},"      \u003C/div>\n",[187,50000,50001],{"class":189,"line":17464},[187,50002,16660],{},[187,50004,50005],{"class":189,"line":17476},[187,50006,6847],{},[187,50008,50009],{"class":189,"line":17488},[187,50010,1309],{},[187,50012,50013],{"class":189,"line":17501},[187,50014,50015],{},"{% endraw %}\n",[11,50017,50018],{},"I had so much fun putting together this tic-tac-toe app that I decided to write another one of my favorite games called Gomoku. This game is somwhat similar to tic-tac-toe, but the objective is to connect 5 stones of the same color on a much larger board.",[11,50020,50021],{},"Here's a look at the result of my Gomoku game:",[11,50023,50024],{},[511,50025],{"alt":7255,"src":50026},"/static/react/wuziqi.png",[11,50028,49378,50029],{},[15,50030,1321],{"href":50031,"target":37968},"/static/react/wuziqi/wuziqi.html",[11,50033,50034],{},"Here's the main component for this game with heavy commenting:",[26,50036,50038],{"className":6362,"code":50037,"language":6364,"meta":35,"style":35},"{% raw %}//import React and Square component\nimport React from 'react';\nimport { Square } from './Square';\nimport { Button } from './Button';\n\n//main board component with game logic\nexport class Board extends React.Component{\n  constructor(props){\n    super(props);\n    this.state = {\n      //white goes first\n      'isWhite':true,\n      //this sets up an empty board\n      //\"+\"\" represenets an empty square, \"b\" is a black stone and \"w\" is a white stone\n      'grid':Array(19).fill().map(x => Array(19).fill(\"+\")),\n    };\n    //bind this word to helper functions\n    this.handleClick = this.handleClick.bind(this);\n    this.handleReset = this.handleReset.bind(this);\n  }\n\n  //generate a new empty grid and set it to the grid state with setState\n  handleReset(){\n    let newGrid = Array(19).fill().map(x => Array(19).fill(\"+\"));\n    this.setState({'grid':newGrid});\n  }\n\n  handleClick(x, y){\n    //only add a peice and check for wins if the clicked square is empty\n    if (this.state.grid[x][y] === '+'){\n      //we don't want to mutate state directly, so we store the reference to 'grid' in a const\n      const g = this.state.grid;\n      //set the grid square cooresponding to the clicked square to the color of the current player\n      g[x][y] = this.state.isWhite === true ? 'w':'b';\n      //set the state with the new grid data\n      this.setState({'grid':g, 'isWhite':!this.state.isWhite})\n\n      //helper function for\n      function checkDir(x_, y_, color){\n        //track how many squares of a given color there are in a given dirention (specified by x_ and y_)\n        //for example checkDir(0,1, 'w') checks how many white stones there are in a row to the right )\n        let tracked = 0;\n        let _x = x;\n        let _y = y;\n        //stop tracking stones when the color is not equal to the specified stone or we have gone past the edge of the board\n        while (g[_x] !== undefined && g[_x][_y] === color){\n          //increment the number of tracked stones\n          tracked += 1;\n          //increment/decrement to check the next square in the specified direction\n          _y += y_;\n          _x += x_;\n        }\n        return tracked;\n      }\n      //sum the directions (left+right, up+down, 2 diagonals)\n      const w_horizontal = checkDir(0, 1, 'w') + checkDir(0, -1, 'w') -1;\n      const b_horizontal = checkDir(0, 1, 'b') + checkDir(0, -1, 'b') -1;\n\n      const w_vertical = checkDir(1, 0, 'w') + checkDir(-1, 0, 'w') -1;\n      const b_vertical = checkDir(1, 0, 'b') + checkDir(-1, 0, 'b') -1;\n\n      const w_diag1 = checkDir(1, 1, 'w') + checkDir(-1, -1, 'w') -1;\n      const b_diag1 = checkDir(1, 1, 'b') + checkDir(-1, -1, 'b') -1;\n\n      const w_diag2 = checkDir(1, 1, 'w') + checkDir(-1, -1, 'w') -1;\n      const b_diag2 = checkDir(-1, 1, 'b') + checkDir(1, -1, 'b') -1;\n\n      //check to see if there are any sums greater than or equal to 5 and alert the players of a win\n      //setTimeout is called so that the alert() function does not hold up the rendering of the board.\n      if (w_horizontal >=  5 || w_vertical >=  5 || w_diag1 >=  5 || w_diag2 >=  5){\n        setTimeout(()=>{alert('white wins')}, 1);\n      }\n\n      if (b_horizontal >= 5 || b_vertical >= 5 || b_diag1 >= 5 || b_diag2 >= 5){\n        setTimeout(()=>{alert('black wins')}, 1);\n      }\n    }\n  }\n  render(){\n    //define styles for the \u003Ctable> element in the return() function below\n    const style={\n             textAlign: \"center\",\n             margin:\"auto\",\n             height: \"auto\",\n             width:\"500px\",\n             border:\"1px solid black\",\n             tableLayout:'fixed',\n           };\n    const g = this.state.grid;\n    //loop through the squares in each row and generate a new Square component,\n    //passing in props to the Square component in the nested map() function\n    const board = g.map((row, i) => { return (\n      \u003Ctr key={\"row_\"+i}>\n        {row.map((col, j) => {\n          //set the color of the square based on state.grid\n          const color_ = g[i][j] === '+' ? '#e4e4a1': g[i][j] === 'w' ? 'white':'black';\n          //return Square component, passing in the following as props:\n          //square color defined above in color_,\n          //a value for the key which React needs (I think) and\n          //a function to handle clicks with grid coordinates passed in as arguments\n          return (\n            \u003CSquare handleClick={()=>this.handleClick(i,j)} color={color_} key={i+\"_\"+j} />\n              )\n            }\n          )\n        }\n      \u003C/tr>)\n    });\n\n    //returns the board with the Square Components in {board},\n    //as well as a simple Button component that takes the handleReset function as a prop\n    //this could be further refactored to separate the layout and styling, but it isn't that complicated so I will leave it like this\n    return (\n      \u003Cdiv style={{ textAlign:'center'}}>\n      \u003Ch2>\u003Ca href=\"https://en.wikipedia.org/wiki/Gomoku\" style={{textDecoration:\"none\"}}>五子棋\u003C/a>\u003C/h2>\n      \u003Cdiv style={{margin: 'auto', width:\"40%\"}}>\n      \u003Ctable cellSpacing=\"0\" style={style}>\n        \u003Ctbody>\n          {board}\n        \u003C/tbody>\n      \u003C/table>\n      \u003C/div>\n      \u003Cbr />\n      \u003CButton onClick={this.handleReset} />\n      \u003C/div>\n    )\n  }\n}\n{% endraw %}\n",[33,50039,50040,50045,50050,50054,50059,50063,50068,50073,50077,50081,50085,50090,50095,50100,50105,50110,50114,50119,50124,50128,50132,50136,50141,50145,50150,50155,50159,50163,50168,50173,50178,50183,50188,50193,50198,50203,50208,50212,50217,50222,50227,50232,50237,50242,50247,50252,50257,50262,50267,50272,50277,50282,50286,50291,50295,50300,50305,50310,50314,50319,50324,50328,50333,50338,50342,50347,50352,50356,50361,50366,50371,50376,50380,50384,50389,50394,50398,50402,50406,50410,50415,50420,50425,50430,50435,50440,50445,50450,50455,50460,50465,50470,50475,50479,50484,50489,50494,50499,50504,50509,50514,50519,50524,50528,50532,50536,50540,50545,50549,50553,50558,50563,50568,50572,50577,50582,50587,50592,50597,50602,50607,50612,50616,50621,50626,50630,50634,50638,50642],{"__ignoreMap":35},[187,50041,50042],{"class":189,"line":190},[187,50043,50044],{},"{% raw %}//import React and Square component\n",[187,50046,50047],{"class":189,"line":249},[187,50048,50049],{},"import React from 'react';\n",[187,50051,50052],{"class":189,"line":312},[187,50053,49404],{},[187,50055,50056],{"class":189,"line":319},[187,50057,50058],{},"import { Button } from './Button';\n",[187,50060,50061],{"class":189,"line":325},[187,50062,316],{"emptyLinePlaceholder":315},[187,50064,50065],{"class":189,"line":686},[187,50066,50067],{},"//main board component with game logic\n",[187,50069,50070],{"class":189,"line":697},[187,50071,50072],{},"export class Board extends React.Component{\n",[187,50074,50075],{"class":189,"line":1291},[187,50076,49418],{},[187,50078,50079],{"class":189,"line":1306},[187,50080,49423],{},[187,50082,50083],{"class":189,"line":1434},[187,50084,49428],{},[187,50086,50087],{"class":189,"line":2599},[187,50088,50089],{},"      //white goes first\n",[187,50091,50092],{"class":189,"line":2607},[187,50093,50094],{},"      'isWhite':true,\n",[187,50096,50097],{"class":189,"line":2621},[187,50098,50099],{},"      //this sets up an empty board\n",[187,50101,50102],{"class":189,"line":2631},[187,50103,50104],{},"      //\"+\"\" represenets an empty square, \"b\" is a black stone and \"w\" is a white stone\n",[187,50106,50107],{"class":189,"line":2642},[187,50108,50109],{},"      'grid':Array(19).fill().map(x => Array(19).fill(\"+\")),\n",[187,50111,50112],{"class":189,"line":2653},[187,50113,49458],{},[187,50115,50116],{"class":189,"line":2665},[187,50117,50118],{},"    //bind this word to helper functions\n",[187,50120,50121],{"class":189,"line":2674},[187,50122,50123],{},"    this.handleClick = this.handleClick.bind(this);\n",[187,50125,50126],{"class":189,"line":2684},[187,50127,49473],{},[187,50129,50130],{"class":189,"line":2694},[187,50131,6847],{},[187,50133,50134],{"class":189,"line":2706},[187,50135,316],{"emptyLinePlaceholder":315},[187,50137,50138],{"class":189,"line":2715},[187,50139,50140],{},"  //generate a new empty grid and set it to the grid state with setState\n",[187,50142,50143],{"class":189,"line":2725},[187,50144,49491],{},[187,50146,50147],{"class":189,"line":2735},[187,50148,50149],{},"    let newGrid = Array(19).fill().map(x => Array(19).fill(\"+\"));\n",[187,50151,50152],{"class":189,"line":2743},[187,50153,50154],{},"    this.setState({'grid':newGrid});\n",[187,50156,50157],{"class":189,"line":2754},[187,50158,6847],{},[187,50160,50161],{"class":189,"line":2762},[187,50162,316],{"emptyLinePlaceholder":315},[187,50164,50165],{"class":189,"line":2770},[187,50166,50167],{},"  handleClick(x, y){\n",[187,50169,50170],{"class":189,"line":2781},[187,50171,50172],{},"    //only add a peice and check for wins if the clicked square is empty\n",[187,50174,50175],{"class":189,"line":2792},[187,50176,50177],{},"    if (this.state.grid[x][y] === '+'){\n",[187,50179,50180],{"class":189,"line":2803},[187,50181,50182],{},"      //we don't want to mutate state directly, so we store the reference to 'grid' in a const\n",[187,50184,50185],{"class":189,"line":2808},[187,50186,50187],{},"      const g = this.state.grid;\n",[187,50189,50190],{"class":189,"line":2816},[187,50191,50192],{},"      //set the grid square cooresponding to the clicked square to the color of the current player\n",[187,50194,50195],{"class":189,"line":2824},[187,50196,50197],{},"      g[x][y] = this.state.isWhite === true ? 'w':'b';\n",[187,50199,50200],{"class":189,"line":2834},[187,50201,50202],{},"      //set the state with the new grid data\n",[187,50204,50205],{"class":189,"line":2845},[187,50206,50207],{},"      this.setState({'grid':g, 'isWhite':!this.state.isWhite})\n",[187,50209,50210],{"class":189,"line":2856},[187,50211,316],{"emptyLinePlaceholder":315},[187,50213,50214],{"class":189,"line":2867},[187,50215,50216],{},"      //helper function for\n",[187,50218,50219],{"class":189,"line":2878},[187,50220,50221],{},"      function checkDir(x_, y_, color){\n",[187,50223,50224],{"class":189,"line":2886},[187,50225,50226],{},"        //track how many squares of a given color there are in a given dirention (specified by x_ and y_)\n",[187,50228,50229],{"class":189,"line":2900},[187,50230,50231],{},"        //for example checkDir(0,1, 'w') checks how many white stones there are in a row to the right )\n",[187,50233,50234],{"class":189,"line":2905},[187,50235,50236],{},"        let tracked = 0;\n",[187,50238,50239],{"class":189,"line":2913},[187,50240,50241],{},"        let _x = x;\n",[187,50243,50244],{"class":189,"line":2921},[187,50245,50246],{},"        let _y = y;\n",[187,50248,50249],{"class":189,"line":2931},[187,50250,50251],{},"        //stop tracking stones when the color is not equal to the specified stone or we have gone past the edge of the board\n",[187,50253,50254],{"class":189,"line":2942},[187,50255,50256],{},"        while (g[_x] !== undefined && g[_x][_y] === color){\n",[187,50258,50259],{"class":189,"line":2953},[187,50260,50261],{},"          //increment the number of tracked stones\n",[187,50263,50264],{"class":189,"line":2964},[187,50265,50266],{},"          tracked += 1;\n",[187,50268,50269],{"class":189,"line":2975},[187,50270,50271],{},"          //increment/decrement to check the next square in the specified direction\n",[187,50273,50274],{"class":189,"line":2983},[187,50275,50276],{},"          _y += y_;\n",[187,50278,50279],{"class":189,"line":2992},[187,50280,50281],{},"          _x += x_;\n",[187,50283,50284],{"class":189,"line":3001},[187,50285,9780],{},[187,50287,50288],{"class":189,"line":3010},[187,50289,50290],{},"        return tracked;\n",[187,50292,50293],{"class":189,"line":3019},[187,50294,49548],{},[187,50296,50297],{"class":189,"line":3028},[187,50298,50299],{},"      //sum the directions (left+right, up+down, 2 diagonals)\n",[187,50301,50302],{"class":189,"line":3033},[187,50303,50304],{},"      const w_horizontal = checkDir(0, 1, 'w') + checkDir(0, -1, 'w') -1;\n",[187,50306,50307],{"class":189,"line":3041},[187,50308,50309],{},"      const b_horizontal = checkDir(0, 1, 'b') + checkDir(0, -1, 'b') -1;\n",[187,50311,50312],{"class":189,"line":3049},[187,50313,316],{"emptyLinePlaceholder":315},[187,50315,50316],{"class":189,"line":3059},[187,50317,50318],{},"      const w_vertical = checkDir(1, 0, 'w') + checkDir(-1, 0, 'w') -1;\n",[187,50320,50321],{"class":189,"line":3070},[187,50322,50323],{},"      const b_vertical = checkDir(1, 0, 'b') + checkDir(-1, 0, 'b') -1;\n",[187,50325,50326],{"class":189,"line":3075},[187,50327,316],{"emptyLinePlaceholder":315},[187,50329,50330],{"class":189,"line":3083},[187,50331,50332],{},"      const w_diag1 = checkDir(1, 1, 'w') + checkDir(-1, -1, 'w') -1;\n",[187,50334,50335],{"class":189,"line":3091},[187,50336,50337],{},"      const b_diag1 = checkDir(1, 1, 'b') + checkDir(-1, -1, 'b') -1;\n",[187,50339,50340],{"class":189,"line":3101},[187,50341,316],{"emptyLinePlaceholder":315},[187,50343,50344],{"class":189,"line":3111},[187,50345,50346],{},"      const w_diag2 = checkDir(1, 1, 'w') + checkDir(-1, -1, 'w') -1;\n",[187,50348,50349],{"class":189,"line":3122},[187,50350,50351],{},"      const b_diag2 = checkDir(-1, 1, 'b') + checkDir(1, -1, 'b') -1;\n",[187,50353,50354],{"class":189,"line":3132},[187,50355,316],{"emptyLinePlaceholder":315},[187,50357,50358],{"class":189,"line":3143},[187,50359,50360],{},"      //check to see if there are any sums greater than or equal to 5 and alert the players of a win\n",[187,50362,50363],{"class":189,"line":3151},[187,50364,50365],{},"      //setTimeout is called so that the alert() function does not hold up the rendering of the board.\n",[187,50367,50368],{"class":189,"line":3161},[187,50369,50370],{},"      if (w_horizontal >=  5 || w_vertical >=  5 || w_diag1 >=  5 || w_diag2 >=  5){\n",[187,50372,50373],{"class":189,"line":3170},[187,50374,50375],{},"        setTimeout(()=>{alert('white wins')}, 1);\n",[187,50377,50378],{"class":189,"line":3178},[187,50379,49548],{},[187,50381,50382],{"class":189,"line":3185},[187,50383,316],{"emptyLinePlaceholder":315},[187,50385,50386],{"class":189,"line":3195},[187,50387,50388],{},"      if (b_horizontal >= 5 || b_vertical >= 5 || b_diag1 >= 5 || b_diag2 >= 5){\n",[187,50390,50391],{"class":189,"line":3205},[187,50392,50393],{},"        setTimeout(()=>{alert('black wins')}, 1);\n",[187,50395,50396],{"class":189,"line":3210},[187,50397,49548],{},[187,50399,50400],{"class":189,"line":3216},[187,50401,9799],{},[187,50403,50404],{"class":189,"line":3224},[187,50405,6847],{},[187,50407,50408],{"class":189,"line":3234},[187,50409,49785],{},[187,50411,50412],{"class":189,"line":3242},[187,50413,50414],{},"    //define styles for the \u003Ctable> element in the return() function below\n",[187,50416,50417],{"class":189,"line":3252},[187,50418,50419],{},"    const style={\n",[187,50421,50422],{"class":189,"line":3260},[187,50423,50424],{},"             textAlign: \"center\",\n",[187,50426,50427],{"class":189,"line":3270},[187,50428,50429],{},"             margin:\"auto\",\n",[187,50431,50432],{"class":189,"line":3275},[187,50433,50434],{},"             height: \"auto\",\n",[187,50436,50437],{"class":189,"line":3283},[187,50438,50439],{},"             width:\"500px\",\n",[187,50441,50442],{"class":189,"line":3291},[187,50443,50444],{},"             border:\"1px solid black\",\n",[187,50446,50447],{"class":189,"line":3300},[187,50448,50449],{},"             tableLayout:'fixed',\n",[187,50451,50452],{"class":189,"line":3310},[187,50453,50454],{},"           };\n",[187,50456,50457],{"class":189,"line":3320},[187,50458,50459],{},"    const g = this.state.grid;\n",[187,50461,50462],{"class":189,"line":3325},[187,50463,50464],{},"    //loop through the squares in each row and generate a new Square component,\n",[187,50466,50467],{"class":189,"line":3333},[187,50468,50469],{},"    //passing in props to the Square component in the nested map() function\n",[187,50471,50472],{"class":189,"line":3343},[187,50473,50474],{},"    const board = g.map((row, i) => { return (\n",[187,50476,50477],{"class":189,"line":3354},[187,50478,49839],{},[187,50480,50481],{"class":189,"line":17135},[187,50482,50483],{},"        {row.map((col, j) => {\n",[187,50485,50486],{"class":189,"line":17141},[187,50487,50488],{},"          //set the color of the square based on state.grid\n",[187,50490,50491],{"class":189,"line":17146},[187,50492,50493],{},"          const color_ = g[i][j] === '+' ? '#e4e4a1': g[i][j] === 'w' ? 'white':'black';\n",[187,50495,50496],{"class":189,"line":17152},[187,50497,50498],{},"          //return Square component, passing in the following as props:\n",[187,50500,50501],{"class":189,"line":17164},[187,50502,50503],{},"          //square color defined above in color_,\n",[187,50505,50506],{"class":189,"line":17175},[187,50507,50508],{},"          //a value for the key which React needs (I think) and\n",[187,50510,50511],{"class":189,"line":17188},[187,50512,50513],{},"          //a function to handle clicks with grid coordinates passed in as arguments\n",[187,50515,50516],{"class":189,"line":17199},[187,50517,50518],{},"          return (\n",[187,50520,50521],{"class":189,"line":17207},[187,50522,50523],{},"            \u003CSquare handleClick={()=>this.handleClick(i,j)} color={color_} key={i+\"_\"+j} />\n",[187,50525,50526],{"class":189,"line":17212},[187,50527,49874],{},[187,50529,50530],{"class":189,"line":17217},[187,50531,23609],{},[187,50533,50534],{"class":189,"line":17223},[187,50535,13369],{},[187,50537,50538],{"class":189,"line":17235},[187,50539,9780],{},[187,50541,50542],{"class":189,"line":17248},[187,50543,50544],{},"      \u003C/tr>)\n",[187,50546,50547],{"class":189,"line":17267},[187,50548,12459],{},[187,50550,50551],{"class":189,"line":17278},[187,50552,316],{"emptyLinePlaceholder":315},[187,50554,50555],{"class":189,"line":11081},[187,50556,50557],{},"    //returns the board with the Square Components in {board},\n",[187,50559,50560],{"class":189,"line":17293},[187,50561,50562],{},"    //as well as a simple Button component that takes the handleReset function as a prop\n",[187,50564,50565],{"class":189,"line":17298},[187,50566,50567],{},"    //this could be further refactored to separate the layout and styling, but it isn't that complicated so I will leave it like this\n",[187,50569,50570],{"class":189,"line":17304},[187,50571,49905],{},[187,50573,50574],{"class":189,"line":17332},[187,50575,50576],{},"      \u003Cdiv style={{ textAlign:'center'}}>\n",[187,50578,50579],{"class":189,"line":17337},[187,50580,50581],{},"      \u003Ch2>\u003Ca href=\"https://en.wikipedia.org/wiki/Gomoku\" style={{textDecoration:\"none\"}}>五子棋\u003C/a>\u003C/h2>\n",[187,50583,50584],{"class":189,"line":17343},[187,50585,50586],{},"      \u003Cdiv style={{margin: 'auto', width:\"40%\"}}>\n",[187,50588,50589],{"class":189,"line":17349},[187,50590,50591],{},"      \u003Ctable cellSpacing=\"0\" style={style}>\n",[187,50593,50594],{"class":189,"line":17361},[187,50595,50596],{},"        \u003Ctbody>\n",[187,50598,50599],{"class":189,"line":17373},[187,50600,50601],{},"          {board}\n",[187,50603,50604],{"class":189,"line":17388},[187,50605,50606],{},"        \u003C/tbody>\n",[187,50608,50609],{"class":189,"line":17398},[187,50610,50611],{},"      \u003C/table>\n",[187,50613,50614],{"class":189,"line":17407},[187,50615,49998],{},[187,50617,50618],{"class":189,"line":17412},[187,50619,50620],{},"      \u003Cbr />\n",[187,50622,50623],{"class":189,"line":17417},[187,50624,50625],{},"      \u003CButton onClick={this.handleReset} />\n",[187,50627,50628],{"class":189,"line":17425},[187,50629,49998],{},[187,50631,50632],{"class":189,"line":17430},[187,50633,23653],{},[187,50635,50636],{"class":189,"line":17436},[187,50637,6847],{},[187,50639,50640],{"class":189,"line":17453},[187,50641,1309],{},[187,50643,50644],{"class":189,"line":17458},[187,50645,50015],{},[11,50647,50648],{},"And here is the square component:",[26,50650,50652],{"className":6362,"code":50651,"language":6364,"meta":35,"style":35},"{% raw %}import React from 'react';\n\nexport class Square extends React.Component{\n  render(){\n    const color_ = this.props.color;\n    return (\n      \u003Ctd\n        style={{\n          overflow:'hidden',\n          width:'auto',\n          height:'25px',\n          backgroundColor:'#e4e4a1',\n          color:'red',\n          boarderColor: 'black',\n          border:\".5px solid black\"\n        }}\n      onClick={this.props.handleClick} >\n        \u003Cdiv\n          style={{color:color_,\n                  border:\"1px solid\",\n                  backgroundColor: color_,\n                  borderRadius: \"50%\",\n                  borderColor: color_,\n                  height:25}} >\n        \u003C/div>\n      \u003C/td>\n    )\n  }\n}\n{% endraw %}\n",[33,50653,50654,50658,50662,50667,50671,50676,50680,50685,50690,50695,50700,50705,50710,50715,50720,50725,50730,50735,50740,50745,50750,50755,50760,50765,50770,50775,50780,50784,50788,50792],{"__ignoreMap":35},[187,50655,50656],{"class":189,"line":190},[187,50657,49394],{},[187,50659,50660],{"class":189,"line":249},[187,50661,316],{"emptyLinePlaceholder":315},[187,50663,50664],{"class":189,"line":312},[187,50665,50666],{},"export class Square extends React.Component{\n",[187,50668,50669],{"class":189,"line":319},[187,50670,49785],{},[187,50672,50673],{"class":189,"line":325},[187,50674,50675],{},"    const color_ = this.props.color;\n",[187,50677,50678],{"class":189,"line":686},[187,50679,49905],{},[187,50681,50682],{"class":189,"line":697},[187,50683,50684],{},"      \u003Ctd\n",[187,50686,50687],{"class":189,"line":1291},[187,50688,50689],{},"        style={{\n",[187,50691,50692],{"class":189,"line":1306},[187,50693,50694],{},"          overflow:'hidden',\n",[187,50696,50697],{"class":189,"line":1434},[187,50698,50699],{},"          width:'auto',\n",[187,50701,50702],{"class":189,"line":2599},[187,50703,50704],{},"          height:'25px',\n",[187,50706,50707],{"class":189,"line":2607},[187,50708,50709],{},"          backgroundColor:'#e4e4a1',\n",[187,50711,50712],{"class":189,"line":2621},[187,50713,50714],{},"          color:'red',\n",[187,50716,50717],{"class":189,"line":2631},[187,50718,50719],{},"          boarderColor: 'black',\n",[187,50721,50722],{"class":189,"line":2642},[187,50723,50724],{},"          border:\".5px solid black\"\n",[187,50726,50727],{"class":189,"line":2653},[187,50728,50729],{},"        }}\n",[187,50731,50732],{"class":189,"line":2665},[187,50733,50734],{},"      onClick={this.props.handleClick} >\n",[187,50736,50737],{"class":189,"line":2674},[187,50738,50739],{},"        \u003Cdiv\n",[187,50741,50742],{"class":189,"line":2684},[187,50743,50744],{},"          style={{color:color_,\n",[187,50746,50747],{"class":189,"line":2694},[187,50748,50749],{},"                  border:\"1px solid\",\n",[187,50751,50752],{"class":189,"line":2706},[187,50753,50754],{},"                  backgroundColor: color_,\n",[187,50756,50757],{"class":189,"line":2715},[187,50758,50759],{},"                  borderRadius: \"50%\",\n",[187,50761,50762],{"class":189,"line":2725},[187,50763,50764],{},"                  borderColor: color_,\n",[187,50766,50767],{"class":189,"line":2735},[187,50768,50769],{},"                  height:25}} >\n",[187,50771,50772],{"class":189,"line":2743},[187,50773,50774],{},"        \u003C/div>\n",[187,50776,50777],{"class":189,"line":2754},[187,50778,50779],{},"      \u003C/td>\n",[187,50781,50782],{"class":189,"line":2762},[187,50783,23653],{},[187,50785,50786],{"class":189,"line":2770},[187,50787,6847],{},[187,50789,50790],{"class":189,"line":2781},[187,50791,1309],{},[187,50793,50794],{"class":189,"line":2792},[187,50795,50015],{},[11,50797,50798],{},"So far, I love using React. These examples only scratch the surface of what you can do with the library, but even with these examples you can see how easy it is to keep track of state. By making use of the virtual DOM, React only rerenders the part of the actual DOM for which there has been a change in state.",[11,50800,50801],{},"There's more work to do on these games, but I will be putting them aside to learn more about React's lifecycle methods and how to do routing with react-router. Ultimately I'm working toward building React as the frontend to a Django Rest Framework API in some type of single page application (or not, there are actually a few different ways to combine React with Django and I hope to get into that soon!) I will also be learning more about Redux to see how complicated state can be easily managed by using React and Redux together.",[11,50803,50804],{},"Leave a comment if you have any questions or want to point out any errors I may have made in the code above. Thanks!",[855,50806,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":50808},[],"2017-10-03","To ease into learning ReactJS, I took a shot at implementing a simple tic-tac-toe game with React. This is covered in the official Facebook React tutorial, but I haven't actually looked at how they did this yet. Instead, I wanted to see how far I could get on my own, and then fallback to the tutorial if I needed help. I heard that there are many different ways that React components can be organized and structured in a React project, so I wanted to see how my results compared to what the official tutorial recommends.",{"layout":29014},"/2017/10/03/simple-games-in-react",{"title":49356,"description":50810},"2017/10/03/simple-games-in-react",[50816],"react","vYSdDU8qwg3HjRMQc9F8pFz4kYsabgJdpkm0gBGx2MM",{"id":50819,"title":50820,"body":50821,"comments":315,"date":53382,"description":53383,"draft":872,"extension":873,"external":874,"image":53384,"meta":53385,"navigation":315,"path":53386,"seo":53387,"stem":53388,"tags":53389,"__hash__":53390},"blog/2017/08/03/arch-linux-installation-guide.md","Arch Linux Installation Guide",{"type":8,"value":50822,"toc":53305},[50823,50832,50836,50850,50857,50860,50863,50869,50872,50875,50884,50896,50900,50903,50909,50912,50918,50921,50927,50930,50936,50943,50948,50951,50954,50960,50967,50971,50998,51002,51027,51031,51060,51064,51070,51089,51099,51102,51108,51112,51115,51121,51127,51133,51139,51143,51149,51155,51161,51164,51168,51174,51177,51181,51184,51190,51196,51202,51205,51211,51214,51219,51223,51229,51239,51242,51248,51255,51259,51262,51264,51270,51283,51287,51297,51303,51308,51312,51318,51325,51331,51337,51343,51350,51356,51362,51365,51371,51377,51381,51387,51391,51398,51404,51408,51414,51420,51423,51428,51432,51437,51443,51449,51453,51456,51462,51466,51472,51475,51479,51493,51499,51502,51508,51512,51518,51522,51528,51537,51542,51548,51551,51557,51562,51565,51571,51594,51598,51601,51607,51610,51616,51621,51630,51633,51639,51644,51648,51651,51657,51661,51664,51667,51673,51678,51684,51687,51693,51697,51703,51709,51715,51718,51724,51727,51733,51736,51742,51745,51751,51754,51758,51764,51768,51771,51774,51780,51783,51789,51792,51798,51801,51807,51813,51819,51825,51828,51834,51840,51846,51852,51855,51862,51866,51873,51879,51884,51888,51894,51898,51904,51908,51914,51918,51924,51929,51932,51938,51941,51947,51952,51958,51965,51971,51974,51978,51989,52002,52005,52011,52014,52017,52023,52030,52034,52037,52044,52047,52053,52056,52062,52065,52068,52074,52077,52094,52103,52109,52126,52129,52135,52138,52144,52147,52153,52160,52171,52175,52187,52190,52196,52199,52232,52235,52239,52242,52248,52254,52260,52267,52273,52277,52280,52284,52291,52295,52305,52309,52315,52319,52322,52326,52333,52343,52347,52350,52355,52358,52366,52373,52378,52384,52387,52393,52396,52401,52404,52410,52420,52425,52428,52434,52443,52458,52464,52467,52473,52484,52488,52491,52497,52501,52505,52508,52514,52527,52534,52537,52542,52548,52551,52557,52562,52566,52569,52575,52578,52581,52585,52594,52601,52607,52610,52616,52619,52625,52631,52637,52644,52650,52653,52659,52668,52674,52680,52683,52687,52693,52695,52701,52708,52712,52715,52717,52723,52729,52735,52739,52745,52754,52760,52763,52769,52773,52781,52787,52790,52796,52803,52806,52811,52817,52820,52822,52828,52834,52842,52850,52855,52861,52866,52872,52875,52879,52882,52888,52894,52904,52909,52913,52918,52922,52928,52934,52937,52940,52946,52949,52955,52962,52968,52974,52983,52986,52992,52994,53000,53004,53007,53013,53016,53022,53029,53032,53038,53041,53045,53048,53054,53057,53063,53067,53070,53076,53079,53085,53089,53103,53109,53113,53116,53120,53123,53137,53140,53146,53155,53159,53162,53168,53175,53179,53182,53188,53191,53195,53198,53204,53207,53213,53217,53220,53223],[11,50824,50825,50826,50831],{},"This post will be a comprehensive guide to installing Arch Linux. I will be installing Arch Linux on a refurbished ",[15,50827,50830],{"href":50828,"rel":50829},"http://www3.lenovo.com/us/en/laptops/thinkpad/t-series/t430/",[19],"ThinkPad T430 Laptop",", first as a guest OS on a Windows 10 Pro host, and then on a secondary SSD that will replace the optical drive. I will try to cover absolutely everything you need from creating a bootable USB drive to customizing the desktop interface and installing development environments and tools.",[168,50833,50835],{"id":50834},"put-arch-linux-on-a-usb-drive","Put Arch Linux on a USB drive",[11,50837,50838,50839,50844,50845,50849],{},"I recommend that you download a BitTorrent client such as ",[15,50840,50843],{"href":50841,"rel":50842},"http://dev.deluge-torrent.org/wiki/Download",[19],"Deluge",", and then grab the ISO magnet link from ",[15,50846,50847],{"href":50847,"rel":50848},"https://www.archlinux.org/download/",[19],". It should take just a few minutes to complete the download via BitTorrent.",[11,50851,50852,50853,50856],{},"If you are using a Mac to build the bootable drive, I have read that you can use the ",[33,50854,50855],{},"dd"," command. This didn't work for me.",[11,50858,50859],{},"I found out that my 2011 MacBook Air is not capable of creating Windows 10 bootable media. Only the options for Windows 7 and 8 were available in Boot Camp Assistant (the Mac utility for installing Windows and creating bootable media). I also found that the latest MacBooks can't create bootable media. I was able to find another Mac that had the option for creating \"Windows 7 or later\", and this finally worked.",[11,50861,50862],{},"Another solution that is probably much faster is using Rufus, a Windows/Linux utility for burning ISO images to bootable media. The Arch Wiki has a highlighted note about using Rufus:",[26,50864,50867],{"className":50865,"code":50866,"language":31},[29],"Note: Be sure to select DD mode or the image will be transferred incorrectly.\n",[33,50868,50866],{"__ignoreMap":35},[11,50870,50871],{},"A problem I found is that you can't select the ISO with DD mode selected. I was able to select ISO (in ISO mode), choose the ISO file in my downloads folder, and then select DD mode.",[2215,50873,50874],{"id":44332},"VirtualBox",[11,50876,50877,50878,50883],{},"The T430 came with a copy of Windows Pro, which I installed right away. I also picked up a SSD sleeve that can replace the optical drive for dual booting or additional storage. I have installed Arch Linux on my Windows desktop machine, as well as my MacBook Air. Land of Linux has a ",[15,50879,50882],{"href":50880,"rel":50881},"http://landoflinux.com/linux_install_archlinux_process.html",[19],"great tutorial on installing Arch Linux in Virtual Box",", but it is frustrating because there is no date and no comments. This part of the guide will closely follow this guide and provide some corrections where you might get stuck following the directions command for command.",[11,50885,50886,50887,40575,50889,20174,50892,50895],{},"First we want to set up a new virtual machine with the options Linx / ArchLinux / 64-bit. You are probably using a 64-bit machine (although some old netbooks are 32-bit). The first obstacle is you will probably face with VirtualBox is that there are no options for 64-bit. This is because Intel Virtualization is often disabled by default. You will need to go into the BIOS (by pressing ",[33,50888,19988],{},[33,50890,50891],{},"F10",[33,50893,50894],{},"F12"," at boot time) and then change this option, save the BIOS settings and restart. With this done, you will want to create a virtual machine with at at least 20GB of storage and around 4GB of memory.",[34786,50897,50899],{"id":50898},"quick-note-about-fonts","Quick note about fonts",[11,50901,50902],{},"The default font when installing Arch Linux is quite small, so you may want to increase the font size.",[26,50904,50907],{"className":50905,"code":50906,"language":31},[29],"ls /usr/share/kbd/consolefonts/\n",[33,50908,50906],{"__ignoreMap":35},[11,50910,50911],{},"This command will list the available fonts. To set a font, run the following command:",[26,50913,50916],{"className":50914,"code":50915,"language":31},[29],"setfont sun12x22\n",[33,50917,50915],{"__ignoreMap":35},[11,50919,50920],{},"To revert back to the default font, just run:",[26,50922,50925],{"className":50923,"code":50924,"language":31},[29],"setfont\n",[33,50926,50924],{"__ignoreMap":35},[11,50928,50929],{},"You can also view the current font with",[26,50931,50934],{"className":50932,"code":50933,"language":31},[29],"showconsolefont\n",[33,50935,50933],{"__ignoreMap":35},[911,50937,18073,50939,50942],{"id":50938},"using-fdisk-for-partitioning",[33,50940,50941],{},"fdisk"," for partitioning",[107,50944,50945],{},[11,50946,50947],{},"fdisk is a dialogue-driven program for creation and manipulation of partition tables.",[11,50949,50950],{},"This part of the guide will be similar for installation on virtual machines and regular installions (on harddrives in desktops and laptops). If you are installing on a physical machine that has other disks of similar size to your target disk, I recommend that you first remove all other drives so you don't accidentaly format partitions (I did this by accident, but luckily it was just my Windows Boot Partition and I was able to recover my Windows installation).",[11,50952,50953],{},"We want to run fdisk and enter the device we want to work with as the first argument after the command:",[26,50955,50958],{"className":50956,"code":50957,"language":31},[29],"fdisk /dev/sda\n",[33,50959,50957],{"__ignoreMap":35},[11,50961,50962,50963,50966],{},"On your Virtual Machine it will most likely be ",[338,50964,50965],{},"sda",", but on a physical machine it could vary slightly.",[34786,50968,50970],{"id":50969},"note-about-fdisk","Note about fdisk",[11,50972,50973,50974,15754,50977,15754,50980,50983,50984,50987,50988,30832,50991,50993,50994,50997],{},"When installing Arch Linux on a disk that has Windows or Linux installed (and you want delete the existing Windows or Linux completely to make room for a fresh Arch Linux istallation), you can delete the partitions on the drive (which is usually labeled ",[33,50975,50976],{},"/dev/sda",[33,50978,50979],{},"/dev/sdb",[33,50981,50982],{},"/dev/sdc","). Run ",[33,50985,50986],{},"lsblk"," to determine which drive you want to format, then run ",[33,50989,50990],{},"fdisk /dev/sdX",[33,50992,36314],{}," corresponds to the drive we will be using. You should be inside the fdisk menu. Press ",[33,50995,50996],{},"d"," to start deleting partitions. You may get some warnings about a partition containing some type of trace (such as ext4). This is fine. You can delete all of the partitions.",[911,50999,51001],{"id":51000},"create-a-root-partition","Create a root partition",[11,51003,51004,51005,51007,51008,51010,51011,51014,51015,51018,51019,51022,51023,51026],{},"Press ",[33,51006,10979],{}," and then ",[33,51009,11],{},". When prompted for ",[33,51012,51013],{},"first sector"," press ",[33,51016,51017],{},"ENTER",", and for ",[33,51020,51021],{},"last sector"," enter ",[33,51024,51025],{},"+10G"," for a 10GB partition.",[911,51028,51030],{"id":51029},"create-a-swap-partition","Create a SWAP partition",[11,51032,51004,51033,51035,51036,51038,51039,51041,51042,51044,51045,51048,51049,51052,51053,51055,51056,51059],{},[33,51034,10979],{}," for a new partition, press ",[33,51037,11],{}," for primary partition. Select partition number ",[33,51040,22129],{},". Press ",[33,51043,51017],{}," to select the default start sector, and then type ",[33,51046,51047],{},"+1G"," to create a 1GB swap partition. Press ",[33,51050,51051],{},"t"," to indicate the type of partition this will be. Specify that we are still working with partition ",[33,51054,22129],{},", and then enter ",[33,51057,51058],{},"82",", the code for SWAP partition.",[911,51061,51063],{"id":51062},"create-home-partition","Create home partition",[11,51065,51066,51067,51069],{},"Enter the following ",[33,51068,50941],{}," commands:",[11,51071,51072,637,51074,637,51076,637,51078,637,51080,637,51082,637,51084,637,51086,752],{},[33,51073,10979],{},[33,51075,11],{},[33,51077,34913],{},[33,51079,51017],{},[33,51081,51017],{},[33,51083,51051],{},[33,51085,34913],{},[33,51087,51088],{},"83",[11,51090,51091,51092,51094,51095,51098],{},"Before exiting the ",[33,51093,50941],{}," interface, enter the ",[33,51096,51097],{},"w"," command to write the proposed changes.",[11,51100,51101],{},"We should see the following:",[26,51103,51106],{"className":51104,"code":51105,"language":31},[29],"The partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n",[33,51107,51105],{"__ignoreMap":35},[911,51109,51111],{"id":51110},"create-filesystems-and-format-partitions","Create Filesystems and Format Partitions",[11,51113,51114],{},"Make filesystems for partitions 1 and 3:",[26,51116,51119],{"className":51117,"code":51118,"language":31},[29],"mkfs.ext4 /dev/sda1\nmkfs.ext4 /dev/sda3\n",[33,51120,51118],{"__ignoreMap":35},[26,51122,51125],{"className":51123,"code":51124,"language":31},[29],"mkswap /dev/sda2\nswapon /dev/sda2\n",[33,51126,51124],{"__ignoreMap":35},[911,51128,51130,51131],{"id":51129},"verify-our-layout-with-lsblk","Verify our layout with ",[33,51132,50986],{},[26,51134,51137],{"className":51135,"code":51136,"language":31},[29],"lsblk\n",[33,51138,51136],{"__ignoreMap":35},[911,51140,51142],{"id":51141},"mount-filesystems","Mount Filesystems",[26,51144,51147],{"className":51145,"code":51146,"language":31},[29],"mount /dev/sda1 /mnt\n",[33,51148,51146],{"__ignoreMap":35},[26,51150,51153],{"className":51151,"code":51152,"language":31},[29],"mkdir /mnt/home\n",[33,51154,51152],{"__ignoreMap":35},[26,51156,51159],{"className":51157,"code":51158,"language":31},[29],"mount /dev/sda3 /mnt/home\n",[33,51160,51158],{"__ignoreMap":35},[11,51162,51163],{},"(The swap partition has already been initialized.)",[911,51165,51167],{"id":51166},"backup-mirrorlist","Backup mirrorlist",[26,51169,51172],{"className":51170,"code":51171,"language":31},[29],"cp -vf /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup\n",[33,51173,51171],{"__ignoreMap":35},[11,51175,51176],{},"We can edit the mirrorlist to select the closest mirror, but this is not necessary.",[911,51178,51180],{"id":51179},"install-reflector","Install reflector",[11,51182,51183],{},"This synchronizes package databases:",[26,51185,51188],{"className":51186,"code":51187,"language":31},[29],"pacman -Syy\n",[33,51189,51187],{"__ignoreMap":35},[11,51191,51192,51193,752],{},"Next we want to install our first package: ",[33,51194,51195],{},"reflector",[26,51197,51200],{"className":51198,"code":51199,"language":31},[29],"pacman -S reflector\n",[33,51201,51199],{"__ignoreMap":35},[11,51203,51204],{},"The following command will sort the five best mirrors based on your location:",[26,51206,51209],{"className":51207,"code":51208,"language":31},[29],"reflector --verbose -l 5 --sort rate --save /etc/pacman.d/mirrorlist\n",[33,51210,51208],{"__ignoreMap":35},[11,51212,51213],{},"Re-issue the sync command:",[26,51215,51217],{"className":51216,"code":51187,"language":31},[29],[33,51218,51187],{"__ignoreMap":35},[911,51220,51222],{"id":51221},"installing-the-base","Installing the base",[11,51224,51225,51226,752],{},"The base installation is installed using a special script called ",[33,51227,51228],{},"pacstrap",[11,51230,51231,51232,51234,51235,51238],{},"We will be using the ",[33,51233,11953],{}," group and also the ",[33,51236,51237],{},"base-devel"," group that we will need later.",[11,51240,51241],{},"Issue the following command:",[26,51243,51246],{"className":51244,"code":51245,"language":31},[29],"pacstrap /mnt base base-devel\n",[33,51247,51245],{"__ignoreMap":35},[11,51249,51250,51251,51254],{},"This will install lots of packages; it takes a few minutes. It took me ",[33,51252,51253],{},"4:27.06"," on my MacBook Air and about 2.5 minutes on my ThinkPad.",[911,51256,51258],{"id":51257},"create-etcfstab","Create /etc/fstab",[11,51260,51261],{},"The next step is to create a mount table that will be responsible for automatically mounting filesystems at boot time.",[11,51263,51241],{},[26,51265,51268],{"className":51266,"code":51267,"language":31},[29],"genfstab -U -p /mnt >> /mnt/etc/fstab\n",[33,51269,51267],{"__ignoreMap":35},[11,51271,51272,51273,51276,51277,30832,51280,51282],{},"We can verify the entries with ",[33,51274,51275],{},"cat /mnt/etc/fstab",", or ",[33,51278,51279],{},"blkid /dev/sd#",[33,51281,9984],{}," is the partition number.",[911,51284,51286],{"id":51285},"chroot-configuring-the-base-system","Chroot - Configuring the base system",[11,51288,51289,51290,51293,51294,51296],{},"To configure our new installation, we need to issue the ",[33,51291,51292],{},"arch-chroot"," command. This command is used as follows: ",[33,51295,51292],{}," followed by the new root directory as the first argument.",[26,51298,51301],{"className":51299,"code":51300,"language":31},[29],"root@archiso ~ # arch-chroot /mnt /bin/bash\n[root@archiso /]#\n",[33,51302,51300],{"__ignoreMap":35},[11,51304,51305,51306,752],{},"We could pass in additional arguments to specify a particular shell, but here we are using ",[33,51307,183],{},[911,51309,51311],{"id":51310},"configuring-locale-settings","Configuring locale settings",[26,51313,51316],{"className":51314,"code":51315,"language":31},[29],"nano /etc/locale.gen\n",[33,51317,51315],{"__ignoreMap":35},[11,51319,51320,51321,51324],{},"Uncomment ",[33,51322,51323],{},"en_US.UTF-8 UTF-8"," for US English.",[11,51326,51327,51328,752],{},"Once the desired locales have been uncommented, run ",[33,51329,51330],{},"local-gen",[11,51332,51333,51334,358],{},"Next we will create ",[33,51335,51336],{},"/etc/locale.conf",[26,51338,51341],{"className":51339,"code":51340,"language":31},[29],"echo LANG=en_US.UTF-8 > /etc/locale.conf\nexport LANG=en_US.UTF-8\n",[33,51342,51340],{"__ignoreMap":35},[11,51344,51345,51346,51349],{},"Create the following file: ",[33,51347,51348],{},"/etc/vconsole.conf"," and add the following line:",[26,51351,51354],{"className":51352,"code":51353,"language":31},[29],"KEYMAP=us\n",[33,51355,51353],{"__ignoreMap":35},[11,51357,51358,51359,752],{},"To configure the time zone, we need to create a link to a file called ",[33,51360,51361],{},"/etc/localtime",[11,51363,51364],{},"The following command won't work, we will do this in a later step, so skip the following command for now.",[26,51366,51369],{"className":51367,"code":51368,"language":31},[29],"ln -s /usr/share/zoneinfo/US/Eastern /etc/localtime\n",[33,51370,51368],{"__ignoreMap":35},[11,51372,51373,51374,51376],{},"This worked on my MacBook Air in VirtualBox, but in doing this on my Windows host in VirtualBox trying to link to the file ",[33,51375,51361],{}," gave an error that the file already existed. Skipping this step on my ThinkPad was OK. Looking in the file, it seemed that UTC was already configured.",[911,51378,51380],{"id":51379},"set-the-hardware-clock","Set the Hardware Clock",[26,51382,51385],{"className":51383,"code":51384,"language":31},[29],"hwclock --systohc --utc\n",[33,51386,51384],{"__ignoreMap":35},[911,51388,51390],{"id":51389},"set-the-host-name-for-the-system","Set the Host Name for the System",[11,51392,51393,51394,51397],{},"I'm using the hostname ",[33,51395,51396],{},"archthinkpad",", you can pick whatever host name you want.",[26,51399,51402],{"className":51400,"code":51401,"language":31},[29],"echo archthinkpad > /etc/hostname\n",[33,51403,51401],{"__ignoreMap":35},[911,51405,51407],{"id":51406},"enable-multilib-repository","Enable Multilib Repository",[11,51409,51410,51411,358],{},"Add the following to ",[33,51412,51413],{},"/etc/pacman.conf",[26,51415,51418],{"className":51416,"code":51417,"language":31},[29],"[multilib]\nInclude = /etc/pacman.d/mirrorlist\n",[33,51419,51417],{"__ignoreMap":35},[11,51421,51422],{},"And then run",[26,51424,51426],{"className":51425,"code":51187,"language":31},[29],[33,51427,51187],{"__ignoreMap":35},[911,51429,51431],{"id":51430},"add-support-for-yaourt-package-tool","Add support for Yaourt Package Tool",[11,51433,51434,51435,358],{},"Add the following to the end of ",[33,51436,51413],{},[26,51438,51441],{"className":51439,"code":51440,"language":31},[29],"[archlinuxfr]\nSigLevel = Never\nServer = http://repo.archlinux.fr/$arch\n",[33,51442,51440],{"__ignoreMap":35},[11,51444,51445,51446,752],{},"Then run ",[33,51447,51448],{},"pacman -Syy",[911,51450,51452],{"id":51451},"synchronize-and-update-database-packages","Synchronize and Update Database Packages",[11,51454,51455],{},"Run the \"update system\" command:",[26,51457,51460],{"className":51458,"code":51459,"language":31},[29],"pacman -Syu\n",[33,51461,51459],{"__ignoreMap":35},[911,51463,51465],{"id":51464},"create-a-password-for-the-root-account","Create a password for the root account",[26,51467,51470],{"className":51468,"code":51469,"language":31},[29],"passwd\n",[33,51471,51469],{"__ignoreMap":35},[11,51473,51474],{},"and enter a password for the root user.",[911,51476,51478],{"id":51477},"add-a-new-user-with-sudo-privileges","Add a new user with sudo privileges",[11,51480,51481,51482,51485,51486,51489,51490,752],{},"To add a new user we issue ",[33,51483,51484],{},"useradd"," along with primary and secondary groups. ",[33,51487,51488],{},"wheel"," group members will be able to issue commands prefixed with ",[33,51491,51492],{},"sudo",[26,51494,51497],{"className":51495,"code":51496,"language":31},[29],"useradd -mg users -G wheel,storage,power -s /bin/bash brian\n",[33,51498,51496],{"__ignoreMap":35},[11,51500,51501],{},"Then set a password for this user:",[26,51503,51506],{"className":51504,"code":51505,"language":31},[29],"passwd brian\n",[33,51507,51505],{"__ignoreMap":35},[911,51509,51511],{"id":51510},"install-sudo-package","Install sudo Package",[26,51513,51516],{"className":51514,"code":51515,"language":31},[29],"pacman -S sudo\n",[33,51517,51515],{"__ignoreMap":35},[911,51519,51521],{"id":51520},"configuring-a-sudo-adding-user-to-wheel-group","Configuring a sudo - adding user to wheel group",[11,51523,51524,51525,752],{},"We now need to configure the sudo configuration file. In order to edit this file we must use a special command called ",[33,51526,51527],{},"visudo",[11,51529,51530,51531,15754,51534,51536],{},"This is an editor like ",[33,51532,51533],{},"vi",[33,51535,49087],{}," and it edits the sudo config file.",[11,51538,44099,51539,51541],{},[33,51540,51527],{},", we might see the follwoing error:",[26,51543,51546],{"className":51544,"code":51545,"language":31},[29],"visudo: specified editor (vim) doesn't exist.\n",[33,51547,51545],{"__ignoreMap":35},[11,51549,51550],{},"Let's install vim:",[26,51552,51555],{"className":51553,"code":51554,"language":31},[29],"pacman -S vim\n",[33,51556,51554],{"__ignoreMap":35},[11,51558,51559,51560,752],{},"Now we should be able to run ",[33,51561,51527],{},[11,51563,51564],{},"We need to uncomment the following line:",[26,51566,51569],{"className":51567,"code":51568,"language":31},[29],"# %wheel ALL=(ALL) ALL\n",[33,51570,51568],{"__ignoreMap":35},[11,51572,51573,51574,51577,51578,51580,51581,51584,51585,51007,51587,51584,51589,51591,51592,752],{},"To do this, press the down arrow until we are on the line we need to uncomment, then press ",[33,51575,51576],{},"i",", then remove the ",[33,51579,9984],{}," and space. Next, press ",[33,51582,51583],{},"esc"," and then press ",[33,51586,358],{},[33,51588,2196],{},[33,51590,51017],{},". That should save our changes. Arch Linux warns that this file ONLY be edited with ",[33,51593,49087],{},[911,51595,51597],{"id":51596},"installing-a-bootloader","Installing a bootloader",[11,51599,51600],{},"At this point the directions will fork based on your motherboard firmware. By default grub-install uses target x86_64-efi. If you don't have a UEFI system you should use the following command:",[26,51602,51605],{"className":51603,"code":51604,"language":31},[29],"pacman -S grub\ngrub-install --target=i386-pc --recheck /dev/sdb\n",[33,51606,51604],{"__ignoreMap":35},[11,51608,51609],{},"If this works properly you should see the following:",[26,51611,51614],{"className":51612,"code":51613,"language":31},[29],"Installing for i386-pc platform.\nInstallation finished. No error reported.\n",[33,51615,51613],{"__ignoreMap":35},[107,51617,51618],{},[11,51619,51620],{},"--target=i386-pc instructs grub-install to install for BIOS systems only. It is recommended to always use this option to remove ambiguity in grub-install.",[11,51622,51623,51624,51629],{},"Here is ",[15,51625,51628],{"href":51626,"rel":51627},"https://wiki.archlinux.org/index.php/GRUB",[19],"more information about GRUB"," from the Arch Wiki.",[11,51631,51632],{},"Finally run the following command:",[26,51634,51637],{"className":51635,"code":51636,"language":31},[29],"grub-mkconfig -o /boot/grub/grub.cfg\n",[33,51638,51636],{"__ignoreMap":35},[11,51640,51641],{},[187,51642,51643],{},"Note: If you had a multi OS environment, you could install the \"os-prober\" package with \"pacman -S os-prober\" before running the \"grub-mkconfig\" command. Os-prober will detect other operating systems when generating the grub.cfg file.",[911,51645,51647],{"id":51646},"arch-linux-is-now-installed","Arch Linux is now installed",[11,51649,51650],{},"Issue the following commands to exit:",[26,51652,51655],{"className":51653,"code":51654,"language":31},[29],"exit\numount /mnt/home\numount /mnt\nreboot\n",[33,51656,51654],{"__ignoreMap":35},[911,51658,51660],{"id":51659},"boot-from-existing-os","Boot from existing OS",[11,51662,51663],{},"At the startup screen, select \"Boot existing OS\".",[11,51665,51666],{},"I got a strange error message at login:",[26,51668,51671],{"className":51669,"code":51670,"language":31},[29],"arch20815 login: [9.921142] piix4_smbus 0000:00:07.0: SMBus base address uninitialized - upgrade BIOS or use force_addr=0xaddr\n",[33,51672,51670],{"__ignoreMap":35},[11,51674,51004,51675,51677],{},[33,51676,51017],{}," to return to the login prompt.",[11,51679,51680,51681,51683],{},"Login with the ",[33,51682,51492],{}," user we created.",[11,51685,51686],{},"We can check to see if this user was setup properly by running:",[26,51688,51691],{"className":51689,"code":51690,"language":31},[29],"[brian@arch20815 ~]$ sudo whoami\n\nWe trust you have received the usual lecture from the local System Administrator. It usually boils down to these three things:\n\n    #1) Respect the privacy of others.\n    #2) Think before you type.\n    #3) With great power comes great responsibility.\n\n[sudo] password for brian:\nroot\n[brian@arch20815 ~]$\n",[33,51692,51690],{"__ignoreMap":35},[911,51694,51696],{"id":51695},"configuring-the-network","Configuring the Network",[11,51698,51699,51700,752],{},"After the reboot we will have lost network connectivity. We need to create a new definition to our interface. We can isssue the command ",[33,51701,51702],{},"ip link",[11,51704,1164,51705,51708],{},[33,51706,51707],{},"sudo vi /etc/systemd/network/enp0s3.netork"," and add the following to this file:",[26,51710,51713],{"className":51711,"code":51712,"language":31},[29],"[Match]\nName=en*\n\n[Network]\nDHCP=yes\n",[33,51714,51712],{"__ignoreMap":35},[11,51716,51717],{},"Then we need to run the following commands:",[26,51719,51722],{"className":51720,"code":51721,"language":31},[29],"sudo systemctl restart systemd-networkd\nsudo systemctl enable systemd-networkd\n",[33,51723,51721],{"__ignoreMap":35},[11,51725,51726],{},"Next issue the following command:",[26,51728,51731],{"className":51729,"code":51730,"language":31},[29],"sudo vi /etc/resolv.conf\n",[33,51732,51730],{"__ignoreMap":35},[11,51734,51735],{},"And add the following to this file:",[26,51737,51740],{"className":51738,"code":51739,"language":31},[29],"nameserver 8.8.8.8\nnameserver 8.8.4.4\n",[33,51741,51739],{"__ignoreMap":35},[11,51743,51744],{},"Next we can check connectivity:",[26,51746,51749],{"className":51747,"code":51748,"language":31},[29],"ip a s\nping -c 3 www.google.com\n",[33,51750,51748],{"__ignoreMap":35},[11,51752,51753],{},"We should see that we have been assigned an IP address and that we can ping google.com.",[911,51755,51757],{"id":51756},"run-a-full-system-update","Run a full System update",[26,51759,51762],{"className":51760,"code":51761,"language":31},[29],"sudo pacman -Syu\n",[33,51763,51761],{"__ignoreMap":35},[911,51765,51767],{"id":51766},"installing-the-x-environment","Installing the X Environment",[11,51769,51770],{},"I have had problems with this step in the past.",[11,51772,51773],{},"The next step in the tutorial is to run:",[26,51775,51778],{"className":51776,"code":51777,"language":31},[29],"sudo pacman -S xorg-server xorg-xinit xorg-utils xorg-server-utils mesa xorg-twm xterm xorg-xclock\n",[33,51779,51777],{"__ignoreMap":35},[11,51781,51782],{},"But this will result in:",[26,51784,51787],{"className":51785,"code":51786,"language":31},[29],"error: target not found: xorg-utils\nerror: target not found: xorg-server-utils\n",[33,51788,51786],{"__ignoreMap":35},[11,51790,51791],{},"It seems that these packages have been deprecated. I think a solution to this is to run:",[26,51793,51796],{"className":51794,"code":51795,"language":31},[29],"sudo pacman -S xorg-apps\n",[33,51797,51795],{"__ignoreMap":35},[11,51799,51800],{},"and then install the remaining packages:",[26,51802,51805],{"className":51803,"code":51804,"language":31},[29],"sudo pacman -S xorg-server xorg-xinit mesa xorg-twm xterm xorg-xclock\n",[33,51806,51804],{"__ignoreMap":35},[11,51808,51809,51810,51812],{},"Select option ",[33,51811,15625],{}," (don't select the options that include the word nvidia).",[11,51814,51815,51816,358],{},"Next we need to install ",[33,51817,51818],{},"linux-headers",[26,51820,51823],{"className":51821,"code":51822,"language":31},[29],"sudo pacman -S linux-headers\n",[33,51824,51822],{"__ignoreMap":35},[11,51826,51827],{},"Next we need to install some VirtualBox packages:",[26,51829,51832],{"className":51830,"code":51831,"language":31},[29],"sudo pacman -S virtualbox-guest-utils virtualbox-guest-dkms\n",[33,51833,51831],{"__ignoreMap":35},[11,51835,51836,51837,51839],{},"The tutorial didn't mention the ",[33,51838,51818],{}," package, but the error message that comes up if you skip this step say that this package is needed.",[11,51841,51842,51845],{},[33,51843,51844],{},"xorg-server-utils"," seems to have been removed, but it was a meta-package that simply included the following:",[26,51847,51850],{"className":51848,"code":51849,"language":31},[29],"xorg-iceauth xorg-sessreg xorg-xcmsdb xorg-xbacklight xorg-xgamma xorg-xhost xorg-xinput xorg-xmodmap xorg-xrandr xorg-xrdb xorg-xrefresh xorg-xset xorg-xsetroot\n",[33,51851,51849],{"__ignoreMap":35},[11,51853,51854],{},"Installing these packages individually did not help.",[11,51856,51857,51858,51861],{},"This does a full install of xorg (reinstalling many packages I just installed). After running this command, running ",[33,51859,51860],{},"startx"," gives no errors and lets me enter the X environment.",[911,51863,51865],{"id":51864},"installing-gnome-desktop","Installing GNOME Desktop",[11,51867,51868,51869,51872],{},"Next, let's install ",[33,51870,51871],{},"GNOME"," desktop. Issue the following command:",[26,51874,51877],{"className":51875,"code":51876,"language":31},[29],"sudo pacman -S gnome gnome-extra gdm\n",[33,51878,51876],{"__ignoreMap":35},[11,51880,51881,51882,752],{},"Accept all of the defaults and then confirm by pressing ",[33,51883,36318],{},[911,51885,51887],{"id":51886},"installing-networking-tools","Installing Networking Tools",[26,51889,51892],{"className":51890,"code":51891,"language":31},[29],"sudo pacman -S net-tools\n",[33,51893,51891],{"__ignoreMap":35},[911,51895,51897],{"id":51896},"installing-popular-packages","Installing popular packages",[26,51899,51902],{"className":51900,"code":51901,"language":31},[29],"sudo pacman -S pulseaudio pulseaudio-alsa pavucontrol gnome-terminal firefox flashplugin vlc deluge smplayer audacious qmmp gimp xfburn gedit gnome-system-monitor\n",[33,51903,51901],{"__ignoreMap":35},[911,51905,51907],{"id":51906},"installing-codecs-for-audio-and-video","Installing codecs for audio and video",[26,51909,51912],{"className":51910,"code":51911,"language":31},[29],"sudo pacman -S a52dec faac faad2 flac jasper lame libdca libdv libmad libmpeg2 libtheora libvorbis libxv wavpack x264 xvidcore\n",[33,51913,51911],{"__ignoreMap":35},[911,51915,51917],{"id":51916},"install-yaourt","Install Yaourt",[26,51919,51922],{"className":51920,"code":51921,"language":31},[29],"sudo pacman -S yaourt\n",[33,51923,51921],{"__ignoreMap":35},[11,51925,51926],{},[338,51927,51928],{},"NEVER run yaourt with sudo or as root",[11,51930,51931],{},"Finally, enable GNOME to start automatically at boot:",[26,51933,51936],{"className":51934,"code":51935,"language":31},[29],"sudo systemctl enable gdm\n",[33,51937,51935],{"__ignoreMap":35},[11,51939,51940],{},"The last issue I had was getting GNOME Desktop to display in full-screen in VirtualBox. To do this, click on the Devices Tab and then Insert Guest Additions Image.",[26,51942,51945],{"className":51943,"code":51944,"language":31},[29],"reboot\n",[33,51946,51944],{"__ignoreMap":35},[11,51948,51949,51950,358],{},"Login with the user account that you created. At this point, trying to open Terminal will probably not work. This is because of an issue mentioned earlier about a locale file. Let's look at ",[33,51951,51336],{},[26,51953,51956],{"className":51954,"code":51955,"language":31},[29],"cat /etc/locale.conf\n",[33,51957,51955],{"__ignoreMap":35},[11,51959,51960,51961,51964],{},"We just need to make sure that the language we want is uncommented in ",[33,51962,51963],{},"/etc/locale.gen",". We did this previously. Now we need to run the following command:",[26,51966,51969],{"className":51967,"code":51968,"language":31},[29],"sudo locale-gen\n",[33,51970,51968],{"__ignoreMap":35},[11,51972,51973],{},"This should allow us to access the terminal.",[911,51975,51977],{"id":51976},"enable-wi-fi","Enable Wi-Fi",[11,51979,51980,51981,51984,51985,51988],{},"To enable Wi-Fi, first check ",[33,51982,51983],{},"lspci -k"," if you have an onboard wifi card or ",[33,51986,51987],{},"lsusb -v"," if you have a USB wifi adapter.",[11,51990,51991,51992,51994,51995,51997,51998,52001],{},"Next run ",[33,51993,51702],{}," and see if there is an entry that starts with ",[33,51996,51097],{},". This is wireless interface. If you don't have WiFi working it probably says ",[33,51999,52000],{},"state DOWN"," in the description.",[11,52003,52004],{},"We need to run the following command to bring it up:",[26,52006,52009],{"className":52007,"code":52008,"language":31},[29],"sudo ip link set \u003Cthe code that starts with w> up\n",[33,52010,52008],{"__ignoreMap":35},[11,52012,52013],{},"If we go to \"Settings\" we will see an error that says \"NetworkManager needs to be running.\"",[11,52015,52016],{},"To get NetworkManager running, issue the following commands:",[26,52018,52021],{"className":52019,"code":52020,"language":31},[29],"sudo systemctl enable NetworkManager.service\nsudo systemctl start NetworkManager.service\n",[33,52022,52020],{"__ignoreMap":35},[11,52024,52025,52026,52029],{},"This will prompt you for authentication, then you can go back to ",[33,52027,52028],{},"Settings > Network"," and you should be able to find any available Wi-Fi networks.",[911,52031,52033],{"id":52032},"customizing-gnome-desktop","Customizing GNOME Desktop",[11,52035,52036],{},"Next let's work on customizing our GUI.",[11,52038,52039,52040,52043],{},"We will now start making use of the ",[33,52041,52042],{},"yaourt"," utility to build packages for programs we want to install.",[11,52045,52046],{},"Yaourt gives us a useful flag to search packages:",[26,52048,52051],{"className":52049,"code":52050,"language":31},[29],"yaourt -Ss search_term1 search_term2\n",[33,52052,52050],{"__ignoreMap":35},[11,52054,52055],{},"This returns a list of packages which match search terms either in their title or description. When you find the right package you can copy the exact name and then download it with:",[26,52057,52060],{"className":52058,"code":52059,"language":31},[29],"yaourt -S package_name\n",[33,52061,52059],{"__ignoreMap":35},[11,52063,52064],{},"Let's start with nice custom theme, numix.",[11,52066,52067],{},"Run the following:",[26,52069,52072],{"className":52070,"code":52071,"language":31},[29],"yaourt -Ss numix\n",[33,52073,52071],{"__ignoreMap":35},[11,52075,52076],{},"We will see many packages, and there are three that we should install:",[916,52078,52079,52084,52089],{},[919,52080,52081],{},[33,52082,52083],{},"numix-gtk-theme",[919,52085,52086],{},[33,52087,52088],{},"numix-circle-theme-git",[919,52090,52091],{},[33,52092,52093],{},"numix-cursor-theme-git",[11,52095,52096,52097,52099,52100,52102],{},"The installations process for ",[33,52098,52083],{}," is quite long. You need to confirm ",[33,52101,1596],{}," and it will also ask you the following:",[26,52104,52107],{"className":52105,"code":52106,"language":31},[29],"Edit PKGBUILD with:\n",[33,52108,52106],{"__ignoreMap":35},[11,52110,52111,52112,52115,52116,52118,52119,52121,52122,52125],{},"Here you can type ",[33,52113,52114],{},"nano"," and press ",[33,52117,51017],{},". It will then show you a file in ",[33,52120,52114],{}," or the editor you select. You can press ",[33,52123,52124],{},"Ctrl-X"," and the installation script will continue running.",[11,52127,52128],{},"You will also see the following message before being prompted to select an editor:",[26,52130,52133],{"className":52131,"code":52132,"language":31},[29],"Please add $VISUAL to your environment variables.\n",[33,52134,52132],{"__ignoreMap":35},[11,52136,52137],{},"Let's do that now so we are not prompted to do it later:",[26,52139,52142],{"className":52140,"code":52141,"language":31},[29],"sudo nano ~/.bashrc\n",[33,52143,52141],{"__ignoreMap":35},[11,52145,52146],{},"Add the following line to the end of the file:",[26,52148,52151],{"className":52149,"code":52150,"language":31},[29],"export VISUAL=\"nano\"\n",[33,52152,52150],{"__ignoreMap":35},[11,52154,52155,52156,52159],{},"You could also set ",[33,52157,52158],{},"VISUAL=\"vim\""," or some other editor.",[11,52161,52162,52163,52166,52167,52170],{},"Now that we have installed the themes, let's activate them. Press ",[33,52164,52165],{},"ALT F2"," and type ",[33,52168,52169],{},"gnome-tweak-tool",". Under the appearance tab you can select the newly installed GTK+ theme, icons and cursor.",[911,52172,52174],{"id":52173},"gnome-shell-extensions","GNOME Shell Extensions",[11,52176,52177,52178,52181,52182,52186],{},"You can do a lot of neat customization with extensions. You will see an ",[33,52179,52180],{},"Extensions"," tab in the Tweak Tool. Click on \"Get more extensions\". This will take you to the ",[15,52183,52174],{"href":52184,"rel":52185},"https://extensions.gnome.org",[19]," site. Before you start using extensions, you need to add a Firefox extensions and install a package. Click the link in the blue bar at the top of the page and install the plugin.",[11,52188,52189],{},"Next you need to install the following package:",[26,52191,52194],{"className":52192,"code":52193,"language":31},[29],"yaourt -S chrome-gnome-shell-git\n",[33,52195,52193],{"__ignoreMap":35},[11,52197,52198],{},"Now refresh the GNOME Shell Extensions page and the blue message should go away. Here are the extensions I use:",[916,52200,52201,52204,52207,52210,52213,52216,52219,52222,52225],{},[919,52202,52203],{},"Applications menu",[919,52205,52206],{},"Removable Drive menu",[919,52208,52209],{},"Dash to Dock",[919,52211,52212],{},"Drop Down Terminal",[919,52214,52215],{},"Dynamic Top Bar",[919,52217,52218],{},"User Themes",[919,52220,52221],{},"System Monitor",[919,52223,52224],{},"Sensory Perception",[919,52226,52227,52228,52231],{},"Screen Saver and hibernate buttons (requires ",[33,52229,52230],{},"gnome-screensaver"," package and a GNOME Shell Extension called \"Suspend and Lock Button\")",[11,52233,52234],{},"Dash to Dock lets you place the dock wherever you want, you can also change its appearance and behavior.",[911,52236,52238],{"id":52237},"screenfetch","Screenfetch",[11,52240,52241],{},"I like to use Screenfetch, a package that prints out ASCII art for your distro and some customizable system information.",[26,52243,52246],{"className":52244,"code":52245,"language":31},[29],"yaourt -S screenfetch\n",[33,52247,52245],{"__ignoreMap":35},[11,52249,52250,52251,52253],{},"Now edit ",[33,52252,48623],{}," by adding the following line at the bottom of the file:",[26,52255,52258],{"className":52256,"code":52257,"language":31},[29],"screenfetch\n",[33,52259,52257],{"__ignoreMap":35},[11,52261,52262,52263,52266],{},"Screefetch can be further customized, check out ",[33,52264,52265],{},"man screenfetch"," for more options. For example, you can exlude certain lines about system information or add custom lines like the model of your PC, etc:",[26,52268,52271],{"className":52269,"code":52270,"language":31},[29],"                  -`\n                 .o+`                 brian@archthinkpad\n                `ooo/                 OS: Arch Linux\n               `+oooo:                Kernel: x86_64 Linux 4.12.5-1-ARCH\n              `+oooooo:               Uptime: 5h 8m\n              -+oooooo+:              Packages: 1044\n            `/:-:++oooo+:             Shell: bash 4.4.12\n           `/++++/+++++++:            Resolution: 1366x768\n          `/++++++++++++++:           DE: GNOME\n         `/+++ooooooooooooo/`         WM: GNOME Shell\n        ./ooosssso++osssssso+`        GTK Theme: Numix [GTK2/3]\n       .oossssso-````/ossssss+`       Icon Theme: Numix-Circle\n      -osssssso.      :ssssssso.      Font: Cantarell 11\n     :osssssss/        osssso+++.     CPU: Intel Core i5-3320M @ 4x 3.3GHz [44.0°C]\n    /ossssssss/        +ssssooo/-     GPU: intel\n  `/ossssso+/:-        -:/+osssso+-   RAM: 4315MiB / 7685MiB\n  `+sso+:-`                 `.-/+oso:\n `++:.                           `-/+/\n.`                                 `/\n\n\n",[33,52272,52270],{"__ignoreMap":35},[34786,52274,52276],{"id":52275},"quick-note-on-terminal-colors","Quick Note on Terminal Colors",[11,52278,52279],{},"I don't like the default white background of the terminal, so you can change this by editing the default profile. Unselect \"Use colors from system theme\" and select \"Solarized Dark\" (my preferred color scheme).",[34786,52281,52283],{"id":52282},"quick-note-on-yaourt","Quick Note on yaourt",[11,52285,52286,52287,52290],{},"You can add a ",[33,52288,52289],{},"--no-confirm"," flag to yaourt that will prevent it from asking you if you want to proceed with each step of the installation process.",[911,52292,52294],{"id":52293},"window-options","Window options",[11,52296,52297,52298,52300,52301,52304],{},"By default, GNOME only shows the ",[33,52299,36314],{}," in the top right corner of windows. You can add the maximize and minimize buttons in the ",[33,52302,52303],{},"Windows"," tab of the GNOME Tweak Tool.",[911,52306,52308],{"id":52307},"icons-on-desktop","Icons on Desktop",[11,52310,52311,52312,52314],{},"Another default setting that may be strange for you is \"Icons on Desktop\" which is set to ",[33,52313,24338],{}," by default. The is found under the Desktop tab in the GNOME Tweak Tool.",[911,52316,52318],{"id":52317},"my-other-setup-notes","My other setup notes",[11,52320,52321],{},"I install Chromium and set it to my default browser. My must have extension for Chrome (and any other browser) is Last Pass for account and password management.",[168,52323,52325],{"id":52324},"default-applications","Default Applications",[11,52327,52328,52329,52332],{},"You can set default applications by opening ",[33,52330,52331],{},"Settings > Details > Default Applications",". I use Image Viewer for photos instead of GIMP and VLC for Video.",[11,52334,52335,52338,52339,52342],{},[33,52336,52337],{},"pacman"," offers similar functionality, and so does ",[33,52340,52341],{},"pacaur",". There are several options for how to download software with Arch Linux.",[911,52344,52346],{"id":52345},"other-packages","Other packages",[11,52348,52349],{},"Here's a list of other helpful packages:",[916,52351,52352],{},[919,52353,52354],{},"htop (system monitoring)",[11,52356,52357],{},"htop can be customized to show some helpful information. Press F2 or click in the terminal where it says \"Setup\" and you will open a menu to select custom options for htop.",[916,52359,52360,52363],{},[919,52361,52362],{},"blender (video editing and 3d modeling)",[919,52364,52365],{},"OBS (screen recording and streaming software)",[11,52367,52368,52369,52372],{},"OBS took a little bit of configuring to get right. Out of the box, doing a screen capture showed just a black screen. I think the solution is to install ",[33,52370,52371],{},"obs-studio-git"," and then reboot your computer.",[916,52374,52375],{},[919,52376,52377],{},"Dropbox",[26,52379,52382],{"className":52380,"code":52381,"language":31},[29],"yaourt -S dropbox\n",[33,52383,52381],{"__ignoreMap":35},[11,52385,52386],{},"Run the following command:",[26,52388,52391],{"className":52389,"code":52390,"language":31},[29],"dropbox\n",[33,52392,52390],{"__ignoreMap":35},[11,52394,52395],{},"And then follow the dialogue to sign into your Dropbox account. You will see a Dropbox folder in the File Manager.",[916,52397,52398],{},[919,52399,52400],{},"Google Drive",[11,52402,52403],{},"You can get access to Google Drive by going into:",[26,52405,52408],{"className":52406,"code":52407,"language":31},[29],"Settings > Online Accounts > Google\n",[33,52409,52407],{"__ignoreMap":35},[11,52411,52412,52413,52416,52417,52419],{},"If you have ",[33,52414,52415],{},"Files"," set to ",[33,52418,24335],{},", you will find your Google Drive Folder in the side bar of the File Viewer. Clicking on the icon mounts your Google Drive and gives you access to the files.",[916,52421,52422],{},[919,52423,52424],{},"hexchat (IRC client)",[11,52426,52427],{},"Internet Relay Chat (IRC) is an easy way to start talking to people in real-time about any topic you can think of. Hexchat is a popular GUI client for IRC. Here is how to install hexchat:",[26,52429,52432],{"className":52430,"code":52431,"language":31},[29],"sudo pacman -S hexchat\n",[33,52433,52431],{"__ignoreMap":35},[11,52435,52436,52437,52442],{},"Once you have installed hexchat, you will want to register a nickname. ",[15,52438,52441],{"href":52439,"rel":52440},"https://freenode.net/kb/answer/registration",[19],"This article"," says that the steps for registering a nickname are to:",[11,52444,52445,52446,52449,52450,52453,52454,52457],{},"Send the following message to ",[33,52447,52448],{},"freenode"," (replace ",[33,52451,52452],{},"some_password"," with a secure password of your choice and ",[33,52455,52456],{},"youremail@example.com"," with your email address):",[26,52459,52462],{"className":52460,"code":52461,"language":31},[29],"/msg NickServ REGISTER some_password youremail@example.com\n",[33,52463,52461],{"__ignoreMap":35},[11,52465,52466],{},"After you send this message, check your email and you will see instructions to confirm your email that look like this:",[26,52468,52471],{"className":52469,"code":52470,"language":31},[29],"/msg NickServ VERIFY REGISTER \u003Cyour_name> \u003Csome_code_they_give_you>\n",[33,52472,52470],{"__ignoreMap":35},[11,52474,52475,52476,52479,52480,52483],{},"This should confirm your nickname. Now you can start chatting on channels. Press ",[33,52477,52478],{},"Alt + S",", and then ",[33,52481,52482],{},"J",", and then type the name of the channel you want to join.",[911,52485,52487],{"id":52486},"configure-vpn","Configure VPN",[11,52489,52490],{},"I setup TorGuard VPN service by installing:",[26,52492,52495],{"className":52493,"code":52494,"language":31},[29],"yaourt -S torguard\n",[33,52496,52494],{"__ignoreMap":35},[168,52498,52500],{"id":52499},"setting-up-development-tools","Setting Up Development Tools",[911,52502,52504],{"id":52503},"atom-and-other-text-editors","Atom and other text editors",[11,52506,52507],{},"My preferred text editor is Atom:",[26,52509,52512],{"className":52510,"code":52511,"language":31},[29],"sudo pacman -S atom\n",[33,52513,52511],{"__ignoreMap":35},[11,52515,52516,52517,52522,52523,52526],{},"Spell checker didn't seem to be working for me, and this seems to be a ",[15,52518,52521],{"href":52519,"rel":52520},"https://github.com/atom/spell-check/issues/129",[19],"known issue",", so I recommend you install the ",[33,52524,52525],{},"atom/spell-checker"," package if you like to use atom and want spell checking.",[11,52528,52529,52530,52533],{},"The Arch Linux installation also came with an interesting editor called ",[33,52531,52532],{},"Builder"," which seems pretty interesting as well.",[911,52535,50874],{"id":52536},"virtualbox-1",[11,52538,52539],{},[4339,52540,52541],{},"Skip this step if you are installing Arch Linux as guest OS in a VirtualBox (you can't run a virtual machine in a virtual machine).",[26,52543,52546],{"className":52544,"code":52545,"language":31},[29],"sudo pacman -S virtualbox\n",[33,52547,52545],{"__ignoreMap":35},[11,52549,52550],{},"You will see a message that says:",[26,52552,52555],{"className":52553,"code":52554,"language":31},[29],":: There are 2 providers available for VIRTUALBOX-HOST-MODULES\n:: Repository community\n   1) virtualbox-host-dkms 2) virtualbox-host-modules-arch\n",[33,52556,52554],{"__ignoreMap":35},[11,52558,52559],{},[338,52560,52561],{},"Select option 2, continue the installation process and then reboot.",[911,52563,52565],{"id":52564},"quick-note-on-deluge","Quick note on deluge",[11,52567,52568],{},"You will probably want to use a BitTorrent client to download ISO images for virtual machines you want to run. Deluge works well for this, and we already downloaded it during setup. We can add one more package for a theme that Deluge uses:",[26,52570,52573],{"className":52571,"code":52572,"language":31},[29],"yaourt -S gtk-engine-murrine\n",[33,52574,52572],{"__ignoreMap":35},[11,52576,52577],{},"Now grab a torrent link or file for an OS you want to download and add it to Deluge.",[11,52579,52580],{},"When it is finished we can launch VirtualBox and create a virtual machine with our ISO.",[911,52582,52584],{"id":52583},"npm-and-nodejs","npm and node.js",[11,52586,52587,52588,52593],{},"Here's ",[15,52589,52592],{"href":52590,"rel":52591},"https://docs.npmjs.com/getting-started/fixing-npm-permissions",[19],"a helpful article"," that is important to following when setting up npm and node.js. Be very careful here as some of the steps can completely mess up permissions on your entire system.",[11,52595,52596,52597,52600],{},"Here's a walkthrough of the steps to get ",[33,52598,52599],{},"npm"," setup correctly.",[26,52602,52605],{"className":52603,"code":52604,"language":31},[29],"sudo pacman -S npm\n",[33,52606,52604],{"__ignoreMap":35},[11,52608,52609],{},"Next run:",[26,52611,52614],{"className":52612,"code":52613,"language":31},[29],"npm config get prefix\n",[33,52615,52613],{"__ignoreMap":35},[11,52617,52618],{},"The output of this command is probably:",[26,52620,52623],{"className":52621,"code":52622,"language":31},[29],"/usr\n",[33,52624,52622],{"__ignoreMap":35},[11,52626,52627,52628,358],{},"If this is the case, we MUST follow the following steps (option 2 from ",[15,52629,30306],{"href":52590,"rel":52630},[19],[26,52632,52635],{"className":52633,"code":52634,"language":31},[29],"mkdir ~/.npm-global\nnpm config set prefix '~/.npm-global'\n",[33,52636,52634],{"__ignoreMap":35},[11,52638,52639,52640,52643],{},"If you don't already have a ",[33,52641,52642],{},"~/.profile"," file, make one, and then add the following line:",[26,52645,52648],{"className":52646,"code":52647,"language":31},[29],"export PATH=~/.npm-global/bin:$PATH\n",[33,52649,52647],{"__ignoreMap":35},[11,52651,52652],{},"Then run:",[26,52654,52657],{"className":52655,"code":52656,"language":31},[29],"source ~/.profile\n",[33,52658,52656],{"__ignoreMap":35},[11,52660,52661,52662,52664,52665,52667],{},"This will \"refresh\" ",[33,52663,52642],{}," so that using the ",[33,52666,52599],{}," command will work without needing sudo.",[11,52669,52670,52671,52673],{},"Next we can test that ",[33,52672,52599],{}," works by trying to install a package:",[26,52675,52678],{"className":52676,"code":52677,"language":31},[29],"npm install -g jshint\n",[33,52679,52677],{"__ignoreMap":35},[11,52681,52682],{},"This should install the package with no errors.",[911,52684,52686],{"id":52685},"heroku","Heroku",[26,52688,52691],{"className":52689,"code":52690,"language":31},[29],"yaourt -S heroku-cli\n",[33,52692,52690],{"__ignoreMap":35},[11,52694,52652],{},[26,52696,52699],{"className":52697,"code":52698,"language":31},[29],"heroku keys:add\n",[33,52700,52698],{"__ignoreMap":35},[11,52702,52703,52704,52707],{},"and then type ",[33,52705,52706],{},"yes"," at the prompt. This will your computer's public key to your Heroku account.",[34786,52709,52711],{"id":52710},"note-about-python-versions-on-heroku","Note about Python versions on Heroku",[11,52713,52714],{},"I have only been able to deploy with Python 3.5.2.",[911,52716,12986],{"id":12985},[26,52718,52721],{"className":52719,"code":52720,"language":31},[29],"sudo pacman -S redis\n",[33,52722,52720],{"__ignoreMap":35},[11,52724,52725,52726],{},"Next, start and enable ",[33,52727,52728],{},"redis.service",[26,52730,52733],{"className":52731,"code":52732,"language":31},[29],"sudo systemctl start redis.service\nsudo systemctl enable redis.service\n",[33,52734,52732],{"__ignoreMap":35},[911,52736,52738],{"id":52737},"ssh","SSH",[26,52740,52743],{"className":52741,"code":52742,"language":31},[29],"sudo pacman -S openssh\n",[33,52744,52742],{"__ignoreMap":35},[911,52746,52748,52749,343],{"id":52747},"autoenv-github","autoenv (",[15,52750,52753],{"href":52751,"rel":52752},"https://github.com/kennethreitz/autoenv",[19],"GitHub",[26,52755,52758],{"className":52756,"code":52757,"language":31},[29],"yaourt -S autoenv\n",[33,52759,52757],{"__ignoreMap":35},[11,52761,52762],{},"You need to source activate.sh in your bashrc afterwards:",[26,52764,52767],{"className":52765,"code":52766,"language":31},[29],"echo 'source /usr/share/autoenv/activate.sh' >> ~/.bashrc\n",[33,52768,52766],{"__ignoreMap":35},[911,52770,52772],{"id":52771},"ruby-and-jekyll","Ruby and Jekyll",[11,52774,52775,52776,752],{},"I use Jekyll for my personal site and blog, which is a static site generator written in Ruby. To start using Jekyll we need to install a ruby gem, and to install the gem we will need to first install ruby. Here is the ",[15,52777,52780],{"href":52778,"rel":52779},"https://wiki.archlinux.org/index.php/ruby",[19],"ruby page from the Arch Wiki",[26,52782,52785],{"className":52783,"code":52784,"language":31},[29],"sudo pacman -S ruby\n",[33,52786,52784],{"__ignoreMap":35},[11,52788,52789],{},"Next we can install the Jekyll gem:",[26,52791,52794],{"className":52792,"code":52793,"language":31},[29],"gem install jekyll bundler\n",[33,52795,52793],{"__ignoreMap":35},[11,52797,52798,52799,752],{},"It should say something like \"20 gems installed\" if everything was successful. You can read more about Jekyll ",[15,52800,1321],{"href":52801,"rel":52802},"https://jekyllrb.com/",[19],[11,52804,52805],{},"For now I will not be installing RVM (Ruby Version Manager), but the Arch Wiki has good notes on how this can be installed.",[11,52807,52808,52809,22038],{},"Before we use the Jekyll gem, we need to add the following to our ",[33,52810,48623],{},[26,52812,52815],{"className":52813,"code":52814,"language":31},[29],"PATH=\"$(ruby -e 'print Gem.user_dir')/bin:$PATH\"\n",[33,52816,52814],{"__ignoreMap":35},[11,52818,52819],{},"This will allow you to run Jekyll commands.",[911,52821,34532],{"id":1125},[11,52823,52824,52825,358],{},"In the installation pIssuesrocess, the latest versions of Python has been installed (3.6.2 at the time of writing this tutorial). We need to get ",[33,52826,52827],{},"pip",[26,52829,52832],{"className":52830,"code":52831,"language":31},[29],"yaourt -S python-pip\n",[33,52833,52831],{"__ignoreMap":35},[11,52835,52587,52836,52841],{},[15,52837,52840],{"href":52838,"rel":52839},"https://docs.python.org/3/library/venv.html",[19],"an article from the Python documentation"," that covers creating virtual environments.",[11,52843,51623,52844,52849],{},[15,52845,52848],{"href":52846,"rel":52847},"https://packaging.python.org/tutorials/installing-packages/",[19],"another article"," about creating virtual environments in python.",[107,52851,52852],{},[11,52853,52854],{},"Currently, there are two viable tools for creating Python virtual environments:\nvenv is available by default in Python 3.3 and later, and installs pip and setuptools into created virtual environments in Python 3.4 and later.\nvirtualenv needs to be installed separately, but supports Python 2.6+ and Python 3.3+, and pip, setuptools and wheel are always installed into created virtual environments by default (regardless of Python version).\nThe basic usage is like so:\nUsing virtualenv:",[26,52856,52859],{"className":52857,"code":52858,"language":31},[29],"virtualenv \u003CDIR>\nsource \u003CDIR>/bin/activate\n",[33,52860,52858],{"__ignoreMap":35},[107,52862,52863],{},[11,52864,52865],{},"Using venv:",[26,52867,52870],{"className":52868,"code":52869,"language":31},[29],"python3 -m venv \u003CDIR>\nsource \u003CDIR>/bin/activate\n",[33,52871,52869],{"__ignoreMap":35},[11,52873,52874],{},"For more information, see the virtualenv docs or the venv docs.",[34786,52876,52878],{"id":52877},"jupyter-notebook-and-ipython-notebook","Jupyter Notebook and IPython Notebook",[11,52880,52881],{},"IPython notebooks (also known as Jupyter notebooks), provide a nice environment for combining python runtime environments, Markdown, images, graphs and other interactive Python tools such as Bokeh. You can install both Jupyter Notebooks and IPython Notebooks:",[26,52883,52886],{"className":52884,"code":52885,"language":31},[29],"yaort -S jupyter-notebook\n",[33,52887,52885],{"__ignoreMap":35},[26,52889,52892],{"className":52890,"code":52891,"language":31},[29],"yaourt -S ipython2-notebook\n",[33,52893,52891],{"__ignoreMap":35},[11,52895,52896,52897,15754,52900,52903],{},"You can launch either of these with ",[33,52898,52899],{},"jupyter notebook",[33,52901,52902],{},"ipython notebook"," from the terminal.",[11,52905,52906],{},[4339,52907,52908],{},"There seems to be an issue with LaTeX",[34786,52910,52912],{"id":52911},"anaconda-distribution-from-continuum-analytics","Anaconda Distribution from Continuum Analytics",[11,52914,52915,52916,30978],{},"If you are doing scientific computing, statistical analysis or an type of machine learning, you will want to download the Anaconda distribution. This is Python bundled with a bunch of great packages and C libraries for doing heavy lifting. Anaconda uses a slightly different tool for managing virtual environments and package management, but it is very easy to do both using the ",[33,52917,44655],{},[911,52919,52921],{"id":52920},"postgresql","PostgreSQL",[26,52923,52926],{"className":52924,"code":52925,"language":31},[29],"sudo pacman -S postgresql\n",[33,52927,52925],{"__ignoreMap":35},[26,52929,52932],{"className":52930,"code":52931,"language":31},[29],"sudo -u postgres -i\n",[33,52933,52931],{"__ignoreMap":35},[11,52935,52936],{},"This switches you to the PostgreSQL user.",[11,52938,52939],{},"Before PostgreSQL can function correctly, the database cluster must be initialized:",[26,52941,52944],{"className":52942,"code":52943,"language":31},[29],"initdb --locale $LANG -E UTF8 -D '/var/lib/postgres/data'\n",[33,52945,52943],{"__ignoreMap":35},[11,52947,52948],{},"You should see the following:",[26,52950,52953],{"className":52951,"code":52952,"language":31},[29],"[postgres@archthinkpad ~]$ initdb --locale $LANG -E UTF8 -D '/var/lib/postgres/data'\nThe files belonging to this database system will be owned by user \"postgres\".\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale \"en_US.UTF-8\".\nThe default text search configuration will be set to \"english\".\n\nData page checksums are disabled.\n\nfixing permissions on existing directory /var/lib/postgres/data ... ok\ncreating subdirectories ... ok\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting dynamic shared memory implementation ... posix\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\n\nWARNING: enabling \"trust\" authentication for local connections\nYou can change this by editing pg_hba.conf or using the option -A, or\n--auth-local and --auth-host, the next time you run initdb.\n\nSuccess. You can now start the database server using:\n\n    pg_ctl -D /var/lib/postgres/data -l logfile start\n",[33,52954,52952],{"__ignoreMap":35},[11,52956,52957,52958,52961],{},"Now type ",[33,52959,52960],{},"exit"," to return to your regular user.",[11,52963,52725,52964,52967],{},[33,52965,52966],{},"postgresql.service"," as root:",[26,52969,52972],{"className":52970,"code":52971,"language":31},[29],"[brian@archthinkpad ~]$ sudo systemctl start postgresql.service\n[sudo] password for brian:\n[brian@archthinkpad ~]$ sudo systemctl enable postgresql.service\nCreated symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /usr/lib/systemd/system/postgresql.service.\n[brian@archthinkpad ~]$\n",[33,52973,52971],{"__ignoreMap":35},[11,52975,52976,52977,52982],{},"See the ",[15,52978,52981],{"href":52979,"rel":52980},"https://wiki.archlinux.org/index.php/PostgreSQL",[19],"Arch Wiki article on PostgreSQL"," for more information.",[911,52984,52985],{"id":52985},"tmux",[26,52987,52990],{"className":52988,"code":52989,"language":31},[29],"sudo pacman -S tmux\n",[33,52991,52989],{"__ignoreMap":35},[911,52993,33936],{"id":33936},[26,52995,52998],{"className":52996,"code":52997,"language":31},[29],"sudo pacman -S tree\n",[33,52999,52997],{"__ignoreMap":35},[911,53001,53003],{"id":53002},"languages-and-input-sources","Languages and Input Sources",[11,53005,53006],{},"Here is how to add support for Chinese characters and also Chinese pinyin input:",[26,53008,53011],{"className":53009,"code":53010,"language":31},[29],"yaourt -S adobe-source-han-sans-cn-fonts\n",[33,53012,53010],{"__ignoreMap":35},[11,53014,53015],{},"This will make Chinese characters visible. In order to type Chinese using the pinyin input method, run the following commands:",[26,53017,53020],{"className":53018,"code":53019,"language":31},[29],"sudo pacman -S ibus\nsudo pacman -S ibus-libpinyin\n",[33,53021,53019],{"__ignoreMap":35},[11,53023,53024,53025,53028],{},"可以打汉字了！其它语言和文字 packages 可以参考",[15,53026,53027],{"href":35},"这个 Arch Wiki 文章","。",[11,53030,53031],{},"Next you can go into:",[26,53033,53036],{"className":53034,"code":53035,"language":31},[29],"Settings > Region & Language > Input Source > + > Other\n",[33,53037,53035],{"__ignoreMap":35},[11,53039,53040],{},"Select Chinese (Intelligent Pin Yin) from the list and you should see a language menu in the top bar. At this point I was able to see both English and Chinese in Language menu in the top bar, but the Chinese pinyin only worked after rebooting.",[911,53042,53044],{"id":53043},"spotify","Spotify",[11,53046,53047],{},"Get some tunes going with the Spotfy app. It is not officially supported, but it seems to work alright. Facebook login did not work for me. You can create a new account with your existing email like this:",[26,53049,53052],{"className":53050,"code":53051,"language":31},[29],"youremail+spotify@email.com\n",[33,53053,53051],{"__ignoreMap":35},[11,53055,53056],{},"Using my main email to sign up for a new account didn't work because that email was linked to the Facebook login.",[26,53058,53061],{"className":53059,"code":53060,"language":31},[29],"yaourt -S spotify\n",[33,53062,53060],{"__ignoreMap":35},[911,53064,53066],{"id":53065},"word-processing-libre-office","Word Processing (Libre Office)",[11,53068,53069],{},"Libre Office is an open-source Office Suite similar in functionality to Microsoft Word.",[26,53071,53074],{"className":53072,"code":53073,"language":31},[29],"sudo pacman -S libreoffice-fresh\n",[33,53075,53073],{"__ignoreMap":35},[11,53077,53078],{},"You can also install any language packs you may need for Libre Office:",[26,53080,53083],{"className":53081,"code":53082,"language":31},[29],"yaourt -S libreoffice-fresh-zh-CN\n",[33,53084,53082],{"__ignoreMap":35},[911,53086,53088],{"id":53087},"disk-usage-visualizations","Disk Usage Visualizations",[11,53090,53091,53092,53095,53096,1172,53099,53102],{},"You can get disk usage with the ",[33,53093,53094],{},"du"," command. There are other programs for disk space visualizations, so far I have found ",[33,53097,53098],{},"gdmap",[33,53100,53101],{},"filelight"," to be helpful. Both are available in the AUR:",[26,53104,53107],{"className":53105,"code":53106,"language":31},[29],"yaourt -S gdmap filelight\n",[33,53108,53106],{"__ignoreMap":35},[911,53110,53112],{"id":53111},"sign-up-for-the-arch-wiki","Sign up for the Arch Wiki",[11,53114,53115],{},"The best way to ask for help is to ask the community. You should sign up for the Arch Wiki and post any questions you have after doing research and trying a few different solutions. The more detailed you are in your post, the better help you will get.",[168,53117,53119],{"id":53118},"miscelaneous-items","Miscelaneous Items",[11,53121,53122],{},"I have experienced unexpected behavior with some programs. Here's a list of some major issues and workarounds I have found:",[916,53124,53125],{},[919,53126,53127,53130,53131,53136],{},[338,53128,53129],{},"Google Hangouts",": I use Google Hangouts a lot for screen sharing and video conferencing. Trying to share my screen only showed a black screen with a cursor, although sharing invidual windows seemed to work fine (similar to what was happening with OBS). I found a ",[15,53132,53135],{"href":53133,"rel":53134},"https://superuser.com/questions/1166765/google-hangouts-screen-share-black-screen-error",[19],"Superuser thread"," describing the same issue. The solution is simply to select an Xorg session on login (instead of GNOME Classic or regular GNOME session). This is something I still don't know much about, but the solution worked for me and I have been able to share my entire screen in Google Hangouts.",[11,53138,53139],{},"You can check which session type you are using with the following command:",[26,53141,53144],{"className":53142,"code":53143,"language":31},[29],"echo $XDG_SESSION_TYPE\n",[33,53145,53143],{"__ignoreMap":35},[11,53147,53148,53149,15754,53152,752],{},"and you should either see ",[33,53150,53151],{},"x11",[33,53153,53154],{},"wayland",[911,53156,53158],{"id":53157},"keyboard-shortcuts","Keyboard shortcuts",[11,53160,53161],{},"You can configure keyboard shortcuts by going into:",[26,53163,53166],{"className":53164,"code":53165,"language":31},[29],"Settings > Keyboard\n",[33,53167,53165],{"__ignoreMap":35},[11,53169,53170,53171,53174],{},"Search for or find the setting that says \"Hide all normal windows\". I set mine to ",[33,53172,53173],{},"Shift + Alt + D",". You can configure other keyboard shortcuts here.",[911,53176,53178],{"id":53177},"night-light","Night Light",[11,53180,53181],{},"If you are coming from Mac or Windows, you might be used to using f.lux to dim the blue colors from your screen. There are few options for Linux such as Red Shift and xflux. I finf that in GNOME, using the built in Nigh Light works the best for reduce eye strain. Just go into:",[26,53183,53186],{"className":53184,"code":53185,"language":31},[29],"Settings > Displays\n",[33,53187,53185],{"__ignoreMap":35},[11,53189,53190],{},"and click on Night Light. Like flux, you can set it for regular hours. There aren't as many modes or options, but it does a pretty goog job of what I need it to do.",[911,53192,53194],{"id":53193},"helpful-commands","Helpful Commands",[11,53196,53197],{},"To view all available commands, run the following command:",[26,53199,53202],{"className":53200,"code":53201,"language":31},[29],"compgen -c\n",[33,53203,53201],{"__ignoreMap":35},[11,53205,53206],{},"To view all installed packages, run:",[26,53208,53211],{"className":53209,"code":53210,"language":31},[29],"pacman -Q\n",[33,53212,53210],{"__ignoreMap":35},[168,53214,53216],{"id":53215},"conclusion-and-next-steps","Conclusion and Next Steps",[11,53218,53219],{},"Installing Arch Linux is like trying to build a house with just a spoon. You won't get very far with a spoon, but you have the benefit of being able to order unlimited copies of free parts and materials online (the AUR, or Arch User Repository), and a great community of people that are extremely knowledgable about \"building houses\". Pretty soon you are able to quickly put together a foundation, scaffolding, wiring, appliances and yes even wallpaper. Arch Linux stays with its rolling release cycle, so you don't have to rebuild your house every 9 months to stay current.",[11,53221,53222],{},"Arch Linux is a lot of work to set up compared to popular Linux distributions like Ubuntu. Just like everyone says, you learn a lot by going through the process, breaking things and starting the installations process from scratch. I totally messed up my permissions while installing and quickly found that the only solutions was to reinstall Arch. This guide is primarily for personal use, and I am sure there are things that can be improved and even done completely differently. Here's a list of things I can start with:",[916,53224,53225,53241,53247,53253,53259,53265,53271,53277,53283,53289,53297,53300,53303],{},[919,53226,53227,585,53230,53237,53238,1737],{},[338,53228,53229],{},"Fix OBS Studio",[53231,53232,53233,53234],"del",{},"I got this to work in my earlier install and can't seem to remember how I fixed it. Currently OBS displays a black screen when set to ",[33,53235,53236],{},"Screen Capture",". I finally got OBS studio to work and I think the issue was as simple as rebooting (or possibly an update with ",[33,53239,53240],{},"sudo pacman -Syu",[919,53242,53243,53246],{},[338,53244,53245],{},"Encrypting the home folder",": this is good practice and will make my computer more secure, it shouldnt be too difficult either.",[919,53248,53249,53252],{},[338,53250,53251],{},"Adding a boot partition and fixing GRUB",": the guides I worked off of didn't include this, and I think it would be very important to figure out how this works if I want to include Windows 10 for doing Windows-specific tasks on my laptop as well without having to go into the bios each time I want to switch OSs.",[919,53254,53255,53258],{},[338,53256,53257],{},"Using HDDs",": On my desktop I would like to be able to figure out how to mount my HDD that I use for mass file storage on my Windows machine. For simplicity I have kept everything on one SSD, but it would be good to figure out how to easily add additional disks at boot time.",[919,53260,53261,53264],{},[338,53262,53263],{},"NVIDIA drivers",": this does not apply to my laptop, but I would like to figure out how I can get the best drivers in Arch Linux for my NVIDIA GPU on my desktop machine. This is one thing that running Linux in a Virtual Machine really doesn't allow you to do (pass a GPU through a VM), as far as I know.",[919,53266,53267,53270],{},[338,53268,53269],{},"All other drivers",": I still have lots of questions about how to make sure that I am running things properly on my desktop PC. I feel like drivers for the laptop install were pretty automatic, but this may not be the case with additional hardware components on my desktop, such a closed-loop water cooler.",[919,53272,53273,53276],{},[338,53274,53275],{},"Additional Customization, Themes, Window Managers, etc.",": There is so much that can be done with Arch Linux in terms of GUI. I love the setup I have but it would be interesting to explore some additional options like i3, for example.",[919,53278,53279,53282],{},[338,53280,53281],{},"Cusomt kernels",": I am also interested in learning how I could swap out kernels. I have seen people add LTS kernels to Arch Linux for various reasons, and I'm interested to learn how this works.",[919,53284,53285,53288],{},[338,53286,53287],{},"Maximzing battery life",": During the install process I saw the topic discussed but didn't look into it. It would be nice to see if I could make changes to get more out of the battery in my refurbished ThinkPad.",[919,53290,53291,585,53294],{},[338,53292,53293],{},"Other areas for improvement",[187,53295,53296],{},"add here",[919,53298,53299],{},"Install RVM",[919,53301,53302],{},"Install EMACS",[919,53304],{},{"title":35,"searchDepth":249,"depth":249,"links":53306},[53307,53351,53355,53376,53381],{"id":50834,"depth":249,"text":50835,"children":53308},[53309,53311,53312,53313,53314,53315,53317,53318,53319,53320,53321,53322,53323,53324,53325,53326,53327,53328,53329,53330,53331,53332,53333,53334,53335,53336,53337,53338,53339,53340,53341,53342,53343,53344,53345,53346,53347,53348,53349,53350],{"id":50938,"depth":312,"text":53310},"Using fdisk for partitioning",{"id":51000,"depth":312,"text":51001},{"id":51029,"depth":312,"text":51030},{"id":51062,"depth":312,"text":51063},{"id":51110,"depth":312,"text":51111},{"id":51129,"depth":312,"text":53316},"Verify our layout with lsblk",{"id":51141,"depth":312,"text":51142},{"id":51166,"depth":312,"text":51167},{"id":51179,"depth":312,"text":51180},{"id":51221,"depth":312,"text":51222},{"id":51257,"depth":312,"text":51258},{"id":51285,"depth":312,"text":51286},{"id":51310,"depth":312,"text":51311},{"id":51379,"depth":312,"text":51380},{"id":51389,"depth":312,"text":51390},{"id":51406,"depth":312,"text":51407},{"id":51430,"depth":312,"text":51431},{"id":51451,"depth":312,"text":51452},{"id":51464,"depth":312,"text":51465},{"id":51477,"depth":312,"text":51478},{"id":51510,"depth":312,"text":51511},{"id":51520,"depth":312,"text":51521},{"id":51596,"depth":312,"text":51597},{"id":51646,"depth":312,"text":51647},{"id":51659,"depth":312,"text":51660},{"id":51695,"depth":312,"text":51696},{"id":51756,"depth":312,"text":51757},{"id":51766,"depth":312,"text":51767},{"id":51864,"depth":312,"text":51865},{"id":51886,"depth":312,"text":51887},{"id":51896,"depth":312,"text":51897},{"id":51906,"depth":312,"text":51907},{"id":51916,"depth":312,"text":51917},{"id":51976,"depth":312,"text":51977},{"id":52032,"depth":312,"text":52033},{"id":52173,"depth":312,"text":52174},{"id":52237,"depth":312,"text":52238},{"id":52293,"depth":312,"text":52294},{"id":52307,"depth":312,"text":52308},{"id":52317,"depth":312,"text":52318},{"id":52324,"depth":249,"text":52325,"children":53352},[53353,53354],{"id":52345,"depth":312,"text":52346},{"id":52486,"depth":312,"text":52487},{"id":52499,"depth":249,"text":52500,"children":53356},[53357,53358,53359,53360,53361,53362,53363,53364,53366,53367,53368,53369,53370,53371,53372,53373,53374,53375],{"id":52503,"depth":312,"text":52504},{"id":52536,"depth":312,"text":50874},{"id":52564,"depth":312,"text":52565},{"id":52583,"depth":312,"text":52584},{"id":52685,"depth":312,"text":52686},{"id":12985,"depth":312,"text":12986},{"id":52737,"depth":312,"text":52738},{"id":52747,"depth":312,"text":53365},"autoenv (GitHub)",{"id":52771,"depth":312,"text":52772},{"id":1125,"depth":312,"text":34532},{"id":52920,"depth":312,"text":52921},{"id":52985,"depth":312,"text":52985},{"id":33936,"depth":312,"text":33936},{"id":53002,"depth":312,"text":53003},{"id":53043,"depth":312,"text":53044},{"id":53065,"depth":312,"text":53066},{"id":53087,"depth":312,"text":53088},{"id":53111,"depth":312,"text":53112},{"id":53118,"depth":249,"text":53119,"children":53377},[53378,53379,53380],{"id":53157,"depth":312,"text":53158},{"id":53177,"depth":312,"text":53178},{"id":53193,"depth":312,"text":53194},{"id":53215,"depth":249,"text":53216},"2017-08-03","A comprehensive guide to installing Arch Linux","/static/aur/arch.png",{"layout":29014},"/2017/08/03/arch-linux-installation-guide",{"title":50820,"description":53383},"2017/08/03/arch-linux-installation-guide",[45897],"l7wFYBuinN5MITDJN77MHPUHs20PAli0KzIvWyGhogE",{"id":53392,"title":53393,"body":53394,"comments":315,"date":53425,"description":53426,"draft":872,"extension":873,"external":874,"image":53409,"meta":53427,"navigation":315,"path":53428,"seo":53429,"stem":53430,"tags":53431,"__hash__":53433},"blog/2017/05/09/my-first-attempt-at-photogrammetry.md","Building a 3D model from 60 photographs with VisualSFM and Meshlab",{"type":8,"value":53395,"toc":53423},[53396,53405,53410,53413],[11,53397,53398,53399,53404],{},"I found an interesting ",[15,53400,53403],{"href":53401,"rel":53402},"http://wedidstuff.heavyimage.com/index.php/2013/07/12/open-source-photogrammetry-workflow/",[19],"photogrammetry tutorial"," and decided to take a shot at building a 3D model of a sun hat. Here's the result:",[11,53406,53407],{},[511,53408],{"alt":7255,"src":53409},"/static/sunhat.png",[11,53411,53412],{},"And here is an interactive model that you can view in 3D:",[10229,53414,53418],{"margin":53415,"className":53416},"auto",[53417],"sketchfab-embed-wrapper",[10159,53419],{"width":53420,"height":53421,"src":53422,"frameBorder":10165,"allowvr":35,"allowFullScreen":315,"mozallowfullscreen":6630,"webkitallowfullscreen":6630},640,480,"https://sketchfab.com/models/ff952a9d9cae4d26a178ad74e099e96b/embed",{"title":35,"searchDepth":249,"depth":249,"links":53424},[],"2017-05-09","My first attempt at photogrammetry",{"layout":29014},"/2017/05/09/my-first-attempt-at-photogrammetry",{"title":53393,"description":53426},"2017/05/09/my-first-attempt-at-photogrammetry",[53432],"photogrammetry","QfEWaUo2T0pIAclGYgcFXQUifDRTCubKvI25kYYVfUc",{"id":53435,"title":53436,"body":53437,"comments":315,"date":53425,"description":53465,"draft":872,"extension":873,"external":874,"image":53452,"meta":53466,"navigation":315,"path":53467,"seo":53468,"stem":53469,"tags":53470,"__hash__":53474},"blog/2017/05/09/rendering-sketchup-models-with-kerkythea.md","Rendering SketchUp models with Kerkythea",{"type":8,"value":53438,"toc":53463},[53439,53448,53453,53458],[11,53440,53441,53442,53447],{},"These are some results of an architectural model I rendered with Kerkythea. ",[15,53443,53446],{"href":53444,"rel":53445},"http://www.kerkythea.net/cms/",[19],"Kerkythea"," is an open source rendering program that has a SketchUp plugin. I worked with an architect who made the original plans by hand.",[11,53449,53450],{},[511,53451],{"alt":7255,"src":53452},"/static/sketchup/sketchup_1.jpg",[11,53454,53455],{},[511,53456],{"alt":7255,"src":53457},"/static/sketchup/sketchup_2.jpg",[11,53459,53460],{},[511,53461],{"alt":7255,"src":53462},"/static/sketchup/sketchup_3.jpg",{"title":35,"searchDepth":249,"depth":249,"links":53464},[],"These are some results of an architectural model I rendered with Kerkythea. Kerkythea is an open source rendering program that has a SketchUp plugin. I worked with an architect who made the original plans by hand.",{"layout":29014},"/2017/05/09/rendering-sketchup-models-with-kerkythea",{"title":53436,"description":53465},"2017/05/09/rendering-sketchup-models-with-kerkythea",[53471,53472,53473],"sketchup","kerkythea","3d-modeling","cInP-o-_HwdNry_m_XQD9OhxPGZ-IX6Z52dQFsSKImQ",{"id":53476,"title":53477,"body":53478,"comments":315,"date":57245,"description":57246,"draft":872,"extension":873,"external":874,"image":57247,"meta":57248,"navigation":315,"path":57249,"seo":57250,"stem":57251,"tags":57252,"__hash__":57254},"blog/2017/04/02/langton-ant-notebook.md","Python script for generating 2D n-state Langton's Ant animations",{"type":8,"value":53479,"toc":57243},[53480,53485,53497,53506,53511,53520,53535,53553,53562,53624,55031,55035,55042,55045,55101,55115,55124,55130,55139,55364,55375,55384,55393,55399,55402,55411,55420,55440,55449,55788,55797,55800,55840,55846,55851,55860,55866,55869,55878,55887,55893,55896,55905,55911,55914,55938,55944,55949,55952,55991,55997,56002,56011,56017,56030,56033,56042,56051,56057,56060,56069,56072,56100,56106,56111,56114,56123,56129,56138,56141,56324,56333,56339,56344,56353,56359,56364,56373,56379,56384,56393,56399,56404,56413,56419,56424,56433,56439,56444,56453,56459,56464,56473,56479,56484,56493,56499,56504,56507,56516,56522,56527,56535,56540,56545,56554,56560,56565,56574,56579,56584,56593,56599,56604,56613,56619,56624,56633,56639,56644,56653,56659,56664,56673,56679,56684,56693,56699,56704,56713,56719,56724,56733,56739,56744,56753,56759,56764,56772,56777,56782,56791,56797,56802,56811,56817,56822,56831,56837,56842,56851,56857,56862,56870,56875,56880,56889,56895,56900,56909,56915,56920,56929,56935,56940,56949,56955,56960,56969,56975,56980,56989,56995,57000,57009,57015,57020,57029,57035,57040,57049,57055,57060,57063,57067,57070,57079,57088,57097,57121,57130,57136,57139,57148,57157,57163,57166,57181,57186,57191,57206,57211,57216,57219,57221,57224,57241],[11,53481,53482],{},[511,53483],{"alt":7255,"src":53484},"/static/LLRRRLRRRRR.png",[11,53486,53487,53488,53493,53494,752],{},"This is an old project that I would like to refactor. I'm copying the contents of ",[15,53489,53492],{"href":53490,"rel":53491},"https://github.com/briancaffey/cellular-automata/blob/master/ants.ipynb",[19],"this Jupyter notebook"," into this article with the ",[33,53495,53496],{},"jupyter nbconvert ants.ipynb --to markdown",[11,53498,53499,53500,53505],{},"This notebook explores a type of Turing Machine known as ",[15,53501,53504],{"href":53502,"rel":53503},"https://en.wikipedia.org/wiki/Turmite",[19],"termites",". The first part is a script I wrote a few years ago when I was first learning Python. If you are new to learning Python, I suggest you give it a try before reading the script; there's a lot you will learn about flow control and data structures. My script is far from perfect and every time I come back to it there is an idiom I can add and areas that can be refactored and cleaned up. It generates images of 2-dimensional n-state termites on an $a$ x $b$ rectangular grid, or it can generate multiple images (frames) of a single termite as it grows to make a video. Here's an example of a termite animatino that I made using the script below:",[10159,53507],{"width":10161,"height":27000,"src":53508,"frameBorder":10165,"gesture":53509,"allow":53510,"allowFullScreen":315},"https://www.youtube-nocookie.com/embed/Du2DorTLAo4?rel=0","media","encrypted-media",[11,53512,53513,53514,53519],{},"The type of termite explored here is a modified version of a type of cellular automata known as ",[15,53515,53518],{"href":53516,"rel":53517},"https://en.wikipedia.org/wiki/Langton%27s_ant",[19],"Langton's Ant",". Langton's Ant has a simple ruleset: an ant is placed on a 2-dimensional grid of 2-state cells (black or white) with a directional orientation. If the ant is on a black cell at $t=n$, the ant enters the cell on the immediate left at $t=n+1$ and the state of the cell it exits changes to white. If the state of the cell that the ant enters is white, the ant enters the cell immediately to the right and the cell it exits turns black. Around 11,000 steps, the ant enters a 'highway' which results in a repeated motion that moves the ant continually in one direction.",[11,53521,53522,53523,53526,53527,53530,53531,53534],{},"Instead of black and white cells, we can define $n$ number of states (colors) and assign any combination of $n$ instructions (eg. LRLLLRLLLRL). The script below generates generates an arbitrary number of ",[33,53524,53525],{},"ants",". Each number in ",[33,53528,53529],{},"range(ants)"," is converted to binary and then 1s and 0s of the corresponding binary number represent the left and right turns for each individual ant. For example: ",[33,53532,53533],{},"bin(23)"," corresponds to a 5-state ant with the following rules: RLRRR. This method avoids generating isotropes (RLRRR is the same ant as LRLLL).",[11,53536,53537,53538,28395,53541,53543,53544,53547,53548,752],{},"If ",[33,53539,53540],{},"record",[33,53542,44250],{},", one frame will be captured every ",[33,53545,53546],{},"frame_interval"," number of steps. These images can be converted into video easily with open-source programs like ",[15,53549,53552],{"href":53550,"rel":53551},"https://www.blender.org/",[19],"Blender",[11,53554,53555,53556,53561],{},"The last part of the notebook attempts to use new methods from the latest version of scikit-learn (",[15,53557,53560],{"href":53558,"rel":53559},"http://scikit-learn.org/stable/whats_new.html",[19],"0.18.1",") to cluster ants by their behavior: k-means (for clustering) and Isolation Forests (for detecting outliers).",[26,53563,53565],{"className":1383,"code":53564,"language":1125,"meta":35,"style":35},"import PIL\nfrom PIL import Image\nimport random\nimport os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom __future__ import print_function\nfrom sklearn import cluster\nimport seaborn as sns\n%matplotlib inline\n",[33,53566,53567,53571,53575,53579,53583,53587,53592,53596,53601,53605,53610,53615,53620],{"__ignoreMap":35},[187,53568,53569],{"class":189,"line":190},[187,53570,26193],{},[187,53572,53573],{"class":189,"line":249},[187,53574,26048],{},[187,53576,53577],{"class":189,"line":312},[187,53578,26012],{},[187,53580,53581],{"class":189,"line":319},[187,53582,10345],{},[187,53584,53585],{"class":189,"line":325},[187,53586,37567],{},[187,53588,53589],{"class":189,"line":686},[187,53590,53591],{},"import pandas as pd\n",[187,53593,53594],{"class":189,"line":697},[187,53595,10563],{},[187,53597,53598],{"class":189,"line":1291},[187,53599,53600],{},"import scipy\n",[187,53602,53603],{"class":189,"line":1306},[187,53604,26035],{},[187,53606,53607],{"class":189,"line":1434},[187,53608,53609],{},"from __future__ import print_function\n",[187,53611,53612],{"class":189,"line":2599},[187,53613,53614],{},"from sklearn import cluster\n",[187,53616,53617],{"class":189,"line":2607},[187,53618,53619],{},"import seaborn as sns\n",[187,53621,53622],{"class":189,"line":2621},[187,53623,26216],{},[26,53625,53627],{"className":1383,"code":53626,"language":1125,"meta":35,"style":35},"#this script generates an image (or a series of images) for n-state 2D Langton's Ant cellular automaton.\n\n#SETTINGS\n\n#number of ants to run\nants = 65536\n#ants = 100\n#set record to True to record frames once every frame_interval steps\nrecord = False\nframe_interval = 5000\n\n#Boolean for recording final image\nrecord_final_image = False\n\n#set scale to scale the resulting images in save_image(i) function\nscale = 1\n\n#set the length and width of the square image canvas\nwidth = int(200)\nlength = int(200)\n\n#initialize ant in the center of the grid\n#grid contains length_width**2 cells\nant_pos = int((length*width)/2) + int(width/2)\n\n#boolean to check if the ant touches the border (out of bounds)\noob = False\n\n#number of steps that the ant will take on each walk\niterations = 100000\n\n#for naming the image file below\nnumber = str(iterations)\n\n#set the direction of the ant's first step: 1 --> Right; -1 --> Left. Eliminates mirror images (isotropes) from dataset\ndirection = 1\n\n#Ininitialize a blank square image\nim1 = Image.new('RGBA', (width,length),'white')\n\n#color selection\nwhite = (255,255,255,255)\nred = (255,0,0,255)\norange = (255,128,0,255)\nyellow = (255,255,0,255)\nyellow_green = (128,255,0,255)\ngreen = (0,255,0,255)\nteal = (0,255,255,255)\nlight_blue = (0,128,255,255)\nblue = (0,0,255,255)\npurple = (127,0,255,255)\nblack = (0,0,0,255)\ngrey = (150,150,150,255)\nother = (40,100,50,255)\nbrown = (130,90,44,255)\npink = (244,114,208,255)\nmauve = (118,96,138,255)\nmagenta = (216,0,115,255)\n\ncolor_choices = [red, orange, yellow, light_blue, yellow_green, blue, purple, black, grey, green, teal, light_blue, other, brown, pink, mauve, magenta]\n\n#convert an integer to binary and then convert\ndef num_to_string(num):\n    binary = bin(num)\n    moves = \"\"\n    for x in str(binary)[2:]:\n        if x == '1':\n            moves += \"R\"\n        else:\n            moves += \"L\"\n    return moves\n\n#moves list includes all 16 length moves\n#moves_list = [num_to_string(ant) for ant in range(32768,65536)]\n\n#defines the list of strings that is used for the main loop bellow\nmoves_list = [num_to_string(ant) for ant in range(ants)]\n\n#a list of the dictionaries to by passed into the pandas dataframe for later analysis\ndf_row_list = []\n\n#dataframe object for later analysis\ndf = pd.DataFrame() #index=[0]\n\n#functions for moving the postition of the ant right, left, up or down\ndef move_right():\n    global ant_pos\n    #move ant_pos one pixel to the right\n    ant_pos += 1\n    return ant_pos\ndef move_left():\n    global ant_pos\n    #move ant_pos one pixel to the left\n    ant_pos -= 1\n    return ant_pos\ndef move_up():\n    global ant_pos\n    #move ant_pos one pixel up\n    ant_pos += width\n    return ant_pos\ndef move_down():\n    global ant_pos\n    #move ant_pos one pixel down\n    ant_pos -= width\n    return ant_pos\n\ndef move(color,d):\n    global direction\n    while True:\n        #this part is a little confusing and may need to be rewritten\n        #it uses the current direction of the ant to determine the appropriate direction for the next turn\n        #breaks are used\n        if pix_list[ant_pos][2] == color and direction == width*d:\n            #set the color to the next color in the list, or loop back to the beginning of the list if the end has been reached\n            pix_list[ant_pos][2] = (color + 1) % len(pixel_colors)\n            #save the current postion of the ant\n            init = ant_pos\n            #move the ant\n            move_right()\n            #save the updated position of the ant\n            end = ant_pos\n            #calculate the new direction of the ant by taking the difference between end and init\n            direction = end - init\n            break\n\n        #same idea as above\n        elif pix_list[ant_pos][2] == color and direction == -1*width*d:\n            pix_list[ant_pos][2] = (color + 1) % len(pixel_colors)\n            init = ant_pos\n            move_left()\n            end = ant_pos\n            direction = end - init\n            break\n        #same idea as above\n        elif pix_list[ant_pos][2] == color and direction == 1*d:\n            pix_list[ant_pos][2] = (color + 1) % len(pixel_colors)\n            init = ant_pos\n            move_down()\n            end = ant_pos\n            direction = end - init\n            break\n        #same idea as above\n        elif pix_list[ant_pos][2] == color and direction == -1*d:\n            pix_list[ant_pos][2] = (color + 1) % len(pixel_colors)\n            init = ant_pos\n            move_up()\n            end = ant_pos\n            direction = end - init\n            break\n        break\n\n#captures series of pixels used for generating images\ndef get_pix_series():\n    global pix_series\n    pix_series = []\n    for x in range(len(pix_list)):\n        for y in range(len(pixel_colors)):\n            if pix_list[x][2] == y:\n                pixel = pixel_colors[y]\n                pix_series.append(pixel)\n\n#runs ant along the grid according to the moves (defined above) for the number of steps in iterations (defined above)\ndef run():\n    #variable the tracks the step number if the ant goes out of bounds\n    global oob\n    #converts moves string into a list of 0s and 1s; these numbers correspond to direction and are passed into the move() function\n    moves1 = [1 if x == 'R' else -1 for x in moves]\n    for step in range(iterations):\n        #exit the loop if the ant reaches the edge of the grid\n        if ant_pos \u003C width or ant_pos % width == 0:\n            oob = step\n            return\n        #loop through the moves\n        for index, direction in enumerate(moves1):\n            try:\n                #remember the ant position\n                not_moved = ant_pos\n                #try to move the ant position\n                move(index,direction)\n                #check to see if the position was moved\n                if ant_pos != not_moved:\n                    #set record to false in the settings to turn of frame recording\n                    if record == True:\n                        #records a new frame every frame_interval frame\n                        if step % frame_interval == 0:\n                            counter += 1\n                            print(\"Generating frame number \" + str(counter))\n                            get_pix_series()\n                            save_image(counter)\n                    break\n                else:\n                    continue\n            except:\n                #print(\"Out of bounds at step number \" + str(step))\n                oob = step\n                return\n\ndef save_image(i):\n    #give access to the image instantiated at the beginning of the script\n    global im1\n    #fill blank image canvas with pix_series pixel data\n    im1.putdata(pix_series)\n    #to rescale the image, set the scale variable in settings and call resize on im1\n    im1.resize((scale*im1.size[0],scale*im1.size[1])).save('%s.png' % (moves))\n\n#builds a dictionary to count pixels by color\ndef build_df_row():\n    colors_dict = {str(val): 0 for val, color in enumerate(pixel_colors)}\n    moves_dict = {'moves':moves}\n    last_step = {'last_step':oob}\n    row_dict = dict(colors_dict.items()+moves_dict.items()+last_step.items())\n    for x in pix_list:\n        pixel_color = str(x[2])\n        #print(pixel_color)\n        row_dict[pixel_color] += 1\n    return row_dict\n\n#uncomment below to overwrite moves_list\n#moves_list = ['LR', 'RRLR']\n\nfor _, moves in enumerate(moves_list):\n    oob = 0\n    dir_path = str(_)\n\n    #make a new directory for each new ant walk in walks based on the the walk number and navigate to that directory\n    if record == True:\n        #make a new directory to record frames for a give ant if record is set to true and that directory does not yet exist\n        if not os.path.isdir(dir_path):\n            os.makedirs(dir_path)\n        #otherwise just change into the directory\n        else:\n            os.chdir(dir_path)\n\n    #set ant at middle of grid\n    ant_pos = int((length*width)/2) + int(width/2)\n\n    #moves = len(moves)\n    pixel_colors = color_choices[:(len(moves))]\n\n    #defines an empty list of elements [x,y,0] where x amd y are the position 0 is the 0ht color in the color list (the base canvas color)\n    pix_list = []\n    for x in range(length):\n        for y in range(width):\n            a = [x,y,0]\n            pix_list.append(a)\n\n    #pix_series is a list of pixels that is passed into the put_data function to generate an image\n    pix_series = []\n\n    #counter keeps track of the frame number (if recording a series of images)\n    counter = 0\n\n    #run the ant\n    run()\n\n    #capture the final state of the grid with get_pix_series\n    get_pix_series()\n\n    #uncomment below to preview images for testing\n    #im1.putdata(pix_series)\n    #im1.resize((scale*im1.size[0],scale*im1.size[1])).show()\n\n    #build a dictionary with pixel counts\n    colors_dict = build_df_row()\n    row_df = pd.DataFrame(colors_dict, index=[0])\n    df = df.append(row_df, ignore_index=True)\n\n    if record_final_image == True:\n        os.chdir(os.path.expanduser('~/Documents/CA_1/imgs/'))\n        save_image(_)\n        os.chdir('../')\n\n    #summary\n    print(str(_), end=' ')\n\nos.chdir(os.path.expanduser('~/Documents/CA_1/'))\ndf.to_csv('ants_hist_.csv', index=False)\n",[33,53628,53629,53634,53638,53643,53647,53652,53657,53662,53667,53672,53677,53681,53686,53691,53695,53700,53705,53709,53714,53719,53724,53728,53733,53738,53743,53747,53752,53757,53761,53766,53771,53775,53780,53785,53789,53794,53799,53803,53808,53813,53817,53822,53827,53832,53837,53842,53847,53852,53857,53862,53867,53872,53877,53882,53887,53892,53897,53902,53907,53911,53916,53920,53925,53930,53935,53940,53945,53950,53955,53959,53964,53969,53973,53978,53983,53987,53992,53997,54001,54006,54011,54015,54020,54025,54029,54034,54039,54044,54049,54054,54059,54064,54068,54073,54078,54082,54087,54091,54096,54101,54105,54110,54114,54119,54124,54128,54132,54137,54142,54147,54152,54157,54162,54167,54172,54177,54182,54187,54192,54197,54202,54207,54212,54217,54222,54226,54231,54236,54240,54244,54249,54253,54257,54261,54265,54270,54274,54278,54283,54287,54291,54295,54299,54304,54308,54312,54317,54321,54325,54329,54334,54338,54343,54348,54353,54358,54363,54368,54373,54378,54383,54387,54392,54397,54402,54407,54412,54417,54422,54427,54432,54437,54442,54447,54452,54456,54461,54466,54471,54476,54481,54486,54491,54496,54501,54506,54511,54516,54521,54526,54531,54536,54541,54546,54552,54558,54564,54569,54575,54581,54587,54593,54599,54605,54611,54616,54622,54628,54634,54640,54646,54652,54658,54664,54670,54676,54682,54687,54693,54699,54704,54710,54716,54722,54727,54733,54739,54745,54751,54757,54763,54768,54774,54779,54785,54791,54796,54802,54808,54813,54819,54825,54831,54837,54843,54849,54854,54860,54865,54870,54876,54882,54887,54893,54899,54904,54910,54916,54921,54927,54933,54939,54944,54950,54956,54962,54968,54973,54979,54985,54991,54997,55002,55008,55014,55019,55025],{"__ignoreMap":35},[187,53630,53631],{"class":189,"line":190},[187,53632,53633],{},"#this script generates an image (or a series of images) for n-state 2D Langton's Ant cellular automaton.\n",[187,53635,53636],{"class":189,"line":249},[187,53637,316],{"emptyLinePlaceholder":315},[187,53639,53640],{"class":189,"line":312},[187,53641,53642],{},"#SETTINGS\n",[187,53644,53645],{"class":189,"line":319},[187,53646,316],{"emptyLinePlaceholder":315},[187,53648,53649],{"class":189,"line":325},[187,53650,53651],{},"#number of ants to run\n",[187,53653,53654],{"class":189,"line":686},[187,53655,53656],{},"ants = 65536\n",[187,53658,53659],{"class":189,"line":697},[187,53660,53661],{},"#ants = 100\n",[187,53663,53664],{"class":189,"line":1291},[187,53665,53666],{},"#set record to True to record frames once every frame_interval steps\n",[187,53668,53669],{"class":189,"line":1306},[187,53670,53671],{},"record = False\n",[187,53673,53674],{"class":189,"line":1434},[187,53675,53676],{},"frame_interval = 5000\n",[187,53678,53679],{"class":189,"line":2599},[187,53680,316],{"emptyLinePlaceholder":315},[187,53682,53683],{"class":189,"line":2607},[187,53684,53685],{},"#Boolean for recording final image\n",[187,53687,53688],{"class":189,"line":2621},[187,53689,53690],{},"record_final_image = False\n",[187,53692,53693],{"class":189,"line":2631},[187,53694,316],{"emptyLinePlaceholder":315},[187,53696,53697],{"class":189,"line":2642},[187,53698,53699],{},"#set scale to scale the resulting images in save_image(i) function\n",[187,53701,53702],{"class":189,"line":2653},[187,53703,53704],{},"scale = 1\n",[187,53706,53707],{"class":189,"line":2665},[187,53708,316],{"emptyLinePlaceholder":315},[187,53710,53711],{"class":189,"line":2674},[187,53712,53713],{},"#set the length and width of the square image canvas\n",[187,53715,53716],{"class":189,"line":2684},[187,53717,53718],{},"width = int(200)\n",[187,53720,53721],{"class":189,"line":2694},[187,53722,53723],{},"length = int(200)\n",[187,53725,53726],{"class":189,"line":2706},[187,53727,316],{"emptyLinePlaceholder":315},[187,53729,53730],{"class":189,"line":2715},[187,53731,53732],{},"#initialize ant in the center of the grid\n",[187,53734,53735],{"class":189,"line":2725},[187,53736,53737],{},"#grid contains length_width**2 cells\n",[187,53739,53740],{"class":189,"line":2735},[187,53741,53742],{},"ant_pos = int((length*width)/2) + int(width/2)\n",[187,53744,53745],{"class":189,"line":2743},[187,53746,316],{"emptyLinePlaceholder":315},[187,53748,53749],{"class":189,"line":2754},[187,53750,53751],{},"#boolean to check if the ant touches the border (out of bounds)\n",[187,53753,53754],{"class":189,"line":2762},[187,53755,53756],{},"oob = False\n",[187,53758,53759],{"class":189,"line":2770},[187,53760,316],{"emptyLinePlaceholder":315},[187,53762,53763],{"class":189,"line":2781},[187,53764,53765],{},"#number of steps that the ant will take on each walk\n",[187,53767,53768],{"class":189,"line":2792},[187,53769,53770],{},"iterations = 100000\n",[187,53772,53773],{"class":189,"line":2803},[187,53774,316],{"emptyLinePlaceholder":315},[187,53776,53777],{"class":189,"line":2808},[187,53778,53779],{},"#for naming the image file below\n",[187,53781,53782],{"class":189,"line":2816},[187,53783,53784],{},"number = str(iterations)\n",[187,53786,53787],{"class":189,"line":2824},[187,53788,316],{"emptyLinePlaceholder":315},[187,53790,53791],{"class":189,"line":2834},[187,53792,53793],{},"#set the direction of the ant's first step: 1 --> Right; -1 --> Left. Eliminates mirror images (isotropes) from dataset\n",[187,53795,53796],{"class":189,"line":2845},[187,53797,53798],{},"direction = 1\n",[187,53800,53801],{"class":189,"line":2856},[187,53802,316],{"emptyLinePlaceholder":315},[187,53804,53805],{"class":189,"line":2867},[187,53806,53807],{},"#Ininitialize a blank square image\n",[187,53809,53810],{"class":189,"line":2878},[187,53811,53812],{},"im1 = Image.new('RGBA', (width,length),'white')\n",[187,53814,53815],{"class":189,"line":2886},[187,53816,316],{"emptyLinePlaceholder":315},[187,53818,53819],{"class":189,"line":2900},[187,53820,53821],{},"#color selection\n",[187,53823,53824],{"class":189,"line":2905},[187,53825,53826],{},"white = (255,255,255,255)\n",[187,53828,53829],{"class":189,"line":2913},[187,53830,53831],{},"red = (255,0,0,255)\n",[187,53833,53834],{"class":189,"line":2921},[187,53835,53836],{},"orange = (255,128,0,255)\n",[187,53838,53839],{"class":189,"line":2931},[187,53840,53841],{},"yellow = (255,255,0,255)\n",[187,53843,53844],{"class":189,"line":2942},[187,53845,53846],{},"yellow_green = (128,255,0,255)\n",[187,53848,53849],{"class":189,"line":2953},[187,53850,53851],{},"green = (0,255,0,255)\n",[187,53853,53854],{"class":189,"line":2964},[187,53855,53856],{},"teal = (0,255,255,255)\n",[187,53858,53859],{"class":189,"line":2975},[187,53860,53861],{},"light_blue = (0,128,255,255)\n",[187,53863,53864],{"class":189,"line":2983},[187,53865,53866],{},"blue = (0,0,255,255)\n",[187,53868,53869],{"class":189,"line":2992},[187,53870,53871],{},"purple = (127,0,255,255)\n",[187,53873,53874],{"class":189,"line":3001},[187,53875,53876],{},"black = (0,0,0,255)\n",[187,53878,53879],{"class":189,"line":3010},[187,53880,53881],{},"grey = (150,150,150,255)\n",[187,53883,53884],{"class":189,"line":3019},[187,53885,53886],{},"other = (40,100,50,255)\n",[187,53888,53889],{"class":189,"line":3028},[187,53890,53891],{},"brown = (130,90,44,255)\n",[187,53893,53894],{"class":189,"line":3033},[187,53895,53896],{},"pink = (244,114,208,255)\n",[187,53898,53899],{"class":189,"line":3041},[187,53900,53901],{},"mauve = (118,96,138,255)\n",[187,53903,53904],{"class":189,"line":3049},[187,53905,53906],{},"magenta = (216,0,115,255)\n",[187,53908,53909],{"class":189,"line":3059},[187,53910,316],{"emptyLinePlaceholder":315},[187,53912,53913],{"class":189,"line":3070},[187,53914,53915],{},"color_choices = [red, orange, yellow, light_blue, yellow_green, blue, purple, black, grey, green, teal, light_blue, other, brown, pink, mauve, magenta]\n",[187,53917,53918],{"class":189,"line":3075},[187,53919,316],{"emptyLinePlaceholder":315},[187,53921,53922],{"class":189,"line":3083},[187,53923,53924],{},"#convert an integer to binary and then convert\n",[187,53926,53927],{"class":189,"line":3091},[187,53928,53929],{},"def num_to_string(num):\n",[187,53931,53932],{"class":189,"line":3101},[187,53933,53934],{},"    binary = bin(num)\n",[187,53936,53937],{"class":189,"line":3111},[187,53938,53939],{},"    moves = \"\"\n",[187,53941,53942],{"class":189,"line":3122},[187,53943,53944],{},"    for x in str(binary)[2:]:\n",[187,53946,53947],{"class":189,"line":3132},[187,53948,53949],{},"        if x == '1':\n",[187,53951,53952],{"class":189,"line":3143},[187,53953,53954],{},"            moves += \"R\"\n",[187,53956,53957],{"class":189,"line":3151},[187,53958,23551],{},[187,53960,53961],{"class":189,"line":3161},[187,53962,53963],{},"            moves += \"L\"\n",[187,53965,53966],{"class":189,"line":3170},[187,53967,53968],{},"    return moves\n",[187,53970,53971],{"class":189,"line":3178},[187,53972,316],{"emptyLinePlaceholder":315},[187,53974,53975],{"class":189,"line":3185},[187,53976,53977],{},"#moves list includes all 16 length moves\n",[187,53979,53980],{"class":189,"line":3195},[187,53981,53982],{},"#moves_list = [num_to_string(ant) for ant in range(32768,65536)]\n",[187,53984,53985],{"class":189,"line":3205},[187,53986,316],{"emptyLinePlaceholder":315},[187,53988,53989],{"class":189,"line":3210},[187,53990,53991],{},"#defines the list of strings that is used for the main loop bellow\n",[187,53993,53994],{"class":189,"line":3216},[187,53995,53996],{},"moves_list = [num_to_string(ant) for ant in range(ants)]\n",[187,53998,53999],{"class":189,"line":3224},[187,54000,316],{"emptyLinePlaceholder":315},[187,54002,54003],{"class":189,"line":3234},[187,54004,54005],{},"#a list of the dictionaries to by passed into the pandas dataframe for later analysis\n",[187,54007,54008],{"class":189,"line":3242},[187,54009,54010],{},"df_row_list = []\n",[187,54012,54013],{"class":189,"line":3252},[187,54014,316],{"emptyLinePlaceholder":315},[187,54016,54017],{"class":189,"line":3260},[187,54018,54019],{},"#dataframe object for later analysis\n",[187,54021,54022],{"class":189,"line":3270},[187,54023,54024],{},"df = pd.DataFrame() #index=[0]\n",[187,54026,54027],{"class":189,"line":3275},[187,54028,316],{"emptyLinePlaceholder":315},[187,54030,54031],{"class":189,"line":3283},[187,54032,54033],{},"#functions for moving the postition of the ant right, left, up or down\n",[187,54035,54036],{"class":189,"line":3291},[187,54037,54038],{},"def move_right():\n",[187,54040,54041],{"class":189,"line":3300},[187,54042,54043],{},"    global ant_pos\n",[187,54045,54046],{"class":189,"line":3310},[187,54047,54048],{},"    #move ant_pos one pixel to the right\n",[187,54050,54051],{"class":189,"line":3320},[187,54052,54053],{},"    ant_pos += 1\n",[187,54055,54056],{"class":189,"line":3325},[187,54057,54058],{},"    return ant_pos\n",[187,54060,54061],{"class":189,"line":3333},[187,54062,54063],{},"def move_left():\n",[187,54065,54066],{"class":189,"line":3343},[187,54067,54043],{},[187,54069,54070],{"class":189,"line":3354},[187,54071,54072],{},"    #move ant_pos one pixel to the left\n",[187,54074,54075],{"class":189,"line":17135},[187,54076,54077],{},"    ant_pos -= 1\n",[187,54079,54080],{"class":189,"line":17141},[187,54081,54058],{},[187,54083,54084],{"class":189,"line":17146},[187,54085,54086],{},"def move_up():\n",[187,54088,54089],{"class":189,"line":17152},[187,54090,54043],{},[187,54092,54093],{"class":189,"line":17164},[187,54094,54095],{},"    #move ant_pos one pixel up\n",[187,54097,54098],{"class":189,"line":17175},[187,54099,54100],{},"    ant_pos += width\n",[187,54102,54103],{"class":189,"line":17188},[187,54104,54058],{},[187,54106,54107],{"class":189,"line":17199},[187,54108,54109],{},"def move_down():\n",[187,54111,54112],{"class":189,"line":17207},[187,54113,54043],{},[187,54115,54116],{"class":189,"line":17212},[187,54117,54118],{},"    #move ant_pos one pixel down\n",[187,54120,54121],{"class":189,"line":17217},[187,54122,54123],{},"    ant_pos -= width\n",[187,54125,54126],{"class":189,"line":17223},[187,54127,54058],{},[187,54129,54130],{"class":189,"line":17235},[187,54131,316],{"emptyLinePlaceholder":315},[187,54133,54134],{"class":189,"line":17248},[187,54135,54136],{},"def move(color,d):\n",[187,54138,54139],{"class":189,"line":17267},[187,54140,54141],{},"    global direction\n",[187,54143,54144],{"class":189,"line":17278},[187,54145,54146],{},"    while True:\n",[187,54148,54149],{"class":189,"line":11081},[187,54150,54151],{},"        #this part is a little confusing and may need to be rewritten\n",[187,54153,54154],{"class":189,"line":17293},[187,54155,54156],{},"        #it uses the current direction of the ant to determine the appropriate direction for the next turn\n",[187,54158,54159],{"class":189,"line":17298},[187,54160,54161],{},"        #breaks are used\n",[187,54163,54164],{"class":189,"line":17304},[187,54165,54166],{},"        if pix_list[ant_pos][2] == color and direction == width*d:\n",[187,54168,54169],{"class":189,"line":17332},[187,54170,54171],{},"            #set the color to the next color in the list, or loop back to the beginning of the list if the end has been reached\n",[187,54173,54174],{"class":189,"line":17337},[187,54175,54176],{},"            pix_list[ant_pos][2] = (color + 1) % len(pixel_colors)\n",[187,54178,54179],{"class":189,"line":17343},[187,54180,54181],{},"            #save the current postion of the ant\n",[187,54183,54184],{"class":189,"line":17349},[187,54185,54186],{},"            init = ant_pos\n",[187,54188,54189],{"class":189,"line":17361},[187,54190,54191],{},"            #move the ant\n",[187,54193,54194],{"class":189,"line":17373},[187,54195,54196],{},"            move_right()\n",[187,54198,54199],{"class":189,"line":17388},[187,54200,54201],{},"            #save the updated position of the ant\n",[187,54203,54204],{"class":189,"line":17398},[187,54205,54206],{},"            end = ant_pos\n",[187,54208,54209],{"class":189,"line":17407},[187,54210,54211],{},"            #calculate the new direction of the ant by taking the difference between end and init\n",[187,54213,54214],{"class":189,"line":17412},[187,54215,54216],{},"            direction = end - init\n",[187,54218,54219],{"class":189,"line":17417},[187,54220,54221],{},"            break\n",[187,54223,54224],{"class":189,"line":17425},[187,54225,316],{"emptyLinePlaceholder":315},[187,54227,54228],{"class":189,"line":17430},[187,54229,54230],{},"        #same idea as above\n",[187,54232,54233],{"class":189,"line":17436},[187,54234,54235],{},"        elif pix_list[ant_pos][2] == color and direction == -1*width*d:\n",[187,54237,54238],{"class":189,"line":17453},[187,54239,54176],{},[187,54241,54242],{"class":189,"line":17458},[187,54243,54186],{},[187,54245,54246],{"class":189,"line":17464},[187,54247,54248],{},"            move_left()\n",[187,54250,54251],{"class":189,"line":17476},[187,54252,54206],{},[187,54254,54255],{"class":189,"line":17488},[187,54256,54216],{},[187,54258,54259],{"class":189,"line":17501},[187,54260,54221],{},[187,54262,54263],{"class":189,"line":17511},[187,54264,54230],{},[187,54266,54267],{"class":189,"line":17534},[187,54268,54269],{},"        elif pix_list[ant_pos][2] == color and direction == 1*d:\n",[187,54271,54272],{"class":189,"line":17548},[187,54273,54176],{},[187,54275,54276],{"class":189,"line":17553},[187,54277,54186],{},[187,54279,54280],{"class":189,"line":17558},[187,54281,54282],{},"            move_down()\n",[187,54284,54285],{"class":189,"line":17571},[187,54286,54206],{},[187,54288,54289],{"class":189,"line":17576},[187,54290,54216],{},[187,54292,54293],{"class":189,"line":17582},[187,54294,54221],{},[187,54296,54297],{"class":189,"line":17588},[187,54298,54230],{},[187,54300,54301],{"class":189,"line":17594},[187,54302,54303],{},"        elif pix_list[ant_pos][2] == color and direction == -1*d:\n",[187,54305,54306],{"class":189,"line":17600},[187,54307,54176],{},[187,54309,54310],{"class":189,"line":17615},[187,54311,54186],{},[187,54313,54314],{"class":189,"line":17626},[187,54315,54316],{},"            move_up()\n",[187,54318,54319],{"class":189,"line":17637},[187,54320,54206],{},[187,54322,54323],{"class":189,"line":17642},[187,54324,54216],{},[187,54326,54327],{"class":189,"line":17647},[187,54328,54221],{},[187,54330,54331],{"class":189,"line":17663},[187,54332,54333],{},"        break\n",[187,54335,54336],{"class":189,"line":17668},[187,54337,316],{"emptyLinePlaceholder":315},[187,54339,54340],{"class":189,"line":17674},[187,54341,54342],{},"#captures series of pixels used for generating images\n",[187,54344,54345],{"class":189,"line":17687},[187,54346,54347],{},"def get_pix_series():\n",[187,54349,54350],{"class":189,"line":17703},[187,54351,54352],{},"    global pix_series\n",[187,54354,54355],{"class":189,"line":17722},[187,54356,54357],{},"    pix_series = []\n",[187,54359,54360],{"class":189,"line":17733},[187,54361,54362],{},"    for x in range(len(pix_list)):\n",[187,54364,54365],{"class":189,"line":17744},[187,54366,54367],{},"        for y in range(len(pixel_colors)):\n",[187,54369,54370],{"class":189,"line":17757},[187,54371,54372],{},"            if pix_list[x][2] == y:\n",[187,54374,54375],{"class":189,"line":17762},[187,54376,54377],{},"                pixel = pixel_colors[y]\n",[187,54379,54380],{"class":189,"line":17770},[187,54381,54382],{},"                pix_series.append(pixel)\n",[187,54384,54385],{"class":189,"line":17775},[187,54386,316],{"emptyLinePlaceholder":315},[187,54388,54389],{"class":189,"line":17781},[187,54390,54391],{},"#runs ant along the grid according to the moves (defined above) for the number of steps in iterations (defined above)\n",[187,54393,54394],{"class":189,"line":17796},[187,54395,54396],{},"def run():\n",[187,54398,54399],{"class":189,"line":17801},[187,54400,54401],{},"    #variable the tracks the step number if the ant goes out of bounds\n",[187,54403,54404],{"class":189,"line":17806},[187,54405,54406],{},"    global oob\n",[187,54408,54409],{"class":189,"line":17812},[187,54410,54411],{},"    #converts moves string into a list of 0s and 1s; these numbers correspond to direction and are passed into the move() function\n",[187,54413,54414],{"class":189,"line":17818},[187,54415,54416],{},"    moves1 = [1 if x == 'R' else -1 for x in moves]\n",[187,54418,54419],{"class":189,"line":17824},[187,54420,54421],{},"    for step in range(iterations):\n",[187,54423,54424],{"class":189,"line":17836},[187,54425,54426],{},"        #exit the loop if the ant reaches the edge of the grid\n",[187,54428,54429],{"class":189,"line":17848},[187,54430,54431],{},"        if ant_pos \u003C width or ant_pos % width == 0:\n",[187,54433,54434],{"class":189,"line":17863},[187,54435,54436],{},"            oob = step\n",[187,54438,54439],{"class":189,"line":17875},[187,54440,54441],{},"            return\n",[187,54443,54444],{"class":189,"line":17883},[187,54445,54446],{},"        #loop through the moves\n",[187,54448,54449],{"class":189,"line":17888},[187,54450,54451],{},"        for index, direction in enumerate(moves1):\n",[187,54453,54454],{"class":189,"line":17893},[187,54455,23380],{},[187,54457,54458],{"class":189,"line":17899},[187,54459,54460],{},"                #remember the ant position\n",[187,54462,54463],{"class":189,"line":17911},[187,54464,54465],{},"                not_moved = ant_pos\n",[187,54467,54468],{"class":189,"line":17922},[187,54469,54470],{},"                #try to move the ant position\n",[187,54472,54473],{"class":189,"line":17936},[187,54474,54475],{},"                move(index,direction)\n",[187,54477,54478],{"class":189,"line":17945},[187,54479,54480],{},"                #check to see if the position was moved\n",[187,54482,54483],{"class":189,"line":17951},[187,54484,54485],{},"                if ant_pos != not_moved:\n",[187,54487,54488],{"class":189,"line":17956},[187,54489,54490],{},"                    #set record to false in the settings to turn of frame recording\n",[187,54492,54493],{"class":189,"line":17961},[187,54494,54495],{},"                    if record == True:\n",[187,54497,54498],{"class":189,"line":17966},[187,54499,54500],{},"                        #records a new frame every frame_interval frame\n",[187,54502,54503],{"class":189,"line":17974},[187,54504,54505],{},"                        if step % frame_interval == 0:\n",[187,54507,54508],{"class":189,"line":17979},[187,54509,54510],{},"                            counter += 1\n",[187,54512,54513],{"class":189,"line":17985},[187,54514,54515],{},"                            print(\"Generating frame number \" + str(counter))\n",[187,54517,54518],{"class":189,"line":17999},[187,54519,54520],{},"                            get_pix_series()\n",[187,54522,54523],{"class":189,"line":18010},[187,54524,54525],{},"                            save_image(counter)\n",[187,54527,54528],{"class":189,"line":18031},[187,54529,54530],{},"                    break\n",[187,54532,54533],{"class":189,"line":18036},[187,54534,54535],{},"                else:\n",[187,54537,54538],{"class":189,"line":18049},[187,54539,54540],{},"                    continue\n",[187,54542,54543],{"class":189,"line":18054},[187,54544,54545],{},"            except:\n",[187,54547,54549],{"class":189,"line":54548},194,[187,54550,54551],{},"                #print(\"Out of bounds at step number \" + str(step))\n",[187,54553,54555],{"class":189,"line":54554},195,[187,54556,54557],{},"                oob = step\n",[187,54559,54561],{"class":189,"line":54560},196,[187,54562,54563],{},"                return\n",[187,54565,54567],{"class":189,"line":54566},197,[187,54568,316],{"emptyLinePlaceholder":315},[187,54570,54572],{"class":189,"line":54571},198,[187,54573,54574],{},"def save_image(i):\n",[187,54576,54578],{"class":189,"line":54577},199,[187,54579,54580],{},"    #give access to the image instantiated at the beginning of the script\n",[187,54582,54584],{"class":189,"line":54583},200,[187,54585,54586],{},"    global im1\n",[187,54588,54590],{"class":189,"line":54589},201,[187,54591,54592],{},"    #fill blank image canvas with pix_series pixel data\n",[187,54594,54596],{"class":189,"line":54595},202,[187,54597,54598],{},"    im1.putdata(pix_series)\n",[187,54600,54602],{"class":189,"line":54601},203,[187,54603,54604],{},"    #to rescale the image, set the scale variable in settings and call resize on im1\n",[187,54606,54608],{"class":189,"line":54607},204,[187,54609,54610],{},"    im1.resize((scale*im1.size[0],scale*im1.size[1])).save('%s.png' % (moves))\n",[187,54612,54614],{"class":189,"line":54613},205,[187,54615,316],{"emptyLinePlaceholder":315},[187,54617,54619],{"class":189,"line":54618},206,[187,54620,54621],{},"#builds a dictionary to count pixels by color\n",[187,54623,54625],{"class":189,"line":54624},207,[187,54626,54627],{},"def build_df_row():\n",[187,54629,54631],{"class":189,"line":54630},208,[187,54632,54633],{},"    colors_dict = {str(val): 0 for val, color in enumerate(pixel_colors)}\n",[187,54635,54637],{"class":189,"line":54636},209,[187,54638,54639],{},"    moves_dict = {'moves':moves}\n",[187,54641,54643],{"class":189,"line":54642},210,[187,54644,54645],{},"    last_step = {'last_step':oob}\n",[187,54647,54649],{"class":189,"line":54648},211,[187,54650,54651],{},"    row_dict = dict(colors_dict.items()+moves_dict.items()+last_step.items())\n",[187,54653,54655],{"class":189,"line":54654},212,[187,54656,54657],{},"    for x in pix_list:\n",[187,54659,54661],{"class":189,"line":54660},213,[187,54662,54663],{},"        pixel_color = str(x[2])\n",[187,54665,54667],{"class":189,"line":54666},214,[187,54668,54669],{},"        #print(pixel_color)\n",[187,54671,54673],{"class":189,"line":54672},215,[187,54674,54675],{},"        row_dict[pixel_color] += 1\n",[187,54677,54679],{"class":189,"line":54678},216,[187,54680,54681],{},"    return row_dict\n",[187,54683,54685],{"class":189,"line":54684},217,[187,54686,316],{"emptyLinePlaceholder":315},[187,54688,54690],{"class":189,"line":54689},218,[187,54691,54692],{},"#uncomment below to overwrite moves_list\n",[187,54694,54696],{"class":189,"line":54695},219,[187,54697,54698],{},"#moves_list = ['LR', 'RRLR']\n",[187,54700,54702],{"class":189,"line":54701},220,[187,54703,316],{"emptyLinePlaceholder":315},[187,54705,54707],{"class":189,"line":54706},221,[187,54708,54709],{},"for _, moves in enumerate(moves_list):\n",[187,54711,54713],{"class":189,"line":54712},222,[187,54714,54715],{},"    oob = 0\n",[187,54717,54719],{"class":189,"line":54718},223,[187,54720,54721],{},"    dir_path = str(_)\n",[187,54723,54725],{"class":189,"line":54724},224,[187,54726,316],{"emptyLinePlaceholder":315},[187,54728,54730],{"class":189,"line":54729},225,[187,54731,54732],{},"    #make a new directory for each new ant walk in walks based on the the walk number and navigate to that directory\n",[187,54734,54736],{"class":189,"line":54735},226,[187,54737,54738],{},"    if record == True:\n",[187,54740,54742],{"class":189,"line":54741},227,[187,54743,54744],{},"        #make a new directory to record frames for a give ant if record is set to true and that directory does not yet exist\n",[187,54746,54748],{"class":189,"line":54747},228,[187,54749,54750],{},"        if not os.path.isdir(dir_path):\n",[187,54752,54754],{"class":189,"line":54753},229,[187,54755,54756],{},"            os.makedirs(dir_path)\n",[187,54758,54760],{"class":189,"line":54759},230,[187,54761,54762],{},"        #otherwise just change into the directory\n",[187,54764,54766],{"class":189,"line":54765},231,[187,54767,23551],{},[187,54769,54771],{"class":189,"line":54770},232,[187,54772,54773],{},"            os.chdir(dir_path)\n",[187,54775,54777],{"class":189,"line":54776},233,[187,54778,316],{"emptyLinePlaceholder":315},[187,54780,54782],{"class":189,"line":54781},234,[187,54783,54784],{},"    #set ant at middle of grid\n",[187,54786,54788],{"class":189,"line":54787},235,[187,54789,54790],{},"    ant_pos = int((length*width)/2) + int(width/2)\n",[187,54792,54794],{"class":189,"line":54793},236,[187,54795,316],{"emptyLinePlaceholder":315},[187,54797,54799],{"class":189,"line":54798},237,[187,54800,54801],{},"    #moves = len(moves)\n",[187,54803,54805],{"class":189,"line":54804},238,[187,54806,54807],{},"    pixel_colors = color_choices[:(len(moves))]\n",[187,54809,54811],{"class":189,"line":54810},239,[187,54812,316],{"emptyLinePlaceholder":315},[187,54814,54816],{"class":189,"line":54815},240,[187,54817,54818],{},"    #defines an empty list of elements [x,y,0] where x amd y are the position 0 is the 0ht color in the color list (the base canvas color)\n",[187,54820,54822],{"class":189,"line":54821},241,[187,54823,54824],{},"    pix_list = []\n",[187,54826,54828],{"class":189,"line":54827},242,[187,54829,54830],{},"    for x in range(length):\n",[187,54832,54834],{"class":189,"line":54833},243,[187,54835,54836],{},"        for y in range(width):\n",[187,54838,54840],{"class":189,"line":54839},244,[187,54841,54842],{},"            a = [x,y,0]\n",[187,54844,54846],{"class":189,"line":54845},245,[187,54847,54848],{},"            pix_list.append(a)\n",[187,54850,54852],{"class":189,"line":54851},246,[187,54853,316],{"emptyLinePlaceholder":315},[187,54855,54857],{"class":189,"line":54856},247,[187,54858,54859],{},"    #pix_series is a list of pixels that is passed into the put_data function to generate an image\n",[187,54861,54863],{"class":189,"line":54862},248,[187,54864,54357],{},[187,54866,54868],{"class":189,"line":54867},249,[187,54869,316],{"emptyLinePlaceholder":315},[187,54871,54873],{"class":189,"line":54872},250,[187,54874,54875],{},"    #counter keeps track of the frame number (if recording a series of images)\n",[187,54877,54879],{"class":189,"line":54878},251,[187,54880,54881],{},"    counter = 0\n",[187,54883,54885],{"class":189,"line":54884},252,[187,54886,316],{"emptyLinePlaceholder":315},[187,54888,54890],{"class":189,"line":54889},253,[187,54891,54892],{},"    #run the ant\n",[187,54894,54896],{"class":189,"line":54895},254,[187,54897,54898],{},"    run()\n",[187,54900,54902],{"class":189,"line":54901},255,[187,54903,316],{"emptyLinePlaceholder":315},[187,54905,54907],{"class":189,"line":54906},256,[187,54908,54909],{},"    #capture the final state of the grid with get_pix_series\n",[187,54911,54913],{"class":189,"line":54912},257,[187,54914,54915],{},"    get_pix_series()\n",[187,54917,54919],{"class":189,"line":54918},258,[187,54920,316],{"emptyLinePlaceholder":315},[187,54922,54924],{"class":189,"line":54923},259,[187,54925,54926],{},"    #uncomment below to preview images for testing\n",[187,54928,54930],{"class":189,"line":54929},260,[187,54931,54932],{},"    #im1.putdata(pix_series)\n",[187,54934,54936],{"class":189,"line":54935},261,[187,54937,54938],{},"    #im1.resize((scale*im1.size[0],scale*im1.size[1])).show()\n",[187,54940,54942],{"class":189,"line":54941},262,[187,54943,316],{"emptyLinePlaceholder":315},[187,54945,54947],{"class":189,"line":54946},263,[187,54948,54949],{},"    #build a dictionary with pixel counts\n",[187,54951,54953],{"class":189,"line":54952},264,[187,54954,54955],{},"    colors_dict = build_df_row()\n",[187,54957,54959],{"class":189,"line":54958},265,[187,54960,54961],{},"    row_df = pd.DataFrame(colors_dict, index=[0])\n",[187,54963,54965],{"class":189,"line":54964},266,[187,54966,54967],{},"    df = df.append(row_df, ignore_index=True)\n",[187,54969,54971],{"class":189,"line":54970},267,[187,54972,316],{"emptyLinePlaceholder":315},[187,54974,54976],{"class":189,"line":54975},268,[187,54977,54978],{},"    if record_final_image == True:\n",[187,54980,54982],{"class":189,"line":54981},269,[187,54983,54984],{},"        os.chdir(os.path.expanduser('~/Documents/CA_1/imgs/'))\n",[187,54986,54988],{"class":189,"line":54987},270,[187,54989,54990],{},"        save_image(_)\n",[187,54992,54994],{"class":189,"line":54993},271,[187,54995,54996],{},"        os.chdir('../')\n",[187,54998,55000],{"class":189,"line":54999},272,[187,55001,316],{"emptyLinePlaceholder":315},[187,55003,55005],{"class":189,"line":55004},273,[187,55006,55007],{},"    #summary\n",[187,55009,55011],{"class":189,"line":55010},274,[187,55012,55013],{},"    print(str(_), end=' ')\n",[187,55015,55017],{"class":189,"line":55016},275,[187,55018,316],{"emptyLinePlaceholder":315},[187,55020,55022],{"class":189,"line":55021},276,[187,55023,55024],{},"os.chdir(os.path.expanduser('~/Documents/CA_1/'))\n",[187,55026,55028],{"class":189,"line":55027},277,[187,55029,55030],{},"df.to_csv('ants_hist_.csv', index=False)\n",[2215,55032,55034],{"id":55033},"clustering","Clustering",[11,55036,55037,55038,55041],{},"We now have a csv file where each row is a 16-state termite and the columns labeled 0 through 15 count the sum of pixels in each state (the different colors). With ",[33,55039,55040],{},"last_step"," we also track the last step reached in the event that the ant runs into the edge of the grid. This will be helpful in clustering ants that form highways in different groups from those that complete 100000 steps inside the 200 x 200 grid.",[11,55043,55044],{},"First let's read the csv into a pandas DataFrame and look at some of the data.",[26,55046,55047],{"className":1383,"code":53564,"language":1125,"meta":35,"style":35},[33,55048,55049,55053,55057,55061,55065,55069,55073,55077,55081,55085,55089,55093,55097],{"__ignoreMap":35},[187,55050,55051],{"class":189,"line":190},[187,55052,26193],{},[187,55054,55055],{"class":189,"line":249},[187,55056,26048],{},[187,55058,55059],{"class":189,"line":312},[187,55060,26012],{},[187,55062,55063],{"class":189,"line":319},[187,55064,10345],{},[187,55066,55067],{"class":189,"line":325},[187,55068,37567],{},[187,55070,55071],{"class":189,"line":686},[187,55072,53591],{},[187,55074,55075],{"class":189,"line":697},[187,55076,10563],{},[187,55078,55079],{"class":189,"line":1291},[187,55080,53600],{},[187,55082,55083],{"class":189,"line":1306},[187,55084,26035],{},[187,55086,55087],{"class":189,"line":1434},[187,55088,53609],{},[187,55090,55091],{"class":189,"line":2599},[187,55092,53614],{},[187,55094,55095],{"class":189,"line":2607},[187,55096,53619],{},[187,55098,55099],{"class":189,"line":2621},[187,55100,26216],{},[26,55102,55104],{"className":1383,"code":55103,"language":1125,"meta":35,"style":35},"os.chdir(os.path.expanduser('~/Documents/CA_1/'))\ndf1 = pd.read_csv('ants_hist_.csv')\n",[33,55105,55106,55110],{"__ignoreMap":35},[187,55107,55108],{"class":189,"line":190},[187,55109,55024],{},[187,55111,55112],{"class":189,"line":249},[187,55113,55114],{},"df1 = pd.read_csv('ants_hist_.csv')\n",[26,55116,55118],{"className":1383,"code":55117,"language":1125,"meta":35,"style":35},"df1.shape\n",[33,55119,55120],{"__ignoreMap":35},[187,55121,55122],{"class":189,"line":190},[187,55123,55117],{},[26,55125,55128],{"className":55126,"code":55127,"language":31},[29],"(32768, 18)\n",[33,55129,55127],{"__ignoreMap":35},[26,55131,55133],{"className":1383,"code":55132,"language":1125,"meta":35,"style":35},"df1.sample(3)\n",[33,55134,55135],{"__ignoreMap":35},[187,55136,55137],{"class":189,"line":190},[187,55138,55132],{},[10229,55140,55141],{},[1525,55142,20796,55145,20796,55195],{"border":190,"className":55143},[55144],"dataframe",[1528,55146,55147,55148,20796],{},"\n    ",[1531,55149,55151,55152,55151,55154,55151,55156,55151,55158,55151,55160,55151,55163,55151,55165,55151,55167,55151,55170,55151,55173,55151,55175,55151,55177,55151,55179,55151,55181,55151,55183,55151,55186,55151,55188,55151,55190,55151,55192,55147],{"style":55150},"text-align: right;","\n      ",[1534,55153],{},[1534,55155,10165],{},[1534,55157,15625],{},[1534,55159,654],{},[1534,55161,55162],{},"11",[1534,55164,38739],{},[1534,55166,38462],{},[1534,55168,55169],{},"14",[1534,55171,55172],{},"15",[1534,55174,22129],{},[1534,55176,34913],{},[1534,55178,35094],{},[1534,55180,10533],{},[1534,55182,47688],{},[1534,55184,55185],{},"7",[1534,55187,10522],{},[1534,55189,668],{},[1534,55191,55040],{},[1534,55193,55194],{},"moves",[1544,55196,55147,55197,55147,55254,55147,55311,20796],{},[1531,55198,55151,55199,55151,55202,55151,55205,55151,55208,55151,55211,55151,55214,55151,55216,55151,55219,55151,55222,55151,55225,55151,55228,55151,55231,55151,55234,55151,55237,55151,55240,55151,55243,55151,55246,55151,55249,55151,55251,55147],{},[1534,55200,55201],{},"25892",[1549,55203,55204],{},"37741",[1549,55206,55207],{},"212",[1549,55209,55210],{},"135",[1549,55212,55213],{},"148",[1549,55215,55210],{},[1549,55217,55218],{},"152",[1549,55220,55221],{},"119",[1549,55223,55224],{},"115",[1549,55226,55227],{},"176",[1549,55229,55230],{},"168",[1549,55232,55233],{},"129",[1549,55235,55236],{},"182",[1549,55238,55239],{},"155",[1549,55241,55242],{},"130",[1549,55244,55245],{},"162",[1549,55247,55248],{},"141",[1549,55250,10165],{},[1549,55252,55253],{},"RRRLLRLRLLRLLRLL",[1531,55255,55151,55256,55151,55259,55151,55262,55151,55265,55151,55268,55151,55271,55151,55274,55151,55277,55151,55280,55151,55283,55151,55286,55151,55289,55151,55292,55151,55295,55151,55298,55151,55301,55151,55303,55151,55306,55151,55308,55147],{},[1534,55257,55258],{},"27264",[1549,55260,55261],{},"35861",[1549,55263,55264],{},"117",[1549,55266,55267],{},"444",[1549,55269,55270],{},"421",[1549,55272,55273],{},"497",[1549,55275,55276],{},"395",[1549,55278,55279],{},"327",[1549,55281,55282],{},"222",[1549,55284,55285],{},"345",[1549,55287,55288],{},"143",[1549,55290,55291],{},"126",[1549,55293,55294],{},"218",[1549,55296,55297],{},"241",[1549,55299,55300],{},"187",[1549,55302,55210],{},[1549,55304,55305],{},"321",[1549,55307,10165],{},[1549,55309,55310],{},"RRRLRLRLRLLLLLLL",[1531,55312,55151,55313,55151,55316,55151,55319,55151,55322,55151,55325,55151,55327,55151,55330,55151,55332,55151,55335,55151,55338,55151,55341,55151,55344,55151,55347,55151,55349,55151,55351,55151,55354,55151,55356,55151,55359,55151,55361,55147],{},[1534,55314,55315],{},"10181",[1549,55317,55318],{},"37202",[1549,55320,55321],{},"300",[1549,55323,55324],{},"177",[1549,55326,55242],{},[1549,55328,55329],{},"150",[1549,55331,55213],{},[1549,55333,55334],{},"124",[1549,55336,55337],{},"156",[1549,55339,55340],{},"191",[1549,55342,55343],{},"217",[1549,55345,55346],{},"234",[1549,55348,55343],{},[1549,55350,55245],{},[1549,55352,55353],{},"200",[1549,55355,55282],{},[1549,55357,55358],{},"170",[1549,55360,10165],{},[1549,55362,55363],{},"RLRLLRRRRRLLLRLR",[11,55365,55366,55367,55370,55371,55374],{},"There are 32768 unique instructions for 16-state termites ",[33,55368,55369],{},"(2^16)/2 = 32768",". Let's check to see how many of these are duplicates. We want to select only the state-counts and then call ",[33,55372,55373],{},".drop_duplicates"," on that DataFrame.",[26,55376,55378],{"className":1383,"code":55377,"language":1125,"meta":35,"style":35},"df2 = df1.iloc[:,0:16]\n",[33,55379,55380],{"__ignoreMap":35},[187,55381,55382],{"class":189,"line":190},[187,55383,55377],{},[26,55385,55387],{"className":1383,"code":55386,"language":1125,"meta":35,"style":35},"df2.shape[0] - df2.drop_duplicates().shape[0]\n",[33,55388,55389],{"__ignoreMap":35},[187,55390,55391],{"class":189,"line":190},[187,55392,55386],{},[26,55394,55397],{"className":55395,"code":55396,"language":31},[29],"1566\n",[33,55398,55396],{"__ignoreMap":35},[11,55400,55401],{},"1566 of the 16-state termites. It might be helpful to remove these termites from the DataFrame before we cluster them.",[26,55403,55405],{"className":1383,"code":55404,"language":1125,"meta":35,"style":35},"unique_termites_index = df2.drop_duplicates().index\n",[33,55406,55407],{"__ignoreMap":35},[187,55408,55409],{"class":189,"line":190},[187,55410,55404],{},[26,55412,55414],{"className":1383,"code":55413,"language":1125,"meta":35,"style":35},"df = df1.loc[unique_termites_index,:]\n",[33,55415,55416],{"__ignoreMap":35},[187,55417,55418],{"class":189,"line":190},[187,55419,55413],{},[26,55421,55423],{"className":1383,"code":55422,"language":1125,"meta":35,"style":35},"df['steps_taken'] = [100000 if x==0 else x for x in df.last_step]\ndf['file_names'] = [x+'.png' for x in df.moves]\ndf['move_len'] = [len(x) for x in df.moves]\n",[33,55424,55425,55430,55435],{"__ignoreMap":35},[187,55426,55427],{"class":189,"line":190},[187,55428,55429],{},"df['steps_taken'] = [100000 if x==0 else x for x in df.last_step]\n",[187,55431,55432],{"class":189,"line":249},[187,55433,55434],{},"df['file_names'] = [x+'.png' for x in df.moves]\n",[187,55436,55437],{"class":189,"line":312},[187,55438,55439],{},"df['move_len'] = [len(x) for x in df.moves]\n",[26,55441,55443],{"className":1383,"code":55442,"language":1125,"meta":35,"style":35},"df.head()\n",[33,55444,55445],{"__ignoreMap":35},[187,55446,55447],{"class":189,"line":190},[187,55448,55442],{},[10229,55450,55451,55785],{},[1525,55452,20796,55454,20796,55505],{"border":190,"className":55453},[55144],[1528,55455,55147,55456,20796],{},[1531,55457,55151,55458,55151,55460,55151,55462,55151,55464,55151,55466,55151,55468,55151,55470,55151,55472,55151,55474,55151,55476,55151,55478,55151,55480,55151,55482,55151,55484,55151,55486,55151,55488,55151,55490,55151,55492,55151,55494,55151,55496,55151,55499,55151,55502,55147],{"style":55150},[1534,55459],{},[1534,55461,10165],{},[1534,55463,15625],{},[1534,55465,654],{},[1534,55467,55162],{},[1534,55469,38739],{},[1534,55471,38462],{},[1534,55473,55169],{},[1534,55475,55172],{},[1534,55477,22129],{},[1534,55479,34913],{},[1534,55481,12940],{},[1534,55483,10533],{},[1534,55485,47688],{},[1534,55487,55185],{},[1534,55489,10522],{},[1534,55491,668],{},[1534,55493,55040],{},[1534,55495,55194],{},[1534,55497,55498],{},"steps_taken",[1534,55500,55501],{},"file_names",[1534,55503,55504],{},"move_len",[1544,55506,55147,55507,55147,55569,55147,55620,55147,55672,55147,55732,20796],{},[1531,55508,55151,55509,55151,55511,55151,55514,55151,55517,55151,55520,55151,55522,55151,55525,55151,55528,55151,55531,55151,55534,55151,55537,55151,55540,55151,55542,55151,55545,55151,55547,55151,55549,55151,55552,55151,55555,55151,55557,55151,55560,55151,55563,55151,55566,55147],{},[1534,55510,10165],{},[1549,55512,55513],{},"36381",[1549,55515,55516],{},"584",[1549,55518,55519],{},"36",[1549,55521,43467],{},[1549,55523,55524],{},"41",[1549,55526,55527],{},"85",[1549,55529,55530],{},"179",[1549,55532,55533],{},"174",[1549,55535,55536],{},"1864",[1549,55538,55539],{},"325",[1549,55541,12940],{},[1549,55543,55544],{},"45",[1549,55546,55544],{},[1549,55548,43467],{},[1549,55550,55551],{},"31",[1549,55553,55554],{},"37",[1549,55556,10165],{},[1549,55558,55559],{},"RLLLLLLLLLLLLLLL",[1549,55561,55562],{},"100000",[1549,55564,55565],{},"RLLLLLLLLLLLLLLL.png",[1549,55567,55568],{},"16",[1531,55570,55151,55571,55151,55573,55151,55576,55151,55579,55151,55581,55151,55583,55151,55585,55151,55587,55151,55589,55151,55591,55151,55594,55151,55596,55151,55598,55151,55600,55151,55602,55151,55604,55151,55606,55151,55608,55151,55610,55151,55613,55151,55615,55151,55618,55147],{},[1534,55572,15625],{},[1549,55574,55575],{},"39857",[1549,55577,55578],{},"44",[1549,55580,35094],{},[1549,55582,10165],{},[1549,55584,47688],{},[1549,55586,10165],{},[1549,55588,10165],{},[1549,55590,35094],{},[1549,55592,55593],{},"27",[1549,55595,38612],{},[1549,55597,12940],{},[1549,55599,10522],{},[1549,55601,34913],{},[1549,55603,55162],{},[1549,55605,47688],{},[1549,55607,35094],{},[1549,55609,10165],{},[1549,55611,55612],{},"RLLLLLLLLLLLLLLR",[1549,55614,55562],{},[1549,55616,55617],{},"RLLLLLLLLLLLLLLR.png",[1549,55619,55568],{},[1531,55621,55151,55622,55151,55624,55151,55627,55151,55630,55151,55632,55151,55634,55151,55636,55151,55638,55151,55640,55151,55642,55151,55645,55151,55647,55151,55649,55151,55651,55151,55653,55151,55655,55151,55658,55151,55660,55151,55662,55151,55665,55151,55667,55151,55670,55147],{},[1534,55623,22129],{},[1549,55625,55626],{},"39804",[1549,55628,55629],{},"48",[1549,55631,10533],{},[1549,55633,38739],{},[1549,55635,55185],{},[1549,55637,668],{},[1549,55639,55185],{},[1549,55641,10165],{},[1549,55643,55644],{},"24",[1549,55646,38739],{},[1549,55648,12940],{},[1549,55650,55162],{},[1549,55652,668],{},[1549,55654,55185],{},[1549,55656,55657],{},"20",[1549,55659,10533],{},[1549,55661,10165],{},[1549,55663,55664],{},"RLLLLLLLLLLLLLRL",[1549,55666,55562],{},[1549,55668,55669],{},"RLLLLLLLLLLLLLRL.png",[1549,55671,55568],{},[1531,55673,55151,55674,55151,55676,55151,55679,55151,55682,55151,55685,55151,55688,55151,55691,55151,55694,55151,55697,55151,55699,55151,55702,55151,55705,55151,55707,55151,55709,55151,55711,55151,55714,55151,55717,55151,55720,55151,55722,55151,55725,55151,55727,55151,55730,55147],{},[1534,55675,34913],{},[1549,55677,55678],{},"37223",[1549,55680,55681],{},"346",[1549,55683,55684],{},"70",[1549,55686,55687],{},"97",[1549,55689,55690],{},"89",[1549,55692,55693],{},"122",[1549,55695,55696],{},"100",[1549,55698,55334],{},[1549,55700,55701],{},"1150",[1549,55703,55704],{},"216",[1549,55706,12940],{},[1549,55708,55527],{},[1549,55710,51088],{},[1549,55712,55713],{},"61",[1549,55715,55716],{},"60",[1549,55718,55719],{},"55",[1549,55721,10165],{},[1549,55723,55724],{},"RLLLLLLLLLLLLLRR",[1549,55726,55562],{},[1549,55728,55729],{},"RLLLLLLLLLLLLLRR.png",[1549,55731,55568],{},[1531,55733,55151,55734,55151,55736,55151,55739,55151,55742,55151,55745,55151,55747,55151,55749,55151,55751,55151,55753,55151,55755,55151,55758,55151,55760,55151,55762,55151,55764,55151,55766,55151,55768,55151,55771,55151,55773,55151,55775,55151,55778,55151,55780,55151,55783,55147],{},[1534,55735,35094],{},[1549,55737,55738],{},"39678",[1549,55740,55741],{},"76",[1549,55743,55744],{},"22",[1549,55746,55568],{},[1549,55748,38612],{},[1549,55750,10522],{},[1549,55752,15625],{},[1549,55754,22129],{},[1549,55756,55757],{},"47",[1549,55759,38612],{},[1549,55761,12940],{},[1549,55763,55551],{},[1549,55765,55169],{},[1549,55767,55172],{},[1549,55769,55770],{},"26",[1549,55772,55162],{},[1549,55774,10165],{},[1549,55776,55777],{},"RLLLLLLLLLLLLRLL",[1549,55779,55562],{},[1549,55781,55782],{},"RLLLLLLLLLLLLRLL.png",[1549,55784,55568],{},[11,55786,55787],{},"5 rows × 21 columns",[26,55789,55791],{"className":1383,"code":55790,"language":1125,"meta":35,"style":35},"df.index = df.file_names\n",[33,55792,55793],{"__ignoreMap":35},[187,55794,55795],{"class":189,"line":190},[187,55796,55790],{},[11,55798,55799],{},"Here's a quick look at the distribution of the base canvas color (red in the images below) over all of the unique termites.",[26,55801,55803],{"className":1383,"code":55802,"language":1125,"meta":35,"style":35},"x = '0'\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,4))\ndf[(df[x]>0)][x].hist(bins=250)\nplt.xlabel('Count of Cells in state 0')\nplt.ylabel('Count')\nplt.title('Histogram Showing Termite Count by number of cells in state 0')\n",[33,55804,55805,55810,55815,55820,55825,55830,55835],{"__ignoreMap":35},[187,55806,55807],{"class":189,"line":190},[187,55808,55809],{},"x = '0'\n",[187,55811,55812],{"class":189,"line":249},[187,55813,55814],{},"sns.set_style('whitegrid')\n",[187,55816,55817],{"class":189,"line":312},[187,55818,55819],{},"plt.figure(figsize=(12,4))\n",[187,55821,55822],{"class":189,"line":319},[187,55823,55824],{},"df[(df[x]>0)][x].hist(bins=250)\n",[187,55826,55827],{"class":189,"line":325},[187,55828,55829],{},"plt.xlabel('Count of Cells in state 0')\n",[187,55831,55832],{"class":189,"line":686},[187,55833,55834],{},"plt.ylabel('Count')\n",[187,55836,55837],{"class":189,"line":697},[187,55838,55839],{},"plt.title('Histogram Showing Termite Count by number of cells in state 0')\n",[26,55841,55844],{"className":55842,"code":55843,"language":31},[29],"\u003Cmatplotlib.text.Text at 0x116dee10>\n",[33,55845,55843],{"__ignoreMap":35},[11,55847,55848],{},[511,55849],{"alt":7255,"src":55850},"/static/ants_files/ants_20_1.png",[26,55852,55854],{"className":1383,"code":55853,"language":1125,"meta":35,"style":35},"df.shape\n",[33,55855,55856],{"__ignoreMap":35},[187,55857,55858],{"class":189,"line":190},[187,55859,55853],{},[26,55861,55864],{"className":55862,"code":55863,"language":31},[29],"(31202, 21)\n",[33,55865,55863],{"__ignoreMap":35},[11,55867,55868],{},"Now we can prepare a DataFrame that we will feed in to the clustering model. We will take only the pixel counts and the total number of steps taken.",[26,55870,55872],{"className":1383,"code":55871,"language":1125,"meta":35,"style":35},"X = df[df.move_len==16].iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,18]]\n",[33,55873,55874],{"__ignoreMap":35},[187,55875,55876],{"class":189,"line":190},[187,55877,55871],{},[26,55879,55881],{"className":1383,"code":55880,"language":1125,"meta":35,"style":35},"X.columns\n",[33,55882,55883],{"__ignoreMap":35},[187,55884,55885],{"class":189,"line":190},[187,55886,55880],{},[26,55888,55891],{"className":55889,"code":55890,"language":31},[29],"Index([u'0', u'1', u'10', u'11', u'12', u'13', u'14', u'15', u'2', u'3', u'4',\n       u'5', u'6', u'7', u'8', u'9', u'steps_taken'],\n      dtype='object')\n",[33,55892,55890],{"__ignoreMap":35},[11,55894,55895],{},"Most of the termites completed all 100000 steps within the grid boundries.",[26,55897,55899],{"className":1383,"code":55898,"language":1125,"meta":35,"style":35},"X[X.steps_taken==100000].steps_taken.count()\n",[33,55900,55901],{"__ignoreMap":35},[187,55902,55903],{"class":189,"line":190},[187,55904,55898],{},[26,55906,55909],{"className":55907,"code":55908,"language":31},[29],"29955\n",[33,55910,55908],{"__ignoreMap":35},[11,55912,55913],{},"Here'a a histogram of the steps taken by ants that took less than 100000 steps.",[26,55915,55917],{"className":1383,"code":55916,"language":1125,"meta":35,"style":35},"X[X.steps_taken\u003C100000].steps_taken.hist()\nplt.title('Histogram of steps_taken for termites that stayed in bounds')\nplt.xlabel('steps_taken')\nplt.ylabel('Count')\n",[33,55918,55919,55924,55929,55934],{"__ignoreMap":35},[187,55920,55921],{"class":189,"line":190},[187,55922,55923],{},"X[X.steps_taken\u003C100000].steps_taken.hist()\n",[187,55925,55926],{"class":189,"line":249},[187,55927,55928],{},"plt.title('Histogram of steps_taken for termites that stayed in bounds')\n",[187,55930,55931],{"class":189,"line":312},[187,55932,55933],{},"plt.xlabel('steps_taken')\n",[187,55935,55936],{"class":189,"line":319},[187,55937,55834],{},[26,55939,55942],{"className":55940,"code":55941,"language":31},[29],"\u003Cmatplotlib.text.Text at 0x18aa3a58>\n",[33,55943,55941],{"__ignoreMap":35},[11,55945,55946],{},[511,55947],{"alt":7255,"src":55948},"/static/ants_files/ants_28_1.png",[11,55950,55951],{},"Here's another look at the distribution of cells in state 3 over all termites:",[26,55953,55955],{"className":1383,"code":55954,"language":1125,"meta":35,"style":35},"x = '3' #other intereting states: 1, 7, 11, 15\nsns.set_style('whitegrid')\n#plt.figure(figsize=(12,8))\ndf[(df[x]>0)][x].hist(bins=100)\nplt.xlabel('Count of Cells in state 3')\nplt.ylabel('Count (termites)')\nplt.title('Histogram Showing Termite Count by number of cells in state 3')\n",[33,55956,55957,55962,55966,55971,55976,55981,55986],{"__ignoreMap":35},[187,55958,55959],{"class":189,"line":190},[187,55960,55961],{},"x = '3' #other intereting states: 1, 7, 11, 15\n",[187,55963,55964],{"class":189,"line":249},[187,55965,55814],{},[187,55967,55968],{"class":189,"line":312},[187,55969,55970],{},"#plt.figure(figsize=(12,8))\n",[187,55972,55973],{"class":189,"line":319},[187,55974,55975],{},"df[(df[x]>0)][x].hist(bins=100)\n",[187,55977,55978],{"class":189,"line":325},[187,55979,55980],{},"plt.xlabel('Count of Cells in state 3')\n",[187,55982,55983],{"class":189,"line":686},[187,55984,55985],{},"plt.ylabel('Count (termites)')\n",[187,55987,55988],{"class":189,"line":697},[187,55989,55990],{},"plt.title('Histogram Showing Termite Count by number of cells in state 3')\n",[26,55992,55995],{"className":55993,"code":55994,"language":31},[29],"\u003Cmatplotlib.text.Text at 0x15ba6630>\n",[33,55996,55994],{"__ignoreMap":35},[11,55998,55999],{},[511,56000],{"alt":7255,"src":56001},"/static/ants_files/ants_30_1.png",[26,56003,56005],{"className":1383,"code":56004,"language":1125,"meta":35,"style":35},"X.shape\n",[33,56006,56007],{"__ignoreMap":35},[187,56008,56009],{"class":189,"line":190},[187,56010,56004],{},[26,56012,56015],{"className":56013,"code":56014,"language":31},[29],"(31202, 17)\n",[33,56016,56014],{"__ignoreMap":35},[11,56018,56019,56020,56023,56024,56029],{},"To cluster the different termites, we can use an unsupervised learning method called clustering. It is \"unsupervised\" because I don't explicitly tell the model what types termites should be grouped together. Instead, we will tell the model ",[4339,56021,56022],{},"how many different clusters there are."," Of course, I really don't know how many clusters there should be. I do know from looking at the results that there seem to be many different types of behavior, patterns, sizes and other characteristics. We significantly reduce the complexity of clustering task by training the model on the count of pixels by what state the are in. I'm sure that the model won't be able to pick up on all of the nuances that humans can detect by looking at the images, but I have a feeling that it should be able to do a fairly good job. After we take a look at the individual clusters, we can try to find an optimal number of clusters by minimizing the total number of outliers of all the clusters. Here's ",[15,56025,56028],{"href":56026,"rel":56027},"http://papers.nips.cc/paper/5306-on-integrated-clustering-and-outlier-detection.pdf",[19],"an interesting paper"," on integrated clustering and outlier detection.",[11,56031,56032],{},"Here's how we set up the clustering model. For the numebr of clusters, let's start with 75.",[26,56034,56036],{"className":1383,"code":56035,"language":1125,"meta":35,"style":35},"k_means = cluster.KMeans(n_clusters=75, random_state=1)\n",[33,56037,56038],{"__ignoreMap":35},[187,56039,56040],{"class":189,"line":190},[187,56041,56035],{},[26,56043,56045],{"className":1383,"code":56044,"language":1125,"meta":35,"style":35},"k_means.fit(X, y=None)\n",[33,56046,56047],{"__ignoreMap":35},[187,56048,56049],{"class":189,"line":190},[187,56050,56044],{},[26,56052,56055],{"className":56053,"code":56054,"language":31},[29],"KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n    n_clusters=75, n_init=10, n_jobs=1, precompute_distances='auto',\n    random_state=1, tol=0.0001, verbose=0)\n",[33,56056,56054],{"__ignoreMap":35},[11,56058,56059],{},"Then we add the cluster number to each termite:",[26,56061,56063],{"className":1383,"code":56062,"language":1125,"meta":35,"style":35},"X['clusters'] = k_means.labels_\n",[33,56064,56065],{"__ignoreMap":35},[187,56066,56067],{"class":189,"line":190},[187,56068,56062],{},[11,56070,56071],{},"Here's the breakdown of clusters by number of termites in each cluster:",[26,56073,56075],{"className":1383,"code":56074,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,4))\nplt.bar(X.clusters.value_counts().index, X.clusters.value_counts())\nplt.xlabel('Cluster Number')\nplt.ylabel('Count')\nplt.title('Termite count by cluster')\n",[33,56076,56077,56081,56086,56091,56095],{"__ignoreMap":35},[187,56078,56079],{"class":189,"line":190},[187,56080,55819],{},[187,56082,56083],{"class":189,"line":249},[187,56084,56085],{},"plt.bar(X.clusters.value_counts().index, X.clusters.value_counts())\n",[187,56087,56088],{"class":189,"line":312},[187,56089,56090],{},"plt.xlabel('Cluster Number')\n",[187,56092,56093],{"class":189,"line":319},[187,56094,55834],{},[187,56096,56097],{"class":189,"line":325},[187,56098,56099],{},"plt.title('Termite count by cluster')\n",[26,56101,56104],{"className":56102,"code":56103,"language":31},[29],"\u003Cmatplotlib.text.Text at 0x15c4c358>\n",[33,56105,56103],{"__ignoreMap":35},[11,56107,56108],{},[511,56109],{"alt":7255,"src":56110},"/static/ants_files/ants_38_1.png",[11,56112,56113],{},"And here is a list of the data shown above:",[26,56115,56117],{"className":1383,"code":56116,"language":1125,"meta":35,"style":35},"for x, y in zip(X.clusters.value_counts().index, X.clusters.value_counts()): print(' || cluster_num: ' + str(x) , 'count: ' + str(y), end='  ')\n",[33,56118,56119],{"__ignoreMap":35},[187,56120,56121],{"class":189,"line":190},[187,56122,56116],{},[26,56124,56127],{"className":56125,"code":56126,"language":31},[29]," || cluster_num: 16 count: 3293   || cluster_num: 71 count: 3036   || cluster_num: 46 count: 2993   || cluster_num: 0 count: 2752   || cluster_num: 23 count: 2667   || cluster_num: 52 count: 2540   || cluster_num: 50 count: 2185   || cluster_num: 5 count: 1412   || cluster_num: 19 count: 987   || cluster_num: 45 count: 816   || cluster_num: 44 count: 791   || cluster_num: 54 count: 686   || cluster_num: 42 count: 625   || cluster_num: 64 count: 603   || cluster_num: 21 count: 534   || cluster_num: 37 count: 518   || cluster_num: 38 count: 345   || cluster_num: 73 count: 315   || cluster_num: 57 count: 314   || cluster_num: 4 count: 302   || cluster_num: 22 count: 291   || cluster_num: 70 count: 285   || cluster_num: 8 count: 225   || cluster_num: 2 count: 224   || cluster_num: 9 count: 218   || cluster_num: 26 count: 182   || cluster_num: 49 count: 178   || cluster_num: 65 count: 138   || cluster_num: 29 count: 137   || cluster_num: 63 count: 123   || cluster_num: 25 count: 122   || cluster_num: 33 count: 105   || cluster_num: 1 count: 74   || cluster_num: 66 count: 71   || cluster_num: 27 count: 70   || cluster_num: 62 count: 66   || cluster_num: 69 count: 58   || cluster_num: 35 count: 56   || cluster_num: 11 count: 55   || cluster_num: 68 count: 54   || cluster_num: 14 count: 51   || cluster_num: 32 count: 49   || cluster_num: 30 count: 45   || cluster_num: 55 count: 38   || cluster_num: 6 count: 35   || cluster_num: 13 count: 34   || cluster_num: 43 count: 30   || cluster_num: 60 count: 30   || cluster_num: 58 count: 30   || cluster_num: 24 count: 29   || cluster_num: 39 count: 26   || cluster_num: 51 count: 25   || cluster_num: 3 count: 24   || cluster_num: 28 count: 24   || cluster_num: 15 count: 23   || cluster_num: 72 count: 23   || cluster_num: 17 count: 22   || cluster_num: 10 count: 22   || cluster_num: 34 count: 21   || cluster_num: 74 count: 20   || cluster_num: 36 count: 20   || cluster_num: 61 count: 19   || cluster_num: 31 count: 17   || cluster_num: 20 count: 13   || cluster_num: 18 count: 12   || cluster_num: 67 count: 12   || cluster_num: 12 count: 11   || cluster_num: 47 count: 11   || cluster_num: 59 count: 10   || cluster_num: 53 count: 8   || cluster_num: 7 count: 6   || cluster_num: 41 count: 6   || cluster_num: 40 count: 6   || cluster_num: 48 count: 3   || cluster_num: 56 count: 1\n",[33,56128,56126],{"__ignoreMap":35},[26,56130,56132],{"className":1383,"code":56131,"language":1125,"meta":35,"style":35},"cluster_dict = {x: y for x, y in zip(X.clusters.value_counts().index, X.clusters.value_counts())}\n",[33,56133,56134],{"__ignoreMap":35},[187,56135,56136],{"class":189,"line":190},[187,56137,56131],{},[11,56139,56140],{},"Now let's have a look at some of the termite clusters. We can use matplotlib and PIL to display multiple images using subplots.",[26,56142,56144],{"className":1383,"code":56143,"language":1125,"meta":35,"style":35},"#variables to manage the arrangement and spacing of cluster images\ntotal = 0\nrows = 0\nim_length = 0\n\n#set variables for cluster images based on cluster size\ndef set_spacing(files):\n    global total\n    global rows\n    global im_length\n    #columns = 6\n    total = len(files)\n    extras = len(files) % 6\n    if extras > 0:\n        total += (6 - extras)\n    rows = total/6.\n    im_length = rows*(20/9.)\n\n#use matplotlib to show images loaded with PIL\ndef show_images(cluster_num, samples = 0,  files_bool=False, files=None):\n    if files_bool==True:\n        files1 = np.random.choice(files.index, min(files.shape[0], samples), replace=False)\n    if (samples == 0) & (files_bool==False):\n        files1 = X[X.clusters==cluster_num].index\n    if (samples > 0) & (files_bool==False):\n        files1 = np.random.choice(X[X.clusters==cluster_num].index, min(cluster_dict[cluster_num],samples), replace=False)\n    set_spacing(files1)\n    plt.figure(figsize = (14,im_length))\n    os.chdir(os.path.expanduser('~/Documents/CA_1/imgs/'))\n    for num, x in enumerate(files1):\n        img = PIL.Image.open(x)\n        plt.subplot(rows,6,num+1)\n        plt.title(x.split('.')[0])\n        plt.axis('off')\n        plt.imshow(img)\n    print('Cluster #' + str(X.ix[x].clusters) + ' -- Cluster Total: ' + str(cluster_dict[X.ix[x].clusters]))\n",[33,56145,56146,56151,56156,56161,56166,56170,56175,56180,56185,56190,56195,56200,56205,56210,56215,56220,56225,56230,56234,56239,56244,56249,56254,56259,56264,56269,56274,56279,56284,56289,56294,56299,56304,56309,56314,56319],{"__ignoreMap":35},[187,56147,56148],{"class":189,"line":190},[187,56149,56150],{},"#variables to manage the arrangement and spacing of cluster images\n",[187,56152,56153],{"class":189,"line":249},[187,56154,56155],{},"total = 0\n",[187,56157,56158],{"class":189,"line":312},[187,56159,56160],{},"rows = 0\n",[187,56162,56163],{"class":189,"line":319},[187,56164,56165],{},"im_length = 0\n",[187,56167,56168],{"class":189,"line":325},[187,56169,316],{"emptyLinePlaceholder":315},[187,56171,56172],{"class":189,"line":686},[187,56173,56174],{},"#set variables for cluster images based on cluster size\n",[187,56176,56177],{"class":189,"line":697},[187,56178,56179],{},"def set_spacing(files):\n",[187,56181,56182],{"class":189,"line":1291},[187,56183,56184],{},"    global total\n",[187,56186,56187],{"class":189,"line":1306},[187,56188,56189],{},"    global rows\n",[187,56191,56192],{"class":189,"line":1434},[187,56193,56194],{},"    global im_length\n",[187,56196,56197],{"class":189,"line":2599},[187,56198,56199],{},"    #columns = 6\n",[187,56201,56202],{"class":189,"line":2607},[187,56203,56204],{},"    total = len(files)\n",[187,56206,56207],{"class":189,"line":2621},[187,56208,56209],{},"    extras = len(files) % 6\n",[187,56211,56212],{"class":189,"line":2631},[187,56213,56214],{},"    if extras > 0:\n",[187,56216,56217],{"class":189,"line":2642},[187,56218,56219],{},"        total += (6 - extras)\n",[187,56221,56222],{"class":189,"line":2653},[187,56223,56224],{},"    rows = total/6.\n",[187,56226,56227],{"class":189,"line":2665},[187,56228,56229],{},"    im_length = rows*(20/9.)\n",[187,56231,56232],{"class":189,"line":2674},[187,56233,316],{"emptyLinePlaceholder":315},[187,56235,56236],{"class":189,"line":2684},[187,56237,56238],{},"#use matplotlib to show images loaded with PIL\n",[187,56240,56241],{"class":189,"line":2694},[187,56242,56243],{},"def show_images(cluster_num, samples = 0,  files_bool=False, files=None):\n",[187,56245,56246],{"class":189,"line":2706},[187,56247,56248],{},"    if files_bool==True:\n",[187,56250,56251],{"class":189,"line":2715},[187,56252,56253],{},"        files1 = np.random.choice(files.index, min(files.shape[0], samples), replace=False)\n",[187,56255,56256],{"class":189,"line":2725},[187,56257,56258],{},"    if (samples == 0) & (files_bool==False):\n",[187,56260,56261],{"class":189,"line":2735},[187,56262,56263],{},"        files1 = X[X.clusters==cluster_num].index\n",[187,56265,56266],{"class":189,"line":2743},[187,56267,56268],{},"    if (samples > 0) & (files_bool==False):\n",[187,56270,56271],{"class":189,"line":2754},[187,56272,56273],{},"        files1 = np.random.choice(X[X.clusters==cluster_num].index, min(cluster_dict[cluster_num],samples), replace=False)\n",[187,56275,56276],{"class":189,"line":2762},[187,56277,56278],{},"    set_spacing(files1)\n",[187,56280,56281],{"class":189,"line":2770},[187,56282,56283],{},"    plt.figure(figsize = (14,im_length))\n",[187,56285,56286],{"class":189,"line":2781},[187,56287,56288],{},"    os.chdir(os.path.expanduser('~/Documents/CA_1/imgs/'))\n",[187,56290,56291],{"class":189,"line":2792},[187,56292,56293],{},"    for num, x in enumerate(files1):\n",[187,56295,56296],{"class":189,"line":2803},[187,56297,56298],{},"        img = PIL.Image.open(x)\n",[187,56300,56301],{"class":189,"line":2808},[187,56302,56303],{},"        plt.subplot(rows,6,num+1)\n",[187,56305,56306],{"class":189,"line":2816},[187,56307,56308],{},"        plt.title(x.split('.')[0])\n",[187,56310,56311],{"class":189,"line":2824},[187,56312,56313],{},"        plt.axis('off')\n",[187,56315,56316],{"class":189,"line":2834},[187,56317,56318],{},"        plt.imshow(img)\n",[187,56320,56321],{"class":189,"line":2845},[187,56322,56323],{},"    print('Cluster #' + str(X.ix[x].clusters) + ' -- Cluster Total: ' + str(cluster_dict[X.ix[x].clusters]))\n",[26,56325,56327],{"className":1383,"code":56326,"language":1125,"meta":35,"style":35},"show_images(16, samples=12)\n",[33,56328,56329],{"__ignoreMap":35},[187,56330,56331],{"class":189,"line":190},[187,56332,56326],{},[26,56334,56337],{"className":56335,"code":56336,"language":31},[29],"Cluster #16 -- Cluster Total: 3293\n",[33,56338,56336],{"__ignoreMap":35},[11,56340,56341],{},[511,56342],{"alt":7255,"src":56343},"/static/ants_files/ants_44_1.png",[26,56345,56347],{"className":1383,"code":56346,"language":1125,"meta":35,"style":35},"show_images(46, samples=12)\n",[33,56348,56349],{"__ignoreMap":35},[187,56350,56351],{"class":189,"line":190},[187,56352,56346],{},[26,56354,56357],{"className":56355,"code":56356,"language":31},[29],"Cluster #46 -- Cluster Total: 2993\n",[33,56358,56356],{"__ignoreMap":35},[11,56360,56361],{},[511,56362],{"alt":7255,"src":56363},"/static/ants_files/ants_45_1.png",[26,56365,56367],{"className":1383,"code":56366,"language":1125,"meta":35,"style":35},"show_images(51, samples=12)\n",[33,56368,56369],{"__ignoreMap":35},[187,56370,56371],{"class":189,"line":190},[187,56372,56366],{},[26,56374,56377],{"className":56375,"code":56376,"language":31},[29],"Cluster #51 -- Cluster Total: 25\n",[33,56378,56376],{"__ignoreMap":35},[11,56380,56381],{},[511,56382],{"alt":7255,"src":56383},"/static/ants_files/ants_46_1.png",[26,56385,56387],{"className":1383,"code":56386,"language":1125,"meta":35,"style":35},"show_images(18, samples=12)\n",[33,56388,56389],{"__ignoreMap":35},[187,56390,56391],{"class":189,"line":190},[187,56392,56386],{},[26,56394,56397],{"className":56395,"code":56396,"language":31},[29],"Cluster #18 -- Cluster Total: 12\n",[33,56398,56396],{"__ignoreMap":35},[11,56400,56401],{},[511,56402],{"alt":7255,"src":56403},"/static/ants_files/ants_47_1.png",[26,56405,56407],{"className":1383,"code":56406,"language":1125,"meta":35,"style":35},"show_images(4, samples=12)\n",[33,56408,56409],{"__ignoreMap":35},[187,56410,56411],{"class":189,"line":190},[187,56412,56406],{},[26,56414,56417],{"className":56415,"code":56416,"language":31},[29],"Cluster #4 -- Cluster Total: 302\n",[33,56418,56416],{"__ignoreMap":35},[11,56420,56421],{},[511,56422],{"alt":7255,"src":56423},"/static/ants_files/ants_48_1.png",[26,56425,56427],{"className":1383,"code":56426,"language":1125,"meta":35,"style":35},"show_images(5, samples=12)\n",[33,56428,56429],{"__ignoreMap":35},[187,56430,56431],{"class":189,"line":190},[187,56432,56426],{},[26,56434,56437],{"className":56435,"code":56436,"language":31},[29],"Cluster #5 -- Cluster Total: 1412\n",[33,56438,56436],{"__ignoreMap":35},[11,56440,56441],{},[511,56442],{"alt":7255,"src":56443},"/static/ants_files/ants_49_1.png",[26,56445,56447],{"className":1383,"code":56446,"language":1125,"meta":35,"style":35},"show_images(6, samples=12)\n",[33,56448,56449],{"__ignoreMap":35},[187,56450,56451],{"class":189,"line":190},[187,56452,56446],{},[26,56454,56457],{"className":56455,"code":56456,"language":31},[29],"Cluster #6 -- Cluster Total: 35\n",[33,56458,56456],{"__ignoreMap":35},[11,56460,56461],{},[511,56462],{"alt":7255,"src":56463},"/static/ants_files/ants_50_1.png",[26,56465,56467],{"className":1383,"code":56466,"language":1125,"meta":35,"style":35},"show_images(9, samples=12)\n",[33,56468,56469],{"__ignoreMap":35},[187,56470,56471],{"class":189,"line":190},[187,56472,56466],{},[26,56474,56477],{"className":56475,"code":56476,"language":31},[29],"Cluster #9 -- Cluster Total: 218\n",[33,56478,56476],{"__ignoreMap":35},[11,56480,56481],{},[511,56482],{"alt":7255,"src":56483},"/static/ants_files/ants_51_1.png",[26,56485,56487],{"className":1383,"code":56486,"language":1125,"meta":35,"style":35},"show_images(12, samples=12)\n",[33,56488,56489],{"__ignoreMap":35},[187,56490,56491],{"class":189,"line":190},[187,56492,56486],{},[26,56494,56497],{"className":56495,"code":56496,"language":31},[29],"Cluster #12 -- Cluster Total: 11\n",[33,56498,56496],{"__ignoreMap":35},[11,56500,56501],{},[511,56502],{"alt":7255,"src":56503},"/static/ants_files/ants_52_1.png",[11,56505,56506],{},"The next four cluster samples are the largest clusters:",[26,56508,56510],{"className":1383,"code":56509,"language":1125,"meta":35,"style":35},"show_images(30, samples=12)\n",[33,56511,56512],{"__ignoreMap":35},[187,56513,56514],{"class":189,"line":190},[187,56515,56509],{},[26,56517,56520],{"className":56518,"code":56519,"language":31},[29],"Cluster #30 -- Cluster Total: 45\n",[33,56521,56519],{"__ignoreMap":35},[11,56523,56524],{},[511,56525],{"alt":7255,"src":56526},"/static/ants_files/ants_54_1.png",[26,56528,56529],{"className":1383,"code":56509,"language":1125,"meta":35,"style":35},[33,56530,56531],{"__ignoreMap":35},[187,56532,56533],{"class":189,"line":190},[187,56534,56509],{},[26,56536,56538],{"className":56537,"code":56519,"language":31},[29],[33,56539,56519],{"__ignoreMap":35},[11,56541,56542],{},[511,56543],{"alt":7255,"src":56544},"/static/ants_files/ants_55_1.png",[26,56546,56548],{"className":1383,"code":56547,"language":1125,"meta":35,"style":35},"show_images(0, samples=12)\n",[33,56549,56550],{"__ignoreMap":35},[187,56551,56552],{"class":189,"line":190},[187,56553,56547],{},[26,56555,56558],{"className":56556,"code":56557,"language":31},[29],"Cluster #0 -- Cluster Total: 2752\n",[33,56559,56557],{"__ignoreMap":35},[11,56561,56562],{},[511,56563],{"alt":7255,"src":56564},"/static/ants_files/ants_56_1.png",[26,56566,56568],{"className":1383,"code":56567,"language":1125,"meta":35,"style":35},"show_images(30, samples=15)\n",[33,56569,56570],{"__ignoreMap":35},[187,56571,56572],{"class":189,"line":190},[187,56573,56567],{},[26,56575,56577],{"className":56576,"code":56519,"language":31},[29],[33,56578,56519],{"__ignoreMap":35},[11,56580,56581],{},[511,56582],{"alt":7255,"src":56583},"/static/ants_files/ants_57_1.png",[26,56585,56587],{"className":1383,"code":56586,"language":1125,"meta":35,"style":35},"show_images(63, samples=12)\n",[33,56588,56589],{"__ignoreMap":35},[187,56590,56591],{"class":189,"line":190},[187,56592,56586],{},[26,56594,56597],{"className":56595,"code":56596,"language":31},[29],"Cluster #63 -- Cluster Total: 123\n",[33,56598,56596],{"__ignoreMap":35},[11,56600,56601],{},[511,56602],{"alt":7255,"src":56603},"/static/ants_files/ants_58_1.png",[26,56605,56607],{"className":1383,"code":56606,"language":1125,"meta":35,"style":35},"show_images(60, samples=12)\n",[33,56608,56609],{"__ignoreMap":35},[187,56610,56611],{"class":189,"line":190},[187,56612,56606],{},[26,56614,56617],{"className":56615,"code":56616,"language":31},[29],"Cluster #60 -- Cluster Total: 30\n",[33,56618,56616],{"__ignoreMap":35},[11,56620,56621],{},[511,56622],{"alt":7255,"src":56623},"/static/ants_files/ants_59_1.png",[26,56625,56627],{"className":1383,"code":56626,"language":1125,"meta":35,"style":35},"show_images(59, samples=12)\n",[33,56628,56629],{"__ignoreMap":35},[187,56630,56631],{"class":189,"line":190},[187,56632,56626],{},[26,56634,56637],{"className":56635,"code":56636,"language":31},[29],"Cluster #59 -- Cluster Total: 10\n",[33,56638,56636],{"__ignoreMap":35},[11,56640,56641],{},[511,56642],{"alt":7255,"src":56643},"/static/ants_files/ants_60_1.png",[26,56645,56647],{"className":1383,"code":56646,"language":1125,"meta":35,"style":35},"show_images(57, samples=12)\n",[33,56648,56649],{"__ignoreMap":35},[187,56650,56651],{"class":189,"line":190},[187,56652,56646],{},[26,56654,56657],{"className":56655,"code":56656,"language":31},[29],"Cluster #57 -- Cluster Total: 314\n",[33,56658,56656],{"__ignoreMap":35},[11,56660,56661],{},[511,56662],{"alt":7255,"src":56663},"/static/ants_files/ants_61_1.png",[26,56665,56667],{"className":1383,"code":56666,"language":1125,"meta":35,"style":35},"show_images(56, samples=12)\n",[33,56668,56669],{"__ignoreMap":35},[187,56670,56671],{"class":189,"line":190},[187,56672,56666],{},[26,56674,56677],{"className":56675,"code":56676,"language":31},[29],"Cluster #56 -- Cluster Total: 1\n",[33,56678,56676],{"__ignoreMap":35},[11,56680,56681],{},[511,56682],{"alt":7255,"src":56683},"/static/ants_files/ants_62_1.png",[26,56685,56687],{"className":1383,"code":56686,"language":1125,"meta":35,"style":35},"show_images(55, samples=12)\n",[33,56688,56689],{"__ignoreMap":35},[187,56690,56691],{"class":189,"line":190},[187,56692,56686],{},[26,56694,56697],{"className":56695,"code":56696,"language":31},[29],"Cluster #55 -- Cluster Total: 38\n",[33,56698,56696],{"__ignoreMap":35},[11,56700,56701],{},[511,56702],{"alt":7255,"src":56703},"/static/ants_files/ants_63_1.png",[26,56705,56707],{"className":1383,"code":56706,"language":1125,"meta":35,"style":35},"show_images(54, samples=12)\n",[33,56708,56709],{"__ignoreMap":35},[187,56710,56711],{"class":189,"line":190},[187,56712,56706],{},[26,56714,56717],{"className":56715,"code":56716,"language":31},[29],"Cluster #54 -- Cluster Total: 686\n",[33,56718,56716],{"__ignoreMap":35},[11,56720,56721],{},[511,56722],{"alt":7255,"src":56723},"/static/ants_files/ants_64_1.png",[26,56725,56727],{"className":1383,"code":56726,"language":1125,"meta":35,"style":35},"show_images(53, samples=12)\n",[33,56728,56729],{"__ignoreMap":35},[187,56730,56731],{"class":189,"line":190},[187,56732,56726],{},[26,56734,56737],{"className":56735,"code":56736,"language":31},[29],"Cluster #53 -- Cluster Total: 8\n",[33,56738,56736],{"__ignoreMap":35},[11,56740,56741],{},[511,56742],{"alt":7255,"src":56743},"/static/ants_files/ants_65_1.png",[26,56745,56747],{"className":1383,"code":56746,"language":1125,"meta":35,"style":35},"show_images(52, samples=12)\n",[33,56748,56749],{"__ignoreMap":35},[187,56750,56751],{"class":189,"line":190},[187,56752,56746],{},[26,56754,56757],{"className":56755,"code":56756,"language":31},[29],"Cluster #52 -- Cluster Total: 2540\n",[33,56758,56756],{"__ignoreMap":35},[11,56760,56761],{},[511,56762],{"alt":7255,"src":56763},"/static/ants_files/ants_66_1.png",[26,56765,56766],{"className":1383,"code":56366,"language":1125,"meta":35,"style":35},[33,56767,56768],{"__ignoreMap":35},[187,56769,56770],{"class":189,"line":190},[187,56771,56366],{},[26,56773,56775],{"className":56774,"code":56376,"language":31},[29],[33,56776,56376],{"__ignoreMap":35},[11,56778,56779],{},[511,56780],{"alt":7255,"src":56781},"/static/ants_files/ants_67_1.png",[26,56783,56785],{"className":1383,"code":56784,"language":1125,"meta":35,"style":35},"show_images(50, samples=12)\n",[33,56786,56787],{"__ignoreMap":35},[187,56788,56789],{"class":189,"line":190},[187,56790,56784],{},[26,56792,56795],{"className":56793,"code":56794,"language":31},[29],"Cluster #50 -- Cluster Total: 2185\n",[33,56796,56794],{"__ignoreMap":35},[11,56798,56799],{},[511,56800],{"alt":7255,"src":56801},"/static/ants_files/ants_68_1.png",[26,56803,56805],{"className":1383,"code":56804,"language":1125,"meta":35,"style":35},"show_images(49, samples=12)\n",[33,56806,56807],{"__ignoreMap":35},[187,56808,56809],{"class":189,"line":190},[187,56810,56804],{},[26,56812,56815],{"className":56813,"code":56814,"language":31},[29],"Cluster #49 -- Cluster Total: 178\n",[33,56816,56814],{"__ignoreMap":35},[11,56818,56819],{},[511,56820],{"alt":7255,"src":56821},"/static/ants_files/ants_69_1.png",[26,56823,56825],{"className":1383,"code":56824,"language":1125,"meta":35,"style":35},"show_images(48, samples=12)\n",[33,56826,56827],{"__ignoreMap":35},[187,56828,56829],{"class":189,"line":190},[187,56830,56824],{},[26,56832,56835],{"className":56833,"code":56834,"language":31},[29],"Cluster #48 -- Cluster Total: 3\n",[33,56836,56834],{"__ignoreMap":35},[11,56838,56839],{},[511,56840],{"alt":7255,"src":56841},"/static/ants_files/ants_70_1.png",[26,56843,56845],{"className":1383,"code":56844,"language":1125,"meta":35,"style":35},"show_images(47, samples=12)\n",[33,56846,56847],{"__ignoreMap":35},[187,56848,56849],{"class":189,"line":190},[187,56850,56844],{},[26,56852,56855],{"className":56853,"code":56854,"language":31},[29],"Cluster #47 -- Cluster Total: 11\n",[33,56856,56854],{"__ignoreMap":35},[11,56858,56859],{},[511,56860],{"alt":7255,"src":56861},"/static/ants_files/ants_71_1.png",[26,56863,56864],{"className":1383,"code":56346,"language":1125,"meta":35,"style":35},[33,56865,56866],{"__ignoreMap":35},[187,56867,56868],{"class":189,"line":190},[187,56869,56346],{},[26,56871,56873],{"className":56872,"code":56356,"language":31},[29],[33,56874,56356],{"__ignoreMap":35},[11,56876,56877],{},[511,56878],{"alt":7255,"src":56879},"/static/ants_files/ants_72_1.png",[26,56881,56883],{"className":1383,"code":56882,"language":1125,"meta":35,"style":35},"show_images(45, samples=12)\n",[33,56884,56885],{"__ignoreMap":35},[187,56886,56887],{"class":189,"line":190},[187,56888,56882],{},[26,56890,56893],{"className":56891,"code":56892,"language":31},[29],"Cluster #45 -- Cluster Total: 816\n",[33,56894,56892],{"__ignoreMap":35},[11,56896,56897],{},[511,56898],{"alt":7255,"src":56899},"/static/ants_files/ants_73_1.png",[26,56901,56903],{"className":1383,"code":56902,"language":1125,"meta":35,"style":35},"show_images(44, samples=12)\n",[33,56904,56905],{"__ignoreMap":35},[187,56906,56907],{"class":189,"line":190},[187,56908,56902],{},[26,56910,56913],{"className":56911,"code":56912,"language":31},[29],"Cluster #44 -- Cluster Total: 791\n",[33,56914,56912],{"__ignoreMap":35},[11,56916,56917],{},[511,56918],{"alt":7255,"src":56919},"/static/ants_files/ants_74_1.png",[26,56921,56923],{"className":1383,"code":56922,"language":1125,"meta":35,"style":35},"show_images(43, samples=12)\n",[33,56924,56925],{"__ignoreMap":35},[187,56926,56927],{"class":189,"line":190},[187,56928,56922],{},[26,56930,56933],{"className":56931,"code":56932,"language":31},[29],"Cluster #43 -- Cluster Total: 30\n",[33,56934,56932],{"__ignoreMap":35},[11,56936,56937],{},[511,56938],{"alt":7255,"src":56939},"/static/ants_files/ants_75_1.png",[26,56941,56943],{"className":1383,"code":56942,"language":1125,"meta":35,"style":35},"show_images(42, samples=12)\n",[33,56944,56945],{"__ignoreMap":35},[187,56946,56947],{"class":189,"line":190},[187,56948,56942],{},[26,56950,56953],{"className":56951,"code":56952,"language":31},[29],"Cluster #42 -- Cluster Total: 625\n",[33,56954,56952],{"__ignoreMap":35},[11,56956,56957],{},[511,56958],{"alt":7255,"src":56959},"/static/ants_files/ants_76_1.png",[26,56961,56963],{"className":1383,"code":56962,"language":1125,"meta":35,"style":35},"show_images(41, samples=12)\n",[33,56964,56965],{"__ignoreMap":35},[187,56966,56967],{"class":189,"line":190},[187,56968,56962],{},[26,56970,56973],{"className":56971,"code":56972,"language":31},[29],"Cluster #41 -- Cluster Total: 6\n",[33,56974,56972],{"__ignoreMap":35},[11,56976,56977],{},[511,56978],{"alt":7255,"src":56979},"/static/ants_files/ants_77_1.png",[26,56981,56983],{"className":1383,"code":56982,"language":1125,"meta":35,"style":35},"show_images(40, samples=12)\n",[33,56984,56985],{"__ignoreMap":35},[187,56986,56987],{"class":189,"line":190},[187,56988,56982],{},[26,56990,56993],{"className":56991,"code":56992,"language":31},[29],"Cluster #40 -- Cluster Total: 6\n",[33,56994,56992],{"__ignoreMap":35},[11,56996,56997],{},[511,56998],{"alt":7255,"src":56999},"/static/ants_files/ants_78_1.png",[26,57001,57003],{"className":1383,"code":57002,"language":1125,"meta":35,"style":35},"show_images(39, samples=12)\n",[33,57004,57005],{"__ignoreMap":35},[187,57006,57007],{"class":189,"line":190},[187,57008,57002],{},[26,57010,57013],{"className":57011,"code":57012,"language":31},[29],"Cluster #39 -- Cluster Total: 26\n",[33,57014,57012],{"__ignoreMap":35},[11,57016,57017],{},[511,57018],{"alt":7255,"src":57019},"/static/ants_files/ants_79_1.png",[26,57021,57023],{"className":1383,"code":57022,"language":1125,"meta":35,"style":35},"show_images(38, samples=12)\n",[33,57024,57025],{"__ignoreMap":35},[187,57026,57027],{"class":189,"line":190},[187,57028,57022],{},[26,57030,57033],{"className":57031,"code":57032,"language":31},[29],"Cluster #38 -- Cluster Total: 345\n",[33,57034,57032],{"__ignoreMap":35},[11,57036,57037],{},[511,57038],{"alt":7255,"src":57039},"/static/ants_files/ants_80_1.png",[26,57041,57043],{"className":1383,"code":57042,"language":1125,"meta":35,"style":35},"show_images(37, samples=12)\n",[33,57044,57045],{"__ignoreMap":35},[187,57046,57047],{"class":189,"line":190},[187,57048,57042],{},[26,57050,57053],{"className":57051,"code":57052,"language":31},[29],"Cluster #37 -- Cluster Total: 518\n",[33,57054,57052],{"__ignoreMap":35},[11,57056,57057],{},[511,57058],{"alt":7255,"src":57059},"/static/ants_files/ants_81_1.png",[11,57061,57062],{},"The vast majority of termites seem to form nondescript blobs after 100000 steps. There are perhaps many thousands of termites that didn't yet reach a . Setting the clusters parameter to 75 is probably too high. Many of the groups have similar behaviour. There were several cluster groups that formed 'highways'. It may make more sense to filter out these termites and cluster termites that didn't form highways.",[2215,57064,57066],{"id":57065},"outlier-detection","Outlier Detection",[11,57068,57069],{},"It could also be interesting to see how many outliers are present in each cluster for various values of k in the k-means algorithm. This may help us choose a more fitting number of clusters by which the termites can be grouped. here's how we could do that:",[26,57071,57073],{"className":1383,"code":57072,"language":1125,"meta":35,"style":35},"from sklearn.ensemble import IsolationForest\n",[33,57074,57075],{"__ignoreMap":35},[187,57076,57077],{"class":189,"line":190},[187,57078,57072],{},[26,57080,57082],{"className":1383,"code":57081,"language":1125,"meta":35,"style":35},"X_ = X.loc[X.clusters==37, :] #16\n",[33,57083,57084],{"__ignoreMap":35},[187,57085,57086],{"class":189,"line":190},[187,57087,57081],{},[26,57089,57091],{"className":1383,"code":57090,"language":1125,"meta":35,"style":35},"X_ = X.loc[X.clusters==37, :]\n",[33,57092,57093],{"__ignoreMap":35},[187,57094,57095],{"class":189,"line":190},[187,57096,57090],{},[26,57098,57100],{"className":1383,"code":57099,"language":1125,"meta":35,"style":35},"clf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X_)\n\ny_pred_train = clf.predict(X_)\n\n",[33,57101,57102,57107,57112,57116],{"__ignoreMap":35},[187,57103,57104],{"class":189,"line":190},[187,57105,57106],{},"clf = IsolationForest(max_samples=100, random_state=rng)\n",[187,57108,57109],{"class":189,"line":249},[187,57110,57111],{},"clf.fit(X_)\n",[187,57113,57114],{"class":189,"line":312},[187,57115,316],{"emptyLinePlaceholder":315},[187,57117,57118],{"class":189,"line":319},[187,57119,57120],{},"y_pred_train = clf.predict(X_)\n",[26,57122,57124],{"className":1383,"code":57123,"language":1125,"meta":35,"style":35},"y_pred_train.mean()\n",[33,57125,57126],{"__ignoreMap":35},[187,57127,57128],{"class":189,"line":190},[187,57129,57123],{},[26,57131,57134],{"className":57132,"code":57133,"language":31},[29],"0.79922779922779918\n",[33,57135,57133],{"__ignoreMap":35},[11,57137,57138],{},"The following values gives us the average of the predicted values (1 for inlier, -1 for outlier), so this value doesn't correspond to a percentage accuracy. The accuracy is about 89% (the model determined that 89% of termites in cluster 37 are inliers and the remaining 11% are outliers.",[26,57140,57142],{"className":1383,"code":57141,"language":1125,"meta":35,"style":35},"X_['anom'] = y_pred_train\n",[33,57143,57144],{"__ignoreMap":35},[187,57145,57146],{"class":189,"line":190},[187,57147,57141],{},[26,57149,57151],{"className":1383,"code":57150,"language":1125,"meta":35,"style":35},"X_.anom.value_counts()\n",[33,57152,57153],{"__ignoreMap":35},[187,57154,57155],{"class":189,"line":190},[187,57156,57150],{},[26,57158,57161],{"className":57159,"code":57160,"language":31},[29]," 1    466\n-1     52\nName: anom, dtype: int64\n",[33,57162,57160],{"__ignoreMap":35},[11,57164,57165],{},"Let's compare some of the inliers with the outliers:",[26,57167,57169],{"className":1383,"code":57168,"language":1125,"meta":35,"style":35},"files_normal = X_[X_.anom==(1)]\nshow_images(0, samples = 24, files_bool=True, files=files_normal)\n",[33,57170,57171,57176],{"__ignoreMap":35},[187,57172,57173],{"class":189,"line":190},[187,57174,57175],{},"files_normal = X_[X_.anom==(1)]\n",[187,57177,57178],{"class":189,"line":249},[187,57179,57180],{},"show_images(0, samples = 24, files_bool=True, files=files_normal)\n",[26,57182,57184],{"className":57183,"code":57052,"language":31},[29],[33,57185,57052],{"__ignoreMap":35},[11,57187,57188],{},[511,57189],{"alt":7255,"src":57190},"/static/ants_files/ants_92_1.png",[26,57192,57194],{"className":1383,"code":57193,"language":1125,"meta":35,"style":35},"files_abnormal = X_[X_.anom==(-1)]\nshow_images(0, samples = 24, files_bool=True, files=files_abnormal)\n",[33,57195,57196,57201],{"__ignoreMap":35},[187,57197,57198],{"class":189,"line":190},[187,57199,57200],{},"files_abnormal = X_[X_.anom==(-1)]\n",[187,57202,57203],{"class":189,"line":249},[187,57204,57205],{},"show_images(0, samples = 24, files_bool=True, files=files_abnormal)\n",[26,57207,57209],{"className":57208,"code":57052,"language":31},[29],[33,57210,57052],{"__ignoreMap":35},[11,57212,57213],{},[511,57214],{"alt":7255,"src":57215},"/static/ants_files/ants_93_1.png",[11,57217,57218],{},"This sample of outliers seems to have slightly different characteristics compared with the inlier sample. This can be seen in the patches of solid colors (pink, purple, teal, grey).",[2215,57220,2413],{"id":15198},[11,57222,57223],{},"Using k-means and Isolation Forests with this set of over 30,000 termites offers a quick and easy way to sort out major trends that these deterministic systems display. As you can see in the cluster samples above, the classification is far from perfect. Some near-identical termites are in different clusters. It would be interesting to tweak some aspects of this experiment in the future:",[916,57225,57226,57229,57232,57235,57238],{},[919,57227,57228],{},"Larger number of states (>16)",[919,57230,57231],{},"More steps (>100000) / bigger grid",[919,57233,57234],{},"Random \"noise\" on the grid at step 0",[919,57236,57237],{},"Variation on the rules",[919,57239,57240],{},"Segmenting 'highway' termites before clustering",[855,57242,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":57244},[],"2017-04-02","Visualizing variations of the classic Langton's Ant cellular automata","/static/ant.png",{"layout":29014},"/2017/04/02/langton-ant-notebook",{"title":53477,"description":57246},"2017/04/02/langton-ant-notebook",[1125,57253],"cellular-automata","9L-g-36gKrOKSXBi5q-6lD8oIyZbucVRMaotpNzQvHM",{"id":57256,"title":57257,"body":57258,"comments":315,"date":58624,"description":57262,"draft":872,"extension":873,"external":874,"image":58625,"meta":58626,"navigation":315,"path":58627,"seo":58628,"stem":58629,"tags":58630,"__hash__":58631},"blog/2017/03/04/settinging-up-django-with-heroku.md","Setting up a Django app on Heroku",{"type":8,"value":57259,"toc":58622},[57260,57263,57265,57268,57274,57277,57283,57289,57297,57333,57342,57346,57369,57394,57400,57403,57409,57412,57418,57423,57429,57432,57438,57443,57449,57455,57461,57468,57471,57477,57484,57488,57494,57509,57516,57519,57525,57535,57544,57550,57556,57559,57565,57568,57574,57577,57583,57590,57593,57598,57604,57607,57613,57616,57622,57630,57662,57665,57670,57673,57679,57687,57693,57696,57702,57708,57715,57720,57736,57742,57745,57749,57817,57820,57826,57831,57900,57907,57956,57962,57968,57975,57981,57989,57995,57998,58002,58005,58008,58014,58019,58026,58065,58071,58114,58127,58133,58139,58145,58154,58168,58186,58193,58199,58205,58603,58611,58617,58619],[11,57261,57262],{},"This is a simple guide to setting up a Django project on Heroku.",[11,57264,38063],{},[11,57266,57267],{},"The first step is to create a virutal environment in a new directory:",[26,57269,57272],{"className":57270,"code":57271,"language":31},[29],"$ mkdir proj && cd proj\n$ virtualenv -p python3 .\n$ source bin/activate\n(proj) $ mkdir src\n(proj) $ cd src\n(proj) $ pip install django==1.10.5\n(proj) $ django-admin.py startproject myproj .\n(proj) $ ls\nmyproj        manage.py\n",[33,57273,57271],{"__ignoreMap":35},[11,57275,57276],{},"This sets up a virtual environment and creates an empty Django project. The next step is to create a settings module.",[26,57278,57281],{"className":57279,"code":57280,"language":31},[29],"(proj) $ cd myproj\n(proj) $ mkdir settings && cd settings\n",[33,57282,57280],{"__ignoreMap":35},[11,57284,57285,57286,57288],{},"Next we want to add ",[33,57287,44114],{}," to settings to make it a python module.",[11,57290,57291],{},[4339,57292,57293,57294,57296],{},"src/myproj/settings/",[338,57295,11524],{},".py",[26,57298,57299],{"className":1383,"code":44568,"language":1125,"meta":35,"style":35},[33,57300,57301,57305,57309,57313,57317,57321,57325,57329],{"__ignoreMap":35},[187,57302,57303],{"class":189,"line":190},[187,57304,44575],{},[187,57306,57307],{"class":189,"line":249},[187,57308,316],{"emptyLinePlaceholder":315},[187,57310,57311],{"class":189,"line":312},[187,57312,44584],{},[187,57314,57315],{"class":189,"line":319},[187,57316,316],{"emptyLinePlaceholder":315},[187,57318,57319],{"class":189,"line":325},[187,57320,44593],{},[187,57322,57323],{"class":189,"line":686},[187,57324,44598],{},[187,57326,57327],{"class":189,"line":697},[187,57328,44603],{},[187,57330,57331],{"class":189,"line":1291},[187,57332,44608],{},[11,57334,57335,57336,57339,57340,358],{},"Next we want to change the ",[33,57337,57338],{},"BASE_DIR"," (base directory) in ",[33,57341,31438],{},[11,57343,57344],{},[4339,57345,31438],{},[26,57347,57349],{"className":1383,"code":57348,"language":1125,"meta":35,"style":35},"[...]\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n[...]\n",[33,57350,57351,57355,57360,57365],{"__ignoreMap":35},[187,57352,57353],{"class":189,"line":190},[187,57354,47849],{},[187,57356,57357],{"class":189,"line":249},[187,57358,57359],{},"# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n",[187,57361,57362],{"class":189,"line":312},[187,57363,57364],{},"BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",[187,57366,57367],{"class":189,"line":319},[187,57368,47849],{},[11,57370,57371,57372,57374,57375,57377,57378,57381,57382,57384,57385,1172,57388,57391,57392,752],{},"Next we need to move ",[33,57373,31438],{}," into the ",[33,57376,29715],{}," folder and rename it ",[33,57379,57380],{},"base.py"," and then copy ",[33,57383,57380],{}," twice as ",[33,57386,57387],{},"local.py",[33,57389,57390],{},"production.py"," these three files will live in ",[33,57393,29715],{},[26,57395,57398],{"className":57396,"code":57397,"language":31},[29],"(proj) $ mv settings.py settings\n(proj) $ cd settings\n(proj) $ mv settings.py base.py\n(proj) $ cp base.py local.py\n(proj) $ cp base.py production.py\n",[33,57399,57397],{"__ignoreMap":35},[11,57401,57402],{},"Next we need to install PostgreSQL:",[26,57404,57407],{"className":57405,"code":57406,"language":31},[29],"(proj) $ pip install psycopg2\n(proj) $ pip install gunicorn dj-database-url\n(proj) $ pip install django-crispy-forms\n(proj) $ pip install pillow\n",[33,57408,57406],{"__ignoreMap":35},[11,57410,57411],{},"At this point we can check to see if everything installed correctly:",[26,57413,57416],{"className":57414,"code":57415,"language":31},[29],"(proj) $ pip freeze\nappdirs==1.4.3\ndj-database-url==0.4.2\nDjango==1.10.5\ndjango-crispy-forms==1.6.1\ngunicorn==19.7.1\nolefile==0.44\npackaging==16.8\nPillow==4.0.0\npsycopg2==2.7.1\npyparsing==2.2.0\nsix==1.10.0\n",[33,57417,57415],{"__ignoreMap":35},[11,57419,57420,57421,358],{},"And then we can add these to a file in our base directory called ",[33,57422,23123],{},[26,57424,57427],{"className":57425,"code":57426,"language":31},[29],"(proj) $ pip freeze > requirements.txt\n",[33,57428,57426],{"__ignoreMap":35},[11,57430,57431],{},"Next we can run migrations and create a superuser:",[26,57433,57436],{"className":57434,"code":57435,"language":31},[29],"(proj) $ python manage.py migrate\n(proj) $ python manage.py createsuperuser\n",[33,57437,57435],{"__ignoreMap":35},[11,57439,57440,57441,358],{},"Next we need to initialize our git repository and create ",[33,57442,783],{},[26,57444,57447],{"className":57445,"code":57446,"language":31},[29],"(proj) $ git init\n",[33,57448,57446],{"__ignoreMap":35},[11,57450,57451,57452,57454],{},"We can put ",[33,57453,783],{}," in our base directory and add the following:",[26,57456,57459],{"className":57457,"code":57458,"language":31,"meta":35},[29],"myproj/settings/local.py\n",[33,57460,57458],{"__ignoreMap":35},[11,57462,57463,57464,752],{},"We also want to ignore several other python-related files in our directory. An easy way to do this is to add python gitignore. This can be found ",[15,57465,1321],{"href":57466,"rel":57467},"https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore",[19],[11,57469,57470],{},"Next we can make our first commit:",[26,57472,57475],{"className":57473,"code":57474,"language":31},[29],"(proj) $ git add --all\n(proj) $ git commit -m \"initial commit\"\n",[33,57476,57474],{"__ignoreMap":35},[11,57478,57479,57480,57483],{},"The next step involves setting up Heroku. First we need to create a ",[33,57481,57482],{},"Procfile"," in our base directory:",[11,57485,57486],{},[4339,57487,57482],{},[26,57489,57492],{"className":57490,"code":57491,"language":31,"meta":35},[29],"web: gunicorn myproj.wsgi --log-file -\n",[33,57493,57491],{"__ignoreMap":35},[11,57495,57496,57497,765,57500,4313,57503,637,57505,1172,57507,752],{},"Next we can try to run Heroku locally, but first we need to add ",[33,57498,57499],{},"0.0.0.0",[33,57501,57502],{},"ALLOWED_HOSTS",[33,57504,57390],{},[33,57506,57380],{},[33,57508,57387],{},[11,57510,57511,57512,57515],{},"We should now see \"It worked!\" at ",[33,57513,57514],{},"0.0.0.0:5000",", which tells us that everything is working properly.",[11,57517,57518],{},"Next we need to create the project on Heroku:",[26,57520,57523],{"className":57521,"code":57522,"language":31},[29],"(proj) $ heroku login\n(proj) $ heroku create my-unique-project-name-123\n",[33,57524,57522],{"__ignoreMap":35},[11,57526,57527,57528,57531,57532],{},"Now we can see our project at ",[33,57529,57530],{},"my-unique-project-name-123.herokuapp.com",", and it should say ",[33,57533,57534],{},"Heroku | Welcome to your new app!",[11,57536,57537,57538,765,57540,4313,57542,752],{},"Next we will want to add ",[33,57539,57530],{},[33,57541,57502],{},[33,57543,31438],{},[11,57545,57546,57547,57483],{},"The very last step is to add the specific version of Python to a file called ",[33,57548,57549],{},"runtime.txt",[26,57551,57554],{"className":57552,"code":57553,"language":31},[29],"(proj) $ python -V\nPython 3.4.3\n(proj) $ echo \"python-3.4.3\" > runtime.txt\n",[33,57555,57553],{"__ignoreMap":35},[11,57557,57558],{},"Before we push to Heroku we need to change a setting on Heroku related to static files:",[26,57560,57563],{"className":57561,"code":57562,"language":31},[29],"(proj) $ heroku config:set DISABLE_COLLECTSTATIC=1\n",[33,57564,57562],{"__ignoreMap":35},[11,57566,57567],{},"Now we can finally push the git repository to Heroku:",[26,57569,57572],{"className":57570,"code":57571,"language":31},[29],"(proj) $ git push heroku master\n",[33,57573,57571],{"__ignoreMap":35},[11,57575,57576],{},"Now if we go to our site on heroku we should see:",[26,57578,57581],{"className":57579,"code":57580,"language":31},[29],"Not Found\n\nThe requested URL / was not found on this server.\n",[33,57582,57580],{"__ignoreMap":35},[11,57584,57585,57586,752],{},"There is a helpful guide on deploying Python and Django apps on Heroku's website ",[15,57587,1321],{"href":57588,"rel":57589},"https://devcenter.heroku.com/articles/deploying-python",[19],[11,57591,57592],{},"Here's an important excerpt regarding databases:",[107,57594,57595],{},[11,57596,57597],{},"For Django applications, a Heroku Postgres hobby-dev database is automatically provisioned. This populates the DATABASE_URL environment variable.\nNo add-ons are automatically provisioned if a pure Python application is detected. If you need a SQL database for your app, add one explicitly:",[26,57599,57602],{"className":57600,"code":57601,"language":31},[29],"$ heroku addons:create heroku-postgresql:hobby-dev\n",[33,57603,57601],{"__ignoreMap":35},[11,57605,57606],{},"So we need to run this command:",[26,57608,57611],{"className":57609,"code":57610,"language":31},[29],"\n(proj) $ heroku addons:create heroku-postgresql:hobby-dev\n\n",[33,57612,57610],{"__ignoreMap":35},[11,57614,57615],{},"Next we need to access the terminal on our Heroku server:",[26,57617,57620],{"className":57618,"code":57619,"language":31},[29],"(proj) $ heroku run bash\n",[33,57621,57619],{"__ignoreMap":35},[11,57623,57624,57625,22088,57627,57629],{},"Next we need to add database-related information to ",[33,57626,57390],{},[33,57628,31517],{}," section:",[26,57631,57633],{"className":1383,"code":57632,"language":1125,"meta":35,"style":35},"[...]\nimport dj_database_url\n\ndb_from_env = dj_database_url.config()\nDATABASES['default'].update(db_from_env)\n[...]\n",[33,57634,57635,57639,57644,57648,57653,57658],{"__ignoreMap":35},[187,57636,57637],{"class":189,"line":190},[187,57638,47849],{},[187,57640,57641],{"class":189,"line":249},[187,57642,57643],{},"import dj_database_url\n",[187,57645,57646],{"class":189,"line":312},[187,57647,316],{"emptyLinePlaceholder":315},[187,57649,57650],{"class":189,"line":319},[187,57651,57652],{},"db_from_env = dj_database_url.config()\n",[187,57654,57655],{"class":189,"line":325},[187,57656,57657],{},"DATABASES['default'].update(db_from_env)\n",[187,57659,57660],{"class":189,"line":686},[187,57661,47849],{},[11,57663,57664],{},"Now we can push to Heroku:",[26,57666,57668],{"className":57667,"code":57571,"language":31},[29],[33,57669,57571],{"__ignoreMap":35},[11,57671,57672],{},"Next we can run our migrations:",[26,57674,57677],{"className":57675,"code":57676,"language":31},[29],"(proj) $ heroku run python manage.py makemigrations\n",[33,57678,57676],{"__ignoreMap":35},[11,57680,57681,57682,1172,57684,57686],{},"Next we can run ",[33,57683,13074],{},[33,57685,32452],{}," on our Heroku server:",[26,57688,57691],{"className":57689,"code":57690,"language":31},[29],"(proj) $ heroku run python manage.py migrate && heroku run python manage.py createsuperuser\n",[33,57692,57690],{"__ignoreMap":35},[11,57694,57695],{},"Now we should be able to login to the admin page with the account we just created, but CSS is still not working at this point. Here's how we configure static files to get CSS to work:",[26,57697,57700],{"className":57698,"code":57699,"language":31},[29],"(proj) $ pip install whitenoise\n",[33,57701,57699],{"__ignoreMap":35},[11,57703,57704,57707],{},[33,57705,57706],{},"whitenoise"," is needed so Heroku can run our static files.",[11,57709,57710,57711,4313,57713,358],{},"Next we want to make sure to include ",[33,57712,57706],{},[33,57714,23123],{},[26,57716,57718],{"className":57717,"code":57426,"language":31},[29],[33,57719,57426],{"__ignoreMap":35},[11,57721,57722,57723,57726,57727,57729,57730,57732,57733,358],{},"Next we need to add the following item to the list of ",[33,57724,57725],{},"MIDDLEWARE"," components. This needs to be added to BOTH ",[33,57728,57390],{}," AND ",[33,57731,57380],{}," in order for the local files to show on both our heroku site and the locally served site with ",[33,57734,57735],{},"heroku local web",[26,57737,57740],{"className":57738,"code":57739,"language":31,"meta":35},[29],"MIDDLEWARE = [\n[...]\n'whitenoise.middleware.WhiteNoiseMiddleware',\n[...]\n]\n",[33,57741,57739],{"__ignoreMap":35},[11,57743,57744],{},"Next we have a few more items to add to our settings files:",[11,57746,57747],{},[4339,57748,57390],{},[26,57750,57752],{"className":1383,"code":57751,"language":1125,"meta":35,"style":35},"\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, \"static\"),\n)\n\nSTATIC_ROOT = os.path.join(BASE_DIR, \"live-static\", \"static-root\")\n\nSTATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'\n\n#STATIC_ROOT = \"/home/cfedeploy/webapps/cfehome_static_root/\"\n\nMEDIA_URL = \"/media/\"\n\nMEDIA_ROOT = os.path.join(BASE_DIR, \"live-static\", \"media-root\")\n\n",[33,57753,57754,57758,57763,57768,57772,57776,57781,57785,57790,57794,57799,57803,57808,57812],{"__ignoreMap":35},[187,57755,57756],{"class":189,"line":190},[187,57757,316],{"emptyLinePlaceholder":315},[187,57759,57760],{"class":189,"line":249},[187,57761,57762],{},"STATICFILES_DIRS = (\n",[187,57764,57765],{"class":189,"line":312},[187,57766,57767],{},"    os.path.join(BASE_DIR, \"static\"),\n",[187,57769,57770],{"class":189,"line":319},[187,57771,621],{},[187,57773,57774],{"class":189,"line":325},[187,57775,316],{"emptyLinePlaceholder":315},[187,57777,57778],{"class":189,"line":686},[187,57779,57780],{},"STATIC_ROOT = os.path.join(BASE_DIR, \"live-static\", \"static-root\")\n",[187,57782,57783],{"class":189,"line":697},[187,57784,316],{"emptyLinePlaceholder":315},[187,57786,57787],{"class":189,"line":1291},[187,57788,57789],{},"STATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'\n",[187,57791,57792],{"class":189,"line":1306},[187,57793,316],{"emptyLinePlaceholder":315},[187,57795,57796],{"class":189,"line":1434},[187,57797,57798],{},"#STATIC_ROOT = \"/home/cfedeploy/webapps/cfehome_static_root/\"\n",[187,57800,57801],{"class":189,"line":2599},[187,57802,316],{"emptyLinePlaceholder":315},[187,57804,57805],{"class":189,"line":2607},[187,57806,57807],{},"MEDIA_URL = \"/media/\"\n",[187,57809,57810],{"class":189,"line":2621},[187,57811,316],{"emptyLinePlaceholder":315},[187,57813,57814],{"class":189,"line":2631},[187,57815,57816],{},"MEDIA_ROOT = os.path.join(BASE_DIR, \"live-static\", \"media-root\")\n",[11,57818,57819],{},"And we also need to add some files to our base directory that will hold our static files:",[26,57821,57824],{"className":57822,"code":57823,"language":31},[29],"(proj) $ mkdir static\n(proj) $ echo \"body {color:#000;}\" > static/main.css\n(proj) $ mkdir live-static\n(proj) $ mkdir live-static/static-root\n(proj) $ mkdir live-static/media-root\n(proj) $\n(proj) $\n\n",[33,57825,57823],{"__ignoreMap":35},[11,57827,57828,57829,358],{},"And we also need to add the following to the end of ",[33,57830,57390],{},[26,57832,57834],{"className":1383,"code":57833,"language":1125,"meta":35,"style":35},"[...]\nSTATIC_URL = '/static/'\n\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, \"static\"),\n)\n\nSTATIC_ROOT = os.path.join(BASE_DIR, \"live-static\", \"static-root\")\n\nSTATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'\n\n#STATIC_ROOT = \"/home/cfedeploy/webapps/cfehome_static_root/\"\n\nMEDIA_URL = \"/media/\"\n\nMEDIA_ROOT = os.path.join(BASE_DIR, \"live-static\", \"media-root\")\n",[33,57835,57836,57840,57844,57848,57852,57856,57860,57864,57868,57872,57876,57880,57884,57888,57892,57896],{"__ignoreMap":35},[187,57837,57838],{"class":189,"line":190},[187,57839,47849],{},[187,57841,57842],{"class":189,"line":249},[187,57843,33441],{},[187,57845,57846],{"class":189,"line":312},[187,57847,316],{"emptyLinePlaceholder":315},[187,57849,57850],{"class":189,"line":319},[187,57851,57762],{},[187,57853,57854],{"class":189,"line":325},[187,57855,57767],{},[187,57857,57858],{"class":189,"line":686},[187,57859,621],{},[187,57861,57862],{"class":189,"line":697},[187,57863,316],{"emptyLinePlaceholder":315},[187,57865,57866],{"class":189,"line":1291},[187,57867,57780],{},[187,57869,57870],{"class":189,"line":1306},[187,57871,316],{"emptyLinePlaceholder":315},[187,57873,57874],{"class":189,"line":1434},[187,57875,57789],{},[187,57877,57878],{"class":189,"line":2599},[187,57879,316],{"emptyLinePlaceholder":315},[187,57881,57882],{"class":189,"line":2607},[187,57883,57798],{},[187,57885,57886],{"class":189,"line":2621},[187,57887,316],{"emptyLinePlaceholder":315},[187,57889,57890],{"class":189,"line":2631},[187,57891,57807],{},[187,57893,57894],{"class":189,"line":2642},[187,57895,316],{"emptyLinePlaceholder":315},[187,57897,57898],{"class":189,"line":2653},[187,57899,57816],{},[11,57901,57902,1172,57904,57906],{},[33,57903,57380],{},[33,57905,57387],{}," should have the following:",[26,57908,57910],{"className":1383,"code":57909,"language":1125,"meta":35,"style":35},"STATIC_URL = '/static/'\n\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, \"static\"),\n)\n\nSTATIC_ROOT = os.path.join(BASE_DIR, \"live-static\", \"static-root\")\n\nMEDIA_URL = \"/media/\"\n\nMEDIA_ROOT = os.path.join(BASE_DIR, \"live-static\", \"media-root\")\n",[33,57911,57912,57916,57920,57924,57928,57932,57936,57940,57944,57948,57952],{"__ignoreMap":35},[187,57913,57914],{"class":189,"line":190},[187,57915,33441],{},[187,57917,57918],{"class":189,"line":249},[187,57919,316],{"emptyLinePlaceholder":315},[187,57921,57922],{"class":189,"line":312},[187,57923,57762],{},[187,57925,57926],{"class":189,"line":319},[187,57927,57767],{},[187,57929,57930],{"class":189,"line":325},[187,57931,621],{},[187,57933,57934],{"class":189,"line":686},[187,57935,316],{"emptyLinePlaceholder":315},[187,57937,57938],{"class":189,"line":697},[187,57939,57780],{},[187,57941,57942],{"class":189,"line":1291},[187,57943,316],{"emptyLinePlaceholder":315},[187,57945,57946],{"class":189,"line":1306},[187,57947,57807],{},[187,57949,57950],{"class":189,"line":1434},[187,57951,316],{"emptyLinePlaceholder":315},[187,57953,57954],{"class":189,"line":2599},[187,57955,57816],{},[11,57957,57958,57959,57961],{},"Next we want to run ",[33,57960,13071],{}," locally:",[26,57963,57966],{"className":57964,"code":57965,"language":31},[29],"(proj) $ python manage.py collectstatic\n",[33,57967,57965],{"__ignoreMap":35},[11,57969,57970,57971,57974],{},"We also want to add a blank file to ",[33,57972,57973],{},"live-static/media-root"," so that it becomes tracked in git:",[26,57976,57979],{"className":57977,"code":57978,"language":31},[29],"(proj) $ echo \"some text\" > live-static/media-root/blank.txt\n",[33,57980,57978],{"__ignoreMap":35},[11,57982,57983,57984,765,57987,358],{},"Next we can commit these changes and push to Heroku, and check to see if the static files are working in the admin panel. We also want to set ",[33,57985,57986],{},"DISABLE_COLLECTSTATIC",[33,57988,10165],{},[26,57990,57993],{"className":57991,"code":57992,"language":31},[29],"(proj) $ git add --all\n(proj) $ git commit -m \"added static files\"\n(proj) $ git push heroku master\n(proj) $ heroku config:set DISABLE_COLLECTSTATIC=0\n",[33,57994,57992],{"__ignoreMap":35},[11,57996,57997],{},"Now we should be able to see the admin panel with working CSS on both the live and local heroku sites.",[2215,57999,58001],{"id":58000},"adding-an-app-and-configuring-bootstrap","Adding an app and configuring Bootstrap",[11,58003,58004],{},"Now that everything seems to be working we can start building our app.",[11,58006,58007],{},"Let's start by creating a new app:",[26,58009,58012],{"className":58010,"code":58011,"language":31},[29],"(proj) $ python manage.py startapp pages\n",[33,58013,58011],{"__ignoreMap":35},[11,58015,58016,58018],{},[33,58017,20539],{}," will be the name of an app that we create here.",[11,58020,58021,58022,58025],{},"In ",[33,58023,58024],{},"pages/views.py"," we can add a class-based view that will serve as the homepage:",[26,58027,58029],{"className":1383,"code":58028,"language":1125,"meta":35,"style":35},"from django.shortcuts import render\nfrom django.views.generic import View\n# Create your views here.\n\nclass HomeView(View):\n    def get(self, request, *args, **kwargs):\n        return render(request, 'pages/home.html', {})\n\n",[33,58030,58031,58036,58041,58046,58050,58055,58060],{"__ignoreMap":35},[187,58032,58033],{"class":189,"line":190},[187,58034,58035],{},"from django.shortcuts import render\n",[187,58037,58038],{"class":189,"line":249},[187,58039,58040],{},"from django.views.generic import View\n",[187,58042,58043],{"class":189,"line":312},[187,58044,58045],{},"# Create your views here.\n",[187,58047,58048],{"class":189,"line":319},[187,58049,316],{"emptyLinePlaceholder":315},[187,58051,58052],{"class":189,"line":325},[187,58053,58054],{},"class HomeView(View):\n",[187,58056,58057],{"class":189,"line":686},[187,58058,58059],{},"    def get(self, request, *args, **kwargs):\n",[187,58061,58062],{"class":189,"line":697},[187,58063,58064],{},"        return render(request, 'pages/home.html', {})\n",[11,58066,58067,58068,58070],{},"We then update ",[33,58069,33855],{}," to include our new view:",[26,58072,58074],{"className":1383,"code":58073,"language":1125,"meta":35,"style":35},"from django.conf.urls import url\nfrom django.contrib import admin\nfrom services.views import HomeView\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r'^$', HomeView.as_view(), name='home'),\n]\n\n",[33,58075,58076,58081,58086,58091,58095,58100,58105,58110],{"__ignoreMap":35},[187,58077,58078],{"class":189,"line":190},[187,58079,58080],{},"from django.conf.urls import url\n",[187,58082,58083],{"class":189,"line":249},[187,58084,58085],{},"from django.contrib import admin\n",[187,58087,58088],{"class":189,"line":312},[187,58089,58090],{},"from services.views import HomeView\n",[187,58092,58093],{"class":189,"line":319},[187,58094,316],{"emptyLinePlaceholder":315},[187,58096,58097],{"class":189,"line":325},[187,58098,58099],{},"urlpatterns = [\n",[187,58101,58102],{"class":189,"line":686},[187,58103,58104],{},"    url(r'^admin/', admin.site.urls),\n",[187,58106,58107],{"class":189,"line":697},[187,58108,58109],{},"    url(r'^$', HomeView.as_view(), name='home'),\n",[187,58111,58112],{"class":189,"line":1291},[187,58113,1437],{},[11,58115,58116,58117,765,58119,58122,58123,1172,58125,752],{},"Next we have to add ",[33,58118,20539],{},[33,58120,58121],{},"INSTALLED_APPS"," in both ",[33,58124,57390],{},[33,58126,57387],{},[11,58128,58129,58130,58132],{},"Next we need to make some folders within our new ",[33,58131,20539],{}," app:",[26,58134,58137],{"className":58135,"code":58136,"language":31},[29],"(proj) $ mkdir page/templates && cd pages/templates\n(proj) $ mkdir pages\n(proj) $ touch home.html\n",[33,58138,58136],{"__ignoreMap":35},[11,58140,58141,58142,358],{},"Then we need to add the following to ",[33,58143,58144],{},"home.html",[26,58146,58148],{"className":6715,"code":58147,"language":6717,"meta":35,"style":35},"{% extends \"base.html\" %} {% block content %} {% endblock content%}\n",[33,58149,58150],{"__ignoreMap":35},[187,58151,58152],{"class":189,"line":190},[187,58153,58147],{"class":577},[11,58155,58156,58157,4313,58159,4313,58162,637,58164,1172,58166,358],{},"Next we need to update ",[33,58158,41229],{},[33,58160,58161],{},"TEMPLATES",[33,58163,57380],{},[33,58165,57387],{},[33,58167,57390],{},[26,58169,58171],{"className":1383,"code":58170,"language":1125,"meta":35,"style":35},"[...]\n'DIRS': [os.path.join(BASE_DIR, 'templates')],\n[...]\n",[33,58172,58173,58177,58182],{"__ignoreMap":35},[187,58174,58175],{"class":189,"line":190},[187,58176,47849],{},[187,58178,58179],{"class":189,"line":249},[187,58180,58181],{},"'DIRS': [os.path.join(BASE_DIR, 'templates')],\n",[187,58183,58184],{"class":189,"line":312},[187,58185,47849],{},[11,58187,58188,58189,58192],{},"Next we need to add a ",[33,58190,58191],{},"templates"," folder to the root of our project:",[26,58194,58197],{"className":58195,"code":58196,"language":31},[29],"(proj) $ mkdir templates\n(proj) $ touch tempates/base.html\n",[33,58198,58196],{"__ignoreMap":35},[11,58200,58201,58202,358],{},"Next we can add a basic Bootstrap template to ",[33,58203,58204],{},"base.html",[26,58206,58208],{"className":6715,"code":58207,"language":6717,"meta":35,"style":35},"\u003C!DOCTYPE html>\n\u003Chtml lang=\"en\">\n  \u003Chead>\n    \u003Cmeta charset=\"utf-8\" />\n    \u003Cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n    \u003Cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    \u003C!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->\n    \u003Ctitle>Bootstrap 101 Template\u003C/title>\n\n    \u003C!-- Latest compiled and minified CSS -->\n    \u003Clink\n      rel=\"stylesheet\"\n      href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\"\n      integrity=\"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\"\n      crossorigin=\"anonymous\"\n    />\n\n    \u003C!-- Optional theme -->\n    \u003Clink\n      rel=\"stylesheet\"\n      href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\"\n      integrity=\"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\"\n      crossorigin=\"anonymous\"\n    />\n\n    \u003C!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->\n    \u003C!-- WARNING: Respond.js doesn't work if you view the page via file:// -->\n    \u003C!--[if lt IE 9]>\n      \u003Cscript src=\"https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js\">\u003C/script>\n      \u003Cscript src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\">\u003C/script>\n    \u003C![endif]-->\n  \u003C/head>\n  \u003Cbody>\n    \u003Ch1>Hello, world!\u003C/h1>\n\n    \u003C!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->\n    \u003Cscript src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\">\u003C/script>\n    \u003C!-- Include all compiled plugins (below), or include individual files as needed -->\n    \u003C!-- Latest compiled and minified JavaScript -->\n    \u003Cscript\n      src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\"\n      integrity=\"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\"\n      crossorigin=\"anonymous\"\n    >\u003C/script>\n  \u003C/body>\n\u003C/html>\n",[33,58209,58210,58223,58239,58247,58264,58288,58310,58315,58328,58332,58337,58344,58354,58364,58374,58384,58389,58393,58398,58404,58412,58421,58430,58438,58442,58446,58451,58456,58461,58466,58471,58476,58484,58493,58506,58510,58515,58535,58540,58545,58552,58562,58571,58579,58587,58595],{"__ignoreMap":35},[187,58211,58212,58215,58218,58221],{"class":189,"line":190},[187,58213,58214],{"class":577},"\u003C!",[187,58216,58217],{"class":2516},"DOCTYPE",[187,58219,58220],{"class":193}," html",[187,58222,6730],{"class":577},[187,58224,58225,58227,58229,58232,58234,58237],{"class":189,"line":249},[187,58226,6724],{"class":577},[187,58228,6717],{"class":2516},[187,58230,58231],{"class":193}," lang",[187,58233,595],{"class":577},[187,58235,58236],{"class":196},"\"en\"",[187,58238,6730],{"class":577},[187,58240,58241,58243,58245],{"class":189,"line":312},[187,58242,19179],{"class":577},[187,58244,7848],{"class":2516},[187,58246,6730],{"class":577},[187,58248,58249,58251,58254,58257,58259,58262],{"class":189,"line":319},[187,58250,19865],{"class":577},[187,58252,58253],{"class":2516},"meta",[187,58255,58256],{"class":193}," charset",[187,58258,595],{"class":577},[187,58260,58261],{"class":196},"\"utf-8\"",[187,58263,19871],{"class":577},[187,58265,58266,58268,58270,58273,58275,58278,58281,58283,58286],{"class":189,"line":325},[187,58267,19865],{"class":577},[187,58269,58253],{"class":2516},[187,58271,58272],{"class":193}," http-equiv",[187,58274,595],{"class":577},[187,58276,58277],{"class":196},"\"X-UA-Compatible\"",[187,58279,58280],{"class":193}," content",[187,58282,595],{"class":577},[187,58284,58285],{"class":196},"\"IE=edge\"",[187,58287,19871],{"class":577},[187,58289,58290,58292,58294,58296,58298,58301,58303,58305,58308],{"class":189,"line":686},[187,58291,19865],{"class":577},[187,58293,58253],{"class":2516},[187,58295,39209],{"class":193},[187,58297,595],{"class":577},[187,58299,58300],{"class":196},"\"viewport\"",[187,58302,58280],{"class":193},[187,58304,595],{"class":577},[187,58306,58307],{"class":196},"\"width=device-width, initial-scale=1\"",[187,58309,19871],{"class":577},[187,58311,58312],{"class":189,"line":697},[187,58313,58314],{"class":295},"    \u003C!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->\n",[187,58316,58317,58319,58321,58324,58326],{"class":189,"line":1291},[187,58318,19865],{"class":577},[187,58320,6537],{"class":2516},[187,58322,58323],{"class":577},">Bootstrap 101 Template\u003C/",[187,58325,6537],{"class":2516},[187,58327,6730],{"class":577},[187,58329,58330],{"class":189,"line":1306},[187,58331,316],{"emptyLinePlaceholder":315},[187,58333,58334],{"class":189,"line":1434},[187,58335,58336],{"class":295},"    \u003C!-- Latest compiled and minified CSS -->\n",[187,58338,58339,58341],{"class":189,"line":2599},[187,58340,19865],{"class":577},[187,58342,58343],{"class":2516},"link\n",[187,58345,58346,58349,58351],{"class":189,"line":2607},[187,58347,58348],{"class":193},"      rel",[187,58350,595],{"class":577},[187,58352,58353],{"class":196},"\"stylesheet\"\n",[187,58355,58356,58359,58361],{"class":189,"line":2621},[187,58357,58358],{"class":193},"      href",[187,58360,595],{"class":577},[187,58362,58363],{"class":196},"\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\"\n",[187,58365,58366,58369,58371],{"class":189,"line":2631},[187,58367,58368],{"class":193},"      integrity",[187,58370,595],{"class":577},[187,58372,58373],{"class":196},"\"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\"\n",[187,58375,58376,58379,58381],{"class":189,"line":2642},[187,58377,58378],{"class":193},"      crossorigin",[187,58380,595],{"class":577},[187,58382,58383],{"class":196},"\"anonymous\"\n",[187,58385,58386],{"class":189,"line":2653},[187,58387,58388],{"class":577},"    />\n",[187,58390,58391],{"class":189,"line":2665},[187,58392,316],{"emptyLinePlaceholder":315},[187,58394,58395],{"class":189,"line":2674},[187,58396,58397],{"class":295},"    \u003C!-- Optional theme -->\n",[187,58399,58400,58402],{"class":189,"line":2684},[187,58401,19865],{"class":577},[187,58403,58343],{"class":2516},[187,58405,58406,58408,58410],{"class":189,"line":2694},[187,58407,58348],{"class":193},[187,58409,595],{"class":577},[187,58411,58353],{"class":196},[187,58413,58414,58416,58418],{"class":189,"line":2706},[187,58415,58358],{"class":193},[187,58417,595],{"class":577},[187,58419,58420],{"class":196},"\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css\"\n",[187,58422,58423,58425,58427],{"class":189,"line":2715},[187,58424,58368],{"class":193},[187,58426,595],{"class":577},[187,58428,58429],{"class":196},"\"sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp\"\n",[187,58431,58432,58434,58436],{"class":189,"line":2725},[187,58433,58378],{"class":193},[187,58435,595],{"class":577},[187,58437,58383],{"class":196},[187,58439,58440],{"class":189,"line":2735},[187,58441,58388],{"class":577},[187,58443,58444],{"class":189,"line":2743},[187,58445,316],{"emptyLinePlaceholder":315},[187,58447,58448],{"class":189,"line":2754},[187,58449,58450],{"class":295},"    \u003C!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->\n",[187,58452,58453],{"class":189,"line":2762},[187,58454,58455],{"class":295},"    \u003C!-- WARNING: Respond.js doesn't work if you view the page via file:// -->\n",[187,58457,58458],{"class":189,"line":2770},[187,58459,58460],{"class":295},"    \u003C!--[if lt IE 9]>\n",[187,58462,58463],{"class":189,"line":2781},[187,58464,58465],{"class":295},"      \u003Cscript src=\"https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js\">\u003C/script>\n",[187,58467,58468],{"class":189,"line":2792},[187,58469,58470],{"class":295},"      \u003Cscript src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\">\u003C/script>\n",[187,58472,58473],{"class":189,"line":2803},[187,58474,58475],{"class":295},"    \u003C![endif]-->\n",[187,58477,58478,58480,58482],{"class":189,"line":2808},[187,58479,19938],{"class":577},[187,58481,7848],{"class":2516},[187,58483,6730],{"class":577},[187,58485,58486,58488,58491],{"class":189,"line":2816},[187,58487,19179],{"class":577},[187,58489,58490],{"class":2516},"body",[187,58492,6730],{"class":577},[187,58494,58495,58497,58499,58502,58504],{"class":189,"line":2824},[187,58496,19865],{"class":577},[187,58498,2215],{"class":2516},[187,58500,58501],{"class":577},">Hello, world!\u003C/",[187,58503,2215],{"class":2516},[187,58505,6730],{"class":577},[187,58507,58508],{"class":189,"line":2834},[187,58509,316],{"emptyLinePlaceholder":315},[187,58511,58512],{"class":189,"line":2845},[187,58513,58514],{"class":295},"    \u003C!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->\n",[187,58516,58517,58519,58521,58524,58526,58529,58531,58533],{"class":189,"line":2856},[187,58518,19865],{"class":577},[187,58520,6727],{"class":2516},[187,58522,58523],{"class":193}," src",[187,58525,595],{"class":577},[187,58527,58528],{"class":196},"\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"",[187,58530,40395],{"class":577},[187,58532,6727],{"class":2516},[187,58534,6730],{"class":577},[187,58536,58537],{"class":189,"line":2867},[187,58538,58539],{"class":295},"    \u003C!-- Include all compiled plugins (below), or include individual files as needed -->\n",[187,58541,58542],{"class":189,"line":2878},[187,58543,58544],{"class":295},"    \u003C!-- Latest compiled and minified JavaScript -->\n",[187,58546,58547,58549],{"class":189,"line":2886},[187,58548,19865],{"class":577},[187,58550,58551],{"class":2516},"script\n",[187,58553,58554,58557,58559],{"class":189,"line":2900},[187,58555,58556],{"class":193},"      src",[187,58558,595],{"class":577},[187,58560,58561],{"class":196},"\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\"\n",[187,58563,58564,58566,58568],{"class":189,"line":2905},[187,58565,58368],{"class":193},[187,58567,595],{"class":577},[187,58569,58570],{"class":196},"\"sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa\"\n",[187,58572,58573,58575,58577],{"class":189,"line":2913},[187,58574,58378],{"class":193},[187,58576,595],{"class":577},[187,58578,58383],{"class":196},[187,58580,58581,58583,58585],{"class":189,"line":2921},[187,58582,24699],{"class":577},[187,58584,6727],{"class":2516},[187,58586,6730],{"class":577},[187,58588,58589,58591,58593],{"class":189,"line":2931},[187,58590,19938],{"class":577},[187,58592,58490],{"class":2516},[187,58594,6730],{"class":577},[187,58596,58597,58599,58601],{"class":189,"line":2942},[187,58598,6856],{"class":577},[187,58600,6717],{"class":2516},[187,58602,6730],{"class":577},[11,58604,58605,58606,752],{},"This html was taken from ",[15,58607,58610],{"href":58608,"rel":58609},"http://getbootstrap.com/getting-started/",[19],"Bootstrap's 'Get Started' page",[11,58612,58613,58614,58616],{},"Now we can run ",[33,58615,57735],{}," and confirm that we see \"Hello, world!\" from the Bootstrap template.",[11,58618,38120],{},[855,58620,58621],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .s9eBZ, html code.shiki .s9eBZ{--shiki-default:#22863A;--shiki-dark:#85E89D}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}",{"title":35,"searchDepth":249,"depth":249,"links":58623},[],"2017-03-04","/static/django-heroku.png",{"layout":29014},"/2017/03/04/settinging-up-django-with-heroku",{"title":57257,"description":57262},"2017/03/04/settinging-up-django-with-heroku",[15290,52685],"fI3zwlnA0iVbc743ycOqtREOk702TVbHs0wyYnE2ztM",{"id":58633,"title":58634,"body":58635,"comments":315,"date":60496,"description":60497,"draft":872,"extension":873,"external":874,"image":60498,"meta":60499,"navigation":315,"path":60500,"seo":60501,"stem":60502,"tags":60503,"__hash__":60505},"blog/2017/03/03/graph_subreddit.md","Related subreddit graph exploration with NetworkX",{"type":8,"value":58636,"toc":60494},[58637,58641,58648,58651,58661,58688,58746,58752,59002,59005,59008,59016,59021,59027,59030,59036,59042,59196,59199,59213,59216,59573,59576,59590,59597,59600,59605,59608,59622,59625,59710,59715,59719,59727,59742,59751,59766,59781,59806,59821,59827,59830,59839,59845,59849,59852,59861,59867,59870,59909,59916,59925,59931,59939,59945,59953,59959,59967,59973,59981,59987,59995,60001,60009,60015,60023,60029,60037,60043,60051,60057,60065,60071,60079,60085,60093,60099,60107,60113,60128,60134,60145,60152,60212,60215,60220,60224,60231,60236,60243,60252,60255,60264,60270,60279,60284,60287,60316,60325,60331,60334,60364,60369,60373,60376,60391,60397,60406,60412,60427,60433,60436,60445,60451,60454,60463,60469,60478,60492],[2215,58638,58640],{"id":58639},"graphing-subreddits","Graphing Subreddits",[11,58642,58643,58644,58647],{},"This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from ",[15,58645,58646],{"href":58646},"reddit.com"," with the goal of visualizing the network of related subreddits (forums on specific topics) and related data.",[11,58649,58650],{},"Reddit is home over 600,000 communities, known as subreddits, where people come to share information, opinions, links, etc. and discuss things in a open forum. Most subreddits display links to related subreddits. For example, /r/apple (the Apple subreddit) links to /r/iPhone, a subreddit all about the iPhone, and over a dozen other Apple-related subreddits.",[11,58652,58653,58654,58656,58657,58660],{},"If you visit reddit.com as a guest, you will see a list of popular subreddits. This list is located inside an ",[33,58655,6717],{}," tag called ",[33,58658,58659],{},"drop-choices",". Here it is:",[26,58662,58664],{"className":1383,"code":58663,"language":1125,"meta":35,"style":35},"from selenium import webdriver\nimport re\nimport time\nimport numpy as np\nfrom bs4 import BeautifulSoup\n",[33,58665,58666,58671,58675,58679,58683],{"__ignoreMap":35},[187,58667,58668],{"class":189,"line":190},[187,58669,58670],{},"from selenium import webdriver\n",[187,58672,58673],{"class":189,"line":249},[187,58674,37572],{},[187,58676,58677],{"class":189,"line":312},[187,58678,10573],{},[187,58680,58681],{"class":189,"line":319},[187,58682,10563],{},[187,58684,58685],{"class":189,"line":325},[187,58686,58687],{},"from bs4 import BeautifulSoup\n",[26,58689,58691],{"className":1383,"code":58690,"language":1125,"meta":35,"style":35},"driver = webdriver.PhantomJS()\ndriver.get('https://www.reddit.com/')\ntime.sleep(4 + np.random.random())\nhtml = driver.page_source.encode('utf-8')\n\ns = BeautifulSoup(html)\ndefaults = s.find('div', attrs={'class':'drop-choices'})\nsubs = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\ndefault_subreddits = list(set(subs.findall(str(defaults))))\n\nfor x in default_subreddits: print '[' + x + '](https://reddit.com'+ x + '), ',\n",[33,58692,58693,58698,58703,58708,58713,58717,58722,58727,58732,58737,58741],{"__ignoreMap":35},[187,58694,58695],{"class":189,"line":190},[187,58696,58697],{},"driver = webdriver.PhantomJS()\n",[187,58699,58700],{"class":189,"line":249},[187,58701,58702],{},"driver.get('https://www.reddit.com/')\n",[187,58704,58705],{"class":189,"line":312},[187,58706,58707],{},"time.sleep(4 + np.random.random())\n",[187,58709,58710],{"class":189,"line":319},[187,58711,58712],{},"html = driver.page_source.encode('utf-8')\n",[187,58714,58715],{"class":189,"line":325},[187,58716,316],{"emptyLinePlaceholder":315},[187,58718,58719],{"class":189,"line":686},[187,58720,58721],{},"s = BeautifulSoup(html)\n",[187,58723,58724],{"class":189,"line":697},[187,58725,58726],{},"defaults = s.find('div', attrs={'class':'drop-choices'})\n",[187,58728,58729],{"class":189,"line":1291},[187,58730,58731],{},"subs = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n",[187,58733,58734],{"class":189,"line":1306},[187,58735,58736],{},"default_subreddits = list(set(subs.findall(str(defaults))))\n",[187,58738,58739],{"class":189,"line":1434},[187,58740,316],{"emptyLinePlaceholder":315},[187,58742,58743],{"class":189,"line":2599},[187,58744,58745],{},"for x in default_subreddits: print '[' + x + '](https://reddit.com'+ x + '), ',\n",[11,58747,58748,58749,358],{},"Here are the elements of ",[33,58750,58751],{},"default_subreddits",[107,58753,58754],{},[11,58755,58756,637,58761,637,58766,637,58771,637,58776,637,58781,637,58786,637,58791,637,58796,637,58801,637,58806,637,58811,637,58816,637,58821,637,58826,637,58831,637,58836,637,58841,637,58846,637,58851,637,58856,637,58861,637,58866,637,58871,637,58876,637,58881,637,58886,637,58891,637,58896,637,58901,637,58906,637,58911,637,58916,637,58921,637,58926,637,58931,637,58936,637,58941,637,58946,637,58951,637,58956,637,58961,637,58966,637,58971,637,58976,637,58981,637,58986,637,58991,637,58996,59001],{},[15,58757,58760],{"href":58758,"rel":58759},"https://reddit.com/r/LifeProTips/",[19],"/r/LifeProTips/",[15,58762,58765],{"href":58763,"rel":58764},"https://reddit.com/r/Futurology/",[19],"/r/Futurology/",[15,58767,58770],{"href":58768,"rel":58769},"https://reddit.com/r/OldSchoolCool/",[19],"/r/OldSchoolCool/",[15,58772,58775],{"href":58773,"rel":58774},"https://reddit.com/r/mildlyinteresting/",[19],"/r/mildlyinteresting/",[15,58777,58780],{"href":58778,"rel":58779},"https://reddit.com/r/askscience/",[19],"/r/askscience/",[15,58782,58785],{"href":58783,"rel":58784},"https://reddit.com/r/UpliftingNews/",[19],"/r/UpliftingNews/",[15,58787,58790],{"href":58788,"rel":58789},"https://reddit.com/r/aww/",[19],"/r/aww/",[15,58792,58795],{"href":58793,"rel":58794},"https://reddit.com/r/GetMotivated/",[19],"/r/GetMotivated/",[15,58797,58800],{"href":58798,"rel":58799},"https://reddit.com/r/personalfinance/",[19],"/r/personalfinance/",[15,58802,58805],{"href":58803,"rel":58804},"https://reddit.com/r/gadgets/",[19],"/r/gadgets/",[15,58807,58810],{"href":58808,"rel":58809},"https://reddit.com/r/science/",[19],"/r/science/",[15,58812,58815],{"href":58813,"rel":58814},"https://reddit.com/r/dataisbeautiful/",[19],"/r/dataisbeautiful/",[15,58817,58820],{"href":58818,"rel":58819},"https://reddit.com/r/DIY/",[19],"/r/DIY/",[15,58822,58825],{"href":58823,"rel":58824},"https://reddit.com/r/AskReddit/",[19],"/r/AskReddit/",[15,58827,58830],{"href":58828,"rel":58829},"https://reddit.com/r/space/",[19],"/r/space/",[15,58832,58835],{"href":58833,"rel":58834},"https://reddit.com/r/nosleep/",[19],"/r/nosleep/",[15,58837,58840],{"href":58838,"rel":58839},"https://reddit.com/r/Documentaries/",[19],"/r/Documentaries/",[15,58842,58845],{"href":58843,"rel":58844},"https://reddit.com/r/todayilearned/",[19],"/r/todayilearned/",[15,58847,58850],{"href":58848,"rel":58849},"https://reddit.com/r/television/",[19],"/r/television/",[15,58852,58855],{"href":58853,"rel":58854},"https://reddit.com/r/IAmA/",[19],"/r/IAmA/",[15,58857,58860],{"href":58858,"rel":58859},"https://reddit.com/r/Art/",[19],"/r/Art/",[15,58862,58865],{"href":58863,"rel":58864},"https://reddit.com/r/EarthPorn/",[19],"/r/EarthPorn/",[15,58867,58870],{"href":58868,"rel":58869},"https://reddit.com/r/books/",[19],"/r/books/",[15,58872,58875],{"href":58873,"rel":58874},"https://reddit.com/r/gifs/",[19],"/r/gifs/",[15,58877,58880],{"href":58878,"rel":58879},"https://reddit.com/r/Showerthoughts/",[19],"/r/Showerthoughts/",[15,58882,58885],{"href":58883,"rel":58884},"https://reddit.com/r/blog/",[19],"/r/blog/",[15,58887,58890],{"href":58888,"rel":58889},"https://reddit.com/r/news/",[19],"/r/news/",[15,58892,58895],{"href":58893,"rel":58894},"https://reddit.com/r/Jokes/",[19],"/r/Jokes/",[15,58897,58900],{"href":58898,"rel":58899},"https://reddit.com/r/TwoXChromosomes/",[19],"/r/TwoXChromosomes/",[15,58902,58905],{"href":58903,"rel":58904},"https://reddit.com/r/videos/",[19],"/r/videos/",[15,58907,58910],{"href":58908,"rel":58909},"https://reddit.com/r/philosophy/",[19],"/r/philosophy/",[15,58912,58915],{"href":58913,"rel":58914},"https://reddit.com/r/nottheonion/",[19],"/r/nottheonion/",[15,58917,58920],{"href":58918,"rel":58919},"https://reddit.com/r/explainlikeimfive/",[19],"/r/explainlikeimfive/",[15,58922,58925],{"href":58923,"rel":58924},"https://reddit.com/r/movies/",[19],"/r/movies/",[15,58927,58930],{"href":58928,"rel":58929},"https://reddit.com/r/Music/",[19],"/r/Music/",[15,58932,58935],{"href":58933,"rel":58934},"https://reddit.com/r/WritingPrompts/",[19],"/r/WritingPrompts/",[15,58937,58940],{"href":58938,"rel":58939},"https://reddit.com/r/worldnews/",[19],"/r/worldnews/",[15,58942,58945],{"href":58943,"rel":58944},"https://reddit.com/r/pics/",[19],"/r/pics/",[15,58947,58950],{"href":58948,"rel":58949},"https://reddit.com/r/history/",[19],"/r/history/",[15,58952,58955],{"href":58953,"rel":58954},"https://reddit.com/r/listentothis/",[19],"/r/listentothis/",[15,58957,58960],{"href":58958,"rel":58959},"https://reddit.com/r/sports/",[19],"/r/sports/",[15,58962,58965],{"href":58963,"rel":58964},"https://reddit.com/r/food/",[19],"/r/food/",[15,58967,58970],{"href":58968,"rel":58969},"https://reddit.com/r/creepy/",[19],"/r/creepy/",[15,58972,58975],{"href":58973,"rel":58974},"https://reddit.com/r/announcements/",[19],"/r/announcements/",[15,58977,58980],{"href":58978,"rel":58979},"https://reddit.com/r/gaming/",[19],"/r/gaming/",[15,58982,58985],{"href":58983,"rel":58984},"https://reddit.com/r/tifu/",[19],"/r/tifu/",[15,58987,58990],{"href":58988,"rel":58989},"https://reddit.com/r/funny/",[19],"/r/funny/",[15,58992,58995],{"href":58993,"rel":58994},"https://reddit.com/r/photoshopbattles/",[19],"/r/photoshopbattles/",[15,58997,59000],{"href":58998,"rel":58999},"https://reddit.com/r/InternetIsBeautiful/",[19],"/r/InternetIsBeautiful/",",",[11,59003,59004],{},"My goal here is to see how many subreddits we can reach as we branch off of these \"default\" subreddits into their related subreddits.",[11,59006,59007],{},"First, we need to set up data structures to hold data for subreddits and their related subreddits. And we need to define an algorithm for collecting data.",[11,59009,59010,59011,358],{},"Here's an intrdoduction to graphs from ",[15,59012,59015],{"href":59013,"rel":59014},"https://www.python.org/doc/essays/graphs/",[19],"python.org",[107,59017,59018],{},[11,59019,59020],{},"Few programming languages provide direct support for graphs as a data type, and Python is no exception. However, graphs are easily built out of lists and dictionaries. For instance, here's a simple graph (I can't use drawings in these columns, so I write down the graph's arcs):",[26,59022,59025],{"className":59023,"code":59024,"language":31,"meta":35},[29],"A -> B\nA -> C\nB -> C\nB -> D\nC -> D\nD -> C\nE -> F\nF -> C\n",[33,59026,59024],{"__ignoreMap":35},[11,59028,59029],{},"This graph has six nodes (A-F) and eight arcs. It can be represented by the following Python data structure:",[26,59031,59034],{"className":59032,"code":59033,"language":31,"meta":35},[29],"graph =     {'A': ['B', 'C'],\n             'B': ['C', 'D'],\n             'C': ['D'],\n             'D': ['C'],\n             'E': ['F'],\n             'F': ['C']}\n",[33,59035,59033],{"__ignoreMap":35},[11,59037,59038,59039,59041],{},"First let's define how we would go only one branch deep into this graph (i.e. find the related subreddits for ",[4339,59040,34602],{}," the default subreddits). To collect the data, I first looped through the default subreddits and save the html of each subreddit to its own text file. Here's a script with comments:",[26,59043,59045],{"className":1383,"code":59044,"language":1125,"meta":35,"style":35},"#first we navigate to the correct folder where we will store the first level of related subreddits\nos.chdir(os.path.expanduser('~/Documents/Projects/Data/Subreddits/one/'))\n\n#next we instantiate the webdriver we will be using: PhantomJS\ndriver = webdriver.PhantomJS()\n\n#loop through the list of default subreddits\nfor num, subreddit in enumerate(default_subreddits):\n\n    #for each subreddit, we append the /r/subreddit path to the base URL (reddit.com)\n    driver.get('https://www.reddit.com'+subreddit)\n\n    #wait for two seconds\n    time.sleep(2 + np.random.random())\n\n    #save the html of the loaded page to a variable: html\n    html = driver.page_source.encode('utf-8')\n\n    #remove '/r/' from the subreddit name string\n    name = subreddit.split('/')[2]\n\n    #open a new file and give it the name of the subreddit we just scraped\n    subreddit_html_file = open(name+'.txt', 'w+')\n\n    #write the html contents to the file\n    subreddit_html_file.write(html)\n\n    #clost the file\n    subreddit_html_file.close()\n\n    #print out the number and name of the subreddit we just scrapped to make sure things are working\n    print str(num) + ' ' + subreddit,\n\n",[33,59046,59047,59052,59057,59061,59066,59070,59074,59079,59084,59088,59093,59098,59102,59107,59112,59116,59121,59126,59130,59135,59140,59144,59149,59154,59158,59163,59168,59172,59177,59182,59186,59191],{"__ignoreMap":35},[187,59048,59049],{"class":189,"line":190},[187,59050,59051],{},"#first we navigate to the correct folder where we will store the first level of related subreddits\n",[187,59053,59054],{"class":189,"line":249},[187,59055,59056],{},"os.chdir(os.path.expanduser('~/Documents/Projects/Data/Subreddits/one/'))\n",[187,59058,59059],{"class":189,"line":312},[187,59060,316],{"emptyLinePlaceholder":315},[187,59062,59063],{"class":189,"line":319},[187,59064,59065],{},"#next we instantiate the webdriver we will be using: PhantomJS\n",[187,59067,59068],{"class":189,"line":325},[187,59069,58697],{},[187,59071,59072],{"class":189,"line":686},[187,59073,316],{"emptyLinePlaceholder":315},[187,59075,59076],{"class":189,"line":697},[187,59077,59078],{},"#loop through the list of default subreddits\n",[187,59080,59081],{"class":189,"line":1291},[187,59082,59083],{},"for num, subreddit in enumerate(default_subreddits):\n",[187,59085,59086],{"class":189,"line":1306},[187,59087,316],{"emptyLinePlaceholder":315},[187,59089,59090],{"class":189,"line":1434},[187,59091,59092],{},"    #for each subreddit, we append the /r/subreddit path to the base URL (reddit.com)\n",[187,59094,59095],{"class":189,"line":2599},[187,59096,59097],{},"    driver.get('https://www.reddit.com'+subreddit)\n",[187,59099,59100],{"class":189,"line":2607},[187,59101,316],{"emptyLinePlaceholder":315},[187,59103,59104],{"class":189,"line":2621},[187,59105,59106],{},"    #wait for two seconds\n",[187,59108,59109],{"class":189,"line":2631},[187,59110,59111],{},"    time.sleep(2 + np.random.random())\n",[187,59113,59114],{"class":189,"line":2642},[187,59115,316],{"emptyLinePlaceholder":315},[187,59117,59118],{"class":189,"line":2653},[187,59119,59120],{},"    #save the html of the loaded page to a variable: html\n",[187,59122,59123],{"class":189,"line":2665},[187,59124,59125],{},"    html = driver.page_source.encode('utf-8')\n",[187,59127,59128],{"class":189,"line":2674},[187,59129,316],{"emptyLinePlaceholder":315},[187,59131,59132],{"class":189,"line":2684},[187,59133,59134],{},"    #remove '/r/' from the subreddit name string\n",[187,59136,59137],{"class":189,"line":2694},[187,59138,59139],{},"    name = subreddit.split('/')[2]\n",[187,59141,59142],{"class":189,"line":2706},[187,59143,316],{"emptyLinePlaceholder":315},[187,59145,59146],{"class":189,"line":2715},[187,59147,59148],{},"    #open a new file and give it the name of the subreddit we just scraped\n",[187,59150,59151],{"class":189,"line":2725},[187,59152,59153],{},"    subreddit_html_file = open(name+'.txt', 'w+')\n",[187,59155,59156],{"class":189,"line":2735},[187,59157,316],{"emptyLinePlaceholder":315},[187,59159,59160],{"class":189,"line":2743},[187,59161,59162],{},"    #write the html contents to the file\n",[187,59164,59165],{"class":189,"line":2754},[187,59166,59167],{},"    subreddit_html_file.write(html)\n",[187,59169,59170],{"class":189,"line":2762},[187,59171,316],{"emptyLinePlaceholder":315},[187,59173,59174],{"class":189,"line":2770},[187,59175,59176],{},"    #clost the file\n",[187,59178,59179],{"class":189,"line":2781},[187,59180,59181],{},"    subreddit_html_file.close()\n",[187,59183,59184],{"class":189,"line":2792},[187,59185,316],{"emptyLinePlaceholder":315},[187,59187,59188],{"class":189,"line":2803},[187,59189,59190],{},"    #print out the number and name of the subreddit we just scrapped to make sure things are working\n",[187,59192,59193],{"class":189,"line":2808},[187,59194,59195],{},"    print str(num) + ' ' + subreddit,\n",[11,59197,59198],{},"Next, we want to go through each file and extract the information we want. Here's what we will be getting:",[916,59200,59201,59204,59207,59210],{},[919,59202,59203],{},"Number of subscribers",[919,59205,59206],{},"Subreddit description",[919,59208,59209],{},"Date created",[919,59211,59212],{},"Related subreddits",[11,59214,59215],{},"For this type of project, I prefer to loop through each page and creating several small dictionaries for each data point, then combine the small dictionaries into a large dictionary, and then append the dictionary to a list of dictionaries. Once I have looped through all of the pages, I can create a pandas DataFrame from the list of dictionaries. This allows me to easily manipulate the data. Here's the script that I used to do this:",[26,59217,59219],{"className":1383,"code":59218,"language":1125,"meta":35,"style":35},"#navigate to where the html files are stored (I moved them around a bit so it is not consistent with the script above)\nos.chdir('E://DATA/Subreddits/subreddits_html/')\n\n#generate a list of files that we will loop through\nfiles = os.listdir('E://DATA/Subreddits/subreddits_html/')\n\n#set up an empty list that we will append dictionaries to\ndict_list = []\n\n#loop through the files\nfor file_ in files:\n\n    #print out the name of the current file in the loop\n    print file_,\n\n    #open the file\n    f = open(file_, 'r')\n    #read the file contents to a local variable\n    html = f.read()\n    #create a BeautifulSoup object that we will use to parse the HTML\n    b = BeautifulSoup(html, 'lxml')\n\n    #get the subreddit name that we are working with (from the `file` variable)\n    subreddit_name = '/r/' + file_[:-4].lower()\n    #put the name into a dictionary\n    subreddit_name_dict = {'subreddit':subreddit_name}\n\n    #get number of subscribers\n    subs = b.find('span', attrs={'class':'subscribers'})\n    #if the number of subscribers is displayed on the page, then we find it and add it to a dictionary\n    if subs:\n        subs = b.find('span', attrs={'class':'subscribers'}).find('span', attrs={'class':'number'}).text.replace(',', '')\n        subs_dict = {'subscribers':int(subs)}\n    #if the number of subscribers is not displayed on the page, then we set the number of subscribers in the dictionary to None\n    else:\n        subs_dict = {'subscribers':None}\n\n    #similar process for the description: if the description is displayed, get it and save it to desc\n    #if it is not available, then desc will be set to `None`\n    desc = b.find('div', attrs={'class':'md'})\n    if desc:\n        desc = b.find('div', attrs={'class':'md'}).text\n        desc = desc.replace('\\n', ' ')\n    desc_dict = {'description':desc}\n\n    #here we use regular expressions to find links anywhere on the page that have the structure: \"/r/something/\"\n    rel_subr = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n    #make a list of these links based on the \"/r/something/\" pattern\n    related_subreddits = rel_subr.findall(html)\n\n    #save the list to a dictionary\n    subreddits_dict = {'related':related_subreddits}\n\n    #same processes for recording the date that the subreddit was created: get the date from an HTML element,\n    #then save it to a dictionary. There were two different formats available in the HTML so I grabbed both\n    age = b.find('span', attrs={'class':'age'})\n    if age:\n        time1 = age.find('time')['title']\n        time2 = age.find('time')['datetime']\n\n    #save the date to a dictionary\n    time_dict = {\"date1\":time1, \"date2\":time2}\n\n    #take all the dictionaries we just created and put them together into one big dictionary\n    dictionary = dict(subs_dict.items()+desc_dict.items()+subreddits_dict.items()+subreddit_name_dict.items()+time_dict.items())\n\n    #append the big dictionary to the list that we defined right before the beginning of the loop\n    dict_list.append(dictionary)\n\n    #deconstruct the Beautiful Soup object (this can eat up memory very quickly, so it is very important when processing lots of data)\n    b.decompose()\n\n    #clost the file\n    f.close()\n",[33,59220,59221,59226,59231,59235,59240,59245,59249,59254,59259,59263,59268,59273,59277,59282,59287,59291,59296,59301,59306,59311,59316,59321,59325,59330,59335,59340,59345,59349,59354,59359,59364,59369,59374,59379,59384,59388,59393,59397,59402,59407,59412,59417,59422,59427,59432,59436,59441,59446,59451,59456,59460,59465,59470,59474,59479,59484,59489,59494,59499,59504,59508,59513,59518,59522,59527,59532,59536,59541,59546,59550,59555,59560,59564,59568],{"__ignoreMap":35},[187,59222,59223],{"class":189,"line":190},[187,59224,59225],{},"#navigate to where the html files are stored (I moved them around a bit so it is not consistent with the script above)\n",[187,59227,59228],{"class":189,"line":249},[187,59229,59230],{},"os.chdir('E://DATA/Subreddits/subreddits_html/')\n",[187,59232,59233],{"class":189,"line":312},[187,59234,316],{"emptyLinePlaceholder":315},[187,59236,59237],{"class":189,"line":319},[187,59238,59239],{},"#generate a list of files that we will loop through\n",[187,59241,59242],{"class":189,"line":325},[187,59243,59244],{},"files = os.listdir('E://DATA/Subreddits/subreddits_html/')\n",[187,59246,59247],{"class":189,"line":686},[187,59248,316],{"emptyLinePlaceholder":315},[187,59250,59251],{"class":189,"line":697},[187,59252,59253],{},"#set up an empty list that we will append dictionaries to\n",[187,59255,59256],{"class":189,"line":1291},[187,59257,59258],{},"dict_list = []\n",[187,59260,59261],{"class":189,"line":1306},[187,59262,316],{"emptyLinePlaceholder":315},[187,59264,59265],{"class":189,"line":1434},[187,59266,59267],{},"#loop through the files\n",[187,59269,59270],{"class":189,"line":2599},[187,59271,59272],{},"for file_ in files:\n",[187,59274,59275],{"class":189,"line":2607},[187,59276,316],{"emptyLinePlaceholder":315},[187,59278,59279],{"class":189,"line":2621},[187,59280,59281],{},"    #print out the name of the current file in the loop\n",[187,59283,59284],{"class":189,"line":2631},[187,59285,59286],{},"    print file_,\n",[187,59288,59289],{"class":189,"line":2642},[187,59290,316],{"emptyLinePlaceholder":315},[187,59292,59293],{"class":189,"line":2653},[187,59294,59295],{},"    #open the file\n",[187,59297,59298],{"class":189,"line":2665},[187,59299,59300],{},"    f = open(file_, 'r')\n",[187,59302,59303],{"class":189,"line":2674},[187,59304,59305],{},"    #read the file contents to a local variable\n",[187,59307,59308],{"class":189,"line":2684},[187,59309,59310],{},"    html = f.read()\n",[187,59312,59313],{"class":189,"line":2694},[187,59314,59315],{},"    #create a BeautifulSoup object that we will use to parse the HTML\n",[187,59317,59318],{"class":189,"line":2706},[187,59319,59320],{},"    b = BeautifulSoup(html, 'lxml')\n",[187,59322,59323],{"class":189,"line":2715},[187,59324,316],{"emptyLinePlaceholder":315},[187,59326,59327],{"class":189,"line":2725},[187,59328,59329],{},"    #get the subreddit name that we are working with (from the `file` variable)\n",[187,59331,59332],{"class":189,"line":2735},[187,59333,59334],{},"    subreddit_name = '/r/' + file_[:-4].lower()\n",[187,59336,59337],{"class":189,"line":2743},[187,59338,59339],{},"    #put the name into a dictionary\n",[187,59341,59342],{"class":189,"line":2754},[187,59343,59344],{},"    subreddit_name_dict = {'subreddit':subreddit_name}\n",[187,59346,59347],{"class":189,"line":2762},[187,59348,316],{"emptyLinePlaceholder":315},[187,59350,59351],{"class":189,"line":2770},[187,59352,59353],{},"    #get number of subscribers\n",[187,59355,59356],{"class":189,"line":2781},[187,59357,59358],{},"    subs = b.find('span', attrs={'class':'subscribers'})\n",[187,59360,59361],{"class":189,"line":2792},[187,59362,59363],{},"    #if the number of subscribers is displayed on the page, then we find it and add it to a dictionary\n",[187,59365,59366],{"class":189,"line":2803},[187,59367,59368],{},"    if subs:\n",[187,59370,59371],{"class":189,"line":2808},[187,59372,59373],{},"        subs = b.find('span', attrs={'class':'subscribers'}).find('span', attrs={'class':'number'}).text.replace(',', '')\n",[187,59375,59376],{"class":189,"line":2816},[187,59377,59378],{},"        subs_dict = {'subscribers':int(subs)}\n",[187,59380,59381],{"class":189,"line":2824},[187,59382,59383],{},"    #if the number of subscribers is not displayed on the page, then we set the number of subscribers in the dictionary to None\n",[187,59385,59386],{"class":189,"line":2834},[187,59387,23313],{},[187,59389,59390],{"class":189,"line":2845},[187,59391,59392],{},"        subs_dict = {'subscribers':None}\n",[187,59394,59395],{"class":189,"line":2856},[187,59396,316],{"emptyLinePlaceholder":315},[187,59398,59399],{"class":189,"line":2867},[187,59400,59401],{},"    #similar process for the description: if the description is displayed, get it and save it to desc\n",[187,59403,59404],{"class":189,"line":2878},[187,59405,59406],{},"    #if it is not available, then desc will be set to `None`\n",[187,59408,59409],{"class":189,"line":2886},[187,59410,59411],{},"    desc = b.find('div', attrs={'class':'md'})\n",[187,59413,59414],{"class":189,"line":2900},[187,59415,59416],{},"    if desc:\n",[187,59418,59419],{"class":189,"line":2905},[187,59420,59421],{},"        desc = b.find('div', attrs={'class':'md'}).text\n",[187,59423,59424],{"class":189,"line":2913},[187,59425,59426],{},"        desc = desc.replace('\\n', ' ')\n",[187,59428,59429],{"class":189,"line":2921},[187,59430,59431],{},"    desc_dict = {'description':desc}\n",[187,59433,59434],{"class":189,"line":2931},[187,59435,316],{"emptyLinePlaceholder":315},[187,59437,59438],{"class":189,"line":2942},[187,59439,59440],{},"    #here we use regular expressions to find links anywhere on the page that have the structure: \"/r/something/\"\n",[187,59442,59443],{"class":189,"line":2953},[187,59444,59445],{},"    rel_subr = re.compile(r\"\\/r\\/[\\w.]+\\/?\")\n",[187,59447,59448],{"class":189,"line":2964},[187,59449,59450],{},"    #make a list of these links based on the \"/r/something/\" pattern\n",[187,59452,59453],{"class":189,"line":2975},[187,59454,59455],{},"    related_subreddits = rel_subr.findall(html)\n",[187,59457,59458],{"class":189,"line":2983},[187,59459,316],{"emptyLinePlaceholder":315},[187,59461,59462],{"class":189,"line":2992},[187,59463,59464],{},"    #save the list to a dictionary\n",[187,59466,59467],{"class":189,"line":3001},[187,59468,59469],{},"    subreddits_dict = {'related':related_subreddits}\n",[187,59471,59472],{"class":189,"line":3010},[187,59473,316],{"emptyLinePlaceholder":315},[187,59475,59476],{"class":189,"line":3019},[187,59477,59478],{},"    #same processes for recording the date that the subreddit was created: get the date from an HTML element,\n",[187,59480,59481],{"class":189,"line":3028},[187,59482,59483],{},"    #then save it to a dictionary. There were two different formats available in the HTML so I grabbed both\n",[187,59485,59486],{"class":189,"line":3033},[187,59487,59488],{},"    age = b.find('span', attrs={'class':'age'})\n",[187,59490,59491],{"class":189,"line":3041},[187,59492,59493],{},"    if age:\n",[187,59495,59496],{"class":189,"line":3049},[187,59497,59498],{},"        time1 = age.find('time')['title']\n",[187,59500,59501],{"class":189,"line":3059},[187,59502,59503],{},"        time2 = age.find('time')['datetime']\n",[187,59505,59506],{"class":189,"line":3070},[187,59507,316],{"emptyLinePlaceholder":315},[187,59509,59510],{"class":189,"line":3075},[187,59511,59512],{},"    #save the date to a dictionary\n",[187,59514,59515],{"class":189,"line":3083},[187,59516,59517],{},"    time_dict = {\"date1\":time1, \"date2\":time2}\n",[187,59519,59520],{"class":189,"line":3091},[187,59521,316],{"emptyLinePlaceholder":315},[187,59523,59524],{"class":189,"line":3101},[187,59525,59526],{},"    #take all the dictionaries we just created and put them together into one big dictionary\n",[187,59528,59529],{"class":189,"line":3111},[187,59530,59531],{},"    dictionary = dict(subs_dict.items()+desc_dict.items()+subreddits_dict.items()+subreddit_name_dict.items()+time_dict.items())\n",[187,59533,59534],{"class":189,"line":3122},[187,59535,316],{"emptyLinePlaceholder":315},[187,59537,59538],{"class":189,"line":3132},[187,59539,59540],{},"    #append the big dictionary to the list that we defined right before the beginning of the loop\n",[187,59542,59543],{"class":189,"line":3143},[187,59544,59545],{},"    dict_list.append(dictionary)\n",[187,59547,59548],{"class":189,"line":3151},[187,59549,316],{"emptyLinePlaceholder":315},[187,59551,59552],{"class":189,"line":3161},[187,59553,59554],{},"    #deconstruct the Beautiful Soup object (this can eat up memory very quickly, so it is very important when processing lots of data)\n",[187,59556,59557],{"class":189,"line":3170},[187,59558,59559],{},"    b.decompose()\n",[187,59561,59562],{"class":189,"line":3178},[187,59563,316],{"emptyLinePlaceholder":315},[187,59565,59566],{"class":189,"line":3185},[187,59567,59176],{},[187,59569,59570],{"class":189,"line":3195},[187,59571,59572],{},"    f.close()\n",[11,59574,59575],{},"Next, let's save the results into a csv file. This let's us load the results quickly without having to scrape everyting again. To do this we can use the pandas library.",[26,59577,59579],{"className":1383,"code":59578,"language":1125,"meta":35,"style":35},"import pandas as pd\ndf0 = pd.DataFrame(dict_list, index=None)\n",[33,59580,59581,59585],{"__ignoreMap":35},[187,59582,59583],{"class":189,"line":190},[187,59584,53591],{},[187,59586,59587],{"class":189,"line":249},[187,59588,59589],{},"df0 = pd.DataFrame(dict_list, index=None)\n",[11,59591,59592,59593,59596],{},"At this point, we can go through the ",[33,59594,59595],{},"related"," column in the DataFrame and put together a list of all the related subreddits. With this list, we can simply repeat the process over and over again. However, each time we start with a new list of subreddits, we want to make sure that they have not already been collected.",[11,59598,59599],{},"Next I will read in one DataFrame that represents related subreddits \"three levels deep\" relative to the default subreddits.",[11,59601,59602],{},[338,59603,59604],{},"Default --> Related --> Related --> Related",[11,59606,59607],{},"This DataFrame represents the collection of subreddits from all of these \"layers\" of the graph.",[26,59609,59611],{"className":1383,"code":59610,"language":1125,"meta":35,"style":35},"import pandas as pd\nmaster_df = pd.read_pickle('pickle/master_df.p')\n",[33,59612,59613,59617],{"__ignoreMap":35},[187,59614,59615],{"class":189,"line":190},[187,59616,53591],{},[187,59618,59619],{"class":189,"line":249},[187,59620,59621],{},"master_df = pd.read_pickle('pickle/master_df.p')\n",[11,59623,59624],{},"Now we can do a quick visualization of the growth in number of subreddits since the website's start in 2005.",[26,59626,59628],{"className":1383,"code":59627,"language":1125,"meta":35,"style":35},"import warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nmaster_df_ = master_df[master_df.notnull()]\nmaster_df_.date1 = pd.to_datetime(master_df_['date1'])\n\nlist_of_dates = master_df_.date1.sort_values()\n\ncounts = np.arange(0, len(list_of_dates))\n_ = plt.plot(list_of_dates, counts)\n_ = plt.title('Number of subreddits over time')\n_ = plt.xlabel('Date')\n_ = plt.ylabel('Cummulative Count')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/static/subreddit_graph/subreddits_count.png'))\n",[33,59629,59630,59635,59640,59644,59648,59652,59656,59660,59665,59670,59674,59679,59683,59687,59691,59696,59700,59705],{"__ignoreMap":35},[187,59631,59632],{"class":189,"line":190},[187,59633,59634],{},"import warnings\n",[187,59636,59637],{"class":189,"line":249},[187,59638,59639],{},"warnings.filterwarnings('ignore')\n",[187,59641,59642],{"class":189,"line":312},[187,59643,26216],{},[187,59645,59646],{"class":189,"line":319},[187,59647,26035],{},[187,59649,59650],{"class":189,"line":325},[187,59651,53619],{},[187,59653,59654],{"class":189,"line":686},[187,59655,10563],{},[187,59657,59658],{"class":189,"line":697},[187,59659,316],{"emptyLinePlaceholder":315},[187,59661,59662],{"class":189,"line":1291},[187,59663,59664],{},"master_df_ = master_df[master_df.notnull()]\n",[187,59666,59667],{"class":189,"line":1306},[187,59668,59669],{},"master_df_.date1 = pd.to_datetime(master_df_['date1'])\n",[187,59671,59672],{"class":189,"line":1434},[187,59673,316],{"emptyLinePlaceholder":315},[187,59675,59676],{"class":189,"line":2599},[187,59677,59678],{},"list_of_dates = master_df_.date1.sort_values()\n",[187,59680,59681],{"class":189,"line":2607},[187,59682,316],{"emptyLinePlaceholder":315},[187,59684,59685],{"class":189,"line":2621},[187,59686,45192],{},[187,59688,59689],{"class":189,"line":2631},[187,59690,45202],{},[187,59692,59693],{"class":189,"line":2642},[187,59694,59695],{},"_ = plt.title('Number of subreddits over time')\n",[187,59697,59698],{"class":189,"line":2653},[187,59699,45212],{},[187,59701,59702],{"class":189,"line":2665},[187,59703,59704],{},"_ = plt.ylabel('Cummulative Count')\n",[187,59706,59707],{"class":189,"line":2674},[187,59708,59709],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/static/subreddit_graph/subreddits_count.png'))\n",[11,59711,59712],{},[511,59713],{"alt":7255,"src":59714},"/static/subreddit_graph/subreddits_count.png",[2215,59716,59718],{"id":59717},"setting-up-a-graph-with-networkx","Setting up a graph with NetworkX",[11,59720,59721,59722,752],{},"Next we can start to look at the collection of reddits and related subreddits as a graph. I will be using a Python package for network and graph analysis called ",[15,59723,59726],{"href":59724,"rel":59725},"https://networkx.github.io",[19],"NetworkX",[26,59728,59730],{"className":1383,"code":59729,"language":1125,"meta":35,"style":35},"#Let's make sure that we have only unique entries in the dataframe.\nmaster_df_u = master_df_.drop_duplicates('subreddit')\n",[33,59731,59732,59737],{"__ignoreMap":35},[187,59733,59734],{"class":189,"line":190},[187,59735,59736],{},"#Let's make sure that we have only unique entries in the dataframe.\n",[187,59738,59739],{"class":189,"line":249},[187,59740,59741],{},"master_df_u = master_df_.drop_duplicates('subreddit')\n",[26,59743,59745],{"className":1383,"code":59744,"language":1125,"meta":35,"style":35},"master_df_u = master_df_u.drop(master_df_u.index[master_df_u.subreddit=='/r/track__subreddits_'])\n",[33,59746,59747],{"__ignoreMap":35},[187,59748,59749],{"class":189,"line":190},[187,59750,59744],{},[26,59752,59754],{"className":1383,"code":59753,"language":1125,"meta":35,"style":35},"#here we define a dictionary where the keys are subreddits and the values are lists of related subreddits\ngraph = {x:y for x, y in zip(master_df_u.subreddit, master_df_u.related)}\n",[33,59755,59756,59761],{"__ignoreMap":35},[187,59757,59758],{"class":189,"line":190},[187,59759,59760],{},"#here we define a dictionary where the keys are subreddits and the values are lists of related subreddits\n",[187,59762,59763],{"class":189,"line":249},[187,59764,59765],{},"graph = {x:y for x, y in zip(master_df_u.subreddit, master_df_u.related)}\n",[26,59767,59769],{"className":1383,"code":59768,"language":1125,"meta":35,"style":35},"#NetworkX comes with the python Anaconda distribution\nimport networkx as nx\n",[33,59770,59771,59776],{"__ignoreMap":35},[187,59772,59773],{"class":189,"line":190},[187,59774,59775],{},"#NetworkX comes with the python Anaconda distribution\n",[187,59777,59778],{"class":189,"line":249},[187,59779,59780],{},"import networkx as nx\n",[26,59782,59784],{"className":1383,"code":59783,"language":1125,"meta":35,"style":35},"G=nx.Graph()\nG=nx.from_dict_of_lists(graph)\n#making the graph undirected takes all of the vertices between nodes and makes them bi-directional\nG1 = G.to_undirected()\n",[33,59785,59786,59791,59796,59801],{"__ignoreMap":35},[187,59787,59788],{"class":189,"line":190},[187,59789,59790],{},"G=nx.Graph()\n",[187,59792,59793],{"class":189,"line":249},[187,59794,59795],{},"G=nx.from_dict_of_lists(graph)\n",[187,59797,59798],{"class":189,"line":312},[187,59799,59800],{},"#making the graph undirected takes all of the vertices between nodes and makes them bi-directional\n",[187,59802,59803],{"class":189,"line":319},[187,59804,59805],{},"G1 = G.to_undirected()\n",[26,59807,59809],{"className":1383,"code":59808,"language":1125,"meta":35,"style":35},"choice = np.random.choice(master_df_u.subreddit, 2)\nprint choice\n",[33,59810,59811,59816],{"__ignoreMap":35},[187,59812,59813],{"class":189,"line":190},[187,59814,59815],{},"choice = np.random.choice(master_df_u.subreddit, 2)\n",[187,59817,59818],{"class":189,"line":249},[187,59819,59820],{},"print choice\n",[26,59822,59825],{"className":59823,"code":59824,"language":31},[29],"['/r/streetboarding' '/r/stephenking']\n",[33,59826,59824],{"__ignoreMap":35},[11,59828,59829],{},"Let's test out some of the functions from NetworkX for graph analysis. First, let's take the two randomly selected nodes defined above and test to see if there exists a path between them:",[26,59831,59833],{"className":1383,"code":59832,"language":1125,"meta":35,"style":35},"nx.has_path(G1, choice[0], choice[1])\n",[33,59834,59835],{"__ignoreMap":35},[187,59836,59837],{"class":189,"line":190},[187,59838,59832],{},[26,59840,59843],{"className":59841,"code":59842,"language":31},[29],"True\n",[33,59844,59842],{"__ignoreMap":35},[2215,59846,59848],{"id":59847},"shortest-path","Shortest path",[11,59850,59851],{},"Now let's see (at least one of) the shortest path that exists between these nodes:",[26,59853,59855],{"className":1383,"code":59854,"language":1125,"meta":35,"style":35},"nx.shortest_path(G1, choice[0], choice[1])\n",[33,59856,59857],{"__ignoreMap":35},[187,59858,59859],{"class":189,"line":190},[187,59860,59854],{},[26,59862,59865],{"className":59863,"code":59864,"language":31},[29],"['/r/streetboarding',\n '/r/freebord',\n '/r/adrenaline',\n '/r/imaginaryadrenaline',\n '/r/imaginarystephenking',\n '/r/stephenking']\n",[33,59866,59864],{"__ignoreMap":35},[11,59868,59869],{},"Let's write a function that selects two random subreddits and then prints a shortest path if it exists:",[26,59871,59873],{"className":1383,"code":59872,"language":1125,"meta":35,"style":35},"def short_path():\n    choices = np.random.choice(master_df_u.subreddit, 2)\n    if nx.has_path(G1, choices[0], choices[1]) == True:\n        path = nx.shortest_path(G1, choices[0], choices[1])\n        print choices[0] + ' and ' + choices[1] + ' are joined by: \\n' + str(path)\n    else:\n        print \"No path exists between \" + choices[0] + ' and ' + choices[1]\n",[33,59874,59875,59880,59885,59890,59895,59900,59904],{"__ignoreMap":35},[187,59876,59877],{"class":189,"line":190},[187,59878,59879],{},"def short_path():\n",[187,59881,59882],{"class":189,"line":249},[187,59883,59884],{},"    choices = np.random.choice(master_df_u.subreddit, 2)\n",[187,59886,59887],{"class":189,"line":312},[187,59888,59889],{},"    if nx.has_path(G1, choices[0], choices[1]) == True:\n",[187,59891,59892],{"class":189,"line":319},[187,59893,59894],{},"        path = nx.shortest_path(G1, choices[0], choices[1])\n",[187,59896,59897],{"class":189,"line":325},[187,59898,59899],{},"        print choices[0] + ' and ' + choices[1] + ' are joined by: \\n' + str(path)\n",[187,59901,59902],{"class":189,"line":686},[187,59903,23313],{},[187,59905,59906],{"class":189,"line":697},[187,59907,59908],{},"        print \"No path exists between \" + choices[0] + ' and ' + choices[1]\n",[11,59910,59911,59912,59915],{},"Here's a collection of results from the ",[33,59913,59914],{},"short_path"," function defined above that start to paint a picuture of the broad set of topics covered by reddit.com:",[26,59917,59919],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},"short_path()\n",[33,59920,59921],{"__ignoreMap":35},[187,59922,59923],{"class":189,"line":190},[187,59924,59918],{},[26,59926,59929],{"className":59927,"code":59928,"language":31},[29],"/r/personalizationadvice and /r/beautifulfemales are joined by:\n['/r/personalizationadvice', '/r/coloranalysis', '/r/fashion', '/r/redcarpet', '/r/gentlemanboners', '/r/beautifulfemales']\n",[33,59930,59928],{"__ignoreMap":35},[26,59932,59933],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,59934,59935],{"__ignoreMap":35},[187,59936,59937],{"class":189,"line":190},[187,59938,59918],{},[26,59940,59943],{"className":59941,"code":59942,"language":31},[29],"/r/caffeine and /r/shittyramen are joined by:\n['/r/caffeine', '/r/toast', '/r/cooking', '/r/ramen', '/r/shittyramen']\n",[33,59944,59942],{"__ignoreMap":35},[26,59946,59947],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,59948,59949],{"__ignoreMap":35},[187,59950,59951],{"class":189,"line":190},[187,59952,59918],{},[26,59954,59957],{"className":59955,"code":59956,"language":31},[29],"/r/watchingcongress and /r/iwantthatonashirt are joined by:\n['/r/watchingcongress', '/r/stand', '/r/snowden', '/r/undelete', '/r/trees', '/r/iwantthatonashirt']\n",[33,59958,59956],{"__ignoreMap":35},[26,59960,59961],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,59962,59963],{"__ignoreMap":35},[187,59964,59965],{"class":189,"line":190},[187,59966,59918],{},[26,59968,59971],{"className":59969,"code":59970,"language":31},[29],"/r/asksciencediscussion and /r/dogsonhardwoodfloors are joined by:\n['/r/asksciencediscussion', '/r/badscience', '/r/badlinguistics', '/r/animalsbeingjerks', '/r/startledcats', '/r/dogsonhardwoodfloors']\n",[33,59972,59970],{"__ignoreMap":35},[26,59974,59975],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,59976,59977],{"__ignoreMap":35},[187,59978,59979],{"class":189,"line":190},[187,59980,59918],{},[26,59982,59985],{"className":59983,"code":59984,"language":31},[29],"/r/randommail and /r/mini are joined by:\n['/r/randommail', '/r/spiceexchange', '/r/cameraswapping', '/r/itookapicture', '/r/carporn', '/r/mini']\n",[33,59986,59984],{"__ignoreMap":35},[26,59988,59989],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,59990,59991],{"__ignoreMap":35},[187,59992,59993],{"class":189,"line":190},[187,59994,59918],{},[26,59996,59999],{"className":59997,"code":59998,"language":31},[29],"/r/catsinsinks and /r/nzmovies are joined by:\n['/r/catsinsinks', '/r/wetcats', '/r/tinysubredditoftheday', '/r/sheep', '/r/nzmetahub', '/r/nzmovies']\n",[33,60000,59998],{"__ignoreMap":35},[26,60002,60003],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60004,60005],{"__ignoreMap":35},[187,60006,60007],{"class":189,"line":190},[187,60008,59918],{},[26,60010,60013],{"className":60011,"code":60012,"language":31},[29],"/r/thoriumreactor and /r/sailing are joined by:\n['/r/thoriumreactor', '/r/energy', '/r/spev', '/r/sailing']\n",[33,60014,60012],{"__ignoreMap":35},[26,60016,60017],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60018,60019],{"__ignoreMap":35},[187,60020,60021],{"class":189,"line":190},[187,60022,59918],{},[26,60024,60027],{"className":60025,"code":60026,"language":31},[29],"/r/deathnote and /r/vegetarianism are joined by:\n['/r/deathnote', '/r/television', '/r/netflixbestof', '/r/naturefilms', '/r/environment', '/r/vegetarianism']\n",[33,60028,60026],{"__ignoreMap":35},[26,60030,60031],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60032,60033],{"__ignoreMap":35},[187,60034,60035],{"class":189,"line":190},[187,60036,59918],{},[26,60038,60041],{"className":60039,"code":60040,"language":31},[29],"/r/mississippir4r and /r/mathematics are joined by:\n['/r/mississippir4r', '/r/mississippi', '/r/prisonreform', '/r/socialscience', '/r/alltech', '/r/mathematics']\n",[33,60042,60040],{"__ignoreMap":35},[26,60044,60045],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60046,60047],{"__ignoreMap":35},[187,60048,60049],{"class":189,"line":190},[187,60050,59918],{},[26,60052,60055],{"className":60053,"code":60054,"language":31},[29],"/r/britainsgottalent and /r/irelandbaldwin are joined by:\n['/r/britainsgottalent', '/r/britishtv', '/r/that70sshow', '/r/mila_kunis', '/r/christinaricci', '/r/irelandbaldwin']\n",[33,60056,60054],{"__ignoreMap":35},[26,60058,60059],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60060,60061],{"__ignoreMap":35},[187,60062,60063],{"class":189,"line":190},[187,60064,59918],{},[26,60066,60069],{"className":60067,"code":60068,"language":31},[29],"/r/the_donald and /r/ladybusiness are joined by:\n['/r/the_donald', '/r/shitliberalssay', '/r/trollxchromosomes', '/r/ladybusiness']\n",[33,60070,60068],{"__ignoreMap":35},[26,60072,60073],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60074,60075],{"__ignoreMap":35},[187,60076,60077],{"class":189,"line":190},[187,60078,59918],{},[26,60080,60083],{"className":60081,"code":60082,"language":31},[29],"/r/selfharm and /r/medlabprofessionals are joined by:\n['/r/selfharm', '/r/adhd', '/r/neuroimaging', '/r/pharmacy', '/r/medlabprofessionals']\n",[33,60084,60082],{"__ignoreMap":35},[26,60086,60087],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60088,60089],{"__ignoreMap":35},[187,60090,60091],{"class":189,"line":190},[187,60092,59918],{},[26,60094,60097],{"className":60095,"code":60096,"language":31},[29],"/r/coverart and /r/phillycraftbeer are joined by:\n['/r/coverart', '/r/nostalgia', '/r/upvotedbecausegirl', '/r/wtf', '/r/remindsmeofdf', '/r/beer', '/r/phillycraftbeer']\n",[33,60098,60096],{"__ignoreMap":35},[26,60100,60101],{"className":1383,"code":59918,"language":1125,"meta":35,"style":35},[33,60102,60103],{"__ignoreMap":35},[187,60104,60105],{"class":189,"line":190},[187,60106,59918],{},[26,60108,60111],{"className":60109,"code":60110,"language":31},[29],"/r/hotguyswithlonghair and /r/castles are joined by:\n['/r/hotguyswithlonghair', '/r/majesticmanes', '/r/ladyboners', '/r/imaginaryladyboners', '/r/imaginarycastles', '/r/castles']\n",[33,60112,60110],{"__ignoreMap":35},[11,60114,60115,60116,60121,60122,60127],{},"Taking a look ",[15,60117,60120],{"href":60118,"rel":60119},"http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/unweighted.html?highlight=bidirectional_shortest_path",[19],"under the hood"," of NetworkX and examining the algorith that finds the ",[15,60123,60126],{"href":60124,"rel":60125},"http://networkx.readthedocs.io/en/networkx-1.11/_modules/networkx/algorithms/shortest_paths/generic.html#shortest_path",[19],"shortest path"," between any two nodes in a graph, we find that it simply boils down to:",[26,60129,60132],{"className":60130,"code":60131,"language":31},[29],"def shortest_path(G, source=None, target=None, weight=None):\n    paths=nx.bidirectional_shortest_path(G,source,target)\n    return paths\n",[33,60133,60131],{"__ignoreMap":35},[11,60135,60136,60137,60140,60141,60144],{},"You can read more about the ",[33,60138,60139],{},"bidirectional_shortest_path"," function ",[15,60142,1321],{"href":60118,"rel":60143},[19]," in the NetworkX documentation.",[11,60146,60147,60148,60151],{},"When I was first experimenting with graph algorithms, I had an interesting result using an algorithm intruduced ",[15,60149,1321],{"href":59013,"rel":60150},[19]," in the Python documentation. Here's the algorithm:",[26,60153,60155],{"className":1383,"code":60154,"language":1125,"meta":35,"style":35},"def find_path(graph, start, end, path=[]):\n    path = path + [start]\n    if start == end:\n        return path\n    if not graph.has_key(start):\n        return None\n    for node in graph[start]:\n        if node not in path:\n            newpath = find_path(graph, node, end, path)\n            if newpath: return newpath\n    return None\n",[33,60156,60157,60162,60167,60172,60177,60182,60187,60192,60197,60202,60207],{"__ignoreMap":35},[187,60158,60159],{"class":189,"line":190},[187,60160,60161],{},"def find_path(graph, start, end, path=[]):\n",[187,60163,60164],{"class":189,"line":249},[187,60165,60166],{},"    path = path + [start]\n",[187,60168,60169],{"class":189,"line":312},[187,60170,60171],{},"    if start == end:\n",[187,60173,60174],{"class":189,"line":319},[187,60175,60176],{},"        return path\n",[187,60178,60179],{"class":189,"line":325},[187,60180,60181],{},"    if not graph.has_key(start):\n",[187,60183,60184],{"class":189,"line":686},[187,60185,60186],{},"        return None\n",[187,60188,60189],{"class":189,"line":697},[187,60190,60191],{},"    for node in graph[start]:\n",[187,60193,60194],{"class":189,"line":1291},[187,60195,60196],{},"        if node not in path:\n",[187,60198,60199],{"class":189,"line":1306},[187,60200,60201],{},"            newpath = find_path(graph, node, end, path)\n",[187,60203,60204],{"class":189,"line":1434},[187,60205,60206],{},"            if newpath: return newpath\n",[187,60208,60209],{"class":189,"line":2599},[187,60210,60211],{},"    return None\n",[11,60213,60214],{},"The above algorthim uses a process called backtracking to exaustively try all possibilities until it returns a solution. It creates an interesting \"random walk\" through groups of related subreddits. Here's the result of calling the above function on our graph (only 2 layers deep) with two random nodes: /r/persianrap and /r/nosleep:",[107,60216,60217],{},[11,60218,60219],{},"/r/persianrap /r/middleeasternmusic /r/arabic /r/arabs /r/libyancrisis /r/syriancivilwar /r/yemenicrisis /r/sinaiinsurgency /r/jihadinfocus /r/credibledefense /r/geopolitics /r/forgottennews /r/libyanconflict /r/menaconflicts /r/iran /r/iranianlgbt /r/zoroastrianism /r/kurdistan /r/rojava /r/anarchism /r/imaginarypolitics /r/imaginaryimmortals /r/imaginaryclerics /r/imaginarylakes /r/imaginaryaliens /r/imaginarygnomes /r/imaginaryladyboners /r/imaginaryturtleworlds /r/imaginarysunnydale /r/imaginarydwarves /r/imaginarywizards /r/imaginaryvikings /r/imaginarycolorscapes /r/imaginarysteampunk /r/imaginarytemples /r/imaginaryblueprints /r/comicbookart /r/imaginarytechnology /r/mtgporn /r/imaginaryoldkingdom /r/imaginaryfactories /r/imaginaryfederation /r/imaginarylovers /r/imaginarynarnia /r/imaginarydwellings /r/imaginaryscience /r/imaginarytaverns /r/imaginarybattlefields /r/cityporn /r/japanpics /r/nationalphotosubs /r/austriapics /r/southkoreapics /r/taiwanpics /r/ghanapics /r/kenyapics /r/norwaypics /r/vzlapics /r/perupics /r/antarcticapics /r/greatlakespics /r/lakeporn /r/pornoverlords /r/thingscutinhalfporn /r/manufacturing /r/cnc /r/askengineers /r/sciencesubreddits /r/math /r/simulate /r/cosmology /r/reddittothefuture /r/scifi /r/lost /r/the100books /r/the100 /r/theblacklist /r/nbc /r/dundermifflin /r/sonsofanarchy /r/twentyfour /r/banshee /r/hbo /r/siliconvalleyhbo /r/siliconvalley /r/california /r/tahoe /r/skiing /r/snowshoeing /r/xcountryskiing /r/wintergear /r/skijumping /r/winter /r/bigmountain /r/mountaineering /r/campingandhiking /r/earthporn /r/nature /r/birding /r/invasivespecies /r/zoology /r/entomology /r/rainforest /r/botany /r/wildlife /r/allscience /r/earthscience /r/energy /r/biomass /r/renewablenews /r/syngas /r/climatenews /r/composting /r/vermiculture /r/organicfarming /r/livestock /r/animalwelfare /r/randomactsofpetfood /r/animalreddits /r/cockatiel /r/catpics /r/tortoises /r/whales /r/cetacea /r/lifeaquatic /r/hrw /r/green_peace /r/environmental_policy /r/conservation /r/depthhub /r/indepthsports /r/deeperhubbeta /r/lectures /r/spacepolicy /r/skylon /r/ula /r/isro /r/engineteststands /r/jupiters /r/imaginarystarscapes /r/spacequestions /r/spaceflight /r/moon /r/dione /r/europa /r/oortcloud /r/dwarfplanetceres /r/saturn /r/asteroidbelt /r/mars /r/rhea /r/venus /r/astrophys /r/spacevideos /r/transhuman /r/timereddits /r/virtualreality /r/vive /r/oculus /r/learnvrdev /r/unity3d /r/gamedev /r/crowdfunding /r/crowdsourcing /r/mturk /r/swagbucks /r/beermoney /r/flipping /r/shoplifting /r/thriftstorehauls /r/dvdcollection /r/televisionposterporn /r/concertposterporn /r/movieposterporn /r/lv426 /r/predator /r/arnoldschwarzenegger /r/alanpartridge /r/americandad /r/timanderic /r/homemovies /r/gravityfalls /r/homestarrunner /r/telltale /r/thewalkingdeadgame /r/thewalkingdeadgifs /r/twdnomansland /r/heycarl /r/twdroadtosurvival /r/thewalkingdead /r/zombies /r/guns /r/swissguns /r/opencarry /r/libertarian /r/geolibertarianism /r/basicincome /r/basicincomeactivism /r/mhoc /r/modelaustralia /r/rmtk /r/thenetherlands /r/tokkiefeesboek /r/nujijinactie /r/ik_ihe /r/youirl /r/fite_me_irl /r/2meirl4meirl /r/depression /r/randomactsofcards /r/philately /r/coins /r/coins4sale /r/ancientcoins /r/ancientrome /r/flatblue /r/bestofwritingprompts /r/writingprompts /r/promptoftheday /r/flashfiction /r/keepwriting /r/getmotivated /r/mentors /r/favors /r/recordthis /r/videography /r/animation /r/3dsmax /r/computergraphics /r/cinema4d /r/design /r/ui_design /r/designjobs /r/heavymind /r/wtfart /r/alternativeart /r/imaginaryninjas /r/imaginaryruins /r/isometric /r/imaginaryislands /r/imaginaryverse /r/icandrawthat /r/caricatures /r/imaginaryneweden /r/imaginaryequestria /r/imaginaryaww /r/imaginarycyberpunk /r/chinafuturism /r/scifirealism /r/inegentlemanboners /r/imaginarywtf /r/imaginaryelementals /r/imaginarydinosaurs /r/dinosaurs /r/speculativeevolution /r/hybridanimals /r/photoshopbattles /r/cutouts /r/battleshops /r/graphic_design /r/visualization /r/statistics /r/oncourtanalytics /r/nbaanalytics /r/nba /r/pacers /r/atlantahawks /r/basketball /r/mavericks /r/fcdallas /r/theticket /r/dallasstars /r/bostonbruins /r/patriots /r/tennesseetitans /r/nashvillesounds /r/predators /r/flyers /r/hockeyfandom /r/caps /r/nhl /r/detroitredwings /r/sabres /r/floridapanthers /r/habs /r/montrealimpact /r/alouettes /r/cfl /r/stadiumporn /r/nfl /r/madden /r/eurobowl /r/fantasyfb /r/fantasyfootball /r/49ers /r/footballgamefilm /r/footballstrategy /r/cfb /r/collegebaseball /r/mlbdraft /r/baseball /r/cubs /r/cardinals /r/saintlouisfc /r/stlouisblues /r/stlouis /r/stlouisbiking /r/mobicycling /r/bicycling /r/vintage_bicycles /r/miamibiking /r/fatbike /r/cycling /r/strava /r/phillycycling /r/wheelbuild /r/bikewrench /r/velo /r/bikepolo /r/bicycletouring /r/bicyclingcirclejerk /r/bikecommuting /r/ukbike /r/leedscycling /r/londoncycling /r/fixedgearbicycle /r/cyclingfashion /r/peloton /r/mtb /r/climbingporn /r/adrenaline /r/motocross /r/bmxracing /r/wake /r/snowboardingnoobs /r/freebord /r/snowboarding /r/sledding /r/outdoors /r/soposts /r/cordcutters /r/netflixviavpn /r/hulu /r/firetv /r/netflixbestof /r/raisinghope /r/madmen /r/earthsgottalent /r/bobsburgers /r/fringe /r/louie /r/theoriginals /r/iansomerhalder /r/kat_graham /r/indianaevans /r/janelevy /r/gagegolightly /r/sarahhyland /r/starlets /r/ninadobrev /r/kathrynnewton /r/arielwinter /r/ashleygreene /r/gentlemanboners /r/bandporn /r/musicpics /r/listentomusic /r/listentonew /r/subraddits /r/dtipics /r/damnthatsinteresting /r/interestingasfuck /r/unexpected /r/wtf /r/weird /r/animalsbeingderps /r/animalsbeingconfused /r/humansbeingbros /r/hulpdiensten /r/askle /r/protectandserve /r/good_cop_free_donut /r/bad_cop_follow_up /r/amifreetogo /r/copwatch /r/puppycide /r/underreportednews /r/mediaquotes /r/savedyouaclick /r/news /r/neutralnews /r/ask_politics /r/politicalopinions /r/gunsarecool /r/renewableenergy /r/web_design /r/somebodymakethis /r/somethingimade /r/crafts /r/kidscrafts /r/daddit /r/formulafeeders /r/boobsandbottles /r/csectioncentral /r/predaddit /r/dadbloggers /r/mombloggers /r/cutekids /r/bigfeats /r/scienceparents /r/lv9hrvv /r/sahp /r/tryingforababy /r/waiting_to_try /r/pcos /r/infertility /r/birthparents /r/tfabchartstalkers /r/firsttimettc /r/cautiousbtb /r/ttchealthy /r/xxketo /r/ketoscience /r/ketogains /r/leangains /r/gettingshredded /r/bulkorcut /r/gainit /r/decidingtobebetter /r/zen /r/buddhism /r/astralprojection /r/spirituality /r/hinduism /r/yoga /r/veganfitness /r/posture /r/health /r/ukhealthcare /r/pharmacy /r/nursing /r/doctorswithoutborders /r/humanitarian /r/assistance /r/paranormalhelp /r/paranormal /r/333 /r/askparanormal /r/intelligence /r/blackhat /r/netsec /r/technology /r/newyorkfuturistparty /r/rad_decentralization /r/massachusettsfp /r/opensource /r/alabamafp /r/darknetplan /r/torrents /r/i2p /r/privacy /r/badgovnofreedom /r/censorship /r/governmentoppression /r/descentintotyranny /r/wikileaks /r/dncleaks /r/hillaryforprison /r/the_donald /r/shitredditsays /r/srsmythos /r/srstrees /r/entwives /r/lesbients /r/actuallesbians /r/lesbianromance /r/lesbianerotica /r/l4l /r/dyke /r/ladyladyboners /r/bisexual /r/bisexy /r/biwomen /r/pansexual /r/genderqueer /r/transspace /r/lgbtlibrary /r/lgbtnews /r/dixiequeer /r/lgbt /r/sex /r/helpmecope /r/bpd /r/rapecounseling /r/trueoffmychest /r/suicidewatch /r/bipolarsos /r/bipolar /r/mentalpod /r/adhd /r/hoarding /r/declutter /r/thrifty /r/tinyhouses /r/leanfire /r/lowcar /r/zerowaste /r/simpleliving /r/livingofftheland /r/hunting /r/animaltracking /r/survival /r/vedc /r/4x4 /r/classiccars /r/automotivetraining /r/autodetailing /r/cartalk /r/mercedes_benz /r/motorsports /r/rallycross /r/worldrallycross /r/blancpain /r/nascarhometracks /r/arcaracing /r/stadiumsupertrucks /r/hydroplanes /r/sailing /r/boatbuilding /r/woodworking /r/cottage_industry /r/farriers /r/blacksmith /r/bladesmith /r/knives /r/swissarmyknives /r/switzerland /r/bern /r/sanktgallen /r/liechtenstein /r/erasmus /r/de /r/germanpuns /r/schland /r/rvacka /r/sloensko /r/slovakia /r/belarus /r/andorra /r/europe /r/hungary /r/francophonie /r/thailand /r/vietnam /r/vietnampics /r/travel /r/geography /r/climate /r/drought /r/waterutilities /r/drylands /r/irrigation /r/water /r/onthewaterfront /r/wetlands /r/marinelife /r/ocean /r/seasteading /r/frontier_colonization /r/arcology /r/retrofuturism /r/goldenpath /r/politics /r/moderationtheory /r/wdp /r/outoftheloop /r/wherearetheynow /r/entertainment /r/portlandia /r/themichaeljfoxshow /r/backtothefuture /r/bladerunner /r/filmnoir /r/vintageladyboners /r/classicfilms /r/foreignmovies /r/britishfilms /r/canadianfilm /r/newjerseyfilm /r/newzealandfilm /r/newzealand /r/wellington /r/nzmetahub /r/newzealandhistory /r/scottishhistory /r/scots /r/scottishproblems /r/britishproblems /r/swedishproblems /r/pinsamt /r/sweden /r/svenskpolitik /r/arbetarrorelsen /r/socialism /r/shittydebatecommunism /r/shittysocialscience /r/shittyideasforadmins /r/shittytheoryofreddit /r/shittybuildingporn /r/shittylifeprotips /r/shittyshitredditsays /r/shittyquotesporn /r/shittyama /r/askashittyparent /r/shittyprogramming /r/shittyaskalawyer /r/badlegaladvice /r/badscience /r/badeconomics /r/badhistory /r/historicalrage /r/metarage /r/ragenovels /r/fffffffuuuuuuuuuuuu /r/gaaaaaaayyyyyyyyyyyy /r/lgbteens /r/needafriend /r/rant /r/showerthoughts /r/markmywords /r/calledit /r/futurewhatif /r/sportswhatif /r/alternatehistory /r/maps /r/xkcd /r/kerbalspaceprogram /r/spacesimgames /r/eve /r/scifigaming /r/masseffect /r/imaginarymasseffect /r/imaginaryvampires /r/imaginarytowers /r/imaginarybestof /r/pics /r/spaceporn /r/auroraporn /r/weatherporn /r/sfwpornnetwork /r/fwepp /r/shittyearthporn /r/shittyaskreddit /r/askashittyphilosopher /r/shittyaskhistory /r/shittysuboftheweek /r/shittyaskcooking /r/shittyhub /r/coolguides /r/trendingsubreddits /r/monkslookingatbeer /r/beerporn /r/beerwithaview /r/shittybeerwithaview /r/shittyfoodporn /r/enttreats /r/trees /r/eldertrees /r/vaporents /r/crainn /r/eirhub /r/fairepublicofireland /r/gaeltacht /r/westmeath /r/tipperary /r/limerick /r/kilkenny /r/ireland /r/irejobs /r/resumes /r/careerguidance /r/flatone /r/centralillinois /r/chicubs /r/whitesox /r/minnesotatwins /r/minnesotavikings /r/greenbaypackers /r/jaguars /r/miamidolphins /r/nflroundtable /r/detroitlions /r/forhonor /r/vikingstv /r/hannibaltv /r/thepathhulu /r/batesmotel /r/hannibal /r/hitchcock /r/silentmoviegifs /r/moviestunts /r/bollywoodrealism /r/indiamain /r/indianews /r/asia /r/oldindia /r/explorepakistan /r/churchporn /r/medievalporn /r/castles /r/historyporn /r/thewaywewere /r/1970s /r/classicmovietrailers /r/warmovies /r/moviecritic /r/trailers /r/liveaction /r/animedeals /r/dbz /r/toonami /r/regularshow /r/thelifeandtimesoftim /r/aquajail /r/modern_family /r/supernatural /r/mishacollins /r/jaredpadalecki /r/fandomnatural /r/fangirls /r/trollxgirlgamers /r/trollmedia /r/trollgaming /r/trollmua /r/justtrollxthings /r/trollxmoms /r/trollmeta /r/trollychromosome /r/oney /r/askwomen /r/okcupid /r/relationship_advice /r/help /r/bugs /r/redditdev /r/enhancement /r/yoursub /r/horrorreviewed /r/truecreepy /r/metatruereddit /r/truepolitics /r/truehub /r/truegaming /r/askgames /r/freegamesonandroid /r/androidapps /r/apphookup /r/browsemyreddit /r/findareddit /r/trap /r/naut /r/militaryfinance /r/army /r/militarystories /r/nationalguard /r/uscg /r/usa /r/murica /r/lonestar /r/whataburger /r/fastfood /r/cocacola /r/kelloggs /r/kellawwggs /r/awwducational /r/marinebiologygifs /r/biologygifs /r/chemicalreactiongifs /r/homechemistry /r/holdmybeaker /r/holdmybeer /r/movieoftheday /r/sharknado /r/syfy /r/killjoys /r/theexpanse /r/truedetective /r/boardwalkempire /r/mobcast /r/1920s /r/1960s /r/beatles /r/minimaluminiumalism /r/ghostsrights /r/botsrights /r/totallynotrobots /r/robotics /r/manna /r/singularity /r/futureporn /r/singularitarianism /r/automate /r/darkfuturology /r/controlproblem /r/aiethics /r/ainothuman /r/neuraljokes /r/3amjokes /r/mommajokes /r/antijokes /r/absolutelynotme_irl /r/toomeirlformeirl /r/meirl /r/tree_irl /r/fishpost /r/mod_irl /r/pics_irl /r/teleshits /r/bitstrips /r/stopbullyingcomics /r/animalsbeingjerks /r/surfinganimals /r/unorthocat /r/catsubs /r/stuffoncats /r/catsinbusinessattire /r/catsinsinks /r/catsonkeyboards /r/mechanicalkeyboards /r/hackedgadgets /r/techsupportmacgyver /r/techsupport /r/programming /r/algorithms /r/datamining /r/datasets /r/wordcloud /r/datavizrequests /r/funnycharts /r/mapporn /r/mapmaking /r/worldbuilding /r/scificoncepts /r/apocalypseporn /r/imaginaryjerk /r/braveryjerk /r/circlejerk /r/politicaldiscussion /r/politicalfactchecking /r/moderatepolitics /r/truereddit /r/malelifestyle /r/fitness /r/swimming /r/freediving /r/bikeshop /r/climbing /r/climbharder /r/bouldering /r/climbergirls /r/womenshredders /r/skatergirls /r/girlsurfers /r/kiteboarding /r/longboarding /r/streetboarding /r/letsgosnowboarding /r/spliddit /r/backcountry /r/wjdbbl2 /r/caving /r/nationalparks /r/parkrangers /r/thesca /r/searchandrescue /r/wildernessbackpacking /r/campinggear /r/flashlight /r/camping /r/yellowstone /r/wmnf /r/pacificcresttrail /r/cdt /r/ultralight /r/backpacking /r/travelpartners /r/adventures /r/libraryofshadows /r/shortscarystories /r/shortscarystoriesooc /r/nosleepooc /r/nosleep",[2215,60221,60223],{"id":60222},"centrality","Centrality",[11,60225,60226,60227,358],{},"Centrality is anohter important topic in graph theory. Here's a brief introduction to centrality from ",[15,60228,41677],{"href":60229,"rel":60230},"https://en.wikipedia.org/wiki/Centrality",[19],[107,60232,60233],{},[11,60234,60235],{},"In graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease.",[11,60237,60238,60239,60242],{},"There are several different methods of measuring centrality in a graph. Here I use ",[33,60240,60241],{},"eigenvector_centrality_numpy",", a function included in NetworkX. It takes in a graph and returns a dictionary with graph nodes as keys and node centrality as values.",[26,60244,60246],{"className":1383,"code":60245,"language":1125,"meta":35,"style":35},"centrality = nx.eigenvector_centrality_numpy(G1)\n",[33,60247,60248],{"__ignoreMap":35},[187,60249,60250],{"class":189,"line":190},[187,60251,60245],{},[11,60253,60254],{},"Let's see which subreddit has the highest centrality:",[26,60256,60258],{"className":1383,"code":60257,"language":1125,"meta":35,"style":35},"print max(centrality, key=centrality.get), centrality[max(centrality, key=centrality.get)]\n",[33,60259,60260],{"__ignoreMap":35},[187,60261,60262],{"class":189,"line":190},[187,60263,60257],{},[26,60265,60268],{"className":60266,"code":60267,"language":31},[29],"/r/imaginarybattlefields 0.0721530261127\n",[33,60269,60267],{"__ignoreMap":35},[26,60271,60273],{"className":1383,"code":60272,"language":1125,"meta":35,"style":35},"len(centrality) == len(sorted(centrality.values(), reverse=True))\n",[33,60274,60275],{"__ignoreMap":35},[187,60276,60277],{"class":189,"line":190},[187,60278,60272],{},[26,60280,60282],{"className":60281,"code":59842,"language":31},[29],[33,60283,59842],{"__ignoreMap":35},[11,60285,60286],{},"Since all of the centrality values are unique, we can look up nodes by their centrality values.",[26,60288,60290],{"className":1383,"code":60289,"language":1125,"meta":35,"style":35},"subr_list = []\nfor node in centrality:\n    subr_list.append((node, centrality[node]))\n\nsorted_subr_list = subr_list.sort(key=lambda x: x[1])\n",[33,60291,60292,60297,60302,60307,60311],{"__ignoreMap":35},[187,60293,60294],{"class":189,"line":190},[187,60295,60296],{},"subr_list = []\n",[187,60298,60299],{"class":189,"line":249},[187,60300,60301],{},"for node in centrality:\n",[187,60303,60304],{"class":189,"line":312},[187,60305,60306],{},"    subr_list.append((node, centrality[node]))\n",[187,60308,60309],{"class":189,"line":319},[187,60310,316],{"emptyLinePlaceholder":315},[187,60312,60313],{"class":189,"line":325},[187,60314,60315],{},"sorted_subr_list = subr_list.sort(key=lambda x: x[1])\n",[26,60317,60319],{"className":1383,"code":60318,"language":1125,"meta":35,"style":35},"for x in sorted(subr_list, key=lambda x: x[1], reverse=True)[:200]: print x[0],\n",[33,60320,60321],{"__ignoreMap":35},[187,60322,60323],{"class":189,"line":190},[187,60324,60318],{},[26,60326,60329],{"className":60327,"code":60328,"language":31},[29],"/r/imaginarybattlefields /r/imaginarycityscapes /r/imaginarywastelands /r/imaginarywildlands /r/imaginaryleviathans /r/imaginarydragons /r/imaginarystarscapes /r/imaginarywesteros /r/imaginaryartifacts /r/imaginaryangels /r/imaginarymaps /r/imaginarybehemoths /r/imaginarydemons /r/imaginaryelves /r/imaginarycentaurs /r/imaginaryfuturewar /r/imaginarysoldiers /r/imaginaryhistory /r/imaginaryarmor /r/imaginarystarships /r/imaginarynetwork /r/imaginaryjedi /r/imaginarydinosaurs /r/imaginarysteampunk /r/imaginarycyberpunk /r/imaginaryarchers /r/imaginaryvehicles /r/imaginaryanime /r/imaginaryfallout /r/imaginaryastronauts /r/imaginarymusic /r/imaginaryfactories /r/imaginaryequestria /r/imaginarywarships /r/imaginaryazeroth /r/imaginaryarrakis /r/imaginarydisney /r/imaginarypolitics /r/imaginaryhorrors /r/imaginarywinterscapes /r/imaginaryseascapes /r/imaginarypirates /r/imaginarywarriors /r/imaginarymiddleearth /r/imaginarygallifrey /r/imaginarymechs /r/imaginarypropaganda /r/imaginarymerfolk /r/imaginaryvikings /r/imaginaryundead /r/imaginarybeasts /r/imaginarymutants /r/imaginaryruins /r/imaginarytamriel /r/imaginaryforests /r/imaginaryelementals /r/imaginaryskyscapes /r/imaginarymonuments /r/imaginarywaterfalls /r/imaginaryworlds /r/imaginarywizards /r/imaginaryinteriors /r/imaginaryhogwarts /r/imaginarytowers /r/imaginaryarchitecture /r/imaginaryweaponry /r/imaginarygaming /r/imaginarycastles /r/imaginaryrobotics /r/imaginarybooks /r/imaginarygnomes /r/imaginaryvillages /r/imaginarydeserts /r/imaginarywerewolves /r/imaginarydieselpunk /r/imaginaryvampires /r/imaginaryadrenaline /r/imaginarykanto /r/imaginarynatives /r/imaginaryrivers /r/imaginarytemples /r/imaginaryassassins /r/imaginaryvolcanoes /r/imaginaryclerics /r/imaginaryprisons /r/imaginarygiants /r/imaginarycowboys /r/imaginaryhumans /r/imaginarydwarves /r/imaginarycaves /r/imaginarytrolls /r/imaginarywalls /r/imaginarylakes /r/imaginarywitches /r/imaginaryorcs /r/imaginarycanyons /r/imaginaryasylums /r/imaginaryimmortals /r/imaginaryaliens /r/imaginarynobles /r/imaginaryspirits /r/imaginaryaetherpunk /r/imaginarytrees /r/imaginaryislands /r/imaginaryninjas /r/imaginaryscience /r/imaginarymountains /r/imaginaryknights /r/imaginarygoblins /r/imaginaryfaeries /r/imaginarygotham /r/imaginarycybernetics /r/imaginaryooo /r/imaginaryderelicts /r/imaginaryfood /r/imaginaryworldeaters /r/imaginarymindscapes /r/imaginaryaww /r/imaginarymarvel /r/imaginaryweather /r/imaginarynewnewyork /r/imaginaryspidey /r/imaginaryautumnscapes /r/imaginarywarhammer /r/imaginaryfeels /r/imaginarywitcher /r/imaginaryvessels /r/imaginarytaverns /r/imaginarybestof /r/imaginaryairships /r/imaginaryportals /r/imaginaryfashion /r/imaginarylovers /r/imaginarydc /r/imaginaryanimals /r/imaginaryhellscapes /r/imaginarycolorscapes /r/imaginarymonstergirls /r/imaginaryswamps /r/imaginarymythology /r/imaginaryscholars /r/imaginaryladyboners /r/imaginaryfuturism /r/imaginaryaviation /r/imaginarypathways /r/imaginarygatherings /r/imaginarybodyscapes /r/imaginaryoverwatch /r/imaginarydwellings /r/imaginarystephenking /r/specart /r/inegentlemanboners /r/comicbookart /r/imaginarymasseffect /r/imaginaryhalo /r/imaginaryjerk /r/backgroundart /r/futureporn /r/imaginarywallpapers /r/imaginaryfamilies /r/imaginarylibraries /r/imaginaryturtleworlds /r/imaginarydesigns /r/wallpapers /r/apocalypseporn /r/comicbookporn /r/isometric /r/imaginarybakerst /r/imaginaryverse /r/imaginarysunnydale /r/imaginaryfederation /r/imaginarysanctuary /r/starshipporn /r/imaginarystarcraft /r/imaginaryoldkingdom /r/imaginarynarnia /r/imaginarycybertron /r/gameworlds /r/imaginarycarnage /r/imaginaryboners /r/icandrawthat /r/imaginarycosmere /r/imaginaryaperture /r/armoredwomen /r/imaginarywtf /r/unusualart /r/imaginaryblueprints /r/alternativeart /r/sympatheticmonsters /r/adorabledragons /r/imaginarysummerscapes /r/imaginarygayboners /r/imaginarystash /r/artistoftheday /r/imaginaryglaciers /r/imaginaryhybrids /r/imaginaryadventurers /r/imaginarymetropolis /r/craftsoficeandfire /r/popartnouveau\n",[33,60330,60328],{"__ignoreMap":35},[11,60332,60333],{},"There seems to be a network of \"imaginary\" subreddits that have the highest centrality. The members of this network probably all link to themselves as well as many other subreddits as the \"imaginary\" topics span a wide range content. This network may be drowning out other nodes that would otherwise have a high centrality relative to the rest of the subreddits. It might be interesting to eliminate these nodes from the graph and recalculate centrality. Let's look at the distribution of centrality values:",[26,60335,60337],{"className":1383,"code":60336,"language":1125,"meta":35,"style":35},"_ = plt.plot(sorted(centrality.values(), reverse=True)[:1000])\n_ = plt.title('Subreddit Centrality (top 1000)')\n_ = plt.xlabel('Rank')\n_ = plt.ylabel('Centrality')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/subreddit_graph/centrality.png'))\n",[33,60338,60339,60344,60349,60354,60359],{"__ignoreMap":35},[187,60340,60341],{"class":189,"line":190},[187,60342,60343],{},"_ = plt.plot(sorted(centrality.values(), reverse=True)[:1000])\n",[187,60345,60346],{"class":189,"line":249},[187,60347,60348],{},"_ = plt.title('Subreddit Centrality (top 1000)')\n",[187,60350,60351],{"class":189,"line":312},[187,60352,60353],{},"_ = plt.xlabel('Rank')\n",[187,60355,60356],{"class":189,"line":319},[187,60357,60358],{},"_ = plt.ylabel('Centrality')\n",[187,60360,60361],{"class":189,"line":325},[187,60362,60363],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/subreddit_graph/centrality.png'))\n",[11,60365,60366],{},[511,60367],{"alt":7255,"src":60368},"/static/subreddit_graph/centrality.png",[2215,60370,60372],{"id":60371},"connectedness","Connectedness",[11,60374,60375],{},"Let's take a look at the graph as a whole. One thing I'm not sure of is whether or not the entire graph is connected. This means that any node can be reached from any other node. Since we constructed the graph from 49 unrelated nodes, it is possible that the graph is unconnected. This would mean that one or more of the default subreddits and its subreddits is not connected with the rest of the graph. In searching for the shortest path I did not come across any pairs of nodes that did not have a path between themselves. I wouldn't be surprised if there are a handful of nodes that stand on their own.",[26,60377,60379],{"className":1383,"code":60378,"language":1125,"meta":35,"style":35},"#size of graph: nodes and edges (or, subreddits and connecting links)\nprint \"Our graph has \" + str(nx.number_of_nodes(G1)) + ' nodes and ' + str(nx.number_of_edges(G1)) + ' edges.'\n",[33,60380,60381,60386],{"__ignoreMap":35},[187,60382,60383],{"class":189,"line":190},[187,60384,60385],{},"#size of graph: nodes and edges (or, subreddits and connecting links)\n",[187,60387,60388],{"class":189,"line":249},[187,60389,60390],{},"print \"Our graph has \" + str(nx.number_of_nodes(G1)) + ' nodes and ' + str(nx.number_of_edges(G1)) + ' edges.'\n",[26,60392,60395],{"className":60393,"code":60394,"language":31},[29],"Our graph has 29854 nodes and 149491 edges.\n",[33,60396,60394],{"__ignoreMap":35},[26,60398,60400],{"className":1383,"code":60399,"language":1125,"meta":35,"style":35},"print \"True of False: our graph is connected... \" + str(nx.is_connected(G1)) + '!'\n",[33,60401,60402],{"__ignoreMap":35},[187,60403,60404],{"class":189,"line":190},[187,60405,60399],{},[26,60407,60410],{"className":60408,"code":60409,"language":31},[29],"True of False: our graph is connected... False!\n",[33,60411,60409],{"__ignoreMap":35},[26,60413,60415],{"className":1383,"code":60414,"language":1125,"meta":35,"style":35},"Gc = max(nx.connected_component_subgraphs(G1), key=len)\nprint \"The largest connected component subgraph has \" + str(nx.number_of_nodes(Gc)) + \" nodes. \"\n",[33,60416,60417,60422],{"__ignoreMap":35},[187,60418,60419],{"class":189,"line":190},[187,60420,60421],{},"Gc = max(nx.connected_component_subgraphs(G1), key=len)\n",[187,60423,60424],{"class":189,"line":249},[187,60425,60426],{},"print \"The largest connected component subgraph has \" + str(nx.number_of_nodes(Gc)) + \" nodes. \"\n",[26,60428,60431],{"className":60429,"code":60430,"language":31},[29],"The largest connected component subgraph has 29840 nodes.\n",[33,60432,60430],{"__ignoreMap":35},[11,60434,60435],{},"There are 14 nodes that are not connected to the main connected component. Let's list them.",[26,60437,60439],{"className":1383,"code":60438,"language":1125,"meta":35,"style":35},"for x in list(set(nx.to_dict_of_lists(G1, nodelist=None).keys()) - set(nx.to_dict_of_lists(Gc, nodelist=None).keys())): print x,\n",[33,60440,60441],{"__ignoreMap":35},[187,60442,60443],{"class":189,"line":190},[187,60444,60438],{},[26,60446,60449],{"className":60447,"code":60448,"language":31},[29],"/r/spacediscussions /r/wtfit.gif /r/space. /r/subreddit_graph /r/vidalia /r/listentothis. /r/history. /r/all. /r/ghostdriver /r/personalfinance. /r/toombscounty /r/gaming /r/science /r/books.\n",[33,60450,60448],{"__ignoreMap":35},[11,60452,60453],{},"Some of the large communities on reddit include /r/books, /r/gaming and /r/science. These subreddits list related subreddits on separate wiki pages since there are many related subreddits for each one. They were most likely all captured in the subsequent levels of the graph, but they also did not link back to /r/science. Here's an example:",[26,60455,60457],{"className":1383,"code":60456,"language":1125,"meta":35,"style":35},"for x in master_df_u.loc[master_df_u.subreddit=='/r/physics'].related: print x\n",[33,60458,60459],{"__ignoreMap":35},[187,60460,60461],{"class":189,"line":190},[187,60462,60456],{},[26,60464,60467],{"className":60465,"code":60466,"language":31},[29],"['/r/physicsjokes', '/r/gradadmissions', '/r/homeworkhelp', '/r/scienceimages', '/r/askacademia', '/r/physicsgifs', '/r/physicsstudents', '/r/gradschool', '/r/askphysics', '/r/physics']\n",[33,60468,60466],{"__ignoreMap":35},[11,60470,60471,60472,60477],{},"I've got some additional ideas to explore in another post on this topic, such as finding cliques and maximual cliques, and doing graph visualizations with D3.js. If you are interested in playing with the data, you can clone ",[15,60473,60476],{"href":60474,"rel":60475},"https://github.com/briancaffey/reddit-graph-analysis",[19],"my GitHub repo"," and load the pickled DataFrames like this:",[26,60479,60481],{"className":1383,"code":60480,"language":1125,"meta":35,"style":35},"import pandas as pd\ndf = pd.read_pickle('pickle/master_df.p')\n",[33,60482,60483,60487],{"__ignoreMap":35},[187,60484,60485],{"class":189,"line":190},[187,60486,53591],{},[187,60488,60489],{"class":189,"line":249},[187,60490,60491],{},"df = pd.read_pickle('pickle/master_df.p')\n",[855,60493,6258],{},{"title":35,"searchDepth":249,"depth":249,"links":60495},[],"2017-03-03","This notebook explores some basic concepts of graph theory. A few weeks ago I set up a script to scrape data from reddit.com with the goal of visualizing the network of related subreddits (forums on specific topics) and related data.","/static/subreddits.png",{"layout":29014},"/2017/03/03/graph_subreddit",{"title":58634,"description":60497},"2017/03/03/graph_subreddit",[10051,1125,27051,582,60504],"graphs","6iZj-hiDxE2fBFIlIkzBrQQuykMyCBtk3aebWowsoyg",{"id":60507,"title":60508,"body":60509,"comments":315,"date":68358,"description":68359,"draft":872,"extension":873,"external":874,"image":61547,"meta":68360,"navigation":315,"path":68361,"seo":68362,"stem":68363,"tags":68364,"__hash__":68366},"blog/2017/01/01/pc-data.md","PCPartPicker data",{"type":8,"value":60510,"toc":68341},[60511,60519,60539,60543,60546,60642,60651,60654,60744,60751,60754,60867,60881,60887,60890,61066,61072,61080,61531,61534,61543,61548,61559,61562,61567,61570,61596,61599,61603,61606,61721,61724,61814,61824,61832,62202,62206,62209,62254,62257,62385,62390,62393,62396,62436,62446,62489,62497,62502,62527,62533,62587,62596,62606,62697,62702,62722,62728,62737,62742,62751,62754,62757,62772,62775,62784,62789,62792,62801,62815,62818,62862,62867,62870,62873,62908,62913,62916,62919,62964,62969,62972,62974,62977,62988,63013,63018,63021,63063,63068,63077,63082,63088,63093,63139,63144,63147,63185,63190,63193,63244,63249,63252,63255,63378,63381,63406,63412,63444,63512,63518,63625,63630,63633,63642,63647,63650,63653,63662,63667,63675,63678,63686,63691,63700,63705,63718,63721,63724,63727,63731,63737,63784,63789,63792,63796,63799,63802,63844,63849,63852,63855,63896,63901,63904,63951,63956,63960,63968,63973,63976,64025,64030,64033,64085,64090,64093,64142,64147,64150,64202,64207,64211,64214,64217,64267,64272,64275,64281,64286,64289,64335,64340,64343,64399,64404,64406,64409,64412,64517,64522,64525,64528,64646,64651,64659,64662,64766,64771,64774,64780,64783,64789,64792,64799,64805,64813,64820,64823,64870,64875,64878,64883,64886,64935,64940,64943,65062,65067,65070,65073,65125,65130,65134,65137,65140,65191,65196,65199,65240,65245,65248,65304,65309,65312,65365,65370,65373,65429,65434,65437,65483,65488,65491,65528,65533,65536,65591,65596,65599,65773,65778,65781,66009,66014,66017,66166,66171,66177,66181,66190,66195,66204,66278,66283,66286,66289,66343,66348,66351,66408,66413,66416,66476,66481,66484,66488,66491,66538,66543,66546,66549,66667,66672,66677,66680,66789,66794,66799,66802,66805,66809,66812,66864,66869,66886,66894,66899,66902,66905,66909,66912,66915,66918,67089,67094,67097,67100,67163,67168,67172,67175,67212,67217,67220,67224,67229,67233,67238,67242,67247,67251,67256,67260,67265,67268,67271,67274,67278,67287,67349,67355,67361,67368,67425,67428,67503,67509,67520,67523,67548,67551,67556,67561,67566,67569,67574,67579,67584,67587,67596,67599,67603,67611,67616,67619,67731,67768,67773,67776,67780,67783,67786,67799,68049,68053,68063,68338],[11,60512,60513,60514,60518],{},"In the summer of 2016 I built two high-end computers, something I haven't done since 2011. I used ",[15,60515,60517],{"href":60516},"pcpartpicker.com","PCPartPicker"," to research the components and read about PC builds similar to the ones I had in mind. It's a relatively new site that has a strong community of builders, helpful tools to help with part compatibility as well as extensive user reviews on PC components.",[11,60520,60521,60522,60525,60526,60531,60532,60538],{},"Pouring over pages and pages of computer builds, CPUs, video cards and motherboards got me very interested in visualizing the data on both individual components and builds, and the relationship between component specifications and prices. This post will detail the process by which I collected, cleaned, visualized and analyzed all of the data on ",[15,60523,60524],{"href":60516},"PCPartPicker.com",". I've also done some work with natural language process (NLP) to do sentiment analysis on the collection of written user reviews for individual PC parts. If you read to the end you will get to see the two computers I built last summer: ",[15,60527,60528],{"href":9984},[4339,60529,60530],{},"Ascension I"," (my personal machine) and ",[15,60533,60534,60537],{"href":9984},[4339,60535,60536],{},"Beast Mode II"," (BM2)"," (for my cousin).",[168,60540,60542],{"id":60541},"data-collection-cleaning","Data Collection & Cleaning",[11,60544,60545],{},"For the most part, the data I wanted to collect was well organized and conveniently structured. Since the content on almost all of the pages on PCPartPicker is loaded dynamically, I decided to use a JavaScript engine called PhantomJS to retrieve the HTML after the page loaded. Scraping data would otherwise return only a skeleton of the DOM with no data. Here is the JavaScript file that I used with PhantomJS to scrape data:",[26,60547,60549],{"className":6362,"code":60548,"language":6364,"meta":35,"style":35},"'use strict'\nvar system = require('system')\nvar args = system.args\nvar page = require('webpage').create()\n\npage.onConsoleMessage = function (msg) {\n  console.log(msg)\n}\nvar url = args[1]\npage.open(url, function (status) {\n  if (status === 'success') {\n    page.evaluate(function () {\n      console.log(document.body.innerHTML)\n    })\n  } else {\n    console.log('not success')\n  }\n  phantom.exit(1)\n})\n",[33,60550,60551,60556,60561,60566,60571,60575,60580,60585,60589,60594,60599,60604,60609,60614,60619,60624,60629,60633,60638],{"__ignoreMap":35},[187,60552,60553],{"class":189,"line":190},[187,60554,60555],{},"'use strict'\n",[187,60557,60558],{"class":189,"line":249},[187,60559,60560],{},"var system = require('system')\n",[187,60562,60563],{"class":189,"line":312},[187,60564,60565],{},"var args = system.args\n",[187,60567,60568],{"class":189,"line":319},[187,60569,60570],{},"var page = require('webpage').create()\n",[187,60572,60573],{"class":189,"line":325},[187,60574,316],{"emptyLinePlaceholder":315},[187,60576,60577],{"class":189,"line":686},[187,60578,60579],{},"page.onConsoleMessage = function (msg) {\n",[187,60581,60582],{"class":189,"line":697},[187,60583,60584],{},"  console.log(msg)\n",[187,60586,60587],{"class":189,"line":1291},[187,60588,1309],{},[187,60590,60591],{"class":189,"line":1306},[187,60592,60593],{},"var url = args[1]\n",[187,60595,60596],{"class":189,"line":1434},[187,60597,60598],{},"page.open(url, function (status) {\n",[187,60600,60601],{"class":189,"line":2599},[187,60602,60603],{},"  if (status === 'success') {\n",[187,60605,60606],{"class":189,"line":2607},[187,60607,60608],{},"    page.evaluate(function () {\n",[187,60610,60611],{"class":189,"line":2621},[187,60612,60613],{},"      console.log(document.body.innerHTML)\n",[187,60615,60616],{"class":189,"line":2631},[187,60617,60618],{},"    })\n",[187,60620,60621],{"class":189,"line":2642},[187,60622,60623],{},"  } else {\n",[187,60625,60626],{"class":189,"line":2653},[187,60627,60628],{},"    console.log('not success')\n",[187,60630,60631],{"class":189,"line":2665},[187,60632,6847],{},[187,60634,60635],{"class":189,"line":2674},[187,60636,60637],{},"  phantom.exit(1)\n",[187,60639,60640],{"class":189,"line":2684},[187,60641,6515],{},[11,60643,60644,60645,60650],{},"PhantomJS was pretty painful to use, and I've heard of better options for web browser automation and web scrapping like ",[15,60646,60649],{"href":60647,"rel":60648},"http://www.seleniumhq.org/",[19],"Selenium",", so I would probably use that for future projects. PhantomJS works fairly well, and this script, combined with the bash scripts below should be able to serve as a decent template for any similar type of web scraping project.",[11,60652,60653],{},"The first step is to loop through pages containing links of pages for both completed computer builds and computer parts using bash. Here's the bash script I used to scrape links to computer builds:",[26,60655,60657],{"className":181,"code":60656,"language":183,"meta":35,"style":35},"#!/use/bin/env bash\nfor i in `seq 1 1125`;\ndo\n    phantomjs production.js \"https://pcpartpicker.com/builds/#page=\"$i > pages/pages_$i.txt\n    result=$(python -c \"import random;print(random.uniform(2.1, 3.4))\")\n    sleep $result\ndone\n",[33,60658,60659,60664,60685,60689,60714,60732,60740],{"__ignoreMap":35},[187,60660,60661],{"class":189,"line":190},[187,60662,60663],{"class":295},"#!/use/bin/env bash\n",[187,60665,60666,60668,60670,60672,60674,60676,60678,60681,60683],{"class":189,"line":249},[187,60667,16543],{"class":573},[187,60669,47398],{"class":577},[187,60671,16549],{"class":573},[187,60673,47577],{"class":196},[187,60675,47580],{"class":193},[187,60677,680],{"class":588},[187,60679,60680],{"class":588}," 1125",[187,60682,13291],{"class":196},[187,60684,6961],{"class":577},[187,60686,60687],{"class":189,"line":312},[187,60688,16566],{"class":573},[187,60690,60691,60694,60697,60700,60703,60705,60708,60711],{"class":189,"line":319},[187,60692,60693],{"class":193},"    phantomjs",[187,60695,60696],{"class":196}," production.js",[187,60698,60699],{"class":196}," \"https://pcpartpicker.com/builds/#page=\"",[187,60701,60702],{"class":577},"$i ",[187,60704,37205],{"class":573},[187,60706,60707],{"class":196}," pages/pages_",[187,60709,60710],{"class":577},"$i",[187,60712,60713],{"class":196},".txt\n",[187,60715,60716,60719,60721,60723,60725,60727,60730],{"class":189,"line":325},[187,60717,60718],{"class":577},"    result",[187,60720,595],{"class":573},[187,60722,16633],{"class":577},[187,60724,1125],{"class":193},[187,60726,47426],{"class":588},[187,60728,60729],{"class":196}," \"import random;print(random.uniform(2.1, 3.4))\"",[187,60731,621],{"class":577},[187,60733,60734,60737],{"class":189,"line":686},[187,60735,60736],{"class":193},"    sleep",[187,60738,60739],{"class":577}," $result\n",[187,60741,60742],{"class":189,"line":697},[187,60743,17114],{"class":573},[11,60745,60746,60747,60750],{},"The result of ",[33,60748,60749],{},"phantomjs"," saves HTML for a given page in the loop to a text file, and then waits for a few seconds before scraping the next page.",[11,60752,60753],{},"I then looped through these pages and pulled out all of the target links for PC builds and PC parts into separate text files using Beautiful Soup. Here's the script for that in python:",[26,60755,60757],{"className":1383,"code":60756,"language":1125,"meta":35,"style":35},"import os\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nos.chdir('pages')\nall_links = []\nfor i in os.listdir(os.getcwd()):\n    text = open(i, \"r\")\n    html = text.read()\n    b = BeautifulSoup(html)\n    links = b.findAll('a', attrs={\"class\":\"build-link\"})\n    for a in links:\n        path = a['href']\n        all_links.append(path)\n    b.decompose()\n    text.close()\n\nos.chdir('../')\nurl_file = open('links.txt', 'a')\n\nfor x in all_links:\n    url_file.write('%s\\n' % x)\nurl_file.close()\n",[33,60758,60759,60763,60767,60771,60775,60780,60785,60790,60795,60800,60805,60810,60815,60820,60825,60829,60834,60838,60843,60848,60852,60857,60862],{"__ignoreMap":35},[187,60760,60761],{"class":189,"line":190},[187,60762,10345],{},[187,60764,60765],{"class":189,"line":249},[187,60766,53591],{},[187,60768,60769],{"class":189,"line":312},[187,60770,58687],{},[187,60772,60773],{"class":189,"line":319},[187,60774,316],{"emptyLinePlaceholder":315},[187,60776,60777],{"class":189,"line":325},[187,60778,60779],{},"os.chdir('pages')\n",[187,60781,60782],{"class":189,"line":686},[187,60783,60784],{},"all_links = []\n",[187,60786,60787],{"class":189,"line":697},[187,60788,60789],{},"for i in os.listdir(os.getcwd()):\n",[187,60791,60792],{"class":189,"line":1291},[187,60793,60794],{},"    text = open(i, \"r\")\n",[187,60796,60797],{"class":189,"line":1306},[187,60798,60799],{},"    html = text.read()\n",[187,60801,60802],{"class":189,"line":1434},[187,60803,60804],{},"    b = BeautifulSoup(html)\n",[187,60806,60807],{"class":189,"line":2599},[187,60808,60809],{},"    links = b.findAll('a', attrs={\"class\":\"build-link\"})\n",[187,60811,60812],{"class":189,"line":2607},[187,60813,60814],{},"    for a in links:\n",[187,60816,60817],{"class":189,"line":2621},[187,60818,60819],{},"        path = a['href']\n",[187,60821,60822],{"class":189,"line":2631},[187,60823,60824],{},"        all_links.append(path)\n",[187,60826,60827],{"class":189,"line":2642},[187,60828,59559],{},[187,60830,60831],{"class":189,"line":2653},[187,60832,60833],{},"    text.close()\n",[187,60835,60836],{"class":189,"line":2665},[187,60837,316],{"emptyLinePlaceholder":315},[187,60839,60840],{"class":189,"line":2674},[187,60841,60842],{},"os.chdir('../')\n",[187,60844,60845],{"class":189,"line":2684},[187,60846,60847],{},"url_file = open('links.txt', 'a')\n",[187,60849,60850],{"class":189,"line":2694},[187,60851,316],{"emptyLinePlaceholder":315},[187,60853,60854],{"class":189,"line":2706},[187,60855,60856],{},"for x in all_links:\n",[187,60858,60859],{"class":189,"line":2715},[187,60860,60861],{},"    url_file.write('%s\\n' % x)\n",[187,60863,60864],{"class":189,"line":2725},[187,60865,60866],{},"url_file.close()\n",[11,60868,60869,60870,60873,60874,60877,60878,358],{},"The first loop goes through all of the pages scraped with PhantomJS and puts each ",[33,60871,60872],{},"build-link"," in a list called ",[33,60875,60876],{},"all_links",". The second loop writes each link to a new line in ",[33,60879,60880],{},"links.txt",[26,60882,60885],{"className":60883,"code":60884,"language":31,"meta":35},[29],"/b/4x4D4D\n/b/ZhVnTW\n/b/8fNG3C\n/b/7KWZxr\n/b/Qm6hP6\n/b/X4rH99\n...\n",[33,60886,60884],{"__ignoreMap":35},[11,60888,60889],{},"The next step is one more bash script that scrapes HTML from individual builds and saves it to individual text files.",[26,60891,60893],{"className":181,"code":60892,"language":183,"meta":35,"style":35},"#!/use/bin/env bash\n\ncounter=1\nwhile read NAME; do\n    echo \"[STATUS] Scraping build number $counter: $NAME...\"\n    file=\"new_build-\"${NAME//\\/b\\//$counter-}\n    phantomjs scrape_build.js \"https://pcpartpicker.com$NAME\" > builds/$file.txt\n    result=$(python -c \"import random;print(random.uniform(2.5, 4.4))\")\n    echo \"[STATUS] Completed scraping build number $counter ($NAME). Sleeping for $result seconds...\"\n    sleep $result\n    ((counter += 1))\ndone \u003C links.txt\n",[33,60894,60895,60899,60903,60913,60928,60945,60974,60999,61016,61038,61044,61056],{"__ignoreMap":35},[187,60896,60897],{"class":189,"line":190},[187,60898,60663],{"class":295},[187,60900,60901],{"class":189,"line":249},[187,60902,316],{"emptyLinePlaceholder":315},[187,60904,60905,60908,60910],{"class":189,"line":312},[187,60906,60907],{"class":577},"counter",[187,60909,595],{"class":573},[187,60911,60912],{"class":196},"1\n",[187,60914,60915,60917,60920,60923,60926],{"class":189,"line":319},[187,60916,47482],{"class":573},[187,60918,60919],{"class":588}," read",[187,60921,60922],{"class":196}," NAME",[187,60924,60925],{"class":577},"; ",[187,60927,16566],{"class":573},[187,60929,60930,60933,60936,60939,60941,60943],{"class":189,"line":325},[187,60931,60932],{"class":588},"    echo",[187,60934,60935],{"class":196}," \"[STATUS] Scraping build number ",[187,60937,60938],{"class":577},"$counter",[187,60940,585],{"class":196},[187,60942,41374],{"class":577},[187,60944,16818],{"class":196},[187,60946,60947,60950,60952,60955,60958,60961,60964,60967,60969,60971],{"class":189,"line":686},[187,60948,60949],{"class":577},"    file",[187,60951,595],{"class":573},[187,60953,60954],{"class":196},"\"new_build-\"",[187,60956,60957],{"class":577},"${NAME",[187,60959,60960],{"class":573},"//",[187,60962,60963],{"class":588},"\\/",[187,60965,60966],{"class":577},"b",[187,60968,60963],{"class":588},[187,60970,20174],{"class":573},[187,60972,60973],{"class":577},"$counter-}\n",[187,60975,60976,60978,60981,60984,60986,60988,60991,60994,60997],{"class":189,"line":697},[187,60977,60693],{"class":193},[187,60979,60980],{"class":196}," scrape_build.js",[187,60982,60983],{"class":196}," \"https://pcpartpicker.com",[187,60985,41374],{"class":577},[187,60987,16508],{"class":196},[187,60989,60990],{"class":573}," >",[187,60992,60993],{"class":196}," builds/",[187,60995,60996],{"class":577},"$file",[187,60998,60713],{"class":196},[187,61000,61001,61003,61005,61007,61009,61011,61014],{"class":189,"line":1291},[187,61002,60718],{"class":577},[187,61004,595],{"class":573},[187,61006,16633],{"class":577},[187,61008,1125],{"class":193},[187,61010,47426],{"class":588},[187,61012,61013],{"class":196}," \"import random;print(random.uniform(2.5, 4.4))\"",[187,61015,621],{"class":577},[187,61017,61018,61020,61023,61025,61027,61029,61032,61035],{"class":189,"line":1306},[187,61019,60932],{"class":588},[187,61021,61022],{"class":196}," \"[STATUS] Completed scraping build number ",[187,61024,60938],{"class":577},[187,61026,784],{"class":196},[187,61028,41374],{"class":577},[187,61030,61031],{"class":196},"). Sleeping for ",[187,61033,61034],{"class":577},"$result",[187,61036,61037],{"class":196}," seconds...\"\n",[187,61039,61040,61042],{"class":189,"line":1434},[187,61041,60736],{"class":193},[187,61043,60739],{"class":577},[187,61045,61046,61049,61052,61054],{"class":189,"line":2599},[187,61047,61048],{"class":577},"    ((counter ",[187,61050,61051],{"class":573},"+=",[187,61053,680],{"class":588},[187,61055,683],{"class":577},[187,61057,61058,61061,61063],{"class":189,"line":2607},[187,61059,61060],{"class":573},"done",[187,61062,47730],{"class":573},[187,61064,61065],{"class":577}," links.txt\n",[11,61067,61068,61069,61071],{},"The script takes lines from ",[33,61070,60880],{}," and appends them to the base URL, scrapes the HTML from the resulting link and saves it to a text file.",[11,61073,61074,61075,61079],{},"The next step is to go through the text files for each PC build and create a pandas DataFrame containing data for each PC build (~25,000 builds total). Here's a sample PC build link: ",[15,61076,61077],{"href":61077,"rel":61078},"https://pcpartpicker.com/b/VD3bt6",[19],". This heavily commented script shows step-by-step how to scrape all of the relevant data from the HTML for PC builds:",[26,61081,61083],{"className":1383,"code":61082,"language":1125,"meta":35,"style":35},"import os\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nos.chdir('builds/builds_html')\nbuild_dict_list = []\n\nfiles = os.listdir(os.getcwd())\n\nfor i in files:\n\n    try:\n        a = open(i, \"r\")\n        b = BeautifulSoup(a)\n\n        #Labels: Each build list has labels, for example: Video Card: GeForce GTX 1080\n        labels = b.find('ul', attrs={\"class\":\"parts\"}).findAll('p', attrs={\"label\"})\n\n        #The number of labels varies between builds\n        #Some builds have multiple parts of the same type\n        cols = [label.contents[0] for label in labels]\n\n        #This will keep track of how many of each part there are\n        count = [0 for x in range(len(cols))]\n\n        #This handles multiple parts of the same type (e.g. 2 or more graphics cards)\n        for x in range(len(cols)):\n            count[x] = 1 + cols[0:x].count(cols[x])\n\n        #Rename the labels with the corresponding count appended to the end\n        new_cols = [str(x)+\"_\"+str(y) for (x,y) in zip(cols, count)]\n\n        #Values (Names of parts)\n        values = b.find('ul', attrs={\"class\":\"parts\"}).findAll('a', attrs={\"name\"})\n        vals = [value.contents[0] for value in values]\n\n        #Links\n        link_list = b.find('ul', attrs={\"class\":\"parts\"}).findAll('a', attrs={\"name\"})\n        link_list = [link['href'] for link in link_list]\n\n        #Prices: there is a an individual price for each part and a total price for the build\n        prices = b.find('ul', attrs={\"class\":\"parts\"}).findAll('div', attrs={\"price\"})\n        price_list = [price.contents[0].strip() for price in prices]\n\n        #This creates a dictionary for each component of the build\n        #Contains the component name, link and price (the price reported by the user)\n        build_part_dict = {part: {\"Link\":link,\"Price\":price,\"Name\":name} for part, link, price, name in zip(new_cols, link_list, price_list,vals)}\n\n        #Total price\n        total_price = float(b.find(\"li\",attrs={\"class\":\"partlist-total\"}).find('span', attrs={\"class\":\"price\"}).contents[0].strip(\"$\"))\n        total_price_dict = {\"total\":total_price}\n\n        #Description\n        description_paragraphs = b.find('div', attrs={'class':'description block'}).find_all('p')\n        description_clean = ''\n        for paragraph in description_paragraphs:\n            add = paragraph.text\n            description_clean += add + ' '\n        description_dict = {'Description':description_clean}\n\n        #Builder\n        build_name = b.find('h1', attrs={\"class\":\"name\"}).contents[0]\n        builder_link = b.find('p', attrs={\"class\":\"owner\"}).find('a')['href']\n        builder_dict = {\"Builder\":build_name}\n        owner_dict = {\"Owner\":builder_link}\n\n        #Details: Details include clock speed, temperatures, date built, etc.\n        #The process is similar to sraping the components, but there is no need to worry about duplicate parts\n        detail_vals = [x.strip(u\" \").strip(u'\\n').strip() for x in b.find('div', attrs={\"class\":\"part-details\"}) if \"\\n \" in x]\n        details = [x.contents[0] for x in b.find('div', attrs={\"class\":\"part-details\"}).find_all('h4')]\n        detail_dict = {detail:detail_val for detail_val, detail in zip(detail_vals, details)}\n\n        #Permalink\n        perma_link = b.find('input', attrs={\"type\":\"text\"})['value'].split(\"/b/\")[1]\n        build_dict = {\"Permalink\": perma_link, \"Builder\":builder_link}\n\n        #Join all of the above dictionaries into a master build dictionary\n        master_dict = dict(build_dict.items()+builder_dict.items()+build_part_dict.items()+detail_dict.items()+total_price_dict.items()+owner_dict.items() + description_dict.items())\n\n        #Add the dictionary to build_dict_list defined above\n        build_dict_list.append(master_dict)\n\n        #This is important for preventing memory leaks in bs4\n        b.decompose()\n        a.close()\n\n    #Some builds have been deleted, even though the links are still listed, so we pass the build if there are any issues scraping data from it\n    except:\n        pass\n\n#Create a pandas DataFrame from build_dict_list\ndf = pd.DataFrame(build_dict_list)\nos.chdir('/Users/andrewcaffey/Documents/Projects/Data/PCPP/builds/')\n#Save the DataFrame to a csv file\ndf.to_csv('builds.csv', encoding='utf-8')\n",[33,61084,61085,61089,61093,61097,61101,61106,61111,61115,61120,61124,61129,61133,61137,61142,61147,61151,61156,61161,61165,61170,61175,61180,61184,61189,61194,61198,61203,61208,61213,61217,61222,61227,61231,61236,61241,61246,61250,61255,61260,61265,61269,61274,61279,61284,61288,61293,61298,61303,61307,61312,61317,61322,61326,61331,61336,61341,61346,61351,61356,61361,61365,61370,61375,61380,61385,61390,61394,61399,61404,61409,61414,61419,61423,61428,61433,61438,61442,61447,61452,61456,61461,61466,61470,61475,61480,61485,61489,61494,61498,61502,61506,61511,61516,61521,61526],{"__ignoreMap":35},[187,61086,61087],{"class":189,"line":190},[187,61088,10345],{},[187,61090,61091],{"class":189,"line":249},[187,61092,53591],{},[187,61094,61095],{"class":189,"line":312},[187,61096,58687],{},[187,61098,61099],{"class":189,"line":319},[187,61100,316],{"emptyLinePlaceholder":315},[187,61102,61103],{"class":189,"line":325},[187,61104,61105],{},"os.chdir('builds/builds_html')\n",[187,61107,61108],{"class":189,"line":686},[187,61109,61110],{},"build_dict_list = []\n",[187,61112,61113],{"class":189,"line":697},[187,61114,316],{"emptyLinePlaceholder":315},[187,61116,61117],{"class":189,"line":1291},[187,61118,61119],{},"files = os.listdir(os.getcwd())\n",[187,61121,61122],{"class":189,"line":1306},[187,61123,316],{"emptyLinePlaceholder":315},[187,61125,61126],{"class":189,"line":1434},[187,61127,61128],{},"for i in files:\n",[187,61130,61131],{"class":189,"line":2599},[187,61132,316],{"emptyLinePlaceholder":315},[187,61134,61135],{"class":189,"line":2607},[187,61136,5405],{},[187,61138,61139],{"class":189,"line":2621},[187,61140,61141],{},"        a = open(i, \"r\")\n",[187,61143,61144],{"class":189,"line":2631},[187,61145,61146],{},"        b = BeautifulSoup(a)\n",[187,61148,61149],{"class":189,"line":2642},[187,61150,316],{"emptyLinePlaceholder":315},[187,61152,61153],{"class":189,"line":2653},[187,61154,61155],{},"        #Labels: Each build list has labels, for example: Video Card: GeForce GTX 1080\n",[187,61157,61158],{"class":189,"line":2665},[187,61159,61160],{},"        labels = b.find('ul', attrs={\"class\":\"parts\"}).findAll('p', attrs={\"label\"})\n",[187,61162,61163],{"class":189,"line":2674},[187,61164,316],{"emptyLinePlaceholder":315},[187,61166,61167],{"class":189,"line":2684},[187,61168,61169],{},"        #The number of labels varies between builds\n",[187,61171,61172],{"class":189,"line":2694},[187,61173,61174],{},"        #Some builds have multiple parts of the same type\n",[187,61176,61177],{"class":189,"line":2706},[187,61178,61179],{},"        cols = [label.contents[0] for label in labels]\n",[187,61181,61182],{"class":189,"line":2715},[187,61183,316],{"emptyLinePlaceholder":315},[187,61185,61186],{"class":189,"line":2725},[187,61187,61188],{},"        #This will keep track of how many of each part there are\n",[187,61190,61191],{"class":189,"line":2735},[187,61192,61193],{},"        count = [0 for x in range(len(cols))]\n",[187,61195,61196],{"class":189,"line":2743},[187,61197,316],{"emptyLinePlaceholder":315},[187,61199,61200],{"class":189,"line":2754},[187,61201,61202],{},"        #This handles multiple parts of the same type (e.g. 2 or more graphics cards)\n",[187,61204,61205],{"class":189,"line":2762},[187,61206,61207],{},"        for x in range(len(cols)):\n",[187,61209,61210],{"class":189,"line":2770},[187,61211,61212],{},"            count[x] = 1 + cols[0:x].count(cols[x])\n",[187,61214,61215],{"class":189,"line":2781},[187,61216,316],{"emptyLinePlaceholder":315},[187,61218,61219],{"class":189,"line":2792},[187,61220,61221],{},"        #Rename the labels with the corresponding count appended to the end\n",[187,61223,61224],{"class":189,"line":2803},[187,61225,61226],{},"        new_cols = [str(x)+\"_\"+str(y) for (x,y) in zip(cols, count)]\n",[187,61228,61229],{"class":189,"line":2808},[187,61230,316],{"emptyLinePlaceholder":315},[187,61232,61233],{"class":189,"line":2816},[187,61234,61235],{},"        #Values (Names of parts)\n",[187,61237,61238],{"class":189,"line":2824},[187,61239,61240],{},"        values = b.find('ul', attrs={\"class\":\"parts\"}).findAll('a', attrs={\"name\"})\n",[187,61242,61243],{"class":189,"line":2834},[187,61244,61245],{},"        vals = [value.contents[0] for value in values]\n",[187,61247,61248],{"class":189,"line":2845},[187,61249,316],{"emptyLinePlaceholder":315},[187,61251,61252],{"class":189,"line":2856},[187,61253,61254],{},"        #Links\n",[187,61256,61257],{"class":189,"line":2867},[187,61258,61259],{},"        link_list = b.find('ul', attrs={\"class\":\"parts\"}).findAll('a', attrs={\"name\"})\n",[187,61261,61262],{"class":189,"line":2878},[187,61263,61264],{},"        link_list = [link['href'] for link in link_list]\n",[187,61266,61267],{"class":189,"line":2886},[187,61268,316],{"emptyLinePlaceholder":315},[187,61270,61271],{"class":189,"line":2900},[187,61272,61273],{},"        #Prices: there is a an individual price for each part and a total price for the build\n",[187,61275,61276],{"class":189,"line":2905},[187,61277,61278],{},"        prices = b.find('ul', attrs={\"class\":\"parts\"}).findAll('div', attrs={\"price\"})\n",[187,61280,61281],{"class":189,"line":2913},[187,61282,61283],{},"        price_list = [price.contents[0].strip() for price in prices]\n",[187,61285,61286],{"class":189,"line":2921},[187,61287,316],{"emptyLinePlaceholder":315},[187,61289,61290],{"class":189,"line":2931},[187,61291,61292],{},"        #This creates a dictionary for each component of the build\n",[187,61294,61295],{"class":189,"line":2942},[187,61296,61297],{},"        #Contains the component name, link and price (the price reported by the user)\n",[187,61299,61300],{"class":189,"line":2953},[187,61301,61302],{},"        build_part_dict = {part: {\"Link\":link,\"Price\":price,\"Name\":name} for part, link, price, name in zip(new_cols, link_list, price_list,vals)}\n",[187,61304,61305],{"class":189,"line":2964},[187,61306,316],{"emptyLinePlaceholder":315},[187,61308,61309],{"class":189,"line":2975},[187,61310,61311],{},"        #Total price\n",[187,61313,61314],{"class":189,"line":2983},[187,61315,61316],{},"        total_price = float(b.find(\"li\",attrs={\"class\":\"partlist-total\"}).find('span', attrs={\"class\":\"price\"}).contents[0].strip(\"$\"))\n",[187,61318,61319],{"class":189,"line":2992},[187,61320,61321],{},"        total_price_dict = {\"total\":total_price}\n",[187,61323,61324],{"class":189,"line":3001},[187,61325,316],{"emptyLinePlaceholder":315},[187,61327,61328],{"class":189,"line":3010},[187,61329,61330],{},"        #Description\n",[187,61332,61333],{"class":189,"line":3019},[187,61334,61335],{},"        description_paragraphs = b.find('div', attrs={'class':'description block'}).find_all('p')\n",[187,61337,61338],{"class":189,"line":3028},[187,61339,61340],{},"        description_clean = ''\n",[187,61342,61343],{"class":189,"line":3033},[187,61344,61345],{},"        for paragraph in description_paragraphs:\n",[187,61347,61348],{"class":189,"line":3041},[187,61349,61350],{},"            add = paragraph.text\n",[187,61352,61353],{"class":189,"line":3049},[187,61354,61355],{},"            description_clean += add + ' '\n",[187,61357,61358],{"class":189,"line":3059},[187,61359,61360],{},"        description_dict = {'Description':description_clean}\n",[187,61362,61363],{"class":189,"line":3070},[187,61364,316],{"emptyLinePlaceholder":315},[187,61366,61367],{"class":189,"line":3075},[187,61368,61369],{},"        #Builder\n",[187,61371,61372],{"class":189,"line":3083},[187,61373,61374],{},"        build_name = b.find('h1', attrs={\"class\":\"name\"}).contents[0]\n",[187,61376,61377],{"class":189,"line":3091},[187,61378,61379],{},"        builder_link = b.find('p', attrs={\"class\":\"owner\"}).find('a')['href']\n",[187,61381,61382],{"class":189,"line":3101},[187,61383,61384],{},"        builder_dict = {\"Builder\":build_name}\n",[187,61386,61387],{"class":189,"line":3111},[187,61388,61389],{},"        owner_dict = {\"Owner\":builder_link}\n",[187,61391,61392],{"class":189,"line":3122},[187,61393,316],{"emptyLinePlaceholder":315},[187,61395,61396],{"class":189,"line":3132},[187,61397,61398],{},"        #Details: Details include clock speed, temperatures, date built, etc.\n",[187,61400,61401],{"class":189,"line":3143},[187,61402,61403],{},"        #The process is similar to sraping the components, but there is no need to worry about duplicate parts\n",[187,61405,61406],{"class":189,"line":3151},[187,61407,61408],{},"        detail_vals = [x.strip(u\" \").strip(u'\\n').strip() for x in b.find('div', attrs={\"class\":\"part-details\"}) if \"\\n \" in x]\n",[187,61410,61411],{"class":189,"line":3161},[187,61412,61413],{},"        details = [x.contents[0] for x in b.find('div', attrs={\"class\":\"part-details\"}).find_all('h4')]\n",[187,61415,61416],{"class":189,"line":3170},[187,61417,61418],{},"        detail_dict = {detail:detail_val for detail_val, detail in zip(detail_vals, details)}\n",[187,61420,61421],{"class":189,"line":3178},[187,61422,316],{"emptyLinePlaceholder":315},[187,61424,61425],{"class":189,"line":3185},[187,61426,61427],{},"        #Permalink\n",[187,61429,61430],{"class":189,"line":3195},[187,61431,61432],{},"        perma_link = b.find('input', attrs={\"type\":\"text\"})['value'].split(\"/b/\")[1]\n",[187,61434,61435],{"class":189,"line":3205},[187,61436,61437],{},"        build_dict = {\"Permalink\": perma_link, \"Builder\":builder_link}\n",[187,61439,61440],{"class":189,"line":3210},[187,61441,316],{"emptyLinePlaceholder":315},[187,61443,61444],{"class":189,"line":3216},[187,61445,61446],{},"        #Join all of the above dictionaries into a master build dictionary\n",[187,61448,61449],{"class":189,"line":3224},[187,61450,61451],{},"        master_dict = dict(build_dict.items()+builder_dict.items()+build_part_dict.items()+detail_dict.items()+total_price_dict.items()+owner_dict.items() + description_dict.items())\n",[187,61453,61454],{"class":189,"line":3234},[187,61455,316],{"emptyLinePlaceholder":315},[187,61457,61458],{"class":189,"line":3242},[187,61459,61460],{},"        #Add the dictionary to build_dict_list defined above\n",[187,61462,61463],{"class":189,"line":3252},[187,61464,61465],{},"        build_dict_list.append(master_dict)\n",[187,61467,61468],{"class":189,"line":3260},[187,61469,316],{"emptyLinePlaceholder":315},[187,61471,61472],{"class":189,"line":3270},[187,61473,61474],{},"        #This is important for preventing memory leaks in bs4\n",[187,61476,61477],{"class":189,"line":3275},[187,61478,61479],{},"        b.decompose()\n",[187,61481,61482],{"class":189,"line":3283},[187,61483,61484],{},"        a.close()\n",[187,61486,61487],{"class":189,"line":3291},[187,61488,316],{"emptyLinePlaceholder":315},[187,61490,61491],{"class":189,"line":3300},[187,61492,61493],{},"    #Some builds have been deleted, even though the links are still listed, so we pass the build if there are any issues scraping data from it\n",[187,61495,61496],{"class":189,"line":3310},[187,61497,41962],{},[187,61499,61500],{"class":189,"line":3320},[187,61501,41967],{},[187,61503,61504],{"class":189,"line":3325},[187,61505,316],{"emptyLinePlaceholder":315},[187,61507,61508],{"class":189,"line":3333},[187,61509,61510],{},"#Create a pandas DataFrame from build_dict_list\n",[187,61512,61513],{"class":189,"line":3343},[187,61514,61515],{},"df = pd.DataFrame(build_dict_list)\n",[187,61517,61518],{"class":189,"line":3354},[187,61519,61520],{},"os.chdir('/Users/andrewcaffey/Documents/Projects/Data/PCPP/builds/')\n",[187,61522,61523],{"class":189,"line":17135},[187,61524,61525],{},"#Save the DataFrame to a csv file\n",[187,61527,61528],{"class":189,"line":17141},[187,61529,61530],{},"df.to_csv('builds.csv', encoding='utf-8')\n",[11,61532,61533],{},"At this point we can do a quick visualization of the distribution of PC prices:",[26,61535,61537],{"className":1383,"code":61536,"language":1125,"meta":35,"style":35},"df.total[(df.total>0)&(df.total\u003C7000)].hist(bins=100, figsize=(20,10))\n",[33,61538,61539],{"__ignoreMap":35},[187,61540,61541],{"class":189,"line":190},[187,61542,61536],{},[11,61544,61545],{},[511,61546],{"alt":7255,"src":61547},"/static/pcpp/hist.png",[11,61549,61550,61551,61554,61555,61558],{},"The mean PC price is ",[338,61552,61553],{},"$1,292",". However, the price of PCs is not reflected accurately in ",[33,61556,61557],{},"df.total",". I noticed that some builds include multiple monitors while others don't include any and some builders don't include prices for components from their previous PC builds.",[11,61560,61561],{},"The data frame contains 141 columns for parts. Here they are:",[107,61563,61564],{},[11,61565,61566],{},"All-In-One Monitor/Chassis_1 CPU Cooler_1 CPU Cooler_2 CPU Cooler_3 CPU_1 CPU_2 Case Accessory_1 Case Accessory_2 Case Fan_1 Case Fan_10 Case Fan_11 Case Fan_12 Case Fan_13 Case Fan_14 Case Fan_15 Case Fan_16 Case Fan_17 Case Fan_18 Case Fan_19 Case Fan_2 Case Fan_20 Case Fan_21 Case Fan_22 Case Fan_23 Case Fan_24 Case Fan_25 Case Fan_26 Case Fan_27 Case Fan_28 Case Fan_29 Case Fan_3 Case Fan_30 Case Fan_31 Case Fan_32 Case Fan_4 Case Fan_5 Case Fan_6 Case Fan_7 Case Fan_8 Case Fan_9 Case_1 Coolant_1 External Storage_1 External Storage_2 External Storage_3 External Storage_4 Fan Controller_1 Fan Controller_2 Food_1 Food_2 Food_3 Headphones_1 Headphones_2 Headphones_3 Headphones_4 Keyboard_1 Keyboard_2 Keyboard_3 Keyboard_4 Keyboard_5 Keyboard_6 Keyboard_7 Laptop_1 Memory_1 Memory_2 Memory_3 Memory_4 Memory_5 Memory_6 Memory_7 Memory_8 Monitor_1 Monitor_2 Monitor_3 Monitor_4 Monitor_5 Monitor_6 Motherboard_1 Mouse_1 Mouse_2 Mouse_3 Mouse_4 Mouse_5 Operating System_1 Optical Drive_1 Optical Drive_2 Optical Drive_3 Optical Drive_4 Power Supply_1 Radiator_1 Reservoir_1 Software_1 Software_2 Software_3 Software_4 Software_5 Software_6 Sound Card_1 Sound Card_2 Speakers_1 Speakers_2 Storage_1 Storage_10 Storage_11 Storage_12 Storage_13 Storage_14 Storage_15 Storage_16 Storage_17 Storage_18 Storage_19 Storage_2 Storage_20 Storage_21 Storage_22 Storage_23 Storage_24 Storage_25 Storage_3 Storage_4 Storage_5 Storage_6 Storage_7 Storage_8 Storage_9 Thermal Compound_1 Thermal Compound_2 Thermal Compound_3 Thermal Compound_4 UPS_1 Video Card Cooler_1 Video Card Cooler_2 Video Card_1 Video Card_2 Video Card_3 Video Card_4 Wired Network Adapter_1 Wired Network Adapter_2 Wireless Network Adapter_1 Wireless Network Adapter_2",[11,61568,61569],{},"That's right, somebody included mulitple food items in their PC build! One build listed 29 hard drive disks and another listed 32 case fans. So, it will make more sense to look at individual PC builds by their core components:",[916,61571,61572,61575,61578,61581,61584,61587,61590,61593],{},[919,61573,61574],{},"case",[919,61576,61577],{},"CPU",[919,61579,61580],{},"GPU (multiple)",[919,61582,61583],{},"motherboard",[919,61585,61586],{},"memory (multiple)",[919,61588,61589],{},"storage (multiple)",[919,61591,61592],{},"CPU cooler",[919,61594,61595],{},"PSU (power supply unit)",[11,61597,61598],{},"Most of the PC builds have at least one of these core components.",[168,61600,61602],{"id":61601},"part-data","Part Data",[11,61604,61605],{},"Collecting data for individual parts followed the same process as collecting data for completed builds. Here are the counts of parts I collected by type:",[1525,61607,61608,61665],{},[1528,61609,61610],{},[1531,61611,61612,61616,61618,61620,61622,61625,61627,61630,61632,61635,61637,61640,61642,61645,61647,61650,61652,61655,61657,61660,61662],{},[1534,61613,61615],{"align":61614},"center","Case",[1534,61617,677],{},[1534,61619,61577],{"align":61614},[1534,61621,677],{},[1534,61623,61624],{"align":61614},"CPU Coooler",[1534,61626,677],{},[1534,61628,61629],{"align":61614},"Case Fan",[1534,61631,677],{},[1534,61633,61634],{"align":61614},"GPU",[1534,61636,677],{},[1534,61638,61639],{"align":61614},"Hard Drive",[1534,61641,677],{},[1534,61643,61644],{"align":61614},"Memory",[1534,61646,677],{},[1534,61648,61649],{"align":61614},"Monitor",[1534,61651,677],{},[1534,61653,61654],{"align":61614},"Motherboard",[1534,61656,677],{},[1534,61658,61659],{"align":61614},"PSU",[1534,61661,677],{},[1534,61663,61664],{"align":61614},"UPS",[1544,61666,61667],{},[1531,61668,61669,61672,61674,61677,61679,61682,61684,61687,61689,61692,61694,61697,61699,61702,61704,61707,61709,61712,61714,61717,61719],{},[1549,61670,61671],{"align":61614},"2774",[1549,61673],{},[1549,61675,61676],{"align":61614},"886",[1549,61678],{},[1549,61680,61681],{"align":61614},"730",[1549,61683],{},[1549,61685,61686],{"align":61614},"1192",[1549,61688],{},[1549,61690,61691],{"align":61614},"2996",[1549,61693],{},[1549,61695,61696],{"align":61614},"1736",[1549,61698],{},[1549,61700,61701],{"align":61614},"1700",[1549,61703],{},[1549,61705,61706],{"align":61614},"600",[1549,61708],{},[1549,61710,61711],{"align":61614},"2400",[1549,61713],{},[1549,61715,61716],{"align":61614},"1434",[1549,61718],{},[1549,61720,55321],{"align":61614},[11,61722,61723],{},"Here's a quick look at the features I am interested in for each part:",[916,61725,61726,61734,61742,61750,61758,61766,61774,61782,61790,61798,61806],{},[919,61727,61728,61729],{},"Case\n",[916,61730,61731],{},[919,61732,61733],{},"Color, Manufacturer, Name, Dimensions, Volume, Average Price, Type",[919,61735,61736,61737],{},"CPU\n",[916,61738,61739],{},[919,61740,61741],{},"Name, Manufacturer, Lithography, TDP, Operating Frequency, Boost Frequency, Core Count, Hyperthreading, Maximum Supported Memory, Average Price",[919,61743,61744,61745],{},"CPU Cooler\n",[916,61746,61747],{},[919,61748,61749],{},"Manufacturer, Maximum Noise Level, Maximum RPM, Liquid Cooled, Radiator Size, Bearing Type, Height",[919,61751,61752,61753],{},"Case Fan\n",[916,61754,61755],{},[919,61756,61757],{},"RPM",[919,61759,61760,61761],{},"GPU\n",[916,61762,61763],{},[919,61764,61765],{},"Memory (GB), NVIDIA/AMD, Clock Speed (MHz), Boost Clock Speed (MHz), Chipset, Manufacturer, TDP, Model",[919,61767,61768,61769],{},"Hard Drive\n",[916,61770,61771],{},[919,61772,61773],{},"Storage (GB), RPM, SSD/Spinning, Price/GB, Form Factor, Manufacturer",[919,61775,61776,61777],{},"Memory\n",[916,61778,61779],{},[919,61780,61781],{},"Manufacturer', CAS, Price/GB, DDR3/DDR4, Speed, DIMM, Size (GB), Module Count, Module Size, Voltage",[919,61783,61784,61785],{},"Monitor\n",[916,61786,61787],{},[919,61788,61789],{},"Refresh rate, Response Time (ms), Screen Size, Viewing Angle, Aspect Ratio, Brightness, Display Colors, Manufacturer, LED, Recommended Resolution, Wide Screen, Curved Screen",[919,61791,61792,61793],{},"Motherboard\n",[916,61794,61795],{},[919,61796,61797],{},"Socket, Maximum Supported Memory, Memory Slots, Chipset",[919,61799,61800,61801],{},"Power Supply Unit (PSU)\n",[916,61802,61803],{},[919,61804,61805],{},"Modular, Power, Price/Watt, Manufacturer, Efficiency Certification",[919,61807,61808,61809],{},"UPS\n",[916,61810,61811],{},[919,61812,61813],{},"Charge time",[11,61815,61816,61817,61820,61821,61823],{},"There's a lot of data for each part, some parts are missing a lot of price data. Each part has a list of vendors with prices that are in the same neighborhood. To impute missing prices on the ",[33,61818,61819],{},"builds"," DataFrame, I will be imputing the average price. For parts missing pricing data, where it makes sense, I'll be using a few different methods to predict the average price (linear regression, decision tree, random forest) and then use those predicted values to fill missing on the ",[33,61822,61819],{}," DataFrame.",[11,61825,61826,61827,61831],{},"Here's a link to one of the pages that I scraped with this script: ",[15,61828,61829],{"href":61829,"rel":61830},"https://pcpartpicker.com/product/MYH48d/corsair-memory-cmk16gx4m2b3000c15",[19],". I was able to use this script for each type of part thanks to the consitent stucture and DOM naming scheme.",[26,61833,61835],{"className":1383,"code":61834,"language":1125,"meta":35,"style":35},"import os\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n#navigate to the directory containing HTML for PSUs\nos.chdir(\"parts/PSU/parts/\")\npart_list = []\ncomments = []\nfor i in os.listdir(os.getcwd()):\n    a = open(i, 'r')\n    #print a.read()\n    b = BeautifulSoup(a)\n\n    average_rating = b.find('span', attrs={'itemprop': 'ratingValue'})\n    ratings_count = b.find('span', attrs={'itemprop':'ratingCount'})\n    if (average_rating != None) & (ratings_count != None):\n        ratings_dict = {'average_rating':average_rating.text, 'ratings_count':ratings_count.text}\n\n    #part name, kind and link\n    if b.find('h4', attrs={'class':'kind'}) != None:\n        kind = b.find('h4', attrs={'class':'kind'}).text\n        part_name = b.find('h1', attrs={'class':'name'}).text\n        link = b.find('input', attrs={'name':'url'})['value']\n        info_dict = {'Kind':kind, 'Name':part_name, 'Link': link}\n\n        #prices\n        if b.find_all('td', attrs={'class':'base'}) != None:\n            price_list = b.find_all('td', attrs={'class':'base'})\n            price_list = [float(x.text.strip('$')) for x in price_list]\n            #average_price = sum(price_list)/len(price_list)\n            price_dict = {'Prices':price_list,}\n\n        #specs\n        spec_labels = b.find('div', attrs={'class':'specs block'}).find_all('h4')\n        spec_labels = [x.contents[0] for x in spec_labels]\n        spec_values = str(b.find('div', attrs={'class':'specs block'}))\n\n        #this part was a little tricky since the values were placed outside of HTML tags\n        #thankfully I managed to figure out a pattern that would extract everything neatly\n        vals = [x.strip().split('\u003C/h4>')[1].strip('\\n').strip() for x in spec_values.split(\"\u003Ch4>\")[1:]]\n        vals[-1] = vals[-1].split('\\n')[0]\n        spec_values = vals\n\n        spec_values = spec_values[0:len(spec_labels)+1]\n        spec_dict = {spec_label:spec_value for spec_label, spec_value in zip(spec_labels,spec_values)}\n\n        part_dict = dict(spec_dict.items() + info_dict.items() + price_dict.items() + ratings_dict.items())\n        part_list.append(part_dict)\n\n    #ratings\n    if (average_rating != None) & (ratings_count != None):\n        ratings_dict = {'average_rating':average_rating, 'ratings_count':ratings_count}\n    reviews = b.find('div', attrs={'class':'part-reviews'})\n    if reviews != None:\n        reviews = reviews.find_all('div',attrs={'class':'part-review-block'})\n        star_list = [len(reviews[x].find('ul',attrs={'class':'stars'}).find_all('li',attrs={'class':'full-star'})) for x in range(len(reviews))]\n\n        comment_text_list = b.find_all('div', attrs={'class':'comment-message markdown'})\n        comment_text_list = [comment_text_list[x].find_all('p') for x in range(len(comment_text_list))]\n\n        comment_text_list_clean = []\n        for i, x in enumerate(comment_text_list):\n            comment = \"\"\n            for y in x:\n                try:\n                    comment += y.contents[0] + \" \"\n                except:\n                    pass\n            comment_text_list_clean.append(comment)\n\n        review = zip(star_list, comment_text_list_clean)\n        comments.append(review)\n\n    a.close()\n    b.decompose()\n\ndf = pd.DataFrame(part_list)\n",[33,61836,61837,61841,61845,61849,61853,61858,61863,61868,61873,61877,61882,61887,61892,61896,61901,61906,61911,61916,61920,61925,61930,61935,61940,61945,61950,61954,61959,61964,61969,61974,61979,61984,61988,61993,61998,62003,62008,62012,62017,62022,62027,62032,62037,62041,62046,62051,62055,62060,62065,62069,62074,62078,62083,62088,62093,62098,62103,62107,62112,62117,62121,62126,62131,62136,62141,62146,62151,62156,62161,62166,62170,62175,62180,62184,62189,62193,62197],{"__ignoreMap":35},[187,61838,61839],{"class":189,"line":190},[187,61840,10345],{},[187,61842,61843],{"class":189,"line":249},[187,61844,53591],{},[187,61846,61847],{"class":189,"line":312},[187,61848,58687],{},[187,61850,61851],{"class":189,"line":319},[187,61852,316],{"emptyLinePlaceholder":315},[187,61854,61855],{"class":189,"line":325},[187,61856,61857],{},"#navigate to the directory containing HTML for PSUs\n",[187,61859,61860],{"class":189,"line":686},[187,61861,61862],{},"os.chdir(\"parts/PSU/parts/\")\n",[187,61864,61865],{"class":189,"line":697},[187,61866,61867],{},"part_list = []\n",[187,61869,61870],{"class":189,"line":1291},[187,61871,61872],{},"comments = []\n",[187,61874,61875],{"class":189,"line":1306},[187,61876,60789],{},[187,61878,61879],{"class":189,"line":1434},[187,61880,61881],{},"    a = open(i, 'r')\n",[187,61883,61884],{"class":189,"line":2599},[187,61885,61886],{},"    #print a.read()\n",[187,61888,61889],{"class":189,"line":2607},[187,61890,61891],{},"    b = BeautifulSoup(a)\n",[187,61893,61894],{"class":189,"line":2621},[187,61895,316],{"emptyLinePlaceholder":315},[187,61897,61898],{"class":189,"line":2631},[187,61899,61900],{},"    average_rating = b.find('span', attrs={'itemprop': 'ratingValue'})\n",[187,61902,61903],{"class":189,"line":2642},[187,61904,61905],{},"    ratings_count = b.find('span', attrs={'itemprop':'ratingCount'})\n",[187,61907,61908],{"class":189,"line":2653},[187,61909,61910],{},"    if (average_rating != None) & (ratings_count != None):\n",[187,61912,61913],{"class":189,"line":2665},[187,61914,61915],{},"        ratings_dict = {'average_rating':average_rating.text, 'ratings_count':ratings_count.text}\n",[187,61917,61918],{"class":189,"line":2674},[187,61919,316],{"emptyLinePlaceholder":315},[187,61921,61922],{"class":189,"line":2684},[187,61923,61924],{},"    #part name, kind and link\n",[187,61926,61927],{"class":189,"line":2694},[187,61928,61929],{},"    if b.find('h4', attrs={'class':'kind'}) != None:\n",[187,61931,61932],{"class":189,"line":2706},[187,61933,61934],{},"        kind = b.find('h4', attrs={'class':'kind'}).text\n",[187,61936,61937],{"class":189,"line":2715},[187,61938,61939],{},"        part_name = b.find('h1', attrs={'class':'name'}).text\n",[187,61941,61942],{"class":189,"line":2725},[187,61943,61944],{},"        link = b.find('input', attrs={'name':'url'})['value']\n",[187,61946,61947],{"class":189,"line":2735},[187,61948,61949],{},"        info_dict = {'Kind':kind, 'Name':part_name, 'Link': link}\n",[187,61951,61952],{"class":189,"line":2743},[187,61953,316],{"emptyLinePlaceholder":315},[187,61955,61956],{"class":189,"line":2754},[187,61957,61958],{},"        #prices\n",[187,61960,61961],{"class":189,"line":2762},[187,61962,61963],{},"        if b.find_all('td', attrs={'class':'base'}) != None:\n",[187,61965,61966],{"class":189,"line":2770},[187,61967,61968],{},"            price_list = b.find_all('td', attrs={'class':'base'})\n",[187,61970,61971],{"class":189,"line":2781},[187,61972,61973],{},"            price_list = [float(x.text.strip('$')) for x in price_list]\n",[187,61975,61976],{"class":189,"line":2792},[187,61977,61978],{},"            #average_price = sum(price_list)/len(price_list)\n",[187,61980,61981],{"class":189,"line":2803},[187,61982,61983],{},"            price_dict = {'Prices':price_list,}\n",[187,61985,61986],{"class":189,"line":2808},[187,61987,316],{"emptyLinePlaceholder":315},[187,61989,61990],{"class":189,"line":2816},[187,61991,61992],{},"        #specs\n",[187,61994,61995],{"class":189,"line":2824},[187,61996,61997],{},"        spec_labels = b.find('div', attrs={'class':'specs block'}).find_all('h4')\n",[187,61999,62000],{"class":189,"line":2834},[187,62001,62002],{},"        spec_labels = [x.contents[0] for x in spec_labels]\n",[187,62004,62005],{"class":189,"line":2845},[187,62006,62007],{},"        spec_values = str(b.find('div', attrs={'class':'specs block'}))\n",[187,62009,62010],{"class":189,"line":2856},[187,62011,316],{"emptyLinePlaceholder":315},[187,62013,62014],{"class":189,"line":2867},[187,62015,62016],{},"        #this part was a little tricky since the values were placed outside of HTML tags\n",[187,62018,62019],{"class":189,"line":2878},[187,62020,62021],{},"        #thankfully I managed to figure out a pattern that would extract everything neatly\n",[187,62023,62024],{"class":189,"line":2886},[187,62025,62026],{},"        vals = [x.strip().split('\u003C/h4>')[1].strip('\\n').strip() for x in spec_values.split(\"\u003Ch4>\")[1:]]\n",[187,62028,62029],{"class":189,"line":2900},[187,62030,62031],{},"        vals[-1] = vals[-1].split('\\n')[0]\n",[187,62033,62034],{"class":189,"line":2905},[187,62035,62036],{},"        spec_values = vals\n",[187,62038,62039],{"class":189,"line":2913},[187,62040,316],{"emptyLinePlaceholder":315},[187,62042,62043],{"class":189,"line":2921},[187,62044,62045],{},"        spec_values = spec_values[0:len(spec_labels)+1]\n",[187,62047,62048],{"class":189,"line":2931},[187,62049,62050],{},"        spec_dict = {spec_label:spec_value for spec_label, spec_value in zip(spec_labels,spec_values)}\n",[187,62052,62053],{"class":189,"line":2942},[187,62054,316],{"emptyLinePlaceholder":315},[187,62056,62057],{"class":189,"line":2953},[187,62058,62059],{},"        part_dict = dict(spec_dict.items() + info_dict.items() + price_dict.items() + ratings_dict.items())\n",[187,62061,62062],{"class":189,"line":2964},[187,62063,62064],{},"        part_list.append(part_dict)\n",[187,62066,62067],{"class":189,"line":2975},[187,62068,316],{"emptyLinePlaceholder":315},[187,62070,62071],{"class":189,"line":2983},[187,62072,62073],{},"    #ratings\n",[187,62075,62076],{"class":189,"line":2992},[187,62077,61910],{},[187,62079,62080],{"class":189,"line":3001},[187,62081,62082],{},"        ratings_dict = {'average_rating':average_rating, 'ratings_count':ratings_count}\n",[187,62084,62085],{"class":189,"line":3010},[187,62086,62087],{},"    reviews = b.find('div', attrs={'class':'part-reviews'})\n",[187,62089,62090],{"class":189,"line":3019},[187,62091,62092],{},"    if reviews != None:\n",[187,62094,62095],{"class":189,"line":3028},[187,62096,62097],{},"        reviews = reviews.find_all('div',attrs={'class':'part-review-block'})\n",[187,62099,62100],{"class":189,"line":3033},[187,62101,62102],{},"        star_list = [len(reviews[x].find('ul',attrs={'class':'stars'}).find_all('li',attrs={'class':'full-star'})) for x in range(len(reviews))]\n",[187,62104,62105],{"class":189,"line":3041},[187,62106,316],{"emptyLinePlaceholder":315},[187,62108,62109],{"class":189,"line":3049},[187,62110,62111],{},"        comment_text_list = b.find_all('div', attrs={'class':'comment-message markdown'})\n",[187,62113,62114],{"class":189,"line":3059},[187,62115,62116],{},"        comment_text_list = [comment_text_list[x].find_all('p') for x in range(len(comment_text_list))]\n",[187,62118,62119],{"class":189,"line":3070},[187,62120,316],{"emptyLinePlaceholder":315},[187,62122,62123],{"class":189,"line":3075},[187,62124,62125],{},"        comment_text_list_clean = []\n",[187,62127,62128],{"class":189,"line":3083},[187,62129,62130],{},"        for i, x in enumerate(comment_text_list):\n",[187,62132,62133],{"class":189,"line":3091},[187,62134,62135],{},"            comment = \"\"\n",[187,62137,62138],{"class":189,"line":3101},[187,62139,62140],{},"            for y in x:\n",[187,62142,62143],{"class":189,"line":3111},[187,62144,62145],{},"                try:\n",[187,62147,62148],{"class":189,"line":3122},[187,62149,62150],{},"                    comment += y.contents[0] + \" \"\n",[187,62152,62153],{"class":189,"line":3132},[187,62154,62155],{},"                except:\n",[187,62157,62158],{"class":189,"line":3143},[187,62159,62160],{},"                    pass\n",[187,62162,62163],{"class":189,"line":3151},[187,62164,62165],{},"            comment_text_list_clean.append(comment)\n",[187,62167,62168],{"class":189,"line":3161},[187,62169,316],{"emptyLinePlaceholder":315},[187,62171,62172],{"class":189,"line":3170},[187,62173,62174],{},"        review = zip(star_list, comment_text_list_clean)\n",[187,62176,62177],{"class":189,"line":3178},[187,62178,62179],{},"        comments.append(review)\n",[187,62181,62182],{"class":189,"line":3185},[187,62183,316],{"emptyLinePlaceholder":315},[187,62185,62186],{"class":189,"line":3195},[187,62187,62188],{},"    a.close()\n",[187,62190,62191],{"class":189,"line":3205},[187,62192,59559],{},[187,62194,62195],{"class":189,"line":3210},[187,62196,316],{"emptyLinePlaceholder":315},[187,62198,62199],{"class":189,"line":3216},[187,62200,62201],{},"df = pd.DataFrame(part_list)\n",[168,62203,62205],{"id":62204},"power-supply-unit-psu","Power Supply Unit (PSU)",[11,62207,62208],{},"Each type of part required a bit of formatting, especially things measured in MHz/GHz and MB/GB, and specs with lots of missing values. Here's a little bit of the cleaning I did for power supply units (PSUs).",[26,62210,62212],{"className":1383,"code":62211,"language":1125,"meta":35,"style":35},"#strip the word Watts from the Wattage specs\ndf['power'] = [float(x.strip('Watts')) if x != '' else 0 for x in df.Wattage]\n#price per watt\ndf['ppw'] = [x/y for x,y in zip(df.avg, df.power)]\n#dictionary to translate efficiency ratings from text to integers\neff_rank_mapping = {u'80+ Bronze':2, u'80+ Gold':4, u'80+':1, u'80+ Titanium':6, u'80+ Platinum':5, u'-':0, u'80+ Silver':3}\n#map efficiency ratings to integers\ndf['eff_rank'] = df['Efficiency Certification'].map(eff_rank_mapping)\n",[33,62213,62214,62219,62224,62229,62234,62239,62244,62249],{"__ignoreMap":35},[187,62215,62216],{"class":189,"line":190},[187,62217,62218],{},"#strip the word Watts from the Wattage specs\n",[187,62220,62221],{"class":189,"line":249},[187,62222,62223],{},"df['power'] = [float(x.strip('Watts')) if x != '' else 0 for x in df.Wattage]\n",[187,62225,62226],{"class":189,"line":312},[187,62227,62228],{},"#price per watt\n",[187,62230,62231],{"class":189,"line":319},[187,62232,62233],{},"df['ppw'] = [x/y for x,y in zip(df.avg, df.power)]\n",[187,62235,62236],{"class":189,"line":325},[187,62237,62238],{},"#dictionary to translate efficiency ratings from text to integers\n",[187,62240,62241],{"class":189,"line":686},[187,62242,62243],{},"eff_rank_mapping = {u'80+ Bronze':2, u'80+ Gold':4, u'80+':1, u'80+ Titanium':6, u'80+ Platinum':5, u'-':0, u'80+ Silver':3}\n",[187,62245,62246],{"class":189,"line":697},[187,62247,62248],{},"#map efficiency ratings to integers\n",[187,62250,62251],{"class":189,"line":1291},[187,62252,62253],{},"df['eff_rank'] = df['Efficiency Certification'].map(eff_rank_mapping)\n",[11,62255,62256],{},"Let's take a look at some of the data we have so far:",[26,62258,62260],{"className":1383,"code":62259,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.axis([0,1800,0,500])\nplt.title('PSU Wattages vs. PSU Prices', fontsize=16)\nplt.xlabel('Wattage', fontsize=14)\nplt.ylabel('Price', fontsize=14)\n#colors = ['black', 'bronze', 'silver', 'gold', 'green', 'red']\ncolors = ['#000000', '#cd7f32', '#CCCCCC', '#ffd700', '#00ff00', '#ff0000']\n\nef1 = plt.scatter(df[df.eff_rank==1].power, df[df.eff_rank==1].avg, color = colors[0], s=50)\nef2 = plt.scatter(df[df.eff_rank==2].power, df[df.eff_rank==2].avg, color = colors[1], s=50)\nef3 = plt.scatter(df[df.eff_rank==3].power, df[df.eff_rank==3].avg, color = colors[2], s=50)\nef4 = plt.scatter(df[df.eff_rank==4].power, df[df.eff_rank==4].avg, color = colors[3], s=50)\nef5 = plt.scatter(df[df.eff_rank==5].power, df[df.eff_rank==5].avg, color = colors[4], s=50)\nef6 = plt.scatter(df[df.eff_rank==6].power, df[df.eff_rank==6].avg, color = colors[5], s=50)\n\nplt.legend((ef1, ef2, ef3, ef4, ef5, ef6),\n           ('80+', '80+ Bronze', '80+ Silver', '80+ Gold', '80+ Platinum', '80+ Titanium'),\n           title = 'Efficiency Rating',\n           scatterpoints=3,\n           loc='upper left',\n           ncol=2,\n           fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/watts_vs_price.png'))\n",[33,62261,62262,62267,62272,62277,62282,62287,62292,62297,62301,62306,62311,62316,62321,62326,62331,62335,62340,62345,62350,62355,62360,62365,62370,62375,62380],{"__ignoreMap":35},[187,62263,62264],{"class":189,"line":190},[187,62265,62266],{},"plt.figure(figsize=(12,8))\n",[187,62268,62269],{"class":189,"line":249},[187,62270,62271],{},"plt.axis([0,1800,0,500])\n",[187,62273,62274],{"class":189,"line":312},[187,62275,62276],{},"plt.title('PSU Wattages vs. PSU Prices', fontsize=16)\n",[187,62278,62279],{"class":189,"line":319},[187,62280,62281],{},"plt.xlabel('Wattage', fontsize=14)\n",[187,62283,62284],{"class":189,"line":325},[187,62285,62286],{},"plt.ylabel('Price', fontsize=14)\n",[187,62288,62289],{"class":189,"line":686},[187,62290,62291],{},"#colors = ['black', 'bronze', 'silver', 'gold', 'green', 'red']\n",[187,62293,62294],{"class":189,"line":697},[187,62295,62296],{},"colors = ['#000000', '#cd7f32', '#CCCCCC', '#ffd700', '#00ff00', '#ff0000']\n",[187,62298,62299],{"class":189,"line":1291},[187,62300,316],{"emptyLinePlaceholder":315},[187,62302,62303],{"class":189,"line":1306},[187,62304,62305],{},"ef1 = plt.scatter(df[df.eff_rank==1].power, df[df.eff_rank==1].avg, color = colors[0], s=50)\n",[187,62307,62308],{"class":189,"line":1434},[187,62309,62310],{},"ef2 = plt.scatter(df[df.eff_rank==2].power, df[df.eff_rank==2].avg, color = colors[1], s=50)\n",[187,62312,62313],{"class":189,"line":2599},[187,62314,62315],{},"ef3 = plt.scatter(df[df.eff_rank==3].power, df[df.eff_rank==3].avg, color = colors[2], s=50)\n",[187,62317,62318],{"class":189,"line":2607},[187,62319,62320],{},"ef4 = plt.scatter(df[df.eff_rank==4].power, df[df.eff_rank==4].avg, color = colors[3], s=50)\n",[187,62322,62323],{"class":189,"line":2621},[187,62324,62325],{},"ef5 = plt.scatter(df[df.eff_rank==5].power, df[df.eff_rank==5].avg, color = colors[4], s=50)\n",[187,62327,62328],{"class":189,"line":2631},[187,62329,62330],{},"ef6 = plt.scatter(df[df.eff_rank==6].power, df[df.eff_rank==6].avg, color = colors[5], s=50)\n",[187,62332,62333],{"class":189,"line":2642},[187,62334,316],{"emptyLinePlaceholder":315},[187,62336,62337],{"class":189,"line":2653},[187,62338,62339],{},"plt.legend((ef1, ef2, ef3, ef4, ef5, ef6),\n",[187,62341,62342],{"class":189,"line":2665},[187,62343,62344],{},"           ('80+', '80+ Bronze', '80+ Silver', '80+ Gold', '80+ Platinum', '80+ Titanium'),\n",[187,62346,62347],{"class":189,"line":2674},[187,62348,62349],{},"           title = 'Efficiency Rating',\n",[187,62351,62352],{"class":189,"line":2684},[187,62353,62354],{},"           scatterpoints=3,\n",[187,62356,62357],{"class":189,"line":2694},[187,62358,62359],{},"           loc='upper left',\n",[187,62361,62362],{"class":189,"line":2706},[187,62363,62364],{},"           ncol=2,\n",[187,62366,62367],{"class":189,"line":2715},[187,62368,62369],{},"           fontsize=14)\n",[187,62371,62372],{"class":189,"line":2725},[187,62373,62374],{},"plt.xticks(fontsize=14)\n",[187,62376,62377],{"class":189,"line":2735},[187,62378,62379],{},"plt.yticks(fontsize=14)\n",[187,62381,62382],{"class":189,"line":2743},[187,62383,62384],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/watts_vs_price.png'))\n",[11,62386,62387],{},[511,62388],{"alt":7255,"src":62389},"/static/pcpp/psu/watts_vs_price.png",[11,62391,62392],{},"The data points along the x-axis are PSUs with no price data. There's a pretty nice correlation between watts and price, and Efficiency Rating should help make price predictions even more accurate than using wattage alone. A quick and easy way to determine how influential each feature of our data is on the target variable (price) is to train the data on machine learning algorithm called a random forest.",[11,62394,62395],{},"Before we run the random forest, categorical variable values must be replaced with integer values and we also need to remove PSUs with missing values. Here's a quick way to do that:",[26,62397,62399],{"className":1383,"code":62398,"language":1125,"meta":35,"style":35},"#all of the columns we will be working with\ncols = [u'power', u'eff_rank', u'ppw', u'Manufacturer', u'Modular', u'Type', u'avg']\n#feature columns (doesn't include average price or price per watt)\nfeature_cols = [u'power', u'eff_rank', u'Manufacturer', u'Modular', u'Type']\n#drop null values and seperate X and y\nX = df[cols].dropna()[feature_cols]\ny = df[cols].dropna().avg\n",[33,62400,62401,62406,62411,62416,62421,62426,62431],{"__ignoreMap":35},[187,62402,62403],{"class":189,"line":190},[187,62404,62405],{},"#all of the columns we will be working with\n",[187,62407,62408],{"class":189,"line":249},[187,62409,62410],{},"cols = [u'power', u'eff_rank', u'ppw', u'Manufacturer', u'Modular', u'Type', u'avg']\n",[187,62412,62413],{"class":189,"line":312},[187,62414,62415],{},"#feature columns (doesn't include average price or price per watt)\n",[187,62417,62418],{"class":189,"line":319},[187,62419,62420],{},"feature_cols = [u'power', u'eff_rank', u'Manufacturer', u'Modular', u'Type']\n",[187,62422,62423],{"class":189,"line":325},[187,62424,62425],{},"#drop null values and seperate X and y\n",[187,62427,62428],{"class":189,"line":686},[187,62429,62430],{},"X = df[cols].dropna()[feature_cols]\n",[187,62432,62433],{"class":189,"line":697},[187,62434,62435],{},"y = df[cols].dropna().avg\n",[11,62437,30971,62438,62441,62442,62445],{},[33,62439,62440],{},"print X.shape, y.shape"," returns ",[33,62443,62444],{},"((1100, 5), (1100,))",", so we have 1100 observations of PSUs with complete data. We started with 1434 observations of PSUs, so my goal is to make predictions on PSU prices for the values with missing price data. (There may not be good enough feature data to make these predictions, but we won't worry about that for now). The next step is to map categorical variable values from strings to integers:",[26,62447,62449],{"className":1383,"code":62448,"language":1125,"meta":35,"style":35},"type_mapping = {x:y for x,y in zip(df.Type.unique(),range(len(df.Type.unique())))}\ndf.Type = df.Type.map(type_mapping)\n\nmodular_mapping = {x:y for x,y in zip(df.Modular.unique(),range(len(df.Modular.unique())))}\ndf.Modular = df.Modular.map(modular_mapping)\n\nmanufacturer_mapping = {x:y for x,y in zip(df.Manufacturer.unique(),range(len(df.Manufacturer.unique())))}\ndf.Manufacturer = df.Manufacturer.map(manufacturer_mapping)\n",[33,62450,62451,62456,62461,62465,62470,62475,62479,62484],{"__ignoreMap":35},[187,62452,62453],{"class":189,"line":190},[187,62454,62455],{},"type_mapping = {x:y for x,y in zip(df.Type.unique(),range(len(df.Type.unique())))}\n",[187,62457,62458],{"class":189,"line":249},[187,62459,62460],{},"df.Type = df.Type.map(type_mapping)\n",[187,62462,62463],{"class":189,"line":312},[187,62464,316],{"emptyLinePlaceholder":315},[187,62466,62467],{"class":189,"line":319},[187,62468,62469],{},"modular_mapping = {x:y for x,y in zip(df.Modular.unique(),range(len(df.Modular.unique())))}\n",[187,62471,62472],{"class":189,"line":325},[187,62473,62474],{},"df.Modular = df.Modular.map(modular_mapping)\n",[187,62476,62477],{"class":189,"line":686},[187,62478,316],{"emptyLinePlaceholder":315},[187,62480,62481],{"class":189,"line":697},[187,62482,62483],{},"manufacturer_mapping = {x:y for x,y in zip(df.Manufacturer.unique(),range(len(df.Manufacturer.unique())))}\n",[187,62485,62486],{"class":189,"line":1291},[187,62487,62488],{},"df.Manufacturer = df.Manufacturer.map(manufacturer_mapping)\n",[11,62490,62491,62492,358],{},"Now we are ready to setup a random forest model. Here's the description of a random forest regressor from ",[15,62493,62496],{"href":62494,"rel":62495},"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html",[19],"scikit-learn.org",[107,62498,62499],{},[11,62500,62501],{},"A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.",[26,62503,62505],{"className":1383,"code":62504,"language":1125,"meta":35,"style":35},"from sklearn.ensemble import RandomForestRegressor\nrfreg = RandomForestRegressor(n_estimators=150, max_features=4, oob_score=True, random_state=3)\nrfreg.fit(X, y)\nfeature_importance = pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance', ascending=False)\n",[33,62506,62507,62512,62517,62522],{"__ignoreMap":35},[187,62508,62509],{"class":189,"line":190},[187,62510,62511],{},"from sklearn.ensemble import RandomForestRegressor\n",[187,62513,62514],{"class":189,"line":249},[187,62515,62516],{},"rfreg = RandomForestRegressor(n_estimators=150, max_features=4, oob_score=True, random_state=3)\n",[187,62518,62519],{"class":189,"line":312},[187,62520,62521],{},"rfreg.fit(X, y)\n",[187,62523,62524],{"class":189,"line":319},[187,62525,62526],{},"feature_importance = pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance', ascending=False)\n",[11,62528,6131,62529,62532],{},[33,62530,62531],{},"feature_importance"," dataframe assigns a percentage representing how important each feature is in predicting the target variable, price.",[1525,62534,62535,62545],{},[1528,62536,62537],{},[1531,62538,62539,62542],{},[1534,62540,62541],{},"feature",[1534,62543,62544],{},"importance",[1544,62546,62547,62555,62563,62571,62579],{},[1531,62548,62549,62552],{},[1549,62550,62551],{},"power",[1549,62553,62554],{},"0.674090",[1531,62556,62557,62560],{},[1549,62558,62559],{},"eff_rank",[1549,62561,62562],{},"0.170238",[1531,62564,62565,62568],{},[1549,62566,62567],{},"Manufacturer",[1549,62569,62570],{},"0.101809",[1531,62572,62573,62576],{},[1549,62574,62575],{},"Modular",[1549,62577,62578],{},"0.033512",[1531,62580,62581,62584],{},[1549,62582,62583],{},"Type",[1549,62585,62586],{},"0.020352",[11,62588,62589,62590,62592,62593,62595],{},"Looking at feature importance is a quick way evaluate the relative strength of each feature in a model. The results here aren't surprising: ",[33,62591,62551],{}," is by far the most important feature, but ",[33,62594,62559],{}," also has significant pull on the target variable (PSU price). The manufacturer, whether or not the PSU is modular and the type (form factor) are less important and could be ignored altogether in the next model.",[11,62597,62598,62599,62602,62603,62605],{},"We can dig a little bit deeper by searching for a ",[33,62600,62601],{},"n_estimators"," value that will minimize RMSE. ",[33,62604,62601],{}," represents the number of decision trees used in the random forest regressor.",[26,62607,62609],{"className":1383,"code":62608,"language":1125,"meta":35,"style":35},"from sklearn.cross_validation import cross_val_score\n\n#list of values to try for n_estimators\nestimator_range = range(10, 310, 10)\n\n#list to store the average RMSE for each value of n_estimators\nRMSE_scores = []\n\n#use 5-fold cross-validation with each value of n_estimators\nfor estimator in estimator_range:\n    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')\n    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n\n# plot n_estimators (x-axis) versus RMSE (y-axis)\nplt.plot(estimator_range, RMSE_scores)\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE (lower is better)')\n",[33,62610,62611,62616,62620,62625,62630,62634,62639,62644,62648,62653,62658,62663,62668,62673,62677,62682,62687,62692],{"__ignoreMap":35},[187,62612,62613],{"class":189,"line":190},[187,62614,62615],{},"from sklearn.cross_validation import cross_val_score\n",[187,62617,62618],{"class":189,"line":249},[187,62619,316],{"emptyLinePlaceholder":315},[187,62621,62622],{"class":189,"line":312},[187,62623,62624],{},"#list of values to try for n_estimators\n",[187,62626,62627],{"class":189,"line":319},[187,62628,62629],{},"estimator_range = range(10, 310, 10)\n",[187,62631,62632],{"class":189,"line":325},[187,62633,316],{"emptyLinePlaceholder":315},[187,62635,62636],{"class":189,"line":686},[187,62637,62638],{},"#list to store the average RMSE for each value of n_estimators\n",[187,62640,62641],{"class":189,"line":697},[187,62642,62643],{},"RMSE_scores = []\n",[187,62645,62646],{"class":189,"line":1291},[187,62647,316],{"emptyLinePlaceholder":315},[187,62649,62650],{"class":189,"line":1306},[187,62651,62652],{},"#use 5-fold cross-validation with each value of n_estimators\n",[187,62654,62655],{"class":189,"line":1434},[187,62656,62657],{},"for estimator in estimator_range:\n",[187,62659,62660],{"class":189,"line":2599},[187,62661,62662],{},"    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",[187,62664,62665],{"class":189,"line":2607},[187,62666,62667],{},"    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')\n",[187,62669,62670],{"class":189,"line":2621},[187,62671,62672],{},"    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",[187,62674,62675],{"class":189,"line":2631},[187,62676,316],{"emptyLinePlaceholder":315},[187,62678,62679],{"class":189,"line":2642},[187,62680,62681],{},"# plot n_estimators (x-axis) versus RMSE (y-axis)\n",[187,62683,62684],{"class":189,"line":2653},[187,62685,62686],{},"plt.plot(estimator_range, RMSE_scores)\n",[187,62688,62689],{"class":189,"line":2665},[187,62690,62691],{},"plt.xlabel('n_estimators')\n",[187,62693,62694],{"class":189,"line":2674},[187,62695,62696],{},"plt.ylabel('RMSE (lower is better)')\n",[11,62698,62699],{},[511,62700],{"alt":7255,"src":62701},"/static/pcpp/psu/n_est_vs_rmse.png",[11,62703,62704,62705,62707,62708,62710,62711,62714,62715,62718,62719,62721],{},"This graph shows the error in the model for 30 different settings of the parameter ",[33,62706,62601],{},". However, each time we test a new ",[33,62709,62601],{}," value we are caluculating the error using cross-validation. Cross-validataion, or K-fold cross-validation helps improve the accuracy of error estimation by averaging the results of ",[33,62712,62713],{},"k"," models. In the model above, we see that ",[33,62716,62717],{},"cv=5",", so we are running the model 5 times for every value of ",[33,62720,62601],{},", for a total of 150 times.",[11,62723,62724,62725,62727],{},"To find the optimal value of ",[33,62726,62601],{}," we search for the value with the lowest error:",[26,62729,62731],{"className":1383,"code":62730,"language":1125,"meta":35,"style":35},"sorted(zip(RMSE_scores, estimator_range))[0]\n",[33,62732,62733],{"__ignoreMap":35},[187,62734,62735],{"class":189,"line":190},[187,62736,62730],{},[11,62738,62739],{},[33,62740,62741],{},"(28.100834969072217, 160)",[11,62743,62744,62745,62747,62748,62750],{},"So, we get a slightly lower root mean squared error of 28.1 when we choose an ",[33,62746,62601],{}," value of 160 (we started with ",[33,62749,62601],{}," equal to 150).",[11,62752,62753],{},"We can do a quick test of the model by comparing the model's price predictions for certain PSUs to prices on Amazon.",[11,62755,62756],{},"Notice that there is a red point on the x-axis just beyond 1200 Watts. Let's predict the price for that PSU:",[26,62758,62760],{"className":1383,"code":62759,"language":1125,"meta":35,"style":35},"#filter the dataframe by 80+ Titanium rated PSUs that have a price of 0 and power over 1200 W\nX_ = np.array(df[(df['Efficiency Certification']=='80+ Titanium')&(df.power>1200)&(df.avg==0)][feature_cols])\n",[33,62761,62762,62767],{"__ignoreMap":35},[187,62763,62764],{"class":189,"line":190},[187,62765,62766],{},"#filter the dataframe by 80+ Titanium rated PSUs that have a price of 0 and power over 1200 W\n",[187,62768,62769],{"class":189,"line":249},[187,62770,62771],{},"X_ = np.array(df[(df['Efficiency Certification']=='80+ Titanium')&(df.power>1200)&(df.avg==0)][feature_cols])\n",[11,62773,62774],{},"Let's get the index for this PSU:",[26,62776,62778],{"className":1383,"code":62777,"language":1125,"meta":35,"style":35},"df[(df['Efficiency Certification']=='80+ Titanium')&(df.power>1200)&(df.avg==0)].index\n",[33,62779,62780],{"__ignoreMap":35},[187,62781,62782],{"class":189,"line":190},[187,62783,62777],{},[11,62785,62786],{},[33,62787,62788],{},"Int64Index([1260], dtype='int64')",[11,62790,62791],{},"Now we can make a prediction for price of this PSU:",[26,62793,62795],{"className":1383,"code":62794,"language":1125,"meta":35,"style":35},"rfreg.predict(df.ix[1260][feature_cols])\n",[33,62796,62797],{"__ignoreMap":35},[187,62798,62799],{"class":189,"line":190},[187,62800,62794],{},[11,62802,62803,62804,1172,62809,62814],{},"And the prediction we get for this model is $278.09. This product is listed ",[15,62805,62808],{"href":62806,"rel":62807},"https://www.amazon.com/Thermaltake-ToughPower-TITANIUM-256-colors-Management/dp/B019JKM20W",[19],"on Amazon",[15,62810,62813],{"href":62811,"rel":62812},"https://www.newegg.com/Product/Product.aspx?Item=N82E16817153270",[19],"NewEgg"," for $349.99, which means that our prediction fell short of the actual price by quite a bit (by $71.90).",[11,62816,62817],{},"A more practical approach for modeling PSU prices might be to simply make individual linear regressions for each Efficiency Certification. Here's a prediction for the same PSU using a linear regression of only a handful of 80+ Titanium rated PSUs:",[26,62819,62821],{"className":1383,"code":62820,"language":1125,"meta":35,"style":35},"df4 = df[(df.avg>0)&(df['Efficiency Certification']=='80+ Titanium')]\nX = df4[['power']]\ny = df4[['avg']]\n\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(X,y)\nprint reg.predict([1250])\n",[33,62822,62823,62828,62833,62838,62842,62847,62852,62857],{"__ignoreMap":35},[187,62824,62825],{"class":189,"line":190},[187,62826,62827],{},"df4 = df[(df.avg>0)&(df['Efficiency Certification']=='80+ Titanium')]\n",[187,62829,62830],{"class":189,"line":249},[187,62831,62832],{},"X = df4[['power']]\n",[187,62834,62835],{"class":189,"line":312},[187,62836,62837],{},"y = df4[['avg']]\n",[187,62839,62840],{"class":189,"line":319},[187,62841,316],{"emptyLinePlaceholder":315},[187,62843,62844],{"class":189,"line":325},[187,62845,62846],{},"from sklearn.linear_model import LinearRegression\n",[187,62848,62849],{"class":189,"line":686},[187,62850,62851],{},"reg = LinearRegression()\n",[187,62853,62854],{"class":189,"line":697},[187,62855,62856],{},"reg.fit(X,y)\n",[187,62858,62859],{"class":189,"line":1291},[187,62860,62861],{},"print reg.predict([1250])\n",[11,62863,62864],{},[33,62865,62866],{},"[[ 332.5585859]]",[11,62868,62869],{},"This prediction is much more accurate, and it falls right in line with a line-of-best-fit for the red points on the scatter plot above.",[11,62871,62872],{},"By visualizing power and efficiency rating vs. price in the graph above, we can see the strong correlation between wattage and price, and we can observe some general trends between different efficiency certifications: for any given power rating 80+ Titanium is generally more expensive than 80+ Platinum, and 80+ Platinum is generally more expensive than 80+ Bronze. 80+ Gold PSU prices seem to range quite a bit, so let's look at the distribution of 80+ Gold prices by manufacturer and form factor:",[26,62874,62876],{"className":1383,"code":62875,"language":1125,"meta":35,"style":35},"sns.plt.figure(figsize=(12,8))\nplt.title('Average Price per Watt for 80+ Gold PSUs by Manufacturer', fontsize=14)\nplt.xlabel('Manufacturer', fontsize=14)\nplt.ylabel('Average Price per Watt', fontsize=14)\ndf[(df['Efficiency Certification']=='80+ Gold')&(df.power>0)&(df.avg>0)].groupby('Manufacturer').ppw.mean().sort_values().plot(kind='bar')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/average_price_by_manufacturer.png'))\n",[33,62877,62878,62883,62888,62893,62898,62903],{"__ignoreMap":35},[187,62879,62880],{"class":189,"line":190},[187,62881,62882],{},"sns.plt.figure(figsize=(12,8))\n",[187,62884,62885],{"class":189,"line":249},[187,62886,62887],{},"plt.title('Average Price per Watt for 80+ Gold PSUs by Manufacturer', fontsize=14)\n",[187,62889,62890],{"class":189,"line":312},[187,62891,62892],{},"plt.xlabel('Manufacturer', fontsize=14)\n",[187,62894,62895],{"class":189,"line":319},[187,62896,62897],{},"plt.ylabel('Average Price per Watt', fontsize=14)\n",[187,62899,62900],{"class":189,"line":325},[187,62901,62902],{},"df[(df['Efficiency Certification']=='80+ Gold')&(df.power>0)&(df.avg>0)].groupby('Manufacturer').ppw.mean().sort_values().plot(kind='bar')\n",[187,62904,62905],{"class":189,"line":686},[187,62906,62907],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/average_price_by_manufacturer.png'))\n",[11,62909,62910],{},[511,62911],{"alt":7255,"src":62912},"/static/pcpp/psu/average_price_by_manufacturer.png",[11,62914,62915],{},"The most expensive 80+ Gold PSUs are more than twice as expensive as the cheapest PSUs by price per watt. This could explain the significance that the random forest regressor attached to this feature.",[11,62917,62918],{},"Whether or not a PSU is modular refers to the connectivity of power cables that come out of the PSUs. Fully modular means you can unplug all of the cables from the back and plug in only what you need for your PC. Semi modular means that there is one cable you can't unplug from the back (it is usually a 24-pin connector that plugs in to the motherboard), and other cables can be plugged in for graphics cards or other devices. 'No' means that all of the cables that you will need are permanently fixed to the PSU and you can't unplug anything. Here's a graph showing showing the distributions of price per watt by modular type (full, semi and none):",[26,62920,62922],{"className":1383,"code":62921,"language":1125,"meta":35,"style":35},"df[(df['Efficiency Certification']=='80+ Gold')&(df.avg>0)].boxplot(column='ppw', by='Modular', figsize=(12,8))\nplt.ylim([0.07,0.3])\nplt.suptitle('')\nplt.ylabel('Price per Watt', fontsize=14)\nplt.title('Price per Watt distributions of 80+ Gold PSUs by Modular Type', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/price_by_modular.png'))\n",[33,62923,62924,62929,62934,62939,62944,62949,62954,62959],{"__ignoreMap":35},[187,62925,62926],{"class":189,"line":190},[187,62927,62928],{},"df[(df['Efficiency Certification']=='80+ Gold')&(df.avg>0)].boxplot(column='ppw', by='Modular', figsize=(12,8))\n",[187,62930,62931],{"class":189,"line":249},[187,62932,62933],{},"plt.ylim([0.07,0.3])\n",[187,62935,62936],{"class":189,"line":312},[187,62937,62938],{},"plt.suptitle('')\n",[187,62940,62941],{"class":189,"line":319},[187,62942,62943],{},"plt.ylabel('Price per Watt', fontsize=14)\n",[187,62945,62946],{"class":189,"line":325},[187,62947,62948],{},"plt.title('Price per Watt distributions of 80+ Gold PSUs by Modular Type', fontsize=14)\n",[187,62950,62951],{"class":189,"line":686},[187,62952,62953],{},"plt.xticks(fontsize=13)\n",[187,62955,62956],{"class":189,"line":697},[187,62957,62958],{},"plt.yticks(fontsize=13)\n",[187,62960,62961],{"class":189,"line":1291},[187,62962,62963],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/psu/price_by_modular.png'))\n",[11,62965,62966],{},[511,62967],{"alt":7255,"src":62968},"/static/pcpp/psu/price_by_modular.png",[11,62970,62971],{},"The distributions aren't surprising: fully-modular is more expensive than semi-modular, and semi-modular PSUs are more expensive than PSUs that are not modular. However, there is quite a bit of overlap in the prices of each modular type, so this feature only contributes roughly 3% of importance for predicting the price of PSUs with the random forest regressor model.",[168,62973,61654],{"id":61583},[11,62975,62976],{},"Motherboards are a critical part of any PC build. This component determines the compatability for most of the other components in a PC build, such as memory type, CPU socket, SSD drives types and SLI/CrossFire configurations (we'll get to what these mean soon).",[11,62978,62979,62980,62983,62984,62987],{},"There are a lot of features to choose from, and most of the features in ",[33,62981,62982],{},"motherboard_csv.csv"," are categorical variables, some containing over 100 unique values. We can use ",[33,62985,62986],{},"df.groupby()"," to look popular combinations of motherboard features. There are 26 CPU socket types, 11 different form factors and 14 memory slot types. There are a total of 60 unique combinations for these three motherboard features, the combinations with the most motherboards are shown below along with average prices:",[26,62989,62991],{"className":1383,"code":62990,"language":1125,"meta":35,"style":35},"df[(df.avg>0)].groupby(['Memory Slots','Form Factor', 'socket']).avg.agg(['mean', 'count']).sort_values(by='count', ascending=False)[:30].plot(kind='bar', figsize=(10,4))\nplt.title('Prices and counts for top 30 Memory Slot, Form Factor and Socket combinations')\nplt.xlabel('Memory Slot, Form Factor and Socket combinations')\nplt.figure()\n",[33,62992,62993,62998,63003,63008],{"__ignoreMap":35},[187,62994,62995],{"class":189,"line":190},[187,62996,62997],{},"df[(df.avg>0)].groupby(['Memory Slots','Form Factor', 'socket']).avg.agg(['mean', 'count']).sort_values(by='count', ascending=False)[:30].plot(kind='bar', figsize=(10,4))\n",[187,62999,63000],{"class":189,"line":249},[187,63001,63002],{},"plt.title('Prices and counts for top 30 Memory Slot, Form Factor and Socket combinations')\n",[187,63004,63005],{"class":189,"line":312},[187,63006,63007],{},"plt.xlabel('Memory Slot, Form Factor and Socket combinations')\n",[187,63009,63010],{"class":189,"line":319},[187,63011,63012],{},"plt.figure()\n",[11,63014,63015],{},[511,63016],{"alt":7255,"src":63017},"/static/pcpp/motherboard/features_vs_price.png",[11,63019,63020],{},"Of the 2400 motherboards in the dataset, there is price information for only 618 motherboards. The average motherboard price is $157.50. Here's a visualizations of motherboard prices:",[26,63022,63024],{"className":1383,"code":63023,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf[(df.avg!=0)&(df.avg\u003C750)].avg.hist(bins=30)\nplt.title('Motherboard prices', fontsize=14)\nplt.xlabel('Price', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/price_histogram.png'))\n",[33,63025,63026,63030,63035,63040,63045,63050,63054,63058],{"__ignoreMap":35},[187,63027,63028],{"class":189,"line":190},[187,63029,62266],{},[187,63031,63032],{"class":189,"line":249},[187,63033,63034],{},"df[(df.avg!=0)&(df.avg\u003C750)].avg.hist(bins=30)\n",[187,63036,63037],{"class":189,"line":312},[187,63038,63039],{},"plt.title('Motherboard prices', fontsize=14)\n",[187,63041,63042],{"class":189,"line":319},[187,63043,63044],{},"plt.xlabel('Price', fontsize=14)\n",[187,63046,63047],{"class":189,"line":325},[187,63048,63049],{},"plt.ylabel('Count', fontsize=14)\n",[187,63051,63052],{"class":189,"line":686},[187,63053,62953],{},[187,63055,63056],{"class":189,"line":697},[187,63057,62958],{},[187,63059,63060],{"class":189,"line":1291},[187,63061,63062],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/price_histogram.png'))\n",[11,63064,63065],{},[511,63066],{"alt":7255,"src":63067},"/static/pcpp/motherboard/price_histogram.png",[11,63069,63070,63071,63076],{},"Another feature that may be indicative of price is SLI support. Here's a description of SLI from a ",[15,63072,63075],{"href":63073,"rel":63074},"http://superuser.com/questions/562631/what-does-sli-ready-mean-and-how-do-i-use-it",[19],"superuser"," forum post:",[107,63078,63079],{},[11,63080,63081],{},"Scalable Link Interface (SLI) is a brand name for a multi-GPU solution developed by NVIDIA for linking two or more video cards together to produce a single output. SLI is an application of parallel processing for computer graphics, meant to increase the processing power available for graphics.",[11,63083,6131,63084,63087],{},[33,63085,63086],{},"SLI Support"," feature values include NaN, Yes, 3 and 4. Intuition tells me that a motherboard supporting 4 graphics cards will be more expensive than motherboard supporing only graphics cards.",[11,63089,63090,63091,358],{},"Here's a look at motherboard prices by ",[33,63092,63086],{},[26,63094,63096],{"className":1383,"code":63095,"language":1125,"meta":35,"style":35},"df[df.avg>0].boxplot(column='avg', by='SLI Support', figsize=(12,8))\nplt.ylim([0,650])\nplt.suptitle('')\nplt.title('Motherboard Prices by SLI Support',fontsize=14)\nplt.xlabel('SLI Support',fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/SLI_prices.png'))\n",[33,63097,63098,63103,63108,63112,63117,63122,63126,63130,63134],{"__ignoreMap":35},[187,63099,63100],{"class":189,"line":190},[187,63101,63102],{},"df[df.avg>0].boxplot(column='avg', by='SLI Support', figsize=(12,8))\n",[187,63104,63105],{"class":189,"line":249},[187,63106,63107],{},"plt.ylim([0,650])\n",[187,63109,63110],{"class":189,"line":312},[187,63111,62938],{},[187,63113,63114],{"class":189,"line":319},[187,63115,63116],{},"plt.title('Motherboard Prices by SLI Support',fontsize=14)\n",[187,63118,63119],{"class":189,"line":325},[187,63120,63121],{},"plt.xlabel('SLI Support',fontsize=14)\n",[187,63123,63124],{"class":189,"line":686},[187,63125,62286],{},[187,63127,63128],{"class":189,"line":697},[187,63129,62953],{},[187,63131,63132],{"class":189,"line":1291},[187,63133,62958],{},[187,63135,63136],{"class":189,"line":1306},[187,63137,63138],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/SLI_prices.png'))\n",[11,63140,63141],{},[511,63142],{"alt":7255,"src":63143},"/static/pcpp/motherboard/SLI_prices.png",[11,63145,63146],{},"Memory slots on motherboard determine what type of memory and how much memory can be included in a PC. The amount of memory is also limited by the maximum supported memory of the CPU. Here'a a breakdown of the count of motherboards by memory type:",[26,63148,63150],{"className":1383,"code":63149,"language":1125,"meta":35,"style":35},"df['Memory Slots'].value_counts()[:8].plot(kind='bar', rot=45, figsize=(12,8))\nplt.title('Top Eight Motherboard Memory Slot types', fontsize=13)\nplt.ylabel('Count', fontsize=13)\nplt.xlabel('Memory Slot types', fontsize=13)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/motherboard_count_by_mem_type.png'))\n",[33,63151,63152,63157,63162,63167,63172,63176,63180],{"__ignoreMap":35},[187,63153,63154],{"class":189,"line":190},[187,63155,63156],{},"df['Memory Slots'].value_counts()[:8].plot(kind='bar', rot=45, figsize=(12,8))\n",[187,63158,63159],{"class":189,"line":249},[187,63160,63161],{},"plt.title('Top Eight Motherboard Memory Slot types', fontsize=13)\n",[187,63163,63164],{"class":189,"line":312},[187,63165,63166],{},"plt.ylabel('Count', fontsize=13)\n",[187,63168,63169],{"class":189,"line":319},[187,63170,63171],{},"plt.xlabel('Memory Slot types', fontsize=13)\n",[187,63173,63174],{"class":189,"line":325},[187,63175,62953],{},[187,63177,63178],{"class":189,"line":686},[187,63179,62958],{},[187,63181,63182],{"class":189,"line":697},[187,63183,63184],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/motherboard_count_by_mem_type.png'))\n",[11,63186,63187],{},[511,63188],{"alt":7255,"src":63189},"/static/pcpp/motherboard/motherboard_count_by_mem_type.png",[11,63191,63192],{},"And here are price boxplots for the top three most common memory slot types:",[26,63194,63196],{"className":1383,"code":63195,"language":1125,"meta":35,"style":35},"df[(df.avg>0)&((df['Memory Slots']=='4 x 240-pin DIMM')|(df['Memory Slots']=='2 x 240-pin DIMM')|\\\n              (df['Memory Slots']=='4 x 288-pin DIMM'))].boxplot(column='avg', by='Memory Slots', figsize=(12,8))\nplt.ylim([0,400])\nplt.suptitle('')\nplt.title('Motherboard price distributions by top three Memory Slot types', fontsize=14)\nplt.xlabel('Memory Slot Type',fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/prices_by_mem_slot.png'))\n",[33,63197,63198,63203,63208,63213,63217,63222,63227,63231,63235,63239],{"__ignoreMap":35},[187,63199,63200],{"class":189,"line":190},[187,63201,63202],{},"df[(df.avg>0)&((df['Memory Slots']=='4 x 240-pin DIMM')|(df['Memory Slots']=='2 x 240-pin DIMM')|\\\n",[187,63204,63205],{"class":189,"line":249},[187,63206,63207],{},"              (df['Memory Slots']=='4 x 288-pin DIMM'))].boxplot(column='avg', by='Memory Slots', figsize=(12,8))\n",[187,63209,63210],{"class":189,"line":312},[187,63211,63212],{},"plt.ylim([0,400])\n",[187,63214,63215],{"class":189,"line":319},[187,63216,62938],{},[187,63218,63219],{"class":189,"line":325},[187,63220,63221],{},"plt.title('Motherboard price distributions by top three Memory Slot types', fontsize=14)\n",[187,63223,63224],{"class":189,"line":686},[187,63225,63226],{},"plt.xlabel('Memory Slot Type',fontsize=14)\n",[187,63228,63229],{"class":189,"line":697},[187,63230,62286],{},[187,63232,63233],{"class":189,"line":1291},[187,63234,62953],{},[187,63236,63237],{"class":189,"line":1306},[187,63238,62958],{},[187,63240,63241],{"class":189,"line":1434},[187,63242,63243],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/prices_by_mem_slot.png'))\n",[11,63245,63246],{},[511,63247],{"alt":7255,"src":63248},"/static/pcpp/motherboard/prices_by_mem_slot.png",[11,63250,63251],{},"Now let's perform a random forest regression for motherboards in the same way we did for PSUs to rank the importance of these features.",[11,63253,63254],{},"First we need to map categorical variable values to integers:",[26,63256,63258],{"className":1383,"code":63257,"language":1125,"meta":35,"style":35},"#map categorical variable values to integers\nchipset_mapping = {x:y for x,y in zip(df.Chipset.unique(),range(len(df.Chipset.unique())))}\ndf['Chipset_int'] = df.Chipset.map(chipset_mapping)\n\nsocket_mapping = {x:y for x,y in zip(df.socket.unique(),range(len(df.socket.unique())))}\ndf['socket_int'] = df.socket.map(socket_mapping)\n\ndf['Form_Factor'] = df['Form Factor']\nform_factor_mapping = {x:y for x,y in zip(df.Form_Factor.unique(),range(len(df.Form_Factor.unique())))}\ndf['Form_Factor_int'] = df.Form_Factor.map(form_factor_mapping)\n\ndf['Memory_Type'] = df['Memory Type']\nmemory_type_mapping = {x:y for x,y in zip(df.Memory_Type.unique(),range(len(df.Memory_Type.unique())))}\ndf['Memory_Type_int'] = df.Memory_Type.map(memory_type_mapping)\n\ndf['Memory_Slots'] = df['Memory Slots']\nmemory_slots_mapping = {x:y for x,y in zip(df.Memory_Slots.unique(),range(len(df.Memory_Slots.unique())))}\ndf['Memory_Slots_int'] = df.Memory_Slots.map(memory_slots_mapping)\n\nmanufacturer_mapping = {x:y for x,y in zip(df.Manufacturer.unique(),range(len(df.Manufacturer.unique())))}\ndf['Manufacturer_int'] = df.Manufacturer.map(manufacturer_mapping)\n\ndf['SLI_Support'] = df['SLI Support']\nsli_mapping = {x:y for x,y in zip(df.SLI_Support.unique(),range(len(df.SLI_Support.unique())))}\ndf['SLI_Support_int'] = df.SLI_Support.map(sli_mapping)\n",[33,63259,63260,63265,63270,63275,63279,63284,63289,63293,63298,63303,63308,63312,63317,63322,63327,63331,63336,63341,63346,63350,63354,63359,63363,63368,63373],{"__ignoreMap":35},[187,63261,63262],{"class":189,"line":190},[187,63263,63264],{},"#map categorical variable values to integers\n",[187,63266,63267],{"class":189,"line":249},[187,63268,63269],{},"chipset_mapping = {x:y for x,y in zip(df.Chipset.unique(),range(len(df.Chipset.unique())))}\n",[187,63271,63272],{"class":189,"line":312},[187,63273,63274],{},"df['Chipset_int'] = df.Chipset.map(chipset_mapping)\n",[187,63276,63277],{"class":189,"line":319},[187,63278,316],{"emptyLinePlaceholder":315},[187,63280,63281],{"class":189,"line":325},[187,63282,63283],{},"socket_mapping = {x:y for x,y in zip(df.socket.unique(),range(len(df.socket.unique())))}\n",[187,63285,63286],{"class":189,"line":686},[187,63287,63288],{},"df['socket_int'] = df.socket.map(socket_mapping)\n",[187,63290,63291],{"class":189,"line":697},[187,63292,316],{"emptyLinePlaceholder":315},[187,63294,63295],{"class":189,"line":1291},[187,63296,63297],{},"df['Form_Factor'] = df['Form Factor']\n",[187,63299,63300],{"class":189,"line":1306},[187,63301,63302],{},"form_factor_mapping = {x:y for x,y in zip(df.Form_Factor.unique(),range(len(df.Form_Factor.unique())))}\n",[187,63304,63305],{"class":189,"line":1434},[187,63306,63307],{},"df['Form_Factor_int'] = df.Form_Factor.map(form_factor_mapping)\n",[187,63309,63310],{"class":189,"line":2599},[187,63311,316],{"emptyLinePlaceholder":315},[187,63313,63314],{"class":189,"line":2607},[187,63315,63316],{},"df['Memory_Type'] = df['Memory Type']\n",[187,63318,63319],{"class":189,"line":2621},[187,63320,63321],{},"memory_type_mapping = {x:y for x,y in zip(df.Memory_Type.unique(),range(len(df.Memory_Type.unique())))}\n",[187,63323,63324],{"class":189,"line":2631},[187,63325,63326],{},"df['Memory_Type_int'] = df.Memory_Type.map(memory_type_mapping)\n",[187,63328,63329],{"class":189,"line":2642},[187,63330,316],{"emptyLinePlaceholder":315},[187,63332,63333],{"class":189,"line":2653},[187,63334,63335],{},"df['Memory_Slots'] = df['Memory Slots']\n",[187,63337,63338],{"class":189,"line":2665},[187,63339,63340],{},"memory_slots_mapping = {x:y for x,y in zip(df.Memory_Slots.unique(),range(len(df.Memory_Slots.unique())))}\n",[187,63342,63343],{"class":189,"line":2674},[187,63344,63345],{},"df['Memory_Slots_int'] = df.Memory_Slots.map(memory_slots_mapping)\n",[187,63347,63348],{"class":189,"line":2684},[187,63349,316],{"emptyLinePlaceholder":315},[187,63351,63352],{"class":189,"line":2694},[187,63353,62483],{},[187,63355,63356],{"class":189,"line":2706},[187,63357,63358],{},"df['Manufacturer_int'] = df.Manufacturer.map(manufacturer_mapping)\n",[187,63360,63361],{"class":189,"line":2715},[187,63362,316],{"emptyLinePlaceholder":315},[187,63364,63365],{"class":189,"line":2725},[187,63366,63367],{},"df['SLI_Support'] = df['SLI Support']\n",[187,63369,63370],{"class":189,"line":2735},[187,63371,63372],{},"sli_mapping = {x:y for x,y in zip(df.SLI_Support.unique(),range(len(df.SLI_Support.unique())))}\n",[187,63374,63375],{"class":189,"line":2743},[187,63376,63377],{},"df['SLI_Support_int'] = df.SLI_Support.map(sli_mapping)\n",[11,63379,63380],{},"And then drop null values and separate X and y:",[26,63382,63384],{"className":1383,"code":63383,"language":1125,"meta":35,"style":35},"cols = ['avg', 'max_mem', 'Memory_Slots_int', 'SLI_Support_int', 'Memory_Type_int', 'Form_Factor_int', 'socket_int','Chipset_int']\nfeature_cols = ['max_mem', 'Memory_Slots_int', 'SLI_Support_int','Memory_Type_int', 'Form_Factor_int', 'socket_int','Chipset_int']\nX = df[cols][df.avg>0].dropna()[feature_cols]\ny = df[cols][df.avg>0].avg\n",[33,63385,63386,63391,63396,63401],{"__ignoreMap":35},[187,63387,63388],{"class":189,"line":190},[187,63389,63390],{},"cols = ['avg', 'max_mem', 'Memory_Slots_int', 'SLI_Support_int', 'Memory_Type_int', 'Form_Factor_int', 'socket_int','Chipset_int']\n",[187,63392,63393],{"class":189,"line":249},[187,63394,63395],{},"feature_cols = ['max_mem', 'Memory_Slots_int', 'SLI_Support_int','Memory_Type_int', 'Form_Factor_int', 'socket_int','Chipset_int']\n",[187,63397,63398],{"class":189,"line":312},[187,63399,63400],{},"X = df[cols][df.avg>0].dropna()[feature_cols]\n",[187,63402,63403],{"class":189,"line":319},[187,63404,63405],{},"y = df[cols][df.avg>0].avg\n",[11,63407,63408,63409,358],{},"Now we setup the model and take a look at the results ",[33,63410,63411],{},".feature_importances_",[26,63413,63415],{"className":1383,"code":63414,"language":1125,"meta":35,"style":35},"from sklearn.ensemble import RandomForestRegressor\n# max_features=8 is best and n_estimators=150 is sufficiently large\nrfreg = RandomForestRegressor(n_estimators=80, max_features=6, oob_score=True, random_state=5)\nrfreg.fit(X, y)\nfeature_importance = pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance', ascending=False)\nprint feature_importance\n",[33,63416,63417,63421,63426,63431,63435,63439],{"__ignoreMap":35},[187,63418,63419],{"class":189,"line":190},[187,63420,62511],{},[187,63422,63423],{"class":189,"line":249},[187,63424,63425],{},"# max_features=8 is best and n_estimators=150 is sufficiently large\n",[187,63427,63428],{"class":189,"line":312},[187,63429,63430],{},"rfreg = RandomForestRegressor(n_estimators=80, max_features=6, oob_score=True, random_state=5)\n",[187,63432,63433],{"class":189,"line":319},[187,63434,62521],{},[187,63436,63437],{"class":189,"line":325},[187,63438,62526],{},[187,63440,63441],{"class":189,"line":686},[187,63442,63443],{},"print feature_importance\n",[1525,63445,63446,63454],{},[1528,63447,63448],{},[1531,63449,63450,63452],{},[1534,63451,62541],{},[1534,63453,62544],{},[1544,63455,63456,63464,63472,63480,63488,63496,63504],{},[1531,63457,63458,63461],{},[1549,63459,63460],{},"Memory_Slots_int",[1549,63462,63463],{},"0.352454",[1531,63465,63466,63469],{},[1549,63467,63468],{},"SLI_Support_int",[1549,63470,63471],{},"0.155018",[1531,63473,63474,63477],{},[1549,63475,63476],{},"Memory_Type_int",[1549,63478,63479],{},"0.127274",[1531,63481,63482,63485],{},[1549,63483,63484],{},"Form_Factor_int",[1549,63486,63487],{},"0.113944",[1531,63489,63490,63493],{},[1549,63491,63492],{},"Chipset_int",[1549,63494,63495],{},"0.088664",[1531,63497,63498,63501],{},[1549,63499,63500],{},"max_mem",[1549,63502,63503],{},"0.085077",[1531,63505,63506,63509],{},[1549,63507,63508],{},"socket_int",[1549,63510,63511],{},"0.077571",[11,63513,63514,63515,63517],{},"And finally we can test the accuracy of the model by searching for an optimal ",[33,63516,62601],{}," number of decision trees with 5-fold cross validation:",[26,63519,63521],{"className":1383,"code":63520,"language":1125,"meta":35,"style":35},"from sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\n\n# list of values to try for n_estimators\nestimator_range = range(10, 310, 10)\n\n# list to store the average RMSE for each value of n_estimators\nRMSE_scores1 = []\n\n# use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\nfor estimator in estimator_range:\n    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')\n    RMSE_scores1.append(np.mean(np.sqrt(-MSE_scores)))\n\n\nplt.figure(figsize=(12,8))\nplt.xlabel('n_estimators', fontsize=14)\nplt.ylabel('RMSE (root-mean-sqaure error)', fontsize=14)\nplt.plot(estimator_range, RMSE_scores1)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/n_est_vs_rmse.png'))\n",[33,63522,63523,63528,63532,63536,63541,63545,63549,63554,63559,63563,63568,63572,63576,63580,63585,63589,63593,63597,63602,63607,63612,63616,63620],{"__ignoreMap":35},[187,63524,63525],{"class":189,"line":190},[187,63526,63527],{},"from sklearn import metrics\n",[187,63529,63530],{"class":189,"line":249},[187,63531,62615],{},[187,63533,63534],{"class":189,"line":312},[187,63535,316],{"emptyLinePlaceholder":315},[187,63537,63538],{"class":189,"line":319},[187,63539,63540],{},"# list of values to try for n_estimators\n",[187,63542,63543],{"class":189,"line":325},[187,63544,62629],{},[187,63546,63547],{"class":189,"line":686},[187,63548,316],{"emptyLinePlaceholder":315},[187,63550,63551],{"class":189,"line":697},[187,63552,63553],{},"# list to store the average RMSE for each value of n_estimators\n",[187,63555,63556],{"class":189,"line":1291},[187,63557,63558],{},"RMSE_scores1 = []\n",[187,63560,63561],{"class":189,"line":1306},[187,63562,316],{"emptyLinePlaceholder":315},[187,63564,63565],{"class":189,"line":1434},[187,63566,63567],{},"# use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",[187,63569,63570],{"class":189,"line":2599},[187,63571,62657],{},[187,63573,63574],{"class":189,"line":2607},[187,63575,62662],{},[187,63577,63578],{"class":189,"line":2621},[187,63579,62667],{},[187,63581,63582],{"class":189,"line":2631},[187,63583,63584],{},"    RMSE_scores1.append(np.mean(np.sqrt(-MSE_scores)))\n",[187,63586,63587],{"class":189,"line":2642},[187,63588,316],{"emptyLinePlaceholder":315},[187,63590,63591],{"class":189,"line":2653},[187,63592,316],{"emptyLinePlaceholder":315},[187,63594,63595],{"class":189,"line":2665},[187,63596,62266],{},[187,63598,63599],{"class":189,"line":2674},[187,63600,63601],{},"plt.xlabel('n_estimators', fontsize=14)\n",[187,63603,63604],{"class":189,"line":2684},[187,63605,63606],{},"plt.ylabel('RMSE (root-mean-sqaure error)', fontsize=14)\n",[187,63608,63609],{"class":189,"line":2694},[187,63610,63611],{},"plt.plot(estimator_range, RMSE_scores1)\n",[187,63613,63614],{"class":189,"line":2706},[187,63615,62953],{},[187,63617,63618],{"class":189,"line":2715},[187,63619,62958],{},[187,63621,63622],{"class":189,"line":2725},[187,63623,63624],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/motherboard/n_est_vs_rmse.png'))\n",[11,63626,63627],{},[511,63628],{"alt":7255,"src":63629},"/static/pcpp/motherboard/n_est_vs_rmse.png",[11,63631,63632],{},"The predictive accuracy is not great. Let's look at a few random motherboards with no pricing data and compare the model's prediction to prices on Amazon.",[26,63634,63636],{"className":1383,"code":63635,"language":1125,"meta":35,"style":35},"df[df.avg==0].sample(1)['Part #']\n",[33,63637,63638],{"__ignoreMap":35},[187,63639,63640],{"class":189,"line":190},[187,63641,63635],{},[11,63643,63644],{},[33,63645,63646],{},"1710 GA-Z97X-SOC FORCE",[11,63648,63649],{},"This is a random sample with no pricing data, and its index value is 1710.",[11,63651,63652],{},"The following will give us a prediction based on our model:",[26,63654,63656],{"className":1383,"code":63655,"language":1125,"meta":35,"style":35},"rfreg.predict(np.array(df.ix[1710][feature_cols]))\n",[33,63657,63658],{"__ignoreMap":35},[187,63659,63660],{"class":189,"line":190},[187,63661,63655],{},[11,63663,63664],{},[33,63665,63666],{},"array([ 203.90425])",[11,63668,63669,63674],{},[15,63670,63673],{"href":63671,"rel":63672},"https://www.amazon.com/gp/offer-listing/B00JKCHEPS/ref=dp_olp_used_mbc?ie=UTF8&condition=used",[19],"This product"," is available used on Amazon for $249.00, so we have fairly significant error in this one prediction.",[11,63676,63677],{},"Here's one more example that I will cherry-pick:",[26,63679,63680],{"className":1383,"code":63635,"language":1125,"meta":35,"style":35},[33,63681,63682],{"__ignoreMap":35},[187,63683,63684],{"class":189,"line":190},[187,63685,63635],{},[11,63687,63688],{},[33,63689,63690],{},"234 A76ML-K 3.0",[26,63692,63694],{"className":1383,"code":63693,"language":1125,"meta":35,"style":35},"rfreg.predict(np.array(df.ix[234][feature_cols]))\n",[33,63695,63696],{"__ignoreMap":35},[187,63697,63698],{"class":189,"line":190},[187,63699,63693],{},[11,63701,63702],{},[33,63703,63704],{},"array([ 82.61848925])",[11,63706,63707,63711,63712,63717],{},[15,63708,63673],{"href":63709,"rel":63710},"https://www.newegg.com/Product/Product.aspx?Item=N82E16813186215",[19]," is available on ",[15,63713,63716],{"href":63714,"rel":63715},"https://www.newegg.com",[19],"NewEgg.com"," for $57.95 (currently on sale for $39.95). Our model's prediction for this product price is slightly more accurate, but it also reveals a problem with trying to fill the missing data in this data set.",[11,63719,63720],{},"Most of the motherboards with no pricing data in the dataset are out of stock on all major online retailers and are also quite old. In the next part of this project we will be able to see how much pricing data is missing in the collection of 25,000 PC builds on PCPartPicker. I am anticipating that there will be very little missing data since people are building with new hardware.",[168,63722,61577],{"id":63723},"cpu",[11,63725,63726],{},"CPU price and performance are determined by several features, so I'll go through them one by one and show interesting relationships among different features and between features and price.",[2215,63728,63730],{"id":63729},"lithography","Lithography",[11,63732,63733,63734,63736],{},"One interesting feature of CPUs that distinguishes Intel and AMD (the two CPU manufacturers) is ",[4339,63735,63729],{},". Lithography can be described as the average space between a processor's transistors, and it is an important factor that determines clock speed and power consumption. This graph shows lithography vs. CPU price for CPUs priced under $750:",[26,63738,63740],{"className":1383,"code":63739,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,4))\nsns.set_style('whitegrid')\nplt.axis([0,70,0,750])\nplt.xlabel('Lithography', fontsize=14)\nplt.title('CPU Lithography and Price', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.scatter(df[df.Manufacturer==\"Intel\"].Lithography, df[df.Manufacturer==\"Intel\"].avg, alpha=.2, color='blue', s=50)\nplt.scatter(df[df.Manufacturer==\"AMD\"].Lithography, df[df.Manufacturer==\"AMD\"].avg, alpha=.2, color='red', s=50 )\nplt.legend(['Intel','AMD'], fontsize=14)\n",[33,63741,63742,63746,63750,63755,63760,63765,63769,63774,63779],{"__ignoreMap":35},[187,63743,63744],{"class":189,"line":190},[187,63745,55819],{},[187,63747,63748],{"class":189,"line":249},[187,63749,55814],{},[187,63751,63752],{"class":189,"line":312},[187,63753,63754],{},"plt.axis([0,70,0,750])\n",[187,63756,63757],{"class":189,"line":319},[187,63758,63759],{},"plt.xlabel('Lithography', fontsize=14)\n",[187,63761,63762],{"class":189,"line":325},[187,63763,63764],{},"plt.title('CPU Lithography and Price', fontsize=14)\n",[187,63766,63767],{"class":189,"line":686},[187,63768,62286],{},[187,63770,63771],{"class":189,"line":697},[187,63772,63773],{},"plt.scatter(df[df.Manufacturer==\"Intel\"].Lithography, df[df.Manufacturer==\"Intel\"].avg, alpha=.2, color='blue', s=50)\n",[187,63775,63776],{"class":189,"line":1291},[187,63777,63778],{},"plt.scatter(df[df.Manufacturer==\"AMD\"].Lithography, df[df.Manufacturer==\"AMD\"].avg, alpha=.2, color='red', s=50 )\n",[187,63780,63781],{"class":189,"line":1306},[187,63782,63783],{},"plt.legend(['Intel','AMD'], fontsize=14)\n",[11,63785,63786],{},[511,63787],{"alt":7255,"src":63788},"/static/pcpp/cpu/lith_vs_price.png",[11,63790,63791],{},"Intel's 22 nm and 14 nm lithography set it apart from AMD. From what I understand, AMD contracts its semi-conducter fabrication to other companies and Intel has its own advanced fabrication processes.",[2215,63793,63795],{"id":63794},"clock-speed-cpu-cores","Clock speed, CPU cores",[11,63797,63798],{},"CPU speed is measured in clock cycles. A clock cycle is the amount of time between two pulses of an oscillator (something that goes back and forth), and it helps determine the amount of instructions that the CPU can execute per second. Clock cycles used to be the best way to measure the speed of a CPU, but developments around 2005 in CPU architecture (and computer applications) have added other important features that must be considered alongside clock speed when determining the power of a PC.",[11,63800,63801],{},"Engineers had difficulty increasing clock speeds, so they started to develop multi-core processors. It's helpful to think of CPUs and CPUs cores like a kitchen. A single-core CPU is like a CPU with one cook, and is limited by how fast the cook can make food. Adding cores to a CPU can be thought of as adding cooks to the kitchen. The cooks don't get faster, but the productivity of the kitchen increases. Here's a look at the distribution of clock speeds by CPU:",[26,63803,63805],{"className":1383,"code":63804,"language":1125,"meta":35,"style":35},"df[(df.cores>0)&(df.avg>0)].boxplot(column='opfreq', by='cores', figsize=(12,8))\nplt.suptitle('')\nplt.title('CPU Operating Frequency by Core count', fontsize=14)\nplt.xlabel('CPU Core count', fontsize=14)\nplt.ylabel('Clock speed (MHz)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/speed_vs_cores.png'))\n",[33,63806,63807,63812,63816,63821,63826,63831,63835,63839],{"__ignoreMap":35},[187,63808,63809],{"class":189,"line":190},[187,63810,63811],{},"df[(df.cores>0)&(df.avg>0)].boxplot(column='opfreq', by='cores', figsize=(12,8))\n",[187,63813,63814],{"class":189,"line":249},[187,63815,62938],{},[187,63817,63818],{"class":189,"line":312},[187,63819,63820],{},"plt.title('CPU Operating Frequency by Core count', fontsize=14)\n",[187,63822,63823],{"class":189,"line":319},[187,63824,63825],{},"plt.xlabel('CPU Core count', fontsize=14)\n",[187,63827,63828],{"class":189,"line":325},[187,63829,63830],{},"plt.ylabel('Clock speed (MHz)', fontsize=14)\n",[187,63832,63833],{"class":189,"line":686},[187,63834,62953],{},[187,63836,63837],{"class":189,"line":697},[187,63838,62958],{},[187,63840,63841],{"class":189,"line":1291},[187,63842,63843],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/speed_vs_cores.png'))\n",[11,63845,63846],{},[511,63847],{"alt":7255,"src":63848},"/static/pcpp/cpu/speed_vs_cores.png",[11,63850,63851],{},"So once there are 22 cooks in the kitchen, everyone has to go slower, but the team of cooks can easily handle things that would totally swamp an individual cook, like rendering 4K video while live-streaming a AAA title at 1080p60.",[11,63853,63854],{},"You have to pay for each cook, so the core count has a big impact on CPU price:",[26,63856,63858],{"className":1383,"code":63857,"language":1125,"meta":35,"style":35},"df[(df.cores>0)&(df.avg>0)].boxplot(column='avg', by='cores', figsize=(12,8))\nplt.suptitle('')\nplt.title('CPU Prices by Core count', fontsize=14)\nplt.xlabel('CPU Core count', fontsize=14)\nplt.ylabel('Prices', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/price_by_core.png'))\n",[33,63859,63860,63865,63869,63874,63878,63883,63887,63891],{"__ignoreMap":35},[187,63861,63862],{"class":189,"line":190},[187,63863,63864],{},"df[(df.cores>0)&(df.avg>0)].boxplot(column='avg', by='cores', figsize=(12,8))\n",[187,63866,63867],{"class":189,"line":249},[187,63868,62938],{},[187,63870,63871],{"class":189,"line":312},[187,63872,63873],{},"plt.title('CPU Prices by Core count', fontsize=14)\n",[187,63875,63876],{"class":189,"line":319},[187,63877,63825],{},[187,63879,63880],{"class":189,"line":325},[187,63881,63882],{},"plt.ylabel('Prices', fontsize=14)\n",[187,63884,63885],{"class":189,"line":686},[187,63886,62953],{},[187,63888,63889],{"class":189,"line":697},[187,63890,62958],{},[187,63892,63893],{"class":189,"line":1291},[187,63894,63895],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/price_by_core.png'))\n",[11,63897,63898],{},[511,63899],{"alt":7255,"src":63900},"/static/pcpp/cpu/price_by_core.png",[11,63902,63903],{},"A kitchen with 22 cooks gets very hot in the same way that a CPU with 22 cores generates a lot of heat. The next graph shows how much heat (measured in something called TDP, or thermal design power) CPUs generate by core count:",[26,63905,63907],{"className":1383,"code":63906,"language":1125,"meta":35,"style":35},"df.boxplot(column='TDP', by='Cores', figsize=(12,8))\nplt.suptitle('')\nplt.ylim(0,200)\nplt.title('Thermal Design Power by Core count', fontsize=14)\nplt.xlabel('Cores', fontsize=14)\nplt.ylabel('Thermal Design Power (Watts)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/tdp_by_cores.png'))\n",[33,63908,63909,63914,63918,63923,63928,63933,63938,63942,63946],{"__ignoreMap":35},[187,63910,63911],{"class":189,"line":190},[187,63912,63913],{},"df.boxplot(column='TDP', by='Cores', figsize=(12,8))\n",[187,63915,63916],{"class":189,"line":249},[187,63917,62938],{},[187,63919,63920],{"class":189,"line":312},[187,63921,63922],{},"plt.ylim(0,200)\n",[187,63924,63925],{"class":189,"line":319},[187,63926,63927],{},"plt.title('Thermal Design Power by Core count', fontsize=14)\n",[187,63929,63930],{"class":189,"line":325},[187,63931,63932],{},"plt.xlabel('Cores', fontsize=14)\n",[187,63934,63935],{"class":189,"line":686},[187,63936,63937],{},"plt.ylabel('Thermal Design Power (Watts)', fontsize=14)\n",[187,63939,63940],{"class":189,"line":697},[187,63941,62953],{},[187,63943,63944],{"class":189,"line":1291},[187,63945,62958],{},[187,63947,63948],{"class":189,"line":1306},[187,63949,63950],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/tdp_by_cores.png'))\n",[11,63952,63953],{},[511,63954],{"alt":7255,"src":63955},"/static/pcpp/cpu/tdp_by_cores.png",[2215,63957,63959],{"id":63958},"thermal-design-power-tdp","Thermal Design Power (TDP)",[11,63961,63962,63963,358],{},"Here is a desrciption of TDP from the ",[15,63964,63967],{"href":63965,"rel":63966},"https://en.wikipedia.org/wiki/Thermal_design_power",[19],"Wikipedia article on Thermal Design Power",[107,63969,63970],{},[11,63971,63972],{},"The thermal design power (TDP), sometimes called thermal design point, is the maximum amount of heat generated by a computer chip or component (often the CPU or GPU) that the cooling system in a computer is designed to dissipate in typical operation.",[11,63974,63975],{},"This is a specification for any type of processor and it is measured in joules per second (or watts) produced by the CPU while it performs a computationally intensive task. Each point on the following scatter plot shows core clock and boost clock speeds with color representing TDP:",[26,63977,63979],{"className":1383,"code":63978,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.subplot(111)\ndf1 = df[(df.opfreq>0)&(df.turbo>0)]\nplt.scatter(df1.opfreq, df1.turbo ,c=df1.TDP, s=60, cmap='Blues')\nplt.colorbar(label='Thermal Design Power')\nplt.title('Core clock vs. Boost clock and TDP (color)', fontsize=14)\nplt.ylabel('Boost clock (MHz)', fontsize=14)\nplt.xlabel('Core clock (MHz)', fontsize=14)\nplt.axis([1500,5000,1500,5000])\n",[33,63980,63981,63985,63990,63995,64000,64005,64010,64015,64020],{"__ignoreMap":35},[187,63982,63983],{"class":189,"line":190},[187,63984,62266],{},[187,63986,63987],{"class":189,"line":249},[187,63988,63989],{},"plt.subplot(111)\n",[187,63991,63992],{"class":189,"line":312},[187,63993,63994],{},"df1 = df[(df.opfreq>0)&(df.turbo>0)]\n",[187,63996,63997],{"class":189,"line":319},[187,63998,63999],{},"plt.scatter(df1.opfreq, df1.turbo ,c=df1.TDP, s=60, cmap='Blues')\n",[187,64001,64002],{"class":189,"line":325},[187,64003,64004],{},"plt.colorbar(label='Thermal Design Power')\n",[187,64006,64007],{"class":189,"line":686},[187,64008,64009],{},"plt.title('Core clock vs. Boost clock and TDP (color)', fontsize=14)\n",[187,64011,64012],{"class":189,"line":697},[187,64013,64014],{},"plt.ylabel('Boost clock (MHz)', fontsize=14)\n",[187,64016,64017],{"class":189,"line":1291},[187,64018,64019],{},"plt.xlabel('Core clock (MHz)', fontsize=14)\n",[187,64021,64022],{"class":189,"line":1306},[187,64023,64024],{},"plt.axis([1500,5000,1500,5000])\n",[11,64026,64027],{},[511,64028],{"alt":7255,"src":64029},"/static/pcpp/cpu/core_v_boost.png",[11,64031,64032],{},"Here are two more graphs that show the relationship between clock speed, core count, TDP and price:",[26,64034,64036],{"className":1383,"code":64035,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf2 = df[(df.avg>0)&(df['Hyper-Threading']==\"Yes\")]\nplt.scatter(df2.opfreq, df2.Cores, c=df2.avg, s=df2.TDP*2, cmap='Blues')\nplt.colorbar(label='Price')\nplt.title('Frequency vs. Core count for \\n Color: Price; Size:TPD', fontsize=14)\nplt.ylabel('Cores', fontsize=14)\nplt.xlabel('Operating Frequency', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/freq_v_cores.png'))\n",[33,64037,64038,64042,64047,64052,64057,64062,64067,64072,64076,64080],{"__ignoreMap":35},[187,64039,64040],{"class":189,"line":190},[187,64041,62266],{},[187,64043,64044],{"class":189,"line":249},[187,64045,64046],{},"df2 = df[(df.avg>0)&(df['Hyper-Threading']==\"Yes\")]\n",[187,64048,64049],{"class":189,"line":312},[187,64050,64051],{},"plt.scatter(df2.opfreq, df2.Cores, c=df2.avg, s=df2.TDP*2, cmap='Blues')\n",[187,64053,64054],{"class":189,"line":319},[187,64055,64056],{},"plt.colorbar(label='Price')\n",[187,64058,64059],{"class":189,"line":325},[187,64060,64061],{},"plt.title('Frequency vs. Core count for \\n Color: Price; Size:TPD', fontsize=14)\n",[187,64063,64064],{"class":189,"line":686},[187,64065,64066],{},"plt.ylabel('Cores', fontsize=14)\n",[187,64068,64069],{"class":189,"line":697},[187,64070,64071],{},"plt.xlabel('Operating Frequency', fontsize=14)\n",[187,64073,64074],{"class":189,"line":1291},[187,64075,62953],{},[187,64077,64078],{"class":189,"line":1306},[187,64079,62958],{},[187,64081,64082],{"class":189,"line":1434},[187,64083,64084],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/freq_v_cores.png'))\n",[11,64086,64087],{},[511,64088],{"alt":7255,"src":64089},"/static/pcpp/cpu/freq_v_cores.png",[11,64091,64092],{},"The CPUs shown in the graph above are all Intel CPUs that utilize a proprietary technology called hyper-threading. Hyper-threading is a new feature on CPUs that allows them to better schedule the tasks that they do so that they can minimize the time the need to wait for information to process. Another good analogy I have heard to explain hyper-threading is eating M&Ms as fast as possible with one hand vs. two hands. Hyper-threading is like using two hands to eat M&Ms, while you are eating an M&M with your left hand, your right hand is retrieving the next M&M from the bowl. Here is the same data with price on the y-axis and core count by color:",[26,64094,64096],{"className":1383,"code":64095,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf2 = df[(df.avg>0)&(df['Hyper-Threading']==\"Yes\")]\nplt.scatter(df2.opfreq, df2.avg, c=df2.Cores, s=df2.TDP*2, cmap='Blues')\nplt.colorbar(label='Cores')\nplt.title('Frequency vs. Price and TPD (diameter)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xlabel('Operating Frequency', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/freq_v_price.png'))\n",[33,64097,64098,64102,64106,64111,64116,64121,64125,64129,64133,64137],{"__ignoreMap":35},[187,64099,64100],{"class":189,"line":190},[187,64101,62266],{},[187,64103,64104],{"class":189,"line":249},[187,64105,64046],{},[187,64107,64108],{"class":189,"line":312},[187,64109,64110],{},"plt.scatter(df2.opfreq, df2.avg, c=df2.Cores, s=df2.TDP*2, cmap='Blues')\n",[187,64112,64113],{"class":189,"line":319},[187,64114,64115],{},"plt.colorbar(label='Cores')\n",[187,64117,64118],{"class":189,"line":325},[187,64119,64120],{},"plt.title('Frequency vs. Price and TPD (diameter)', fontsize=14)\n",[187,64122,64123],{"class":189,"line":686},[187,64124,62286],{},[187,64126,64127],{"class":189,"line":697},[187,64128,64071],{},[187,64130,64131],{"class":189,"line":1291},[187,64132,62953],{},[187,64134,64135],{"class":189,"line":1306},[187,64136,62958],{},[187,64138,64139],{"class":189,"line":1434},[187,64140,64141],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/freq_v_price.png'))\n",[11,64143,64144],{},[511,64145],{"alt":7255,"src":64146},"/static/pcpp/cpu/freq_v_price.png",[11,64148,64149],{},"And finally, here is one more graph showing the difference in price between CPUs with and without hyperthreading technology:",[26,64151,64153],{"className":1383,"code":64152,"language":1125,"meta":35,"style":35},"df['Hyper_Threading'] = df['Hyper-Threading']\ndf[(df.Manufacturer=='Intel')&(df.avg>0)&((df.Cores==4)|(df.Cores==2)|(df.Cores==6)|(df.Cores==8))].groupby(['Hyper_Threading','Cores']).avg.agg(['mean', 'count']).plot(kind='bar', figsize=(12,8))\nplt.xlabel('Hyper Threading and Core combinations', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.title('Mean Price and Count for Combinations of 2, 4, 6 and 8 Core CPUs with and without Hyperthreading', fontsize=14)\nplt.legend(loc='upper left', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(fontsize=13, loc='upper left')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/hyper_threading_prices.png'))\n",[33,64154,64155,64160,64165,64170,64174,64179,64184,64188,64192,64197],{"__ignoreMap":35},[187,64156,64157],{"class":189,"line":190},[187,64158,64159],{},"df['Hyper_Threading'] = df['Hyper-Threading']\n",[187,64161,64162],{"class":189,"line":249},[187,64163,64164],{},"df[(df.Manufacturer=='Intel')&(df.avg>0)&((df.Cores==4)|(df.Cores==2)|(df.Cores==6)|(df.Cores==8))].groupby(['Hyper_Threading','Cores']).avg.agg(['mean', 'count']).plot(kind='bar', figsize=(12,8))\n",[187,64166,64167],{"class":189,"line":312},[187,64168,64169],{},"plt.xlabel('Hyper Threading and Core combinations', fontsize=14)\n",[187,64171,64172],{"class":189,"line":319},[187,64173,62286],{},[187,64175,64176],{"class":189,"line":325},[187,64177,64178],{},"plt.title('Mean Price and Count for Combinations of 2, 4, 6 and 8 Core CPUs with and without Hyperthreading', fontsize=14)\n",[187,64180,64181],{"class":189,"line":686},[187,64182,64183],{},"plt.legend(loc='upper left', fontsize=14)\n",[187,64185,64186],{"class":189,"line":697},[187,64187,62953],{},[187,64189,64190],{"class":189,"line":1291},[187,64191,62958],{},[187,64193,64194],{"class":189,"line":1306},[187,64195,64196],{},"plt.legend(fontsize=13, loc='upper left')\n",[187,64198,64199],{"class":189,"line":1434},[187,64200,64201],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu/hyper_threading_prices.png'))\n",[11,64203,64204],{},[511,64205],{"alt":7255,"src":64206},"/static/pcpp/cpu/hyper_threading_prices.png",[2215,64208,64210],{"id":64209},"cpu-cooler","CPU Cooler",[11,64212,64213],{},"To manage the small amount of heat that is generated at each clock cycle it is necessary to direct heat away from the CPU. This brings us to CPU coolers, the next major class of PC components. Cooling a CPU is achieved with large blocks of aluminum that are attached to the CPU with thermal paste. Aluminum conducts heat well, so the heat is dispersed into fins and the fins are then cooled with fans, or a liquid passes over the cooling block and is sent through a radiator which is cooled with fans.",[11,64215,64216],{},"Here's a comparison of the number of liquid vs. non-liquid CPU coolers broken down by price:",[26,64218,64220],{"className":1383,"code":64219,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.hist(df[(df.avg>0)&(df.liquid==\"No\")].avg, bins = 30, alpha=.75, label='Non-liquid Cooler')\ndf[(df.avg>0)&(df.liquid==\"Yes\")].avg.hist(bins=30, alpha=.75, label='Liquid Cooler')\nplt.legend(loc='upper right', fontsize=14)\nplt.title('Count of Liquid and Non-Liquid CPU Coolers', fontsize=14)\nplt.xlabel('Price', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/prices_hist.png'))\n",[33,64221,64222,64226,64231,64236,64241,64246,64250,64254,64258,64262],{"__ignoreMap":35},[187,64223,64224],{"class":189,"line":190},[187,64225,62266],{},[187,64227,64228],{"class":189,"line":249},[187,64229,64230],{},"plt.hist(df[(df.avg>0)&(df.liquid==\"No\")].avg, bins = 30, alpha=.75, label='Non-liquid Cooler')\n",[187,64232,64233],{"class":189,"line":312},[187,64234,64235],{},"df[(df.avg>0)&(df.liquid==\"Yes\")].avg.hist(bins=30, alpha=.75, label='Liquid Cooler')\n",[187,64237,64238],{"class":189,"line":319},[187,64239,64240],{},"plt.legend(loc='upper right', fontsize=14)\n",[187,64242,64243],{"class":189,"line":325},[187,64244,64245],{},"plt.title('Count of Liquid and Non-Liquid CPU Coolers', fontsize=14)\n",[187,64247,64248],{"class":189,"line":686},[187,64249,63044],{},[187,64251,64252],{"class":189,"line":697},[187,64253,63049],{},[187,64255,64256],{"class":189,"line":1291},[187,64257,62953],{},[187,64259,64260],{"class":189,"line":1306},[187,64261,62958],{},[187,64263,64264],{"class":189,"line":1434},[187,64265,64266],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/prices_hist.png'))\n",[11,64268,64269],{},[511,64270],{"alt":7255,"src":64271},"/static/pcpp/cpu_cooler/prices_hist.png",[11,64273,64274],{},"Liquid coolers come with radiators in five different sizes. This chart shows average prices for liquid coolrs by radiator length:",[26,64276,64279],{"className":64277,"code":64278,"language":31},[29],"df[df.liquid==\"Yes\"].groupby('rad_size').avg.agg(['mean', 'count']).sort_values('count', ascending=False).plot(kind='bar', figsize=(12,8), rot=0)\nplt.title('CPU Cooler Price and Count by radiator size', fontsize=14)\nplt.legend(loc='upper left', fontsize=14)\nplt.xlabel('Radiator Size (mm)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/rad_vs_price.png'))\n",[33,64280,64278],{"__ignoreMap":35},[11,64282,64283],{},[511,64284],{"alt":7255,"src":64285},"/static/pcpp/cpu_cooler/rad_vs_price.png",[11,64287,64288],{},"Non-liquid coolers can be quite large and bulky to allow for more heat dispersion. Here's a scatterplot showing CPU cooler height and prices:",[26,64290,64292],{"className":1383,"code":64291,"language":1125,"meta":35,"style":35},"df1 = df[(df.avg>0)&(df.cooler_height>0)]\nplt.figure(figsize=(12,8))\nplt.scatter(df1.cooler_height, df1.avg)\nplt.title('CPU Height vs. Price (Non-liquid CPU Coolers only)', fontsize=14)\nplt.xlabel('CPU Cooler Height (mm)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/cooler_height.png'))\n",[33,64293,64294,64299,64303,64308,64313,64318,64322,64326,64330],{"__ignoreMap":35},[187,64295,64296],{"class":189,"line":190},[187,64297,64298],{},"df1 = df[(df.avg>0)&(df.cooler_height>0)]\n",[187,64300,64301],{"class":189,"line":249},[187,64302,62266],{},[187,64304,64305],{"class":189,"line":312},[187,64306,64307],{},"plt.scatter(df1.cooler_height, df1.avg)\n",[187,64309,64310],{"class":189,"line":319},[187,64311,64312],{},"plt.title('CPU Height vs. Price (Non-liquid CPU Coolers only)', fontsize=14)\n",[187,64314,64315],{"class":189,"line":325},[187,64316,64317],{},"plt.xlabel('CPU Cooler Height (mm)', fontsize=14)\n",[187,64319,64320],{"class":189,"line":686},[187,64321,62286],{},[187,64323,64324],{"class":189,"line":697},[187,64325,62953],{},[187,64327,64328],{"class":189,"line":1291},[187,64329,62958],{},[187,64331,64332],{"class":189,"line":1306},[187,64333,64334],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/cooler_height.png'))\n",[11,64336,64337],{},[511,64338],{"alt":7255,"src":64339},"/static/pcpp/cpu_cooler/cooler_height.png",[11,64341,64342],{},"Here's one more graph on non-liquid coolers showing the relationship between maximum fan RPM and the maximum level of noise generated by the cooler:",[26,64344,64346],{"className":1383,"code":64345,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf1 = df[(df.liquid=='No')&(df.avg>0)]\nplt.scatter(df1.rpm_max, df1.max_noise, s=100, c = df1.avg, cmap='Blues')\nplt.colorbar(label='Price')\nplt.axis([0,7500,10,60])\nplt.xlabel('Maximum RPM', fontsize=14)\nplt.ylabel('Maximum Noise', fontsize=14)\nplt.title('Maximum RPM vs. Maximum Noise and Price (color)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/rpm_vs_noise.png'))\n",[33,64347,64348,64352,64357,64362,64366,64371,64376,64381,64386,64390,64394],{"__ignoreMap":35},[187,64349,64350],{"class":189,"line":190},[187,64351,62266],{},[187,64353,64354],{"class":189,"line":249},[187,64355,64356],{},"df1 = df[(df.liquid=='No')&(df.avg>0)]\n",[187,64358,64359],{"class":189,"line":312},[187,64360,64361],{},"plt.scatter(df1.rpm_max, df1.max_noise, s=100, c = df1.avg, cmap='Blues')\n",[187,64363,64364],{"class":189,"line":319},[187,64365,64056],{},[187,64367,64368],{"class":189,"line":325},[187,64369,64370],{},"plt.axis([0,7500,10,60])\n",[187,64372,64373],{"class":189,"line":686},[187,64374,64375],{},"plt.xlabel('Maximum RPM', fontsize=14)\n",[187,64377,64378],{"class":189,"line":697},[187,64379,64380],{},"plt.ylabel('Maximum Noise', fontsize=14)\n",[187,64382,64383],{"class":189,"line":1291},[187,64384,64385],{},"plt.title('Maximum RPM vs. Maximum Noise and Price (color)', fontsize=14)\n",[187,64387,64388],{"class":189,"line":1306},[187,64389,62953],{},[187,64391,64392],{"class":189,"line":1434},[187,64393,62958],{},[187,64395,64396],{"class":189,"line":2599},[187,64397,64398],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/cpu_cooler/rpm_vs_noise.png'))\n",[11,64400,64401],{},[511,64402],{"alt":7255,"src":64403},"/static/pcpp/cpu_cooler/rpm_vs_noise.png",[2215,64405,61644],{"id":3036},[11,64407,64408],{},"Memory is another important PC component that is particularlly important for content creators working with large video files. Memory speed and type are also important for benchmarking performed by hard-core PC gaming. PC memory is analogous to the space on your desk whereas hard drive disks are like file cabinets behind your desk. Things stored in memory can be accessed very quickly, but if you don't have enough memory then papers will start falling off of your desk and your application will crash becuase it won't be able to find what it is looking for, or won't have any space to put new information that it may need.",[11,64410,64411],{},"Here's a linear regression that captures the relationship between memory size and price:",[26,64413,64415],{"className":1383,"code":64414,"language":1125,"meta":35,"style":35},"from sklearn.linear_model import LinearRegression\nlreg = LinearRegression()\ndf1 = df[df.avg>0]\nfeat_cols = [u'size_gb']\nX = df1[feat_cols]\ny = df1.avg.reshape(df1.size_gb.shape[0],1)\nlreg.fit(X, y, sample_weight=None)\n\n#plot memory size vs. price\nplt.figure(figsize=(12,8))\nplt.scatter(df[df.avg>0].size_gb, df[df.avg>0].avg, s = 100, alpha=.05)\nplt.axis([0,130,0,1100])\nplt.title('Memory Kit prices by Size (GB)', fontsize=14)\nplt.xlabel('Memory Kit Size (GB)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\n\n#plot regression line\nsize = df1.size_gb.reshape(df1.size_gb.shape[0],1)\npred = lreg.predict(df1.size_gb.reshape(df1.size_gb.shape[0],1))\nplt.plot(size ,pred, color='red')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/size_vs_price.png'))\n",[33,64416,64417,64421,64426,64431,64436,64441,64446,64451,64455,64460,64464,64469,64474,64479,64484,64488,64492,64497,64502,64507,64512],{"__ignoreMap":35},[187,64418,64419],{"class":189,"line":190},[187,64420,62846],{},[187,64422,64423],{"class":189,"line":249},[187,64424,64425],{},"lreg = LinearRegression()\n",[187,64427,64428],{"class":189,"line":312},[187,64429,64430],{},"df1 = df[df.avg>0]\n",[187,64432,64433],{"class":189,"line":319},[187,64434,64435],{},"feat_cols = [u'size_gb']\n",[187,64437,64438],{"class":189,"line":325},[187,64439,64440],{},"X = df1[feat_cols]\n",[187,64442,64443],{"class":189,"line":686},[187,64444,64445],{},"y = df1.avg.reshape(df1.size_gb.shape[0],1)\n",[187,64447,64448],{"class":189,"line":697},[187,64449,64450],{},"lreg.fit(X, y, sample_weight=None)\n",[187,64452,64453],{"class":189,"line":1291},[187,64454,316],{"emptyLinePlaceholder":315},[187,64456,64457],{"class":189,"line":1306},[187,64458,64459],{},"#plot memory size vs. price\n",[187,64461,64462],{"class":189,"line":1434},[187,64463,62266],{},[187,64465,64466],{"class":189,"line":2599},[187,64467,64468],{},"plt.scatter(df[df.avg>0].size_gb, df[df.avg>0].avg, s = 100, alpha=.05)\n",[187,64470,64471],{"class":189,"line":2607},[187,64472,64473],{},"plt.axis([0,130,0,1100])\n",[187,64475,64476],{"class":189,"line":2621},[187,64477,64478],{},"plt.title('Memory Kit prices by Size (GB)', fontsize=14)\n",[187,64480,64481],{"class":189,"line":2631},[187,64482,64483],{},"plt.xlabel('Memory Kit Size (GB)', fontsize=14)\n",[187,64485,64486],{"class":189,"line":2642},[187,64487,62286],{},[187,64489,64490],{"class":189,"line":2653},[187,64491,316],{"emptyLinePlaceholder":315},[187,64493,64494],{"class":189,"line":2665},[187,64495,64496],{},"#plot regression line\n",[187,64498,64499],{"class":189,"line":2674},[187,64500,64501],{},"size = df1.size_gb.reshape(df1.size_gb.shape[0],1)\n",[187,64503,64504],{"class":189,"line":2684},[187,64505,64506],{},"pred = lreg.predict(df1.size_gb.reshape(df1.size_gb.shape[0],1))\n",[187,64508,64509],{"class":189,"line":2694},[187,64510,64511],{},"plt.plot(size ,pred, color='red')\n",[187,64513,64514],{"class":189,"line":2706},[187,64515,64516],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/size_vs_price.png'))\n",[11,64518,64519],{},[511,64520],{"alt":7255,"src":64521},"/static/pcpp/memory/size_vs_price.png",[11,64523,64524],{},"The R^2 value from the regression is 0.767, which means that 76 percent of the variation in the data is explained by the model. The variation might come from the fact that there are two important features that differentiate regular memory from enthusiast-grade memory: memory speed and CAS.",[11,64526,64527],{},"Memory module speed is measured in megatransfers per second (MT/s). Here is a graph of memory speeds vs. prices:",[26,64529,64531],{"className":1383,"code":64530,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\na=.5\ns=75\n\nddr2_speed = df.ddr_speed[(df.ddr_type == 'DDR2')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nddr2_ppgb = df.ppgb[(df.ddr_type == 'DDR2')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nplt.scatter(ddr2_speed,ddr2_ppgb,alpha = a, c='red', s=s)\n\nddr3_speed = df.ddr_speed[(df.ddr_type == 'DDR3')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nddr3_ppgb = df.ppgb[(df.ddr_type == 'DDR3')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nplt.scatter(ddr3_speed,ddr3_ppgb,alpha = a, c='yellow', s=s)\n\nddr4_speed = df.ddr_speed[(df.ddr_type == 'DDR4')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nddr4_ppgb = df.ppgb[(df.ddr_type == 'DDR4')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\nplt.scatter(ddr4_speed,ddr4_ppgb,alpha = a, c='green', s=s)\n\nplt.axis([500,4000,0,25])\nplt.xlabel('Memory Speed (MT/s)', fontsize=14)\nplt.ylabel('Price per GB', fontsize=14)\nplt.title(\"Memory Speed vs. Price\", fontsize=14)\nplt.legend(['DDR2', 'DDR3', 'DDR4'], loc='lower right', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/speed_vs_price.png'))\n",[33,64532,64533,64537,64542,64547,64551,64556,64561,64566,64570,64575,64580,64585,64589,64594,64599,64604,64608,64613,64618,64623,64628,64633,64637,64641],{"__ignoreMap":35},[187,64534,64535],{"class":189,"line":190},[187,64536,62266],{},[187,64538,64539],{"class":189,"line":249},[187,64540,64541],{},"a=.5\n",[187,64543,64544],{"class":189,"line":312},[187,64545,64546],{},"s=75\n",[187,64548,64549],{"class":189,"line":319},[187,64550,316],{"emptyLinePlaceholder":315},[187,64552,64553],{"class":189,"line":325},[187,64554,64555],{},"ddr2_speed = df.ddr_speed[(df.ddr_type == 'DDR2')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64557,64558],{"class":189,"line":686},[187,64559,64560],{},"ddr2_ppgb = df.ppgb[(df.ddr_type == 'DDR2')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64562,64563],{"class":189,"line":697},[187,64564,64565],{},"plt.scatter(ddr2_speed,ddr2_ppgb,alpha = a, c='red', s=s)\n",[187,64567,64568],{"class":189,"line":1291},[187,64569,316],{"emptyLinePlaceholder":315},[187,64571,64572],{"class":189,"line":1306},[187,64573,64574],{},"ddr3_speed = df.ddr_speed[(df.ddr_type == 'DDR3')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64576,64577],{"class":189,"line":1434},[187,64578,64579],{},"ddr3_ppgb = df.ppgb[(df.ddr_type == 'DDR3')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64581,64582],{"class":189,"line":2599},[187,64583,64584],{},"plt.scatter(ddr3_speed,ddr3_ppgb,alpha = a, c='yellow', s=s)\n",[187,64586,64587],{"class":189,"line":2607},[187,64588,316],{"emptyLinePlaceholder":315},[187,64590,64591],{"class":189,"line":2621},[187,64592,64593],{},"ddr4_speed = df.ddr_speed[(df.ddr_type == 'DDR4')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64595,64596],{"class":189,"line":2631},[187,64597,64598],{},"ddr4_ppgb = df.ppgb[(df.ddr_type == 'DDR4')&(df.size_gb==8)&(df.ddr_speed>0)&(df.avg>0)]\n",[187,64600,64601],{"class":189,"line":2642},[187,64602,64603],{},"plt.scatter(ddr4_speed,ddr4_ppgb,alpha = a, c='green', s=s)\n",[187,64605,64606],{"class":189,"line":2653},[187,64607,316],{"emptyLinePlaceholder":315},[187,64609,64610],{"class":189,"line":2665},[187,64611,64612],{},"plt.axis([500,4000,0,25])\n",[187,64614,64615],{"class":189,"line":2674},[187,64616,64617],{},"plt.xlabel('Memory Speed (MT/s)', fontsize=14)\n",[187,64619,64620],{"class":189,"line":2684},[187,64621,64622],{},"plt.ylabel('Price per GB', fontsize=14)\n",[187,64624,64625],{"class":189,"line":2694},[187,64626,64627],{},"plt.title(\"Memory Speed vs. Price\", fontsize=14)\n",[187,64629,64630],{"class":189,"line":2706},[187,64631,64632],{},"plt.legend(['DDR2', 'DDR3', 'DDR4'], loc='lower right', fontsize=14)\n",[187,64634,64635],{"class":189,"line":2715},[187,64636,62953],{},[187,64638,64639],{"class":189,"line":2725},[187,64640,62958],{},[187,64642,64643],{"class":189,"line":2735},[187,64644,64645],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/speed_vs_price.png'))\n",[11,64647,64648],{},[511,64649],{"alt":7255,"src":64650},"/static/pcpp/memory/speed_vs_price.png",[11,64652,64653,64654,1737],{},"CAS stands for column access strobe (CAS) latency, and it is the delay time between the moment a memory controller tells the memory module to access a particular memory column on a RAM module, and the moment the data from the given array location is available on the module's output pins (from the ",[15,64655,64658],{"href":64656,"rel":64657},"https://en.wikipedia.org/wiki/CAS_latency",[19],"Wikipedia article on CAS latency",[11,64660,64661],{},"On synchronous dynamic random-access memory modules like the ones used in modern PCs, CAS is measured in clock cycles and ranges from 4 to 20. Here's a look at memory speeds and their CAS latency:",[26,64663,64665],{"className":1383,"code":64664,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ns = 100\na = 0.1\n\n#plot ddr3\nddr3_cas = df.CAS[(df.is_ddr4 == False)&(df.avg>0)]\nddr3_speed = df.ddr_speed[(df.is_ddr4 == False)&(df.avg>0)]\nplt.scatter(ddr3_cas,ddr3_speed, c = 'blue', s=s, alpha=a)\n\n#plot ddr4\nddr4_cas = df.CAS[(df.is_ddr4 == True)&(df.avg>0)]\nddr4_speed = df.ddr_speed[(df.is_ddr4 == True)&(df.avg>0)]\nplt.scatter(ddr4_cas,ddr4_speed, c = 'red', s = s, alpha=a)\n\nplt.xlabel('CAS', fontsize=14)\nplt.ylabel('Speed (MHz)', fontsize=14)\nplt.title(\"CAS vs. Clock Speed\", fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(['DDR3', 'DDR4'], loc='upper left', fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/cas_vs_speed.png'))\n",[33,64666,64667,64671,64676,64681,64685,64690,64695,64700,64705,64709,64714,64719,64724,64729,64733,64738,64743,64748,64752,64756,64761],{"__ignoreMap":35},[187,64668,64669],{"class":189,"line":190},[187,64670,62266],{},[187,64672,64673],{"class":189,"line":249},[187,64674,64675],{},"s = 100\n",[187,64677,64678],{"class":189,"line":312},[187,64679,64680],{},"a = 0.1\n",[187,64682,64683],{"class":189,"line":319},[187,64684,316],{"emptyLinePlaceholder":315},[187,64686,64687],{"class":189,"line":325},[187,64688,64689],{},"#plot ddr3\n",[187,64691,64692],{"class":189,"line":686},[187,64693,64694],{},"ddr3_cas = df.CAS[(df.is_ddr4 == False)&(df.avg>0)]\n",[187,64696,64697],{"class":189,"line":697},[187,64698,64699],{},"ddr3_speed = df.ddr_speed[(df.is_ddr4 == False)&(df.avg>0)]\n",[187,64701,64702],{"class":189,"line":1291},[187,64703,64704],{},"plt.scatter(ddr3_cas,ddr3_speed, c = 'blue', s=s, alpha=a)\n",[187,64706,64707],{"class":189,"line":1306},[187,64708,316],{"emptyLinePlaceholder":315},[187,64710,64711],{"class":189,"line":1434},[187,64712,64713],{},"#plot ddr4\n",[187,64715,64716],{"class":189,"line":2599},[187,64717,64718],{},"ddr4_cas = df.CAS[(df.is_ddr4 == True)&(df.avg>0)]\n",[187,64720,64721],{"class":189,"line":2607},[187,64722,64723],{},"ddr4_speed = df.ddr_speed[(df.is_ddr4 == True)&(df.avg>0)]\n",[187,64725,64726],{"class":189,"line":2621},[187,64727,64728],{},"plt.scatter(ddr4_cas,ddr4_speed, c = 'red', s = s, alpha=a)\n",[187,64730,64731],{"class":189,"line":2631},[187,64732,316],{"emptyLinePlaceholder":315},[187,64734,64735],{"class":189,"line":2642},[187,64736,64737],{},"plt.xlabel('CAS', fontsize=14)\n",[187,64739,64740],{"class":189,"line":2653},[187,64741,64742],{},"plt.ylabel('Speed (MHz)', fontsize=14)\n",[187,64744,64745],{"class":189,"line":2665},[187,64746,64747],{},"plt.title(\"CAS vs. Clock Speed\", fontsize=14)\n",[187,64749,64750],{"class":189,"line":2674},[187,64751,62953],{},[187,64753,64754],{"class":189,"line":2684},[187,64755,62958],{},[187,64757,64758],{"class":189,"line":2694},[187,64759,64760],{},"plt.legend(['DDR3', 'DDR4'], loc='upper left', fontsize=13)\n",[187,64762,64763],{"class":189,"line":2706},[187,64764,64765],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/cas_vs_speed.png'))\n",[11,64767,64768],{},[511,64769],{"alt":7255,"src":64770},"/static/pcpp/memory/cas_vs_speed.png",[11,64772,64773],{},"It's a misconception that faster memory has higher latency, because CAS is not actually a good representation of memory's latency. This is because it is measured in clock cycles, which become smaller as the memory clock speed increases. Here's how the math for memory and true latency works out:",[26,64775,64778],{"className":64776,"code":64777,"language":31,"meta":35},[29],"true latency (nanoseconds) = clock cycle time (nanoseconds) x CAS (clock cycles)\n",[33,64779,64777],{"__ignoreMap":35},[11,64781,64782],{},"So here's how we can calculate true latency using the data in the dataset:",[26,64784,64787],{"className":64785,"code":64786,"language":31,"meta":35},[29],"#filter for DDR4 memory with valid CAS\ndf_lat = df[(df.CAS>0)&(df.ddr_speed>0)&(df.ddr_type=='DDR4')]\ndf_lat['true_latency'] = [((x/2.)/1000.)*y for x, y in zip(df_lat.ddr_speed, df_lat.CAS)]\n",[33,64788,64786],{"__ignoreMap":35},[11,64790,64791],{},"Let's calculate the true latency of two different memory modules: DDR4-2666/CAS18 and DDR3-1333/CAS9.",[11,64793,64794,64795,64798],{},"The 'DDR' in DDR memory stands for ",[4339,64796,64797],{},"double data rate",", which means that information is transfared twice per clock cycle. Because of this, we must first take the MT/s rate and divide by 2 to calculate the clock speed. Next we need to find how long each clock cycle takes. To find this amount of time we can divide 1 by the frequency. Finally we multiply the time of one clock cycle by the CAS (the latency in number cycles) to get the latency in seconds (nanoseconds):",[26,64800,64803],{"className":64801,"code":64802,"language":31,"meta":35},[29],"DDR4-2666\n2666 MT/s\n2666000000 T/s\n1333000000 Hz\n1/(1333000000 Hz)\n0.00000000075 s\n0.75 nanoseconds = 1 clock cycle\n\ntrue latency = clock cycle x CAS\ntrue latency = 0.75 ns x 18\ntrue latency = 13.5 nanoseconds\n\nDDR3-1333\n1333 MT/s\n1333000000 T/s\n666500000 Hz\n1/(666500000 Hz)\n0.0000000015 seconds\n1.5 nanoseconds = 1 clock cycle\n\ntrue latency = clock cycle x CAS\ntrue latency = 1.5 ns x 9\ntrue latency = 13.5 nanoseconds\n",[33,64804,64802],{"__ignoreMap":35},[11,64806,64807,64808,64812],{},"Here's an ",[15,64809,6974],{"href":64810,"rel":64811},"http://www.crucial.com/usa/en/memory-performance-speed-latency",[19]," from Crucial, a major memory manufacturer, that explains this idea fully. The main conclusion is that speed is more important than memory, but the two measurements must be considered together.",[11,64814,64815,64816,64819],{},"The equation for ",[33,64817,64818],{},"true_latency"," above divides half the DDR MT/s rate by 1000 to both multiply by 10^6 (for converting MT/s to T/s) and divide by 10^9 (for converting seconds to nano seconds).",[11,64821,64822],{},"Here's a histogram of true latency for DDR4 memory modules:",[26,64824,64826],{"className":1383,"code":64825,"language":1125,"meta":35,"style":35},"df_lat = df[(df.CAS>0)&(df.ddr_speed>0)&((df.ddr_type=='DDR4')|(df.ddr_type=='DDR3'))]\ndf_lat['true_latency'] = [((x/2.)/1000.)*y for x, y in zip(df_lat.ddr_speed, df_lat.CAS)]\ndf_lat.true_latency.hist(bins=25, figsize=(12,8))\nplt.xlabel('True Latency (nanoseconds)', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title(\"Histogram for True Latency of DDR3 and DDR4 Memory modules\", fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/true_latency.png'))\n",[33,64827,64828,64833,64838,64843,64848,64852,64857,64861,64865],{"__ignoreMap":35},[187,64829,64830],{"class":189,"line":190},[187,64831,64832],{},"df_lat = df[(df.CAS>0)&(df.ddr_speed>0)&((df.ddr_type=='DDR4')|(df.ddr_type=='DDR3'))]\n",[187,64834,64835],{"class":189,"line":249},[187,64836,64837],{},"df_lat['true_latency'] = [((x/2.)/1000.)*y for x, y in zip(df_lat.ddr_speed, df_lat.CAS)]\n",[187,64839,64840],{"class":189,"line":312},[187,64841,64842],{},"df_lat.true_latency.hist(bins=25, figsize=(12,8))\n",[187,64844,64845],{"class":189,"line":319},[187,64846,64847],{},"plt.xlabel('True Latency (nanoseconds)', fontsize=14)\n",[187,64849,64850],{"class":189,"line":325},[187,64851,63049],{},[187,64853,64854],{"class":189,"line":686},[187,64855,64856],{},"plt.title(\"Histogram for True Latency of DDR3 and DDR4 Memory modules\", fontsize=14)\n",[187,64858,64859],{"class":189,"line":697},[187,64860,62953],{},[187,64862,64863],{"class":189,"line":1291},[187,64864,62958],{},[187,64866,64867],{"class":189,"line":1306},[187,64868,64869],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/true_latency.png'))\n",[11,64871,64872],{},[511,64873],{"alt":7255,"src":64874},"/static/pcpp/memory/true_latency.png",[11,64876,64877],{},"Here's an graph from the Crucial article showing speed vs true latency:",[11,64879,64880],{},[511,64881],{"alt":7255,"src":64882},"/static/pcpp/memory/crucial_latency.png",[11,64884,64885],{},"And here is a similar graph from our dataset:",[26,64887,64889],{"className":1383,"code":64888,"language":1125,"meta":35,"style":35},"df_lat = df[(df.CAS>0)&(df.ddr_speed>0)&((df.ddr_type=='DDR4')|(df.ddr_type=='DDR3'))]\ndf_lat['true_latency'] = [((x/2.)/1000.)*y for x, y in zip(df_lat.ddr_speed, df_lat.CAS)]\nplt.figure(figsize=(12,8))\nplt.scatter(df_lat.ddr_speed, df_lat.true_latency)\nplt.xlabel('Memory Speed (MT/s)', fontsize=14)\nplt.ylabel('True Latency (nanoseconds)', fontsize=14)\nplt.title('Memory Speed vs. True Latency (nanoseconds) for DDR3 and DDR4 memory', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/speed_vs_true_latency.png'))\n",[33,64890,64891,64895,64899,64903,64908,64912,64917,64922,64926,64930],{"__ignoreMap":35},[187,64892,64893],{"class":189,"line":190},[187,64894,64832],{},[187,64896,64897],{"class":189,"line":249},[187,64898,64837],{},[187,64900,64901],{"class":189,"line":312},[187,64902,62266],{},[187,64904,64905],{"class":189,"line":319},[187,64906,64907],{},"plt.scatter(df_lat.ddr_speed, df_lat.true_latency)\n",[187,64909,64910],{"class":189,"line":325},[187,64911,64617],{},[187,64913,64914],{"class":189,"line":686},[187,64915,64916],{},"plt.ylabel('True Latency (nanoseconds)', fontsize=14)\n",[187,64918,64919],{"class":189,"line":697},[187,64920,64921],{},"plt.title('Memory Speed vs. True Latency (nanoseconds) for DDR3 and DDR4 memory', fontsize=14)\n",[187,64923,64924],{"class":189,"line":1291},[187,64925,62953],{},[187,64927,64928],{"class":189,"line":1306},[187,64929,62958],{},[187,64931,64932],{"class":189,"line":1434},[187,64933,64934],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/speed_vs_true_latency.png'))\n",[11,64936,64937],{},[511,64938],{"alt":7255,"src":64939},"/static/pcpp/memory/speed_vs_true_latency.png",[11,64941,64942],{},"Here's another look at memory prices, showing memory module prices and price per GB of memory:",[26,64944,64946],{"className":1383,"code":64945,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ns_8 = df1[df1.size_gb==8]\ns_16 = df1[df1.size_gb==16]\ns_32 = df1[df1.size_gb==32]\ns_4 = df1[df1.size_gb==4]\ns_64 = df1[df1.size_gb==64]\ns_2 = df1[df1.size_gb==2]\n\nplt.scatter(s_2.avg, s_2.ppgb, c='orange', s=50)\nplt.scatter(s_4.avg, s_4.ppgb, c='red', s=50)\nplt.scatter(s_8.avg, s_8.ppgb, c='black', s=50)\nplt.scatter(s_16.avg, s_16.ppgb, c='green', s=50)\nplt.scatter(s_32.avg, s_32.ppgb, c='blue', s=50)\nplt.scatter(s_64.avg, s_64.ppgb, c='yellow', s=50)\n\n\nplt.axis([0,600,0,30])\nplt.title('Memory Kit Price vs. Price per GB', fontsize=14)\nplt.xlabel('Memory Kit Price', fontsize=14)\nplt.ylabel('Memory Kit Price per GB', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(['2GB','4GB', '8GB', '16GB', '32GB', '64GB'], fontsize=14)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/price_vs_ppgb.png'))\n",[33,64947,64948,64952,64957,64962,64967,64972,64977,64982,64986,64991,64996,65001,65006,65011,65016,65020,65024,65029,65034,65039,65044,65048,65052,65057],{"__ignoreMap":35},[187,64949,64950],{"class":189,"line":190},[187,64951,62266],{},[187,64953,64954],{"class":189,"line":249},[187,64955,64956],{},"s_8 = df1[df1.size_gb==8]\n",[187,64958,64959],{"class":189,"line":312},[187,64960,64961],{},"s_16 = df1[df1.size_gb==16]\n",[187,64963,64964],{"class":189,"line":319},[187,64965,64966],{},"s_32 = df1[df1.size_gb==32]\n",[187,64968,64969],{"class":189,"line":325},[187,64970,64971],{},"s_4 = df1[df1.size_gb==4]\n",[187,64973,64974],{"class":189,"line":686},[187,64975,64976],{},"s_64 = df1[df1.size_gb==64]\n",[187,64978,64979],{"class":189,"line":697},[187,64980,64981],{},"s_2 = df1[df1.size_gb==2]\n",[187,64983,64984],{"class":189,"line":1291},[187,64985,316],{"emptyLinePlaceholder":315},[187,64987,64988],{"class":189,"line":1306},[187,64989,64990],{},"plt.scatter(s_2.avg, s_2.ppgb, c='orange', s=50)\n",[187,64992,64993],{"class":189,"line":1434},[187,64994,64995],{},"plt.scatter(s_4.avg, s_4.ppgb, c='red', s=50)\n",[187,64997,64998],{"class":189,"line":2599},[187,64999,65000],{},"plt.scatter(s_8.avg, s_8.ppgb, c='black', s=50)\n",[187,65002,65003],{"class":189,"line":2607},[187,65004,65005],{},"plt.scatter(s_16.avg, s_16.ppgb, c='green', s=50)\n",[187,65007,65008],{"class":189,"line":2621},[187,65009,65010],{},"plt.scatter(s_32.avg, s_32.ppgb, c='blue', s=50)\n",[187,65012,65013],{"class":189,"line":2631},[187,65014,65015],{},"plt.scatter(s_64.avg, s_64.ppgb, c='yellow', s=50)\n",[187,65017,65018],{"class":189,"line":2642},[187,65019,316],{"emptyLinePlaceholder":315},[187,65021,65022],{"class":189,"line":2653},[187,65023,316],{"emptyLinePlaceholder":315},[187,65025,65026],{"class":189,"line":2665},[187,65027,65028],{},"plt.axis([0,600,0,30])\n",[187,65030,65031],{"class":189,"line":2674},[187,65032,65033],{},"plt.title('Memory Kit Price vs. Price per GB', fontsize=14)\n",[187,65035,65036],{"class":189,"line":2684},[187,65037,65038],{},"plt.xlabel('Memory Kit Price', fontsize=14)\n",[187,65040,65041],{"class":189,"line":2694},[187,65042,65043],{},"plt.ylabel('Memory Kit Price per GB', fontsize=14)\n",[187,65045,65046],{"class":189,"line":2706},[187,65047,62953],{},[187,65049,65050],{"class":189,"line":2715},[187,65051,62958],{},[187,65053,65054],{"class":189,"line":2725},[187,65055,65056],{},"plt.legend(['2GB','4GB', '8GB', '16GB', '32GB', '64GB'], fontsize=14)\n",[187,65058,65059],{"class":189,"line":2735},[187,65060,65061],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/price_vs_ppgb.png'))\n",[11,65063,65064],{},[511,65065],{"alt":7255,"src":65066},"/static/pcpp/memory/price_vs_ppgb.png",[11,65068,65069],{},"This graph shows the variation in price for the most popular memrory kit sizes, and it also shows the variation in pricing data. The price per GB corresponds to one price (which is usually the lowest listed price) and the price is the average of all vendors' prices.",[11,65071,65072],{},"One more thing to note about DDR3 and DDR4 is that DDR4 requires lower voltage:",[26,65074,65076],{"className":1383,"code":65075,"language":1125,"meta":35,"style":35},"df3 = df[(df.voltage>0)]\ndf.boxplot(column='voltage', by='ddr_type', figsize=(12, 8))\nplt.suptitle('')\nplt.title('Voltage for DDR2, DDR3 and DDR4', fontsize=14)\nplt.ylim(1,2)\nplt.xlabel('Memory Type', fontsize=14)\nplt.ylabel('Voltage', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/voltage.png'))\n",[33,65077,65078,65083,65088,65092,65097,65102,65107,65112,65116,65120],{"__ignoreMap":35},[187,65079,65080],{"class":189,"line":190},[187,65081,65082],{},"df3 = df[(df.voltage>0)]\n",[187,65084,65085],{"class":189,"line":249},[187,65086,65087],{},"df.boxplot(column='voltage', by='ddr_type', figsize=(12, 8))\n",[187,65089,65090],{"class":189,"line":312},[187,65091,62938],{},[187,65093,65094],{"class":189,"line":319},[187,65095,65096],{},"plt.title('Voltage for DDR2, DDR3 and DDR4', fontsize=14)\n",[187,65098,65099],{"class":189,"line":325},[187,65100,65101],{},"plt.ylim(1,2)\n",[187,65103,65104],{"class":189,"line":686},[187,65105,65106],{},"plt.xlabel('Memory Type', fontsize=14)\n",[187,65108,65109],{"class":189,"line":697},[187,65110,65111],{},"plt.ylabel('Voltage', fontsize=14)\n",[187,65113,65114],{"class":189,"line":1291},[187,65115,62953],{},[187,65117,65118],{"class":189,"line":1306},[187,65119,62958],{},[187,65121,65122],{"class":189,"line":1434},[187,65123,65124],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/memory/voltage.png'))\n",[11,65126,65127],{},[511,65128],{"alt":7255,"src":65129},"/static/pcpp/memory/voltage.png",[2215,65131,65133],{"id":65132},"video-card-graphics-card-gpu","Video Card / Graphics Card / GPU",[11,65135,65136],{},"Video cards are often the single biggest expense for high-end PCs. They deliver parallel computing performance that is necessary for modern applications like 4K gaming, virtual reality and deep learning. Like the CPU market, the GPU market is dominated by two major players: NVIDIA and AMD. These companies produce graphical processing units, but there are a number of vendors who sell graphics cards using the core chipsets provided by NVIDIA and AMD (and these two companies also sell their own consumer products).",[11,65138,65139],{},"GPUs are attached to the motherboard by PC expansion slots as well as the rear of the case where their display connections are exposed. One GPU metric is the number of DisplayPort type connections. GPUs that support multi-screen setups have higher performance and therefor higher cost, and in general the more screens it can support the more costly the card will be. Here's a breakdown of prices by the number of DisplayPort connections:",[26,65141,65143],{"className":1383,"code":65142,"language":1125,"meta":35,"style":35},"df['DisplayPort_count'] = df['DisplayPort'].fillna(0)\ndf[(df.avg>0)].boxplot(column='avg', by='DisplayPort_count', figsize=(12,8))\nplt.ylim(0,2000)\nplt.suptitle('')\nplt.title('GPU Prices by Display Port Connections', fontsize=14)\nplt.xlabel('DisplayPort Connections', fontsize=14)\nplt.ylabel('Prices', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/prices_by_display.png'))\n",[33,65144,65145,65150,65155,65160,65164,65169,65174,65178,65182,65186],{"__ignoreMap":35},[187,65146,65147],{"class":189,"line":190},[187,65148,65149],{},"df['DisplayPort_count'] = df['DisplayPort'].fillna(0)\n",[187,65151,65152],{"class":189,"line":249},[187,65153,65154],{},"df[(df.avg>0)].boxplot(column='avg', by='DisplayPort_count', figsize=(12,8))\n",[187,65156,65157],{"class":189,"line":312},[187,65158,65159],{},"plt.ylim(0,2000)\n",[187,65161,65162],{"class":189,"line":319},[187,65163,62938],{},[187,65165,65166],{"class":189,"line":325},[187,65167,65168],{},"plt.title('GPU Prices by Display Port Connections', fontsize=14)\n",[187,65170,65171],{"class":189,"line":686},[187,65172,65173],{},"plt.xlabel('DisplayPort Connections', fontsize=14)\n",[187,65175,65176],{"class":189,"line":697},[187,65177,63882],{},[187,65179,65180],{"class":189,"line":1291},[187,65181,62953],{},[187,65183,65184],{"class":189,"line":1306},[187,65185,62958],{},[187,65187,65188],{"class":189,"line":1434},[187,65189,65190],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/prices_by_display.png'))\n",[11,65192,65193],{},[511,65194],{"alt":7255,"src":65195},"/static/pcpp/gpu/prices_by_display.png",[11,65197,65198],{},"GPUs also sometimes the largest components, here's a histogram for the height of all GPUs:",[26,65200,65202],{"className":1383,"code":65201,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf[df.gpu_length>0].gpu_length.hist(bins=25)\nplt.title('GPU Length', fontsize=14)\nplt.xlabel('Length (inches)', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_hist.png'))\n",[33,65203,65204,65208,65213,65218,65223,65227,65231,65235],{"__ignoreMap":35},[187,65205,65206],{"class":189,"line":190},[187,65207,62266],{},[187,65209,65210],{"class":189,"line":249},[187,65211,65212],{},"df[df.gpu_length>0].gpu_length.hist(bins=25)\n",[187,65214,65215],{"class":189,"line":312},[187,65216,65217],{},"plt.title('GPU Length', fontsize=14)\n",[187,65219,65220],{"class":189,"line":319},[187,65221,65222],{},"plt.xlabel('Length (inches)', fontsize=14)\n",[187,65224,65225],{"class":189,"line":325},[187,65226,63049],{},[187,65228,65229],{"class":189,"line":686},[187,65230,62953],{},[187,65232,65233],{"class":189,"line":697},[187,65234,62958],{},[187,65236,65237],{"class":189,"line":1291},[187,65238,65239],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_hist.png'))\n",[11,65241,65242],{},[511,65243],{"alt":7255,"src":65244},"/static/pcpp/gpu/length_hist.png",[11,65246,65247],{},"Length is an interesting feature, because you can pack more GPUs into a graphics card that is larger, and you can also have more fans and cooling equipment on larger cards. Here's a look at the relationship between video card length, price, memory and clock speed:",[26,65249,65251],{"className":1383,"code":65250,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf_len = df[(df.avg>0)&(df.gpu_length>0)]\nplt.scatter(df_len.gpu_length, df_len.avg, c=df_len.memory_mb, cmap='CMRmap', s=df_len.clock_speed_in_mhz/4)\nplt.colorbar(label='Memory (MB)')\nplt.axis([5,14,0,3300])\nplt.title('GPU Length vs. Price, Memory (color) and Clockspeed (diameter)', fontsize=14)\nplt.xlabel('Graphics Card Length (inches)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_vs_price.png'))\n",[33,65252,65253,65257,65262,65267,65272,65277,65282,65287,65291,65295,65299],{"__ignoreMap":35},[187,65254,65255],{"class":189,"line":190},[187,65256,62266],{},[187,65258,65259],{"class":189,"line":249},[187,65260,65261],{},"df_len = df[(df.avg>0)&(df.gpu_length>0)]\n",[187,65263,65264],{"class":189,"line":312},[187,65265,65266],{},"plt.scatter(df_len.gpu_length, df_len.avg, c=df_len.memory_mb, cmap='CMRmap', s=df_len.clock_speed_in_mhz/4)\n",[187,65268,65269],{"class":189,"line":319},[187,65270,65271],{},"plt.colorbar(label='Memory (MB)')\n",[187,65273,65274],{"class":189,"line":325},[187,65275,65276],{},"plt.axis([5,14,0,3300])\n",[187,65278,65279],{"class":189,"line":686},[187,65280,65281],{},"plt.title('GPU Length vs. Price, Memory (color) and Clockspeed (diameter)', fontsize=14)\n",[187,65283,65284],{"class":189,"line":697},[187,65285,65286],{},"plt.xlabel('Graphics Card Length (inches)', fontsize=14)\n",[187,65288,65289],{"class":189,"line":1291},[187,65290,62286],{},[187,65292,65293],{"class":189,"line":1306},[187,65294,62953],{},[187,65296,65297],{"class":189,"line":1434},[187,65298,62958],{},[187,65300,65301],{"class":189,"line":2599},[187,65302,65303],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_vs_price.png'))\n",[11,65305,65306],{},[511,65307],{"alt":7255,"src":65308},"/static/pcpp/gpu/length_vs_price.png",[11,65310,65311],{},"And here is another look at the same data, filtered for graphics cards under $1,100:",[26,65313,65315],{"className":1383,"code":65314,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\ndf_len = df[(df.avg>0)&(df.gpu_length>0)&(df.memory_mb\u003C12000)]\nplt.scatter(df_len.gpu_length, df_len.avg, c=df_len.memory_mb, cmap='CMRmap', s=df_len.clock_speed_in_mhz/4)\nplt.colorbar(label='Memory (MB)')\nplt.axis([5,14,0,1100])\nplt.title('GPU Length vs. Price, Memory and Clockspeed (diameter)', fontsize=14)\nplt.xlabel('Graphics Card Length (inches)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_vs_price_2.png'))\n",[33,65316,65317,65321,65326,65330,65334,65339,65344,65348,65352,65356,65360],{"__ignoreMap":35},[187,65318,65319],{"class":189,"line":190},[187,65320,62266],{},[187,65322,65323],{"class":189,"line":249},[187,65324,65325],{},"df_len = df[(df.avg>0)&(df.gpu_length>0)&(df.memory_mb\u003C12000)]\n",[187,65327,65328],{"class":189,"line":312},[187,65329,65266],{},[187,65331,65332],{"class":189,"line":319},[187,65333,65271],{},[187,65335,65336],{"class":189,"line":325},[187,65337,65338],{},"plt.axis([5,14,0,1100])\n",[187,65340,65341],{"class":189,"line":686},[187,65342,65343],{},"plt.title('GPU Length vs. Price, Memory and Clockspeed (diameter)', fontsize=14)\n",[187,65345,65346],{"class":189,"line":697},[187,65347,65286],{},[187,65349,65350],{"class":189,"line":1291},[187,65351,62286],{},[187,65353,65354],{"class":189,"line":1306},[187,65355,62953],{},[187,65357,65358],{"class":189,"line":1434},[187,65359,62958],{},[187,65361,65362],{"class":189,"line":2599},[187,65363,65364],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/length_vs_price_2.png'))\n",[11,65366,65367],{},[511,65368],{"alt":7255,"src":65369},"/static/pcpp/gpu/length_vs_price_2.png",[11,65371,65372],{},"GPUs are also measured in thermal design power (TDP) that we previously examined in CPUs. This scatterplot shows the relationship between TDP, Price, clockspeed and memory:",[26,65374,65376],{"className":1383,"code":65375,"language":1125,"meta":35,"style":35},"df1 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)]\nplt.figure(figsize=(12,8))\nplt.scatter(df1.tdp, df1.avg, c=df1.clock_speed_in_mhz, s=df1.memory_mb/10, cmap='Blues')\nplt.colorbar(label='Clock Speed (MHz)')\nplt.axis([0,350,0,1000])\nplt.title('GPU TDP vs. Price, Clock Speed (color) and Memory (diameter)', fontsize=14)\nplt.xlabel('Thermal Design Power (TDP)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/tdp_vs_price.png'))\n",[33,65377,65378,65383,65387,65392,65397,65402,65407,65412,65416,65420,65424],{"__ignoreMap":35},[187,65379,65380],{"class":189,"line":190},[187,65381,65382],{},"df1 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)]\n",[187,65384,65385],{"class":189,"line":249},[187,65386,62266],{},[187,65388,65389],{"class":189,"line":312},[187,65390,65391],{},"plt.scatter(df1.tdp, df1.avg, c=df1.clock_speed_in_mhz, s=df1.memory_mb/10, cmap='Blues')\n",[187,65393,65394],{"class":189,"line":319},[187,65395,65396],{},"plt.colorbar(label='Clock Speed (MHz)')\n",[187,65398,65399],{"class":189,"line":325},[187,65400,65401],{},"plt.axis([0,350,0,1000])\n",[187,65403,65404],{"class":189,"line":686},[187,65405,65406],{},"plt.title('GPU TDP vs. Price, Clock Speed (color) and Memory (diameter)', fontsize=14)\n",[187,65408,65409],{"class":189,"line":697},[187,65410,65411],{},"plt.xlabel('Thermal Design Power (TDP)', fontsize=14)\n",[187,65413,65414],{"class":189,"line":1291},[187,65415,62286],{},[187,65417,65418],{"class":189,"line":1306},[187,65419,62953],{},[187,65421,65422],{"class":189,"line":1434},[187,65423,62958],{},[187,65425,65426],{"class":189,"line":2599},[187,65427,65428],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/tdp_vs_price.png'))\n",[11,65430,65431],{},[511,65432],{"alt":7255,"src":65433},"/static/pcpp/gpu/tdp_vs_price.png",[11,65435,65436],{},"Here is a look at the top 20 most common GPU chipsets for graphics cards:",[26,65438,65440],{"className":1383,"code":65439,"language":1125,"meta":35,"style":35},"df1 = df[(df.avg>0)]\nplt.figure(figsize=(12,8))\ndf1.groupby('Chipset').avg.count().sort_values( ascending=False)[:30].plot(kind='bar')\nplt.title('GPUs by Chipset', fontsize=14)\nplt.xlabel('Chipset', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_chipsets.png'))\n",[33,65441,65442,65447,65451,65456,65461,65466,65470,65474,65478],{"__ignoreMap":35},[187,65443,65444],{"class":189,"line":190},[187,65445,65446],{},"df1 = df[(df.avg>0)]\n",[187,65448,65449],{"class":189,"line":249},[187,65450,62266],{},[187,65452,65453],{"class":189,"line":312},[187,65454,65455],{},"df1.groupby('Chipset').avg.count().sort_values( ascending=False)[:30].plot(kind='bar')\n",[187,65457,65458],{"class":189,"line":319},[187,65459,65460],{},"plt.title('GPUs by Chipset', fontsize=14)\n",[187,65462,65463],{"class":189,"line":325},[187,65464,65465],{},"plt.xlabel('Chipset', fontsize=14)\n",[187,65467,65468],{"class":189,"line":686},[187,65469,63049],{},[187,65471,65472],{"class":189,"line":697},[187,65473,62953],{},[187,65475,65476],{"class":189,"line":1291},[187,65477,62958],{},[187,65479,65480],{"class":189,"line":1306},[187,65481,65482],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_chipsets.png'))\n",[11,65484,65485],{},[511,65486],{"alt":7255,"src":65487},"/static/pcpp/gpu/gpu_chipsets.png",[11,65489,65490],{},"And here is the same graph with average prices for the top 20 most common GPU chipsets:",[26,65492,65494],{"className":1383,"code":65493,"language":1125,"meta":35,"style":35},"df[df.avg>0].groupby('Chipset').avg.agg(['mean', 'count']).sort_values('count', ascending=False)[:20].plot(kind='bar', figsize=(12,8))\nplt.title('Average Price of GPU Chipsets (sorted by count)', fontsize=14)\nplt.legend(loc='upper right', fontsize=14)\nplt.xlabel('GPU Chipset', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_chipsets_by_price.png'))\n",[33,65495,65496,65501,65506,65510,65515,65519,65523],{"__ignoreMap":35},[187,65497,65498],{"class":189,"line":190},[187,65499,65500],{},"df[df.avg>0].groupby('Chipset').avg.agg(['mean', 'count']).sort_values('count', ascending=False)[:20].plot(kind='bar', figsize=(12,8))\n",[187,65502,65503],{"class":189,"line":249},[187,65504,65505],{},"plt.title('Average Price of GPU Chipsets (sorted by count)', fontsize=14)\n",[187,65507,65508],{"class":189,"line":312},[187,65509,64240],{},[187,65511,65512],{"class":189,"line":319},[187,65513,65514],{},"plt.xlabel('GPU Chipset', fontsize=14)\n",[187,65516,65517],{"class":189,"line":325},[187,65518,62953],{},[187,65520,65521],{"class":189,"line":686},[187,65522,62958],{},[187,65524,65525],{"class":189,"line":697},[187,65526,65527],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_chipsets_by_price.png'))\n",[11,65529,65530],{},[511,65531],{"alt":7255,"src":65532},"/static/pcpp/gpu/gpu_chipsets_by_price.png",[11,65534,65535],{},"In the next graph we can see clusters of GPU chipset families by plotting the clock speed and prices of video cards:",[26,65537,65539],{"className":1383,"code":65538,"language":1125,"meta":35,"style":35},"df2 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)&(df.memory_mb\u003C10000)]\nplt.figure(figsize=(12,8))\nplt.scatter(df2.clock_speed_in_mhz, df2.avg, s=75, c=df2.memory_mb, cmap='CMRmap_r')\nplt.colorbar(label='Memory (MB)')\nplt.axis([500,1800,0,1000])\nplt.title('Clock Speed (MHz) vs. Price and Memory (color)', fontsize=14)\nplt.xlabel('Clock Speed', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/clock_speed_vs_price_and_memory.png'))\n",[33,65540,65541,65546,65550,65555,65559,65564,65569,65574,65578,65582,65586],{"__ignoreMap":35},[187,65542,65543],{"class":189,"line":190},[187,65544,65545],{},"df2 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)&(df.memory_mb\u003C10000)]\n",[187,65547,65548],{"class":189,"line":249},[187,65549,62266],{},[187,65551,65552],{"class":189,"line":312},[187,65553,65554],{},"plt.scatter(df2.clock_speed_in_mhz, df2.avg, s=75, c=df2.memory_mb, cmap='CMRmap_r')\n",[187,65556,65557],{"class":189,"line":319},[187,65558,65271],{},[187,65560,65561],{"class":189,"line":325},[187,65562,65563],{},"plt.axis([500,1800,0,1000])\n",[187,65565,65566],{"class":189,"line":686},[187,65567,65568],{},"plt.title('Clock Speed (MHz) vs. Price and Memory (color)', fontsize=14)\n",[187,65570,65571],{"class":189,"line":697},[187,65572,65573],{},"plt.xlabel('Clock Speed', fontsize=14)\n",[187,65575,65576],{"class":189,"line":1291},[187,65577,62286],{},[187,65579,65580],{"class":189,"line":1306},[187,65581,62953],{},[187,65583,65584],{"class":189,"line":1434},[187,65585,62958],{},[187,65587,65588],{"class":189,"line":2599},[187,65589,65590],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/clock_speed_vs_price_and_memory.png'))\n",[11,65592,65593],{},[511,65594],{"alt":7255,"src":65595},"/static/pcpp/gpu/clock_speed_vs_price_and_memory.png",[11,65597,65598],{},"Now let's look at six generations of NVIDIA GPUs by clock speed:",[26,65600,65602],{"className":1383,"code":65601,"language":1125,"meta":35,"style":35},"#6 generations of NVIDIA graphics cards\ndf_N = df[(df.make==\"NVIDIA\")&(df.avg>0)]\ndf_N['NVIDIA_Series'] = [10 if 'GTX 10' in x else \\\n                          9 if 'GTX 9' in x else \\\n                          7 if 'GTX 7' in x else \\\n                          6 if 'GTX 6' in x else \\\n                          5 if 'GTX 5' in x else \\\n                          4 if 'GTX 4' in x else \\\n                          'other' for x in df_N.Chipset]\n\nplt.figure(figsize=(15,10))\nplt.axis([500,1800,0,1000])\nplt.title('6 Generations of NVIDIA Graphics Cards: Price vs. Clock Speed', fontsize=14)\nplt.xlabel('Clock Speed MHz', fontsize=14)\nplt.ylabel('Price', fontsize=14)\n\ncolors = ['#76b900', '#8946ff', '#cea503', '#9c0000', '#5c5c5c', '#0b75bd']\ns = 150\nn10 = plt.scatter(df_N[df_N.NVIDIA_Series==10].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==10].avg, color = colors[0], s=s)\nn9 = plt.scatter(df_N[df_N.NVIDIA_Series==9].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==9].avg, color = colors[1], s=s)\nn7 = plt.scatter(df_N[df_N.NVIDIA_Series==7].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==7].avg, color = colors[2], s=s)\nn6 = plt.scatter(df_N[df_N.NVIDIA_Series==6].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==6].avg, color = colors[3], s=s)\nn5 = plt.scatter(df_N[df_N.NVIDIA_Series==5].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==5].avg, color = colors[4], s=s)\nn4 = plt.scatter(df_N[df_N.NVIDIA_Series==4].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==4].avg, color = colors[5], s=s)\n\nplt.legend((n10, n9, n7, n6, n5, n4),\n           ('10 Series', '9 Series', '7 Series', '6 Series', '5 Series', '4 Series'),\n           title = 'NVIDIA GeForce Generations',\n           scatterpoints=3,\n           loc='upper left',\n           ncol=2,\n           fontsize=14)\nsns.despine()\nplt.show()\n\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/six_NVIDIA.png'))\n",[33,65603,65604,65609,65614,65619,65624,65629,65634,65639,65644,65649,65653,65658,65662,65667,65672,65676,65680,65685,65690,65695,65700,65705,65710,65715,65720,65724,65729,65734,65739,65743,65747,65751,65755,65760,65764,65768],{"__ignoreMap":35},[187,65605,65606],{"class":189,"line":190},[187,65607,65608],{},"#6 generations of NVIDIA graphics cards\n",[187,65610,65611],{"class":189,"line":249},[187,65612,65613],{},"df_N = df[(df.make==\"NVIDIA\")&(df.avg>0)]\n",[187,65615,65616],{"class":189,"line":312},[187,65617,65618],{},"df_N['NVIDIA_Series'] = [10 if 'GTX 10' in x else \\\n",[187,65620,65621],{"class":189,"line":319},[187,65622,65623],{},"                          9 if 'GTX 9' in x else \\\n",[187,65625,65626],{"class":189,"line":325},[187,65627,65628],{},"                          7 if 'GTX 7' in x else \\\n",[187,65630,65631],{"class":189,"line":686},[187,65632,65633],{},"                          6 if 'GTX 6' in x else \\\n",[187,65635,65636],{"class":189,"line":697},[187,65637,65638],{},"                          5 if 'GTX 5' in x else \\\n",[187,65640,65641],{"class":189,"line":1291},[187,65642,65643],{},"                          4 if 'GTX 4' in x else \\\n",[187,65645,65646],{"class":189,"line":1306},[187,65647,65648],{},"                          'other' for x in df_N.Chipset]\n",[187,65650,65651],{"class":189,"line":1434},[187,65652,316],{"emptyLinePlaceholder":315},[187,65654,65655],{"class":189,"line":2599},[187,65656,65657],{},"plt.figure(figsize=(15,10))\n",[187,65659,65660],{"class":189,"line":2607},[187,65661,65563],{},[187,65663,65664],{"class":189,"line":2621},[187,65665,65666],{},"plt.title('6 Generations of NVIDIA Graphics Cards: Price vs. Clock Speed', fontsize=14)\n",[187,65668,65669],{"class":189,"line":2631},[187,65670,65671],{},"plt.xlabel('Clock Speed MHz', fontsize=14)\n",[187,65673,65674],{"class":189,"line":2642},[187,65675,62286],{},[187,65677,65678],{"class":189,"line":2653},[187,65679,316],{"emptyLinePlaceholder":315},[187,65681,65682],{"class":189,"line":2665},[187,65683,65684],{},"colors = ['#76b900', '#8946ff', '#cea503', '#9c0000', '#5c5c5c', '#0b75bd']\n",[187,65686,65687],{"class":189,"line":2674},[187,65688,65689],{},"s = 150\n",[187,65691,65692],{"class":189,"line":2684},[187,65693,65694],{},"n10 = plt.scatter(df_N[df_N.NVIDIA_Series==10].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==10].avg, color = colors[0], s=s)\n",[187,65696,65697],{"class":189,"line":2694},[187,65698,65699],{},"n9 = plt.scatter(df_N[df_N.NVIDIA_Series==9].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==9].avg, color = colors[1], s=s)\n",[187,65701,65702],{"class":189,"line":2706},[187,65703,65704],{},"n7 = plt.scatter(df_N[df_N.NVIDIA_Series==7].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==7].avg, color = colors[2], s=s)\n",[187,65706,65707],{"class":189,"line":2715},[187,65708,65709],{},"n6 = plt.scatter(df_N[df_N.NVIDIA_Series==6].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==6].avg, color = colors[3], s=s)\n",[187,65711,65712],{"class":189,"line":2725},[187,65713,65714],{},"n5 = plt.scatter(df_N[df_N.NVIDIA_Series==5].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==5].avg, color = colors[4], s=s)\n",[187,65716,65717],{"class":189,"line":2735},[187,65718,65719],{},"n4 = plt.scatter(df_N[df_N.NVIDIA_Series==4].clock_speed_in_mhz, df_N[df_N.NVIDIA_Series==4].avg, color = colors[5], s=s)\n",[187,65721,65722],{"class":189,"line":2743},[187,65723,316],{"emptyLinePlaceholder":315},[187,65725,65726],{"class":189,"line":2754},[187,65727,65728],{},"plt.legend((n10, n9, n7, n6, n5, n4),\n",[187,65730,65731],{"class":189,"line":2762},[187,65732,65733],{},"           ('10 Series', '9 Series', '7 Series', '6 Series', '5 Series', '4 Series'),\n",[187,65735,65736],{"class":189,"line":2770},[187,65737,65738],{},"           title = 'NVIDIA GeForce Generations',\n",[187,65740,65741],{"class":189,"line":2781},[187,65742,62354],{},[187,65744,65745],{"class":189,"line":2792},[187,65746,62359],{},[187,65748,65749],{"class":189,"line":2803},[187,65750,62364],{},[187,65752,65753],{"class":189,"line":2808},[187,65754,62369],{},[187,65756,65757],{"class":189,"line":2816},[187,65758,65759],{},"sns.despine()\n",[187,65761,65762],{"class":189,"line":2824},[187,65763,26147],{},[187,65765,65766],{"class":189,"line":2834},[187,65767,316],{"emptyLinePlaceholder":315},[187,65769,65770],{"class":189,"line":2845},[187,65771,65772],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/six_NVIDIA.png'))\n",[11,65774,65775],{},[511,65776],{"alt":7255,"src":65777},"/static/pcpp/gpu/six_NVIDIA.png",[11,65779,65780],{},"It's also helpful to compare the most recent two series of NVIDIA GPUs by chipset families:",[26,65782,65784],{"className":1383,"code":65783,"language":1125,"meta":35,"style":35},"#10 Series vs. 9 Series\ndf_N = df[(df.make==\"NVIDIA\")&(df.avg>0)]\ndf_N['NVIDIA_Series_9_10'] = [1080 if 'GTX 108' in x else \\\n                          1070 if 'GTX 1070' in x else \\\n                          1060 if 'GTX 1060' in x else \\\n                          980 if 'GTX 980' in x else \\\n                          970 if 'GTX 970' in x else \\\n                          960 if 'GTX 960' in x else \\\n                          950 if 'GTX 950' in x else \\\n                          'other' for x in df_N.Chipset]\n\nplt.figure(figsize=(15,10))\nplt.axis([800,1800,0,1000])\nplt.title('NVIDIA GeForce GTX: 10 Series vs. 9 Series', fontsize=14)\nplt.xlabel('Clock Speed (MHz)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\n\ncolors = ['#76b900', '#8946ff', '#5c5c5c', '#9c0000', '#cea503', '#0b75bd', 'red']\ns = 150\n\nn1080 = df_N[df_N.NVIDIA_Series_9_10==1080]\nn1070 = df_N[df_N.NVIDIA_Series_9_10==1070]\nn1060 = df_N[df_N.NVIDIA_Series_9_10==1060]\nn980 = df_N[df_N.NVIDIA_Series_9_10==980]\nn970 = df_N[df_N.NVIDIA_Series_9_10==970]\nn960 = df_N[df_N.NVIDIA_Series_9_10==960]\nn950 = df_N[df_N.NVIDIA_Series_9_10==950]\n\nn_1080 = plt.scatter(n1080.clock_speed_in_mhz, n1080.avg, color = colors[0], s=s)\nn_1070 = plt.scatter(n1070.clock_speed_in_mhz, n1070.avg, color = colors[1], s=s)\nn_1060 = plt.scatter(n1060.clock_speed_in_mhz, n1060.avg, color = colors[2], s=s)\nn_980 = plt.scatter(n980.clock_speed_in_mhz, n980.avg, color = colors[3], s=s)\nn_970 = plt.scatter(n970.clock_speed_in_mhz, n970.avg, color = colors[4], s=s)\nn_960 = plt.scatter(n960.clock_speed_in_mhz, n960.avg, color = colors[5], s=s)\nn_950 = plt.scatter(n950.clock_speed_in_mhz, n950.avg, color = colors[6], s=s)\n\nplt.legend((n_1080, n_1070, n_1060, n_980, n_970, n_960, n_950),\n           ('1080', '1070', '1060', '980', '970', '960', '950'),\n           title = 'NVIDIA GeForce',\n           scatterpoints=3,\n           loc='upper left',\n           ncol=1,\n           fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nsns.despine()\nplt.show()\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/9_vs_10.png'))\n",[33,65785,65786,65791,65795,65800,65805,65810,65815,65820,65825,65830,65834,65838,65842,65847,65852,65857,65861,65865,65870,65874,65878,65883,65888,65893,65898,65903,65908,65913,65917,65922,65927,65932,65937,65942,65947,65952,65956,65961,65966,65971,65975,65979,65984,65988,65992,65996,66000,66004],{"__ignoreMap":35},[187,65787,65788],{"class":189,"line":190},[187,65789,65790],{},"#10 Series vs. 9 Series\n",[187,65792,65793],{"class":189,"line":249},[187,65794,65613],{},[187,65796,65797],{"class":189,"line":312},[187,65798,65799],{},"df_N['NVIDIA_Series_9_10'] = [1080 if 'GTX 108' in x else \\\n",[187,65801,65802],{"class":189,"line":319},[187,65803,65804],{},"                          1070 if 'GTX 1070' in x else \\\n",[187,65806,65807],{"class":189,"line":325},[187,65808,65809],{},"                          1060 if 'GTX 1060' in x else \\\n",[187,65811,65812],{"class":189,"line":686},[187,65813,65814],{},"                          980 if 'GTX 980' in x else \\\n",[187,65816,65817],{"class":189,"line":697},[187,65818,65819],{},"                          970 if 'GTX 970' in x else \\\n",[187,65821,65822],{"class":189,"line":1291},[187,65823,65824],{},"                          960 if 'GTX 960' in x else \\\n",[187,65826,65827],{"class":189,"line":1306},[187,65828,65829],{},"                          950 if 'GTX 950' in x else \\\n",[187,65831,65832],{"class":189,"line":1434},[187,65833,65648],{},[187,65835,65836],{"class":189,"line":2599},[187,65837,316],{"emptyLinePlaceholder":315},[187,65839,65840],{"class":189,"line":2607},[187,65841,65657],{},[187,65843,65844],{"class":189,"line":2621},[187,65845,65846],{},"plt.axis([800,1800,0,1000])\n",[187,65848,65849],{"class":189,"line":2631},[187,65850,65851],{},"plt.title('NVIDIA GeForce GTX: 10 Series vs. 9 Series', fontsize=14)\n",[187,65853,65854],{"class":189,"line":2642},[187,65855,65856],{},"plt.xlabel('Clock Speed (MHz)', fontsize=14)\n",[187,65858,65859],{"class":189,"line":2653},[187,65860,62286],{},[187,65862,65863],{"class":189,"line":2665},[187,65864,316],{"emptyLinePlaceholder":315},[187,65866,65867],{"class":189,"line":2674},[187,65868,65869],{},"colors = ['#76b900', '#8946ff', '#5c5c5c', '#9c0000', '#cea503', '#0b75bd', 'red']\n",[187,65871,65872],{"class":189,"line":2684},[187,65873,65689],{},[187,65875,65876],{"class":189,"line":2694},[187,65877,316],{"emptyLinePlaceholder":315},[187,65879,65880],{"class":189,"line":2706},[187,65881,65882],{},"n1080 = df_N[df_N.NVIDIA_Series_9_10==1080]\n",[187,65884,65885],{"class":189,"line":2715},[187,65886,65887],{},"n1070 = df_N[df_N.NVIDIA_Series_9_10==1070]\n",[187,65889,65890],{"class":189,"line":2725},[187,65891,65892],{},"n1060 = df_N[df_N.NVIDIA_Series_9_10==1060]\n",[187,65894,65895],{"class":189,"line":2735},[187,65896,65897],{},"n980 = df_N[df_N.NVIDIA_Series_9_10==980]\n",[187,65899,65900],{"class":189,"line":2743},[187,65901,65902],{},"n970 = df_N[df_N.NVIDIA_Series_9_10==970]\n",[187,65904,65905],{"class":189,"line":2754},[187,65906,65907],{},"n960 = df_N[df_N.NVIDIA_Series_9_10==960]\n",[187,65909,65910],{"class":189,"line":2762},[187,65911,65912],{},"n950 = df_N[df_N.NVIDIA_Series_9_10==950]\n",[187,65914,65915],{"class":189,"line":2770},[187,65916,316],{"emptyLinePlaceholder":315},[187,65918,65919],{"class":189,"line":2781},[187,65920,65921],{},"n_1080 = plt.scatter(n1080.clock_speed_in_mhz, n1080.avg, color = colors[0], s=s)\n",[187,65923,65924],{"class":189,"line":2792},[187,65925,65926],{},"n_1070 = plt.scatter(n1070.clock_speed_in_mhz, n1070.avg, color = colors[1], s=s)\n",[187,65928,65929],{"class":189,"line":2803},[187,65930,65931],{},"n_1060 = plt.scatter(n1060.clock_speed_in_mhz, n1060.avg, color = colors[2], s=s)\n",[187,65933,65934],{"class":189,"line":2808},[187,65935,65936],{},"n_980 = plt.scatter(n980.clock_speed_in_mhz, n980.avg, color = colors[3], s=s)\n",[187,65938,65939],{"class":189,"line":2816},[187,65940,65941],{},"n_970 = plt.scatter(n970.clock_speed_in_mhz, n970.avg, color = colors[4], s=s)\n",[187,65943,65944],{"class":189,"line":2824},[187,65945,65946],{},"n_960 = plt.scatter(n960.clock_speed_in_mhz, n960.avg, color = colors[5], s=s)\n",[187,65948,65949],{"class":189,"line":2834},[187,65950,65951],{},"n_950 = plt.scatter(n950.clock_speed_in_mhz, n950.avg, color = colors[6], s=s)\n",[187,65953,65954],{"class":189,"line":2845},[187,65955,316],{"emptyLinePlaceholder":315},[187,65957,65958],{"class":189,"line":2856},[187,65959,65960],{},"plt.legend((n_1080, n_1070, n_1060, n_980, n_970, n_960, n_950),\n",[187,65962,65963],{"class":189,"line":2867},[187,65964,65965],{},"           ('1080', '1070', '1060', '980', '970', '960', '950'),\n",[187,65967,65968],{"class":189,"line":2878},[187,65969,65970],{},"           title = 'NVIDIA GeForce',\n",[187,65972,65973],{"class":189,"line":2886},[187,65974,62354],{},[187,65976,65977],{"class":189,"line":2900},[187,65978,62359],{},[187,65980,65981],{"class":189,"line":2905},[187,65982,65983],{},"           ncol=1,\n",[187,65985,65986],{"class":189,"line":2913},[187,65987,62369],{},[187,65989,65990],{"class":189,"line":2921},[187,65991,62953],{},[187,65993,65994],{"class":189,"line":2931},[187,65995,62958],{},[187,65997,65998],{"class":189,"line":2942},[187,65999,65759],{},[187,66001,66002],{"class":189,"line":2953},[187,66003,26147],{},[187,66005,66006],{"class":189,"line":2964},[187,66007,66008],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/9_vs_10.png'))\n",[11,66010,66011],{},[511,66012],{"alt":7255,"src":66013},"/static/pcpp/gpu/9_vs_10.png",[11,66015,66016],{},"Finally, as we did for CPUs, we can look at the core memory clock and the boost clock for GPUs:",[26,66018,66020],{"className":1383,"code":66019,"language":1125,"meta":35,"style":35},"df2 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)&(df.boost_clock_speed_mhz>0)&(df.tdp>0)]\n\nfrom sklearn.linear_model import LinearRegression\nlreg = LinearRegression()\n\nx = df2.clock_speed_in_mhz\nY = df2.boost_clock_speed_mhz\n\nx = x.values.reshape(-1,1)\nY = Y.values.reshape(-1,1)\n\nlreg.fit(x, Y, sample_weight=None)\ns = df2.tdp*3\na = 0.3\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,8))\nplt.plot(x,lreg.predict(x), c='y')\nplt.scatter(df2[df.make=='NVIDIA'].clock_speed_in_mhz, df2[df.make=='NVIDIA'].boost_clock_speed_mhz, s=s, c='green', alpha=a)\nplt.scatter(df2[df.make=='AMD'].clock_speed_in_mhz, df2[df.make=='AMD'].boost_clock_speed_mhz, s=s, c='r', alpha=a)\nplt.title('Core Clock vs. Boost Clock and TDP (diameter)', fontsize=14)\nplt.xlabel('Core Clock Speed (MHz)', fontsize=14)\nplt.ylabel('Boost Clock Speed (MHz)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.axis([600,2100,600,2100])\nx_points = [700,1000,1500,2000]\ny_points = [700,1000,1500,2000]\n\nplt.plot(x_points,y_points, c='blue')\nplt.legend([ 'Line of best fit', 'y = x', 'Core vs. Boost Clock (NVIDIA)', 'Core vs. Boost Clock (AMD)'], loc='upper left', fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_clock_vs_boost.png'))\n",[33,66021,66022,66027,66031,66035,66039,66043,66048,66053,66057,66062,66067,66071,66076,66081,66086,66090,66094,66099,66104,66109,66114,66119,66124,66128,66132,66137,66142,66147,66151,66156,66161],{"__ignoreMap":35},[187,66023,66024],{"class":189,"line":190},[187,66025,66026],{},"df2 = df[(df.avg>0)&(df.clock_speed_in_mhz>0)&(df.boost_clock_speed_mhz>0)&(df.tdp>0)]\n",[187,66028,66029],{"class":189,"line":249},[187,66030,316],{"emptyLinePlaceholder":315},[187,66032,66033],{"class":189,"line":312},[187,66034,62846],{},[187,66036,66037],{"class":189,"line":319},[187,66038,64425],{},[187,66040,66041],{"class":189,"line":325},[187,66042,316],{"emptyLinePlaceholder":315},[187,66044,66045],{"class":189,"line":686},[187,66046,66047],{},"x = df2.clock_speed_in_mhz\n",[187,66049,66050],{"class":189,"line":697},[187,66051,66052],{},"Y = df2.boost_clock_speed_mhz\n",[187,66054,66055],{"class":189,"line":1291},[187,66056,316],{"emptyLinePlaceholder":315},[187,66058,66059],{"class":189,"line":1306},[187,66060,66061],{},"x = x.values.reshape(-1,1)\n",[187,66063,66064],{"class":189,"line":1434},[187,66065,66066],{},"Y = Y.values.reshape(-1,1)\n",[187,66068,66069],{"class":189,"line":2599},[187,66070,316],{"emptyLinePlaceholder":315},[187,66072,66073],{"class":189,"line":2607},[187,66074,66075],{},"lreg.fit(x, Y, sample_weight=None)\n",[187,66077,66078],{"class":189,"line":2621},[187,66079,66080],{},"s = df2.tdp*3\n",[187,66082,66083],{"class":189,"line":2631},[187,66084,66085],{},"a = 0.3\n",[187,66087,66088],{"class":189,"line":2642},[187,66089,55814],{},[187,66091,66092],{"class":189,"line":2653},[187,66093,62266],{},[187,66095,66096],{"class":189,"line":2665},[187,66097,66098],{},"plt.plot(x,lreg.predict(x), c='y')\n",[187,66100,66101],{"class":189,"line":2674},[187,66102,66103],{},"plt.scatter(df2[df.make=='NVIDIA'].clock_speed_in_mhz, df2[df.make=='NVIDIA'].boost_clock_speed_mhz, s=s, c='green', alpha=a)\n",[187,66105,66106],{"class":189,"line":2684},[187,66107,66108],{},"plt.scatter(df2[df.make=='AMD'].clock_speed_in_mhz, df2[df.make=='AMD'].boost_clock_speed_mhz, s=s, c='r', alpha=a)\n",[187,66110,66111],{"class":189,"line":2694},[187,66112,66113],{},"plt.title('Core Clock vs. Boost Clock and TDP (diameter)', fontsize=14)\n",[187,66115,66116],{"class":189,"line":2706},[187,66117,66118],{},"plt.xlabel('Core Clock Speed (MHz)', fontsize=14)\n",[187,66120,66121],{"class":189,"line":2715},[187,66122,66123],{},"plt.ylabel('Boost Clock Speed (MHz)', fontsize=14)\n",[187,66125,66126],{"class":189,"line":2725},[187,66127,62953],{},[187,66129,66130],{"class":189,"line":2735},[187,66131,62958],{},[187,66133,66134],{"class":189,"line":2743},[187,66135,66136],{},"plt.axis([600,2100,600,2100])\n",[187,66138,66139],{"class":189,"line":2754},[187,66140,66141],{},"x_points = [700,1000,1500,2000]\n",[187,66143,66144],{"class":189,"line":2762},[187,66145,66146],{},"y_points = [700,1000,1500,2000]\n",[187,66148,66149],{"class":189,"line":2770},[187,66150,316],{"emptyLinePlaceholder":315},[187,66152,66153],{"class":189,"line":2781},[187,66154,66155],{},"plt.plot(x_points,y_points, c='blue')\n",[187,66157,66158],{"class":189,"line":2792},[187,66159,66160],{},"plt.legend([ 'Line of best fit', 'y = x', 'Core vs. Boost Clock (NVIDIA)', 'Core vs. Boost Clock (AMD)'], loc='upper left', fontsize=13)\n",[187,66162,66163],{"class":189,"line":2803},[187,66164,66165],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/gpu_clock_vs_boost.png'))\n",[11,66167,66168],{},[511,66169],{"alt":7255,"src":66170},"/static/pcpp/gpu/gpu_clock_vs_boost.png",[11,66172,66173,66174,752],{},"The latest generation of GPUs at much higher clock speeds than previous generations are able to boost clocks even faster than previous generations, on average. This is shown by the slightly steeper curve of the line of best fit as compared with ",[33,66175,66176],{},"y = x",[2215,66178,66180],{"id":66179},"hard-drives","Hard Drives",[11,66182,66183,66184,66189],{},"Hard drive sizes have been growing at an exponential rate since the 1950s. This ",[15,66185,66188],{"href":66186,"rel":66187},"https://en.wikipedia.org/wiki/File:Hard_drive_capacity_over_time.svg",[19],"chart from Wikipedia"," shows the growth of storage sizes on a logarithmic scale in recent decades:",[11,66191,66192],{},[511,66193],{"alt":7255,"src":66194},"/static/pcpp/storage/storage_growth.png",[11,66196,66197,66198,66203],{},"Linear growth on a logarithmic scale corresponds to exponential growth on a linear scale, which is the basis of Moore's Law. Hard drive disks follow a growth pattern similar to Moore's Law called ",[15,66199,66202],{"href":66200,"rel":66201},"https://en.wikipedia.org/wiki/Mark_Kryder",[19],"Kryder's Law",".\nHere is a scatter plot of storage drive sizes and prices:",[26,66205,66207],{"className":1383,"code":66206,"language":1125,"meta":35,"style":35},"sns.set_style('whitegrid')\nplt.figure(figsize=(12,8))\nplt.axis([0,10500,0,1500])\nssd = df[(df.avg>0)&(df.is_ssd==\"Yes\")]\nhdd = df[(df.avg>0)&(df.is_ssd==\"No\")]\nplt.title('Hard Drive Sizes (GB) vs. Prices ', fontsize=14)\ns=75\nplt.scatter(ssd.storage_gb, ssd.avg, c='black', s=s, alpha=.3)\nplt.scatter(hdd.storage_gb, hdd.avg, c='red', s=s, alpha=.3)\nplt.legend(['SSD', 'HDD'], loc='upper right', fontsize=14)\nplt.xlabel('Hard Drive size (GB)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/storage_vs_price.png'))\n",[33,66208,66209,66213,66217,66222,66227,66232,66237,66241,66246,66251,66256,66261,66265,66269,66273],{"__ignoreMap":35},[187,66210,66211],{"class":189,"line":190},[187,66212,55814],{},[187,66214,66215],{"class":189,"line":249},[187,66216,62266],{},[187,66218,66219],{"class":189,"line":312},[187,66220,66221],{},"plt.axis([0,10500,0,1500])\n",[187,66223,66224],{"class":189,"line":319},[187,66225,66226],{},"ssd = df[(df.avg>0)&(df.is_ssd==\"Yes\")]\n",[187,66228,66229],{"class":189,"line":325},[187,66230,66231],{},"hdd = df[(df.avg>0)&(df.is_ssd==\"No\")]\n",[187,66233,66234],{"class":189,"line":686},[187,66235,66236],{},"plt.title('Hard Drive Sizes (GB) vs. Prices ', fontsize=14)\n",[187,66238,66239],{"class":189,"line":697},[187,66240,64546],{},[187,66242,66243],{"class":189,"line":1291},[187,66244,66245],{},"plt.scatter(ssd.storage_gb, ssd.avg, c='black', s=s, alpha=.3)\n",[187,66247,66248],{"class":189,"line":1306},[187,66249,66250],{},"plt.scatter(hdd.storage_gb, hdd.avg, c='red', s=s, alpha=.3)\n",[187,66252,66253],{"class":189,"line":1434},[187,66254,66255],{},"plt.legend(['SSD', 'HDD'], loc='upper right', fontsize=14)\n",[187,66257,66258],{"class":189,"line":2599},[187,66259,66260],{},"plt.xlabel('Hard Drive size (GB)', fontsize=14)\n",[187,66262,66263],{"class":189,"line":2607},[187,66264,62286],{},[187,66266,66267],{"class":189,"line":2621},[187,66268,62953],{},[187,66270,66271],{"class":189,"line":2631},[187,66272,62958],{},[187,66274,66275],{"class":189,"line":2642},[187,66276,66277],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/storage_vs_price.png'))\n",[11,66279,66280],{},[511,66281],{"alt":7255,"src":66282},"/static/pcpp/storage/storage_vs_price.png",[11,66284,66285],{},"HDD refers to a spinning hard drive disk. Electromechanical magnetic disks spin at high freuencies and a physical arm reads and writes data to and from the spinning disks. SSDs, or Solid state drives, have a number of advantages over spinning drives. SSDs have no moving parts, which makes them more shock resistant and quiter than HDDs. SSDs also have lower access time and lower latency, but they come at a much higher price per GB of storage than HDDs.",[11,66287,66288],{},"The next two graphs show SSDs and HDDs, respectively.",[26,66290,66292],{"className":1383,"code":66291,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.axis([0,2000,0,1500])\nssd = df[(df.avg>0)&(df.is_ssd==\"Yes\")]\n\nplt.scatter(ssd.storage_gb, ssd.avg, s=75, alpha=.2)\nplt.title('SSD Size vs. Price', fontsize=14)\nplt.xlabel('SSD Hard Drive size (GB)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/ssd_storage_vs_price.png'))\n",[33,66293,66294,66298,66303,66307,66311,66316,66321,66326,66330,66334,66338],{"__ignoreMap":35},[187,66295,66296],{"class":189,"line":190},[187,66297,62266],{},[187,66299,66300],{"class":189,"line":249},[187,66301,66302],{},"plt.axis([0,2000,0,1500])\n",[187,66304,66305],{"class":189,"line":312},[187,66306,66226],{},[187,66308,66309],{"class":189,"line":319},[187,66310,316],{"emptyLinePlaceholder":315},[187,66312,66313],{"class":189,"line":325},[187,66314,66315],{},"plt.scatter(ssd.storage_gb, ssd.avg, s=75, alpha=.2)\n",[187,66317,66318],{"class":189,"line":686},[187,66319,66320],{},"plt.title('SSD Size vs. Price', fontsize=14)\n",[187,66322,66323],{"class":189,"line":697},[187,66324,66325],{},"plt.xlabel('SSD Hard Drive size (GB)', fontsize=14)\n",[187,66327,66328],{"class":189,"line":1291},[187,66329,62286],{},[187,66331,66332],{"class":189,"line":1306},[187,66333,62953],{},[187,66335,66336],{"class":189,"line":1434},[187,66337,62958],{},[187,66339,66340],{"class":189,"line":2599},[187,66341,66342],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/ssd_storage_vs_price.png'))\n",[11,66344,66345],{},[511,66346],{"alt":7255,"src":66347},"/static/pcpp/storage/ssd_storage_vs_price.png",[11,66349,66350],{},"For HDDs, the color of each point represent the speed in rotations per minute of the spinning disk:",[26,66352,66354],{"className":1383,"code":66353,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,7))\nplt.axis([0,11000,0,650])\nhdd = df[(df.avg>0)&(df.is_ssd==\"No\")&(df.RPM.notnull())]\nplt.scatter(hdd.storage_gb, hdd.avg, c=hdd.RPM, s=40, cmap=\"Blues_r\")\nplt.colorbar(label='RPM')\nplt.title('HDD Size vs. Price and RPM (color)', fontsize=14)\nplt.xlabel('HDD Hard Drive size (GB)', fontsize=14)\nplt.ylabel('Price', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/hdd_storage_vs_price.png'))\n",[33,66355,66356,66361,66366,66371,66376,66381,66386,66391,66395,66399,66403],{"__ignoreMap":35},[187,66357,66358],{"class":189,"line":190},[187,66359,66360],{},"plt.figure(figsize=(12,7))\n",[187,66362,66363],{"class":189,"line":249},[187,66364,66365],{},"plt.axis([0,11000,0,650])\n",[187,66367,66368],{"class":189,"line":312},[187,66369,66370],{},"hdd = df[(df.avg>0)&(df.is_ssd==\"No\")&(df.RPM.notnull())]\n",[187,66372,66373],{"class":189,"line":319},[187,66374,66375],{},"plt.scatter(hdd.storage_gb, hdd.avg, c=hdd.RPM, s=40, cmap=\"Blues_r\")\n",[187,66377,66378],{"class":189,"line":325},[187,66379,66380],{},"plt.colorbar(label='RPM')\n",[187,66382,66383],{"class":189,"line":686},[187,66384,66385],{},"plt.title('HDD Size vs. Price and RPM (color)', fontsize=14)\n",[187,66387,66388],{"class":189,"line":697},[187,66389,66390],{},"plt.xlabel('HDD Hard Drive size (GB)', fontsize=14)\n",[187,66392,66393],{"class":189,"line":1291},[187,66394,62286],{},[187,66396,66397],{"class":189,"line":1306},[187,66398,62953],{},[187,66400,66401],{"class":189,"line":1434},[187,66402,62958],{},[187,66404,66405],{"class":189,"line":2599},[187,66406,66407],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/hdd_storage_vs_price.png'))\n",[11,66409,66410],{},[511,66411],{"alt":7255,"src":66412},"/static/pcpp/storage/hdd_storage_vs_price.png",[11,66414,66415],{},"This graph shows the price and the price per GB for SSDs and HDDs:",[26,66417,66419],{"className":1383,"code":66418,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.axis([0,5100,0,3])\na=.3\nplt.scatter(ssd.storage_gb,ssd.ppgb, s= 50, alpha=a, color='black')\nplt.scatter(hdd.storage_gb,hdd.ppgb, s= 50, alpha=a, color='red')\nplt.title('Price per GB of Hard Drive vs. Storage Capacity', fontsize=14)\nplt.xlabel('Storage Capacity (GB)', fontsize=14)\nplt.ylabel('Price per GB', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(['SSD', 'HDD'], loc='upper right', fontsize=14)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/storage_vs_ppgb.png'))\n",[33,66420,66421,66425,66430,66435,66440,66445,66450,66455,66459,66463,66467,66471],{"__ignoreMap":35},[187,66422,66423],{"class":189,"line":190},[187,66424,62266],{},[187,66426,66427],{"class":189,"line":249},[187,66428,66429],{},"plt.axis([0,5100,0,3])\n",[187,66431,66432],{"class":189,"line":312},[187,66433,66434],{},"a=.3\n",[187,66436,66437],{"class":189,"line":319},[187,66438,66439],{},"plt.scatter(ssd.storage_gb,ssd.ppgb, s= 50, alpha=a, color='black')\n",[187,66441,66442],{"class":189,"line":325},[187,66443,66444],{},"plt.scatter(hdd.storage_gb,hdd.ppgb, s= 50, alpha=a, color='red')\n",[187,66446,66447],{"class":189,"line":686},[187,66448,66449],{},"plt.title('Price per GB of Hard Drive vs. Storage Capacity', fontsize=14)\n",[187,66451,66452],{"class":189,"line":697},[187,66453,66454],{},"plt.xlabel('Storage Capacity (GB)', fontsize=14)\n",[187,66456,66457],{"class":189,"line":1291},[187,66458,64622],{},[187,66460,66461],{"class":189,"line":1306},[187,66462,62953],{},[187,66464,66465],{"class":189,"line":1434},[187,66466,62958],{},[187,66468,66469],{"class":189,"line":2599},[187,66470,66255],{},[187,66472,66473],{"class":189,"line":2607},[187,66474,66475],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/storage/storage_vs_ppgb.png'))\n",[11,66477,66478],{},[511,66479],{"alt":7255,"src":66480},"/static/pcpp/storage/storage_vs_ppgb.png",[11,66482,66483],{},"Both HDDs and SSDs become less expensive per GB as they scale.",[2215,66485,66487],{"id":66486},"case-fans","Case Fans",[11,66489,66490],{},"Case fans are an important consideration for enthusiast-level PC buidlers. Case fan features include maximum RPM, maximum sound (dbA, or A-weighted decibels), maximum airflow (measured in cubic feet per minute, or CFM), fan size (mm) and static pressure (mm/H2O, a measure of pressure). These fan features are determined by the power of the fan motor and the shape, angle and spacing of the fan blades. Here are some of these features on scatter plots:",[26,66492,66494],{"className":1383,"code":66493,"language":1125,"meta":35,"style":35},"df1 = df[(df.noise>0)&(df.max_flow>0)&(df.rpm_max>0)&(df.avg>0)&(df.avg\u003C75)]\nplt.figure(figsize=(12,8))\nplt.scatter(df1.rpm_max, df1.noise, c=df1.avg, cmap='Blues', s=100)\nplt.colorbar(label='Price')\nplt.axis([0,10000,0,70])\nplt.xlabel('Maximum RPM', fontsize=14)\nplt.ylabel('Maximum Noise (dbA)', fontsize=14)\nplt.title('Case Fan Maximum RPM vs. Maximum Noise (dbA) and Price (color)', fontsize=14)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/rpm_vs_noise.png'))\n",[33,66495,66496,66501,66505,66510,66514,66519,66523,66528,66533],{"__ignoreMap":35},[187,66497,66498],{"class":189,"line":190},[187,66499,66500],{},"df1 = df[(df.noise>0)&(df.max_flow>0)&(df.rpm_max>0)&(df.avg>0)&(df.avg\u003C75)]\n",[187,66502,66503],{"class":189,"line":249},[187,66504,62266],{},[187,66506,66507],{"class":189,"line":312},[187,66508,66509],{},"plt.scatter(df1.rpm_max, df1.noise, c=df1.avg, cmap='Blues', s=100)\n",[187,66511,66512],{"class":189,"line":319},[187,66513,64056],{},[187,66515,66516],{"class":189,"line":325},[187,66517,66518],{},"plt.axis([0,10000,0,70])\n",[187,66520,66521],{"class":189,"line":686},[187,66522,64375],{},[187,66524,66525],{"class":189,"line":697},[187,66526,66527],{},"plt.ylabel('Maximum Noise (dbA)', fontsize=14)\n",[187,66529,66530],{"class":189,"line":1291},[187,66531,66532],{},"plt.title('Case Fan Maximum RPM vs. Maximum Noise (dbA) and Price (color)', fontsize=14)\n",[187,66534,66535],{"class":189,"line":1306},[187,66536,66537],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/rpm_vs_noise.png'))\n",[11,66539,66540],{},[511,66541],{"alt":7255,"src":66542},"/static/pcpp/fan/rpm_vs_noise.png",[11,66544,66545],{},"260 of 1192 case fans in this dataset include a rating for static pressure. Static pressure can be thought of as the force by which air is pushed out of the fan. If you put your hand in front of a fan with low static pressure, you will feel a gentle flow of air. Fans with high static pressure have stronger airflow, but not necessarily more airflow, as measured in CFM.",[11,66547,66548],{},"This graph shows air flow plotted against static pressure, and the weak correlation between the two variables:",[26,66550,66552],{"className":1383,"code":66551,"language":1125,"meta":35,"style":35},"df5 = df[(df.avg>0)&(df.max_flow>0)&(df.static_pressure>0)&(df.rpm_max>0)&(df.static_pressure\u003C15)]\n\nfrom sklearn.linear_model import LinearRegression\nlreg = LinearRegression()\n\nX = df5.max_flow.reshape(df5.max_flow.shape[0],1)\ny = df5.static_pressure.reshape(df5.static_pressure.shape[0],1)\nlreg.fit(X, y, sample_weight=None)\n\nplt.figure(figsize=(12,8))\nplt.scatter(df5.max_flow, df5.static_pressure, s=75)\nplt.title('Air Flow (CFM) vs. Static Pressure (mm/H2O)', fontsize=14)\nplt.xlabel('Air Flow in CFM (cubic feet per minute)', fontsize=14)\nplt.ylabel('Static Pressure (mm/H2O)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.axis([0,200,0,8])\n\n#plot regression line\nflow = df5.max_flow.reshape(df5.max_flow.shape[0],1)\npred = lreg.predict(df5.max_flow.reshape(df5.max_flow.shape[0],1))\nplt.plot(flow ,pred, color='red')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/air_flow_v_static_pressure.png'))\n\nprint lreg.score(X, y, sample_weight=None)\n",[33,66553,66554,66559,66563,66567,66571,66575,66580,66585,66589,66593,66597,66602,66607,66612,66617,66621,66625,66630,66634,66638,66643,66648,66653,66658,66662],{"__ignoreMap":35},[187,66555,66556],{"class":189,"line":190},[187,66557,66558],{},"df5 = df[(df.avg>0)&(df.max_flow>0)&(df.static_pressure>0)&(df.rpm_max>0)&(df.static_pressure\u003C15)]\n",[187,66560,66561],{"class":189,"line":249},[187,66562,316],{"emptyLinePlaceholder":315},[187,66564,66565],{"class":189,"line":312},[187,66566,62846],{},[187,66568,66569],{"class":189,"line":319},[187,66570,64425],{},[187,66572,66573],{"class":189,"line":325},[187,66574,316],{"emptyLinePlaceholder":315},[187,66576,66577],{"class":189,"line":686},[187,66578,66579],{},"X = df5.max_flow.reshape(df5.max_flow.shape[0],1)\n",[187,66581,66582],{"class":189,"line":697},[187,66583,66584],{},"y = df5.static_pressure.reshape(df5.static_pressure.shape[0],1)\n",[187,66586,66587],{"class":189,"line":1291},[187,66588,64450],{},[187,66590,66591],{"class":189,"line":1306},[187,66592,316],{"emptyLinePlaceholder":315},[187,66594,66595],{"class":189,"line":1434},[187,66596,62266],{},[187,66598,66599],{"class":189,"line":2599},[187,66600,66601],{},"plt.scatter(df5.max_flow, df5.static_pressure, s=75)\n",[187,66603,66604],{"class":189,"line":2607},[187,66605,66606],{},"plt.title('Air Flow (CFM) vs. Static Pressure (mm/H2O)', fontsize=14)\n",[187,66608,66609],{"class":189,"line":2621},[187,66610,66611],{},"plt.xlabel('Air Flow in CFM (cubic feet per minute)', fontsize=14)\n",[187,66613,66614],{"class":189,"line":2631},[187,66615,66616],{},"plt.ylabel('Static Pressure (mm/H2O)', fontsize=14)\n",[187,66618,66619],{"class":189,"line":2642},[187,66620,62953],{},[187,66622,66623],{"class":189,"line":2653},[187,66624,62958],{},[187,66626,66627],{"class":189,"line":2665},[187,66628,66629],{},"plt.axis([0,200,0,8])\n",[187,66631,66632],{"class":189,"line":2674},[187,66633,316],{"emptyLinePlaceholder":315},[187,66635,66636],{"class":189,"line":2684},[187,66637,64496],{},[187,66639,66640],{"class":189,"line":2694},[187,66641,66642],{},"flow = df5.max_flow.reshape(df5.max_flow.shape[0],1)\n",[187,66644,66645],{"class":189,"line":2706},[187,66646,66647],{},"pred = lreg.predict(df5.max_flow.reshape(df5.max_flow.shape[0],1))\n",[187,66649,66650],{"class":189,"line":2715},[187,66651,66652],{},"plt.plot(flow ,pred, color='red')\n",[187,66654,66655],{"class":189,"line":2725},[187,66656,66657],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/air_flow_v_static_pressure.png'))\n",[187,66659,66660],{"class":189,"line":2735},[187,66661,316],{"emptyLinePlaceholder":315},[187,66663,66664],{"class":189,"line":2743},[187,66665,66666],{},"print lreg.score(X, y, sample_weight=None)\n",[11,66668,66669],{},[33,66670,66671],{},"0.0345903950712",[11,66673,66674],{},[511,66675],{"alt":7255,"src":66676},"/static/pcpp/fan/air_flow_v_static_pressure.png",[11,66678,66679],{},"Two fan features with a strong correlation are maximum RPM and static pressure:",[26,66681,66683],{"className":1383,"code":66682,"language":1125,"meta":35,"style":35},"df5 = df[(df.avg>0)&(df.max_flow>0)&(df.static_pressure>0)&(df.rpm_max>0)&(df.static_pressure\u003C15)]\n\nfrom sklearn.linear_model import LinearRegression\nlreg = LinearRegression()\nX = df5.rpm_max.reshape(df5.rpm_max.shape[0],1)\ny = df5.static_pressure.reshape(df5.static_pressure.shape[0],1)\nlreg.fit(X, y, sample_weight=None)\n\nplt.figure(figsize=(12,8))\nplt.scatter(df5.rpm_max, df5.static_pressure, s=75)\n\nplt.axis([0,8000,0,16])\nplt.title('Maximum RPM vs. Static Pressure (mm/H2O)', fontsize=14)\nplt.xlabel('Maximum RPM', fontsize=14)\nplt.ylabel('Static Pressure (mm/H2O)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\n\n#plot regression line\nsize = df5.rpm_max.reshape(df5.rpm_max.shape[0],1)\npred = lreg.predict(df5.rpm_max.reshape(df5.rpm_max.shape[0],1))\nplt.plot(size ,pred, color='red')\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/rpm_max_vs_static_pressure.png'))\nprint lreg.score(X,y,sample_weight=None)\n",[33,66684,66685,66689,66693,66697,66701,66706,66710,66714,66718,66722,66727,66731,66736,66741,66745,66749,66753,66757,66761,66765,66770,66775,66779,66784],{"__ignoreMap":35},[187,66686,66687],{"class":189,"line":190},[187,66688,66558],{},[187,66690,66691],{"class":189,"line":249},[187,66692,316],{"emptyLinePlaceholder":315},[187,66694,66695],{"class":189,"line":312},[187,66696,62846],{},[187,66698,66699],{"class":189,"line":319},[187,66700,64425],{},[187,66702,66703],{"class":189,"line":325},[187,66704,66705],{},"X = df5.rpm_max.reshape(df5.rpm_max.shape[0],1)\n",[187,66707,66708],{"class":189,"line":686},[187,66709,66584],{},[187,66711,66712],{"class":189,"line":697},[187,66713,64450],{},[187,66715,66716],{"class":189,"line":1291},[187,66717,316],{"emptyLinePlaceholder":315},[187,66719,66720],{"class":189,"line":1306},[187,66721,62266],{},[187,66723,66724],{"class":189,"line":1434},[187,66725,66726],{},"plt.scatter(df5.rpm_max, df5.static_pressure, s=75)\n",[187,66728,66729],{"class":189,"line":2599},[187,66730,316],{"emptyLinePlaceholder":315},[187,66732,66733],{"class":189,"line":2607},[187,66734,66735],{},"plt.axis([0,8000,0,16])\n",[187,66737,66738],{"class":189,"line":2621},[187,66739,66740],{},"plt.title('Maximum RPM vs. Static Pressure (mm/H2O)', fontsize=14)\n",[187,66742,66743],{"class":189,"line":2631},[187,66744,64375],{},[187,66746,66747],{"class":189,"line":2642},[187,66748,66616],{},[187,66750,66751],{"class":189,"line":2653},[187,66752,62953],{},[187,66754,66755],{"class":189,"line":2665},[187,66756,62958],{},[187,66758,66759],{"class":189,"line":2674},[187,66760,316],{"emptyLinePlaceholder":315},[187,66762,66763],{"class":189,"line":2684},[187,66764,64496],{},[187,66766,66767],{"class":189,"line":2694},[187,66768,66769],{},"size = df5.rpm_max.reshape(df5.rpm_max.shape[0],1)\n",[187,66771,66772],{"class":189,"line":2706},[187,66773,66774],{},"pred = lreg.predict(df5.rpm_max.reshape(df5.rpm_max.shape[0],1))\n",[187,66776,66777],{"class":189,"line":2715},[187,66778,64511],{},[187,66780,66781],{"class":189,"line":2725},[187,66782,66783],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/fan/rpm_max_vs_static_pressure.png'))\n",[187,66785,66786],{"class":189,"line":2735},[187,66787,66788],{},"print lreg.score(X,y,sample_weight=None)\n",[11,66790,66791],{},[33,66792,66793],{},"0.711316685518",[11,66795,66796],{},[511,66797],{"alt":7255,"src":66798},"/static/pcpp/fan/rpm_max_vs_static_pressure.png",[11,66800,66801],{},"The stronger force of the air flow from high static pressure fans is better at blowing hot air off of hot surfaces, so these fans are often attached to liquid CPU cooler radiators and CPU cooler heat blocks.",[11,66803,66804],{},"As we discussed TDP and CPU coolers, we know that managing heat well will improve performance. Every part of a PC generates heat. Some memory sticks even include heat shrouds to help keep temperatures down. Software utilities are available to monitor the temperature of every component in a PC build, and dynamically change the fan speeds to help keep temperatures low and framerates high.",[2215,66806,66808],{"id":66807},"monitors","Monitors",[11,66810,66811],{},"Monitors come in all shapes and sizes. Here's a look at the different dimensions of monitors listed:",[26,66813,66815],{"className":1383,"code":66814,"language":1125,"meta":35,"style":35},"plt.figure(figsize=(12,8))\nplt.scatter(df.screen_x, df.screen_y, alpha=0.01, s=150)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.axis([0,35,0,35./2])\nplt.title('Screen Size')\nplt.xlabel('Screen Width (inches)', fontsize=14)\nplt.ylabel('Screen Height (inches)', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/monitor/screen_size.png'))\n",[33,66816,66817,66821,66826,66831,66836,66841,66846,66851,66855,66859],{"__ignoreMap":35},[187,66818,66819],{"class":189,"line":190},[187,66820,62266],{},[187,66822,66823],{"class":189,"line":249},[187,66824,66825],{},"plt.scatter(df.screen_x, df.screen_y, alpha=0.01, s=150)\n",[187,66827,66828],{"class":189,"line":312},[187,66829,66830],{},"plt.gca().set_aspect('equal', adjustable='box')\n",[187,66832,66833],{"class":189,"line":319},[187,66834,66835],{},"plt.axis([0,35,0,35./2])\n",[187,66837,66838],{"class":189,"line":325},[187,66839,66840],{},"plt.title('Screen Size')\n",[187,66842,66843],{"class":189,"line":686},[187,66844,66845],{},"plt.xlabel('Screen Width (inches)', fontsize=14)\n",[187,66847,66848],{"class":189,"line":697},[187,66849,66850],{},"plt.ylabel('Screen Height (inches)', fontsize=14)\n",[187,66852,66853],{"class":189,"line":1291},[187,66854,62953],{},[187,66856,66857],{"class":189,"line":1306},[187,66858,62958],{},[187,66860,66861],{"class":189,"line":1434},[187,66862,66863],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/monitor/screen_size.png'))\n",[11,66865,66866],{},[511,66867],{"alt":7255,"src":66868},"/static/pcpp/monitor/screen_size.png",[11,66870,66871,66872,66874,66875,66878,66879,66882,66883,66885],{},"Monitors are measured not only in size, but in a number of different measures related to ",[4339,66873,37981],{},". Framerate refers to how many frames per second are rendered in programs, typically games. A display monitor can show as many frames per second as its ",[4339,66876,66877],{},"refresh rate"," allows. Most modern games and monitors use a technology called V-Sync (vertical-sync), which limits the ",[4339,66880,66881],{},"frame rate"," to the maximum ",[4339,66884,66877],{}," that the monitor can handle.",[11,66887,66888,66889,358],{},"Input lag and response time are two other important monitor metrics, particullarly for competitive gamers. Input lag refers to the amount of time it takes for user input from peripherals, such as mouse and keyboard, to be reflected on the screen. Input lag typically ranges from (range), and can make all of the difference in FPS games (first-person shooters). Here's an explination of response time from an enthusiast monitor site, ",[15,66890,66893],{"href":66891,"rel":66892},"http://www.144hzmonitors.com/knowledge-base/what-does-response-time/",[19],"www.144hzmonitors.com",[107,66895,66896],{},[11,66897,66898],{},"Response time is a measure of quickly a pixel can display a change from either black to white or from one shade of gray to another. Lower response times are better. Normal response time right now is 1ms for TN panels and 4ms for IPS panels.",[11,66900,66901],{},"Response time is an important metric for playing fast-paced games and watching action movies. If pixels are not able to fully change color in between the time of each from (typically 17 ms for 60Hz monitors), then images displayed may appear with blurred motion trailing certain objects displayed on screen.",[11,66903,66904],{},"There are lots of features for monitors that would seem to be good predictors of price, but there is too much missing data to draw meaningful conclusions. Instead, we can look at another dimension of the pricing that we still haven't explored.",[168,66906,66908],{"id":66907},"vendor-data","Vendor Data",[11,66910,66911],{},"In the graphs and models used above for exploring various types of PC components, the price associated with an individual part is the average of prices offered by any number of vendors. For example, one CPU may be sold by Amazon for $150.00, NewEgg for $145.00 and NCIX US for $170.00. This CPU would be priced at $155 in the dataset.",[11,66913,66914],{},"I'm very interested in understanding how the different vendors compare to one another on product offering prices. My intuition and (experience using the site) tells me that Amazon has the best deals overall, but I would like to find a way to show this, and possibly rank and score the vendors against one another.",[11,66916,66917],{},"Here's a simple method I've devised for comparing vendors:",[26,66919,66921],{"className":1383,"code":66920,"language":1125,"meta":35,"style":35},"df['Pricing_'] = [ast.literal_eval(x) for x in df.Pricing]\n\ndef score_vendors(prices):\n    mean = np.mean([prices[x] for x in prices])\n    std = np.std([prices[x] for x in prices])\n    def z_score(std, mean, price):\n        diff = float(price - mean)\n        z_score = diff/std\n        return z_score\n    vendor_dict = {}\n    for x in prices:\n        vendor_z_score = z_score(std, mean, prices[x])\n        vendor_dict[x] = vendor_z_score\n    return vendor_dict\n\nvendor_z_score_list = []\nfor x in df.Pricing_:\n    z_score_dict = score_vendors(x)\n    vendor_z_score_list.append(z_score_dict)\n\nvend_df = pd.DataFrame(vendor_z_score_list)\n\nvendor_avg = {}\nfor x in vend_df.columns:\n    vendor_avg[x] = vend_df[x].dropna().sum()/vend_df[x].dropna().count()\n\nvendor_avg_chart = pd.DataFrame(vendor_avg, index=[0])\n\nvendor_avg_chart.T.sort_values(by=0, ascending=True).plot(kind='bar', figsize=(12,8))\nplt.title('Average Z-score for Monitor prices by Vendor', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xlabel('Vendor', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(['Average Z-score'], loc='upper left', fontsize=14)\n",[33,66922,66923,66928,66932,66937,66942,66947,66952,66957,66962,66967,66972,66977,66982,66987,66992,66996,67001,67006,67011,67016,67020,67025,67029,67034,67039,67044,67048,67053,67057,67062,67067,67071,67076,67080,67084],{"__ignoreMap":35},[187,66924,66925],{"class":189,"line":190},[187,66926,66927],{},"df['Pricing_'] = [ast.literal_eval(x) for x in df.Pricing]\n",[187,66929,66930],{"class":189,"line":249},[187,66931,316],{"emptyLinePlaceholder":315},[187,66933,66934],{"class":189,"line":312},[187,66935,66936],{},"def score_vendors(prices):\n",[187,66938,66939],{"class":189,"line":319},[187,66940,66941],{},"    mean = np.mean([prices[x] for x in prices])\n",[187,66943,66944],{"class":189,"line":325},[187,66945,66946],{},"    std = np.std([prices[x] for x in prices])\n",[187,66948,66949],{"class":189,"line":686},[187,66950,66951],{},"    def z_score(std, mean, price):\n",[187,66953,66954],{"class":189,"line":697},[187,66955,66956],{},"        diff = float(price - mean)\n",[187,66958,66959],{"class":189,"line":1291},[187,66960,66961],{},"        z_score = diff/std\n",[187,66963,66964],{"class":189,"line":1306},[187,66965,66966],{},"        return z_score\n",[187,66968,66969],{"class":189,"line":1434},[187,66970,66971],{},"    vendor_dict = {}\n",[187,66973,66974],{"class":189,"line":2599},[187,66975,66976],{},"    for x in prices:\n",[187,66978,66979],{"class":189,"line":2607},[187,66980,66981],{},"        vendor_z_score = z_score(std, mean, prices[x])\n",[187,66983,66984],{"class":189,"line":2621},[187,66985,66986],{},"        vendor_dict[x] = vendor_z_score\n",[187,66988,66989],{"class":189,"line":2631},[187,66990,66991],{},"    return vendor_dict\n",[187,66993,66994],{"class":189,"line":2642},[187,66995,316],{"emptyLinePlaceholder":315},[187,66997,66998],{"class":189,"line":2653},[187,66999,67000],{},"vendor_z_score_list = []\n",[187,67002,67003],{"class":189,"line":2665},[187,67004,67005],{},"for x in df.Pricing_:\n",[187,67007,67008],{"class":189,"line":2674},[187,67009,67010],{},"    z_score_dict = score_vendors(x)\n",[187,67012,67013],{"class":189,"line":2684},[187,67014,67015],{},"    vendor_z_score_list.append(z_score_dict)\n",[187,67017,67018],{"class":189,"line":2694},[187,67019,316],{"emptyLinePlaceholder":315},[187,67021,67022],{"class":189,"line":2706},[187,67023,67024],{},"vend_df = pd.DataFrame(vendor_z_score_list)\n",[187,67026,67027],{"class":189,"line":2715},[187,67028,316],{"emptyLinePlaceholder":315},[187,67030,67031],{"class":189,"line":2725},[187,67032,67033],{},"vendor_avg = {}\n",[187,67035,67036],{"class":189,"line":2735},[187,67037,67038],{},"for x in vend_df.columns:\n",[187,67040,67041],{"class":189,"line":2743},[187,67042,67043],{},"    vendor_avg[x] = vend_df[x].dropna().sum()/vend_df[x].dropna().count()\n",[187,67045,67046],{"class":189,"line":2754},[187,67047,316],{"emptyLinePlaceholder":315},[187,67049,67050],{"class":189,"line":2762},[187,67051,67052],{},"vendor_avg_chart = pd.DataFrame(vendor_avg, index=[0])\n",[187,67054,67055],{"class":189,"line":2770},[187,67056,316],{"emptyLinePlaceholder":315},[187,67058,67059],{"class":189,"line":2781},[187,67060,67061],{},"vendor_avg_chart.T.sort_values(by=0, ascending=True).plot(kind='bar', figsize=(12,8))\n",[187,67063,67064],{"class":189,"line":2792},[187,67065,67066],{},"plt.title('Average Z-score for Monitor prices by Vendor', fontsize=14)\n",[187,67068,67069],{"class":189,"line":2803},[187,67070,63049],{},[187,67072,67073],{"class":189,"line":2808},[187,67074,67075],{},"plt.xlabel('Vendor', fontsize=14)\n",[187,67077,67078],{"class":189,"line":2816},[187,67079,62953],{},[187,67081,67082],{"class":189,"line":2824},[187,67083,62958],{},[187,67085,67086],{"class":189,"line":2834},[187,67087,67088],{},"plt.legend(['Average Z-score'], loc='upper left', fontsize=14)\n",[11,67090,67091],{},[511,67092],{"alt":7255,"src":67093},"/static/pcpp/monitor/average_z_score_by_vendor.png",[11,67095,67096],{},"One problem with this method of scoring is that there is a very small number of price observations for each part, and the price observations generally are not normally distributed. Also, some vendors offer significantly more products than others. However, it does support my guess that Amazon has the most competitive product offerings overall.",[11,67098,67099],{},"The products with the fewest offerings tend to have the highest prices. Here's a look at how many monitors each vendor offers:",[26,67101,67103],{"className":1383,"code":67102,"language":1125,"meta":35,"style":35},"vendor_count = {}\nfor x in vend_df.columns:\n    vendor_count[x] = vend_df[x].dropna().count()\n\nvendor_count_chart = pd.DataFrame(vendor_count, index=[0])\n\nvendor_count_chart.T.sort_values(by=0, ascending=True).plot(kind='bar', figsize=(12,8))\nplt.title('Product count by Vendor', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xlabel('Vendor', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.legend(['Count'], fontsize=14, loc='upper left')\n",[33,67104,67105,67110,67114,67119,67123,67128,67132,67137,67142,67146,67150,67154,67158],{"__ignoreMap":35},[187,67106,67107],{"class":189,"line":190},[187,67108,67109],{},"vendor_count = {}\n",[187,67111,67112],{"class":189,"line":249},[187,67113,67038],{},[187,67115,67116],{"class":189,"line":312},[187,67117,67118],{},"    vendor_count[x] = vend_df[x].dropna().count()\n",[187,67120,67121],{"class":189,"line":319},[187,67122,316],{"emptyLinePlaceholder":315},[187,67124,67125],{"class":189,"line":325},[187,67126,67127],{},"vendor_count_chart = pd.DataFrame(vendor_count, index=[0])\n",[187,67129,67130],{"class":189,"line":686},[187,67131,316],{"emptyLinePlaceholder":315},[187,67133,67134],{"class":189,"line":697},[187,67135,67136],{},"vendor_count_chart.T.sort_values(by=0, ascending=True).plot(kind='bar', figsize=(12,8))\n",[187,67138,67139],{"class":189,"line":1291},[187,67140,67141],{},"plt.title('Product count by Vendor', fontsize=14)\n",[187,67143,67144],{"class":189,"line":1306},[187,67145,63049],{},[187,67147,67148],{"class":189,"line":1434},[187,67149,67075],{},[187,67151,67152],{"class":189,"line":2599},[187,67153,62953],{},[187,67155,67156],{"class":189,"line":2607},[187,67157,62958],{},[187,67159,67160],{"class":189,"line":2621},[187,67161,67162],{},"plt.legend(['Count'], fontsize=14, loc='upper left')\n",[11,67164,67165],{},[511,67166],{"alt":7255,"src":67167},"/static/pcpp/monitor/monitors_by_vendor.png",[168,67169,67171],{"id":67170},"user-reviews","User Reviews",[11,67173,67174],{},"One other interesting dimension of the PC part data is user reviews. Users are able to leave reviews for parts they included in their PCs. Reviews include a short text description with a star-rating (between 1 and 5 stars). Here's a short sample of some of this data:",[26,67176,67178],{"className":1383,"code":67177,"language":1125,"meta":35,"style":35},"df.Stars.value_counts().plot(kind='bar', figsize=(12,8), rot=0)\nplt.title('GPU User Review Star Ratings', fontsize=14)\nplt.xlabel('Star Rating', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/star_ratings.png'))\n",[33,67179,67180,67185,67190,67195,67199,67203,67207],{"__ignoreMap":35},[187,67181,67182],{"class":189,"line":190},[187,67183,67184],{},"df.Stars.value_counts().plot(kind='bar', figsize=(12,8), rot=0)\n",[187,67186,67187],{"class":189,"line":249},[187,67188,67189],{},"plt.title('GPU User Review Star Ratings', fontsize=14)\n",[187,67191,67192],{"class":189,"line":312},[187,67193,67194],{},"plt.xlabel('Star Rating', fontsize=14)\n",[187,67196,67197],{"class":189,"line":319},[187,67198,63049],{},[187,67200,67201],{"class":189,"line":325},[187,67202,62953],{},[187,67204,67205],{"class":189,"line":686},[187,67206,62958],{},[187,67208,67209],{"class":189,"line":697},[187,67210,67211],{},"plt.savefig(os.path.expanduser('~/Documents/GitHub/briancaffey.github.io/img/gpu/star_ratings.png'))\n",[11,67213,67214],{},[511,67215],{"alt":7255,"src":67216},"/static/pcpp/gpu/star_ratings.png",[11,67218,67219],{},"Here's one text rating from each of the 5 categories:",[911,67221,67223],{"id":67222},"_5-star-rating","5 Star Rating",[107,67225,67226],{},[11,67227,67228],{},"Amazing GPU; 4 GB VRAM, and it processes data faster than I thought it would. Not a fancy 900-series one but still very powerful.",[911,67230,67232],{"id":67231},"_4-star-rating","4 Star Rating",[107,67234,67235],{},[11,67236,67237],{},"Runs cool, aesthetically pleasing. -1 star for the VRAM fiasco with Nvidia.",[911,67239,67241],{"id":67240},"_3-star-rating","3 Star Rating",[107,67243,67244],{},[11,67245,67246],{},"I like the aesthetics of this card, but I don’t like that I couldn’t get a 960 like I hoped. I for sure will let people not to go to the store to get a GPU, as they are overpriced at both Best Buy and Micro Center. Also, if you are planning to use this with an analog monitor, be sure that the monitor has a DisplayPort/HDMI/DVI input (and you have a cable of one of those types), as this card DOES NOT in ANY WAY accept analog signals.",[911,67248,67250],{"id":67249},"_2-star-rating","2 Star Rating",[107,67252,67253],{},[11,67254,67255],{},"It's a potato. You cannot play any game at any decent framerates even with a FX-8320.",[911,67257,67259],{"id":67258},"_1-star-rating","1 Star Rating",[107,67261,67262],{},[11,67263,67264],{},"CRAP",[11,67266,67267],{},"With this labeled text data we can use natural language processing (NLP) techniques to predict the sentiment (the star rating in this case) for a new text review. At a very basic level, this works by assigning a probabilities to each word in a review and then generating a binary prediction based on a statistical model.",[11,67269,67270],{},"To simplify the problem, we can have our model predict not how many stars a text review would have, but whether or not the text rating is a 5-star rating. This reduces the complexity of the task.",[11,67272,67273],{},"Before we do any statistical modeling, it is important that we make a very simple prediction based on the most common star rating. For GPUs, 67% of the ratings tend to be 5-star ratings, then we could expect to have an accuracy of 67% percent if we predicted that any new ratings are 5-stars, regardless of the accompanying text. It will be important to check our NLP model against this \"baseline\" prediction; hopefully we can significantly improve on it.",[2215,67275,67277],{"id":67276},"naive-bayes-text-classification","Naive Bayes text classification",[11,67279,67280,67281,67286],{},"I will be using techniques introduced in the 'Working With Text Data' tutorial introduced in the ",[15,67282,67285],{"href":67283,"rel":67284},"http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html",[19],"scikit-learn documentation"," to classify text reviews. First we will put the labeled review data from the different part types into one DataFrame and map the 5-star rating to a binary variable where 5 stars ratings are mapped to 1 and ratings less than 5 stars are mapped to 0.",[26,67288,67290],{"className":1383,"code":67289,"language":1125,"meta":35,"style":35},"os.chdir(os.path.expanduser('~/Documents/Projects/Data/PCPP/parts/x_comment_csv_files/'))\nfiles = os.listdir(os.getcwd())\ndf = pd.DataFrame(index=[0])\nfor x in files:\n    df1 = pd.read_csv(x)\n    df= df.append(df1)\n\n    df = df.dropna()\n\ndf['five'] = ['five' if x == 5 else 'not' for x in df.Stars]\ndf['label'] = df.five.map({'not':0, 'five':1})\nprint df.label.value_counts()\n",[33,67291,67292,67297,67301,67306,67311,67316,67321,67325,67330,67334,67339,67344],{"__ignoreMap":35},[187,67293,67294],{"class":189,"line":190},[187,67295,67296],{},"os.chdir(os.path.expanduser('~/Documents/Projects/Data/PCPP/parts/x_comment_csv_files/'))\n",[187,67298,67299],{"class":189,"line":249},[187,67300,61119],{},[187,67302,67303],{"class":189,"line":312},[187,67304,67305],{},"df = pd.DataFrame(index=[0])\n",[187,67307,67308],{"class":189,"line":319},[187,67309,67310],{},"for x in files:\n",[187,67312,67313],{"class":189,"line":325},[187,67314,67315],{},"    df1 = pd.read_csv(x)\n",[187,67317,67318],{"class":189,"line":686},[187,67319,67320],{},"    df= df.append(df1)\n",[187,67322,67323],{"class":189,"line":697},[187,67324,316],{"emptyLinePlaceholder":315},[187,67326,67327],{"class":189,"line":1291},[187,67328,67329],{},"    df = df.dropna()\n",[187,67331,67332],{"class":189,"line":1306},[187,67333,316],{"emptyLinePlaceholder":315},[187,67335,67336],{"class":189,"line":1434},[187,67337,67338],{},"df['five'] = ['five' if x == 5 else 'not' for x in df.Stars]\n",[187,67340,67341],{"class":189,"line":2599},[187,67342,67343],{},"df['label'] = df.five.map({'not':0, 'five':1})\n",[187,67345,67346],{"class":189,"line":2607},[187,67347,67348],{},"print df.label.value_counts()\n",[26,67350,67353],{"className":67351,"code":67352,"language":31,"meta":35},[29],"1    1197\n0     769\nName: label, dtype: int64\n",[33,67354,67352],{"__ignoreMap":35},[11,67356,67357,67358,343],{},"With 1197 positive reviews and 769 negative reviews, our baseline prediction would would predict that all ratings are postive and we would expect an accuracy of about 60.8% (",[33,67359,67360],{},"1197/(1197+769)",[11,67362,67363,67364,67367],{},"The next step is to split the data into training and testing data, tokenize the text data and transform the structure of the data into a format that we can use to train our model. I'll describe this process in detail in ",[15,67365,67366],{"href":35},"another post",", but here is how we do this with scikit-learn:",[26,67369,67371],{"className":1383,"code":67370,"language":1125,"meta":35,"style":35},"X = df.Comments\ny = df.label\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df.label, test_size=0.2, random_state=4)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\n\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\n",[33,67372,67373,67378,67383,67387,67392,67397,67401,67406,67411,67415,67420],{"__ignoreMap":35},[187,67374,67375],{"class":189,"line":190},[187,67376,67377],{},"X = df.Comments\n",[187,67379,67380],{"class":189,"line":249},[187,67381,67382],{},"y = df.label\n",[187,67384,67385],{"class":189,"line":312},[187,67386,316],{"emptyLinePlaceholder":315},[187,67388,67389],{"class":189,"line":319},[187,67390,67391],{},"from sklearn.cross_validation import train_test_split\n",[187,67393,67394],{"class":189,"line":325},[187,67395,67396],{},"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df.label, test_size=0.2, random_state=4)\n",[187,67398,67399],{"class":189,"line":686},[187,67400,316],{"emptyLinePlaceholder":315},[187,67402,67403],{"class":189,"line":697},[187,67404,67405],{},"from sklearn.feature_extraction.text import CountVectorizer\n",[187,67407,67408],{"class":189,"line":1291},[187,67409,67410],{},"vect = CountVectorizer()\n",[187,67412,67413],{"class":189,"line":1306},[187,67414,316],{"emptyLinePlaceholder":315},[187,67416,67417],{"class":189,"line":1434},[187,67418,67419],{},"X_train_dtm = vect.fit_transform(X_train)\n",[187,67421,67422],{"class":189,"line":2599},[187,67423,67424],{},"X_test_dtm = vect.transform(X_test)\n",[11,67426,67427],{},"Finally, we train and evaluate our model using a Multinomial Naive Bayes model:",[26,67429,67431],{"className":1383,"code":67430,"language":1125,"meta":35,"style":35},"# train a Naive Bayes model using X_train_dtm\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)\n\n# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)\n\n\n# calculate accuracy of class predictions\nfrom sklearn import metrics\nprint 'Accuracy score: ' + str(metrics.accuracy_score(y_test, y_pred_class))\n\n# confusion matrix\nprint \"Confusion Matrix: \\n\" + str(metrics.confusion_matrix(y_test, y_pred_class))\n",[33,67432,67433,67438,67443,67448,67453,67457,67462,67467,67471,67475,67480,67484,67489,67493,67498],{"__ignoreMap":35},[187,67434,67435],{"class":189,"line":190},[187,67436,67437],{},"# train a Naive Bayes model using X_train_dtm\n",[187,67439,67440],{"class":189,"line":249},[187,67441,67442],{},"from sklearn.naive_bayes import MultinomialNB\n",[187,67444,67445],{"class":189,"line":312},[187,67446,67447],{},"nb = MultinomialNB()\n",[187,67449,67450],{"class":189,"line":319},[187,67451,67452],{},"nb.fit(X_train_dtm, y_train)\n",[187,67454,67455],{"class":189,"line":325},[187,67456,316],{"emptyLinePlaceholder":315},[187,67458,67459],{"class":189,"line":686},[187,67460,67461],{},"# make class predictions for X_test_dtm\n",[187,67463,67464],{"class":189,"line":697},[187,67465,67466],{},"y_pred_class = nb.predict(X_test_dtm)\n",[187,67468,67469],{"class":189,"line":1291},[187,67470,316],{"emptyLinePlaceholder":315},[187,67472,67473],{"class":189,"line":1306},[187,67474,316],{"emptyLinePlaceholder":315},[187,67476,67477],{"class":189,"line":1434},[187,67478,67479],{},"# calculate accuracy of class predictions\n",[187,67481,67482],{"class":189,"line":2599},[187,67483,63527],{},[187,67485,67486],{"class":189,"line":2607},[187,67487,67488],{},"print 'Accuracy score: ' + str(metrics.accuracy_score(y_test, y_pred_class))\n",[187,67490,67491],{"class":189,"line":2621},[187,67492,316],{"emptyLinePlaceholder":315},[187,67494,67495],{"class":189,"line":2631},[187,67496,67497],{},"# confusion matrix\n",[187,67499,67500],{"class":189,"line":2642},[187,67501,67502],{},"print \"Confusion Matrix: \\n\" + str(metrics.confusion_matrix(y_test, y_pred_class))\n",[26,67504,67507],{"className":67505,"code":67506,"language":31,"meta":35},[29],"Accuracy score: 0.723350253807\nConfusion Matrix:\n[[ 84  70]\n [ 39 201]]\n",[33,67508,67506],{"__ignoreMap":35},[11,67510,67511,67512,67515,67516,67519],{},"Here's a quick overview of what we just did. First, we split the labeled text data into training data and test data. We trained our model on the training data and then used the trained model to predict the label of 20% percent of our data set. One important thing to do in the training process is to split the train and test data into equal proportions of positive and negative reviews. This is accomplished by including ",[33,67513,67514],{},"stratify=df.label"," as a parameter for our ",[33,67517,67518],{},"train_test_split"," function.",[11,67521,67522],{},"We correctly predicted the label for 285 reviews of the 394 text reviews in our testing data, resulting in an accuracy score of 72.3%, which is a significant improvement over our 60% baseline prediction (predicting that all ratings are 5 star ratings). The confusion matrix adds more dimensions to our accuracy score. Here's what the numbers in the confusion matrix correspond to:",[916,67524,67525,67531,67536,67542],{},[919,67526,67527,67530],{},[338,67528,67529],{},"84",": 84 text reviews were correctly predicted to have less than 5 stars. (true positives)",[919,67532,67533,67535],{},[338,67534,55684],{},": 70 text reviews were predicted to have 5 stars but had less than 5 stars. (false negatives)",[919,67537,67538,67541],{},[338,67539,67540],{},"39",": 39 text reviews were predicted to have less than 5 stars had 5 stars. (false positives)",[919,67543,67544,67547],{},[338,67545,67546],{},"201",": 201 text reviews were correctly predicted to have 5 stars. (true negatives)",[11,67549,67550],{},"Let's take a look at a few of the text reviews for false negatives found in our results (reviews that were predicted to have 5 stars but had less than 5 stars):",[107,67552,67553],{},[11,67554,67555],{},"Great GPU",[107,67557,67558],{},[11,67559,67560],{},"So far so good.",[107,67562,67563],{},[11,67564,67565],{},"I don't play too many graphics-heavy games, so this card is pretty good for me.",[11,67567,67568],{},"And here are a few false positives (reviews predicted to have less than 5 star but rated 5 stars):",[107,67570,67571],{},[11,67572,67573],{},"I bought it because it was yellow",[107,67575,67576],{},[11,67577,67578],{},"Works great, speed is as-advertised. It's a bit boring for putting on display in your case, but at least it's not covered in some lame sticker.",[107,67580,67581],{},[11,67582,67583],{},"RAM is RAM. End of story.",[11,67585,67586],{},"It's easy to see the limitations of such a model, but I think that the accuracy than can be achieved from this Naive Bayes approach is simply amazing given the nuance of natural language. A major limitation is the amount of text data available to train the model, and also the fact that some of the text reviews were not as carefully written as reviews you may see on Amazon.",[11,67588,67589,67590,67595],{},"There is a lot of exciting work being done in the area of sentiment analysis. Google uses similar principles when filtering out spam emails from regular emails in your inbox. A better approach would be to use a Recurrent Neural Network with Long Short Term Memory architure like the famous example of IMDB movie review sentiment analysis on ",[15,67591,67594],{"href":67592,"rel":67593},"http://deeplearning.net/tutorial/lstm.html",[19],"DeepLearning.net",". I'm curious to see how an advanced model would perform against a Naive Bayes model with a limited amount of training data.",[11,67597,67598],{},"We will do more text analysis with the user descriptions of PC builds. Instead of classifying builds, we will attempt to cluster them into distinct categories based on the language used in their descriptions.",[168,67600,67602],{"id":67601},"pc-builds","PC Builds",[11,67604,67605,67606,358],{},"We can now revisit data from the collection of PC builds. Each row in the builds DataFrame contains several links to the parts that are included in the. To do this, we will be merging the part data frames with the PC builds data frame. Here's a bried description of database-style DataFrame joining/merging from the ",[15,67607,67610],{"href":67608,"rel":67609},"http://pandas.pydata.org/pandas-docs/stable/merging.html",[19],"pandas documentation",[107,67612,67613],{},[11,67614,67615],{},"pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R). The reason for this is careful algorithmic design and internal layout of the data in DataFrame.",[11,67617,67618],{},"This will allow us to add additional information (as new columns) for each of the parts in the builds DataFrame. For example, we can add a column indicating the number of watts for the power supply of each build. Let's start with this simple example. Here's how we would perform this operation in pandas:",[26,67620,67622],{"className":1383,"code":67621,"language":1125,"meta":35,"style":35},"#navigate to the builds DataFrame\nos.chdir('/Users/andrewcaffey/Documents/Projects/Data/PCPP/builds/')\n\n#read in the builds DataFrame\ndf = pd.read_csv('master_build_csv.csv', low_memory=False)\n\n#Setup a new DataFrame that contains only unique IDs for the PSU of each build\ndf1 = df[['Power_Supply_1_link']]\n\n#navigate to the PSU DataFrame\nos.chdir(os.path.expanduser('~/Documents/Projects/Data/PCPP/parts/x_csv_files/'))\n\n#read in psu DataFrame\npsu_df = pd.read_csv('psu_csv.csv')\n\n#Filter for certain columns we are interested in\npsu_columns = [u'Modular', u'Name', u'avg', u'short_link', u'power', u'eff_rank', u'ppw', u'Manufacturer', u'Efficiency Certification']\n\n#redefine the DataFrame to include those columns only\npsu_df = psu_df[psu_columns]\n\n#merge DataFrames\ndf1 = pd.merge(df1, psu_df, how='left',left_on='Power_Supply_1_link', right_on='short_link')\n",[33,67623,67624,67629,67633,67637,67642,67647,67651,67656,67661,67665,67670,67675,67679,67684,67689,67693,67698,67703,67707,67712,67717,67721,67726],{"__ignoreMap":35},[187,67625,67626],{"class":189,"line":190},[187,67627,67628],{},"#navigate to the builds DataFrame\n",[187,67630,67631],{"class":189,"line":249},[187,67632,61520],{},[187,67634,67635],{"class":189,"line":312},[187,67636,316],{"emptyLinePlaceholder":315},[187,67638,67639],{"class":189,"line":319},[187,67640,67641],{},"#read in the builds DataFrame\n",[187,67643,67644],{"class":189,"line":325},[187,67645,67646],{},"df = pd.read_csv('master_build_csv.csv', low_memory=False)\n",[187,67648,67649],{"class":189,"line":686},[187,67650,316],{"emptyLinePlaceholder":315},[187,67652,67653],{"class":189,"line":697},[187,67654,67655],{},"#Setup a new DataFrame that contains only unique IDs for the PSU of each build\n",[187,67657,67658],{"class":189,"line":1291},[187,67659,67660],{},"df1 = df[['Power_Supply_1_link']]\n",[187,67662,67663],{"class":189,"line":1306},[187,67664,316],{"emptyLinePlaceholder":315},[187,67666,67667],{"class":189,"line":1434},[187,67668,67669],{},"#navigate to the PSU DataFrame\n",[187,67671,67672],{"class":189,"line":2599},[187,67673,67674],{},"os.chdir(os.path.expanduser('~/Documents/Projects/Data/PCPP/parts/x_csv_files/'))\n",[187,67676,67677],{"class":189,"line":2607},[187,67678,316],{"emptyLinePlaceholder":315},[187,67680,67681],{"class":189,"line":2621},[187,67682,67683],{},"#read in psu DataFrame\n",[187,67685,67686],{"class":189,"line":2631},[187,67687,67688],{},"psu_df = pd.read_csv('psu_csv.csv')\n",[187,67690,67691],{"class":189,"line":2642},[187,67692,316],{"emptyLinePlaceholder":315},[187,67694,67695],{"class":189,"line":2653},[187,67696,67697],{},"#Filter for certain columns we are interested in\n",[187,67699,67700],{"class":189,"line":2665},[187,67701,67702],{},"psu_columns = [u'Modular', u'Name', u'avg', u'short_link', u'power', u'eff_rank', u'ppw', u'Manufacturer', u'Efficiency Certification']\n",[187,67704,67705],{"class":189,"line":2674},[187,67706,316],{"emptyLinePlaceholder":315},[187,67708,67709],{"class":189,"line":2684},[187,67710,67711],{},"#redefine the DataFrame to include those columns only\n",[187,67713,67714],{"class":189,"line":2694},[187,67715,67716],{},"psu_df = psu_df[psu_columns]\n",[187,67718,67719],{"class":189,"line":2706},[187,67720,316],{"emptyLinePlaceholder":315},[187,67722,67723],{"class":189,"line":2715},[187,67724,67725],{},"#merge DataFrames\n",[187,67727,67728],{"class":189,"line":2725},[187,67729,67730],{},"df1 = pd.merge(df1, psu_df, how='left',left_on='Power_Supply_1_link', right_on='short_link')\n",[11,67732,67733,67734,67737,67738,67741,67742,5857,67745,1172,67748,67751,67752,1172,67754,67756,67757,67760,67761,67763,67764,752],{},"The arguments of ",[33,67735,67736],{},"pd.merge()"," define how we merge the information from ",[33,67739,67740],{},"psu_df"," onto ",[33,67743,67744],{},"df1",[33,67746,67747],{},"left_on",[33,67749,67750],{},"right_one"," define the columns that we will use in the ",[33,67753,67744],{},[33,67755,67740],{}," DataFrames to match information. ",[33,67758,67759],{},"how='left'"," essentially specifies that we want to keep all of the original rows of ",[33,67762,67744],{},", even if the PSU links for those rows are missing. There is more information and examples on DataFrame merging in the ",[15,67765,67767],{"href":67608,"rel":67766},[19],"documentation",[11,67769,67770,67772],{},[33,67771,67744],{}," is now a DataFrame that displays the PSU data for each unique build in the collection of nearly 26,000 PCs. This allows us to ask questions about the PC build data that we couldn't ask with link data alone, such as: what does the distribution of power ratings look like for PSUs across all PC builds?",[11,67774,67775],{},"In the next post I will merge all of the individual PC part data frames with the PC builds data frame so we can have a more granular look at the collection of computers and their parts.",[168,67777,67779],{"id":67778},"my-recent-pc-builds","My Recent PC Builds",[11,67781,67782],{},"As promised, here are the two builds that I put together last summer:",[2215,67784,60530],{"id":67785},"ascension-i",[11,67787,67788,67793,67794],{},[15,67789,67792],{"href":67790,"rel":67791},"https://pcpartpicker.com/list/fRx8d6",[19],"PCPartPicker part list"," / ",[15,67795,67798],{"href":67796,"rel":67797},"https://pcpartpicker.com/list/fRx8d6/by_merchant/",[19],"Price breakdown by merchant",[1525,67800,67801,67814],{},[1528,67802,67803],{},[1531,67804,67805,67808,67811],{},[1534,67806,62583],{"align":67807},"left",[1534,67809,67810],{"align":67807},"Item",[1534,67812,67813],{"align":67807},"Price",[1544,67815,67816,67832,67848,67864,67880,67897,67910,67926,67943,67959,67976,67993,68010,68021,68035],{},[1531,67817,67818,67822,67829],{},[1549,67819,67820],{"align":67807},[338,67821,61577],{},[1549,67823,67824],{"align":67807},[15,67825,67828],{"href":67826,"rel":67827},"https://pcpartpicker.com/product/tdmxFT/intel-cpu-bx80662i76700k",[19],"Intel Core i7-6700K 4.0GHz Quad-Core Processor",[1549,67830,67831],{"align":67807},"$329.25 @ OutletPC",[1531,67833,67834,67838,67845],{},[1549,67835,67836],{"align":67807},[338,67837,64210],{},[1549,67839,67840],{"align":67807},[15,67841,67844],{"href":67842,"rel":67843},"https://pcpartpicker.com/product/CrDzK8/corsair-cpu-cooler-cw9060025ww",[19],"Corsair H100i v2 70.7 CFM Liquid CPU Cooler",[1549,67846,67847],{"align":67807},"$99.99 @ B&H",[1531,67849,67850,67854,67861],{},[1549,67851,67852],{"align":67807},[338,67853,61654],{},[1549,67855,67856],{"align":67807},[15,67857,67860],{"href":67858,"rel":67859},"https://pcpartpicker.com/product/tBZ2FT/asus-motherboard-maximusviiihero",[19],"Asus MAXIMUS VIII HERO ATX LGA1151 Motherboard",[1549,67862,67863],{"align":67807},"$209.99 @ B&H",[1531,67865,67866,67870,67877],{},[1549,67867,67868],{"align":67807},[338,67869,61644],{},[1549,67871,67872],{"align":67807},[15,67873,67876],{"href":67874,"rel":67875},"https://pcpartpicker.com/product/dNLypg/crucial-memory-bls2k8g4d240fsa",[19],"Crucial Ballistix Sport 16GB (2 x 8GB) DDR4-2400 Memory",[1549,67878,67879],{"align":67807},"$109.64 @ B&H",[1531,67881,67882,67887,67894],{},[1549,67883,67884],{"align":67807},[338,67885,67886],{},"Storage",[1549,67888,67889],{"align":67807},[15,67890,67893],{"href":67891,"rel":67892},"https://pcpartpicker.com/product/3kL7YJ/samsung-internal-hard-drive-mz75e250bam",[19],"Samsung 850 EVO-Series 250GB 2.5\" Solid State Drive",[1549,67895,67896],{"align":67807},"$97.88 @ OutletPC",[1531,67898,67899,67903,67908],{},[1549,67900,67901],{"align":67807},[338,67902,67886],{},[1549,67904,67905],{"align":67807},[15,67906,67893],{"href":67891,"rel":67907},[19],[1549,67909,67896],{"align":67807},[1531,67911,67912,67916,67923],{},[1549,67913,67914],{"align":67807},[338,67915,67886],{},[1549,67917,67918],{"align":67807},[15,67919,67922],{"href":67920,"rel":67921},"https://pcpartpicker.com/product/Fz2kcf/western-digital-internal-hard-drive-wd1003fzex",[19],"Western Digital BLACK SERIES 1TB 3.5\" 7200RPM Internal Hard Drive",[1549,67924,67925],{"align":67807},"$69.00 @ B&H",[1531,67927,67928,67933,67940],{},[1549,67929,67930],{"align":67807},[338,67931,67932],{},"Video Card",[1549,67934,67935],{"align":67807},[15,67936,67939],{"href":67937,"rel":67938},"https://pcpartpicker.com/product/gRvZxr/msi-video-card-geforcegtx1080foundersedition",[19],"MSI GeForce GTX 1080 8GB Founders Edition Video Card",[1549,67941,67942],{"align":67807},"$660.31 @ Amazon",[1531,67944,67945,67949,67956],{},[1549,67946,67947],{"align":67807},[338,67948,61615],{},[1549,67950,67951],{"align":67807},[15,67952,67955],{"href":67953,"rel":67954},"https://pcpartpicker.com/product/9JvRsY/corsair-case-cc9011049ww",[19],"Corsair 450D ATX Mid Tower Case",[1549,67957,67958],{"align":67807},"$109.99 @ Newegg",[1531,67960,67961,67966,67973],{},[1549,67962,67963],{"align":67807},[338,67964,67965],{},"Power Supply",[1549,67967,67968],{"align":67807},[15,67969,67972],{"href":67970,"rel":67971},"https://pcpartpicker.com/product/DmPzK8/corsair-power-supply-cp9020086",[19],"Corsair 850W 80+ Gold Certified Semi-Modular ATX Power Supply",[1549,67974,67975],{"align":67807},"$120.98 @ Newegg",[1531,67977,67978,67983,67990],{},[1549,67979,67980],{"align":67807},[338,67981,67982],{},"Operating System",[1549,67984,67985],{"align":67807},[15,67986,67989],{"href":67987,"rel":67988},"https://pcpartpicker.com/product/MfH48d/microsoft-os-fqc08930",[19],"Microsoft Windows 10 Pro OEM 64-bit",[1549,67991,67992],{"align":67807},"$94.00 @ Amazon",[1531,67994,67995,68000,68007],{},[1549,67996,67997],{"align":67807},[338,67998,67999],{},"Software",[1549,68001,68002],{"align":67807},[15,68003,68006],{"href":68004,"rel":68005},"https://pcpartpicker.com/product/XwzZxr/eset-software-esshn111rbx2016",[19],"ESET Smart Security 2016 (1 Year Subscription) Software",[1549,68008,68009],{"align":67807},"$62.98 @ Newegg",[1531,68011,68012,68017,68019],{},[1549,68013,68014],{"align":67807},[4339,68015,68016],{},"Prices include shipping, taxes, rebates, and discounts",[1549,68018],{"align":67807},[1549,68020],{"align":67807},[1531,68022,68023,68028,68033],{},[1549,68024,68025],{"align":67807},[338,68026,68027],{},"Total",[1549,68029,68030],{"align":67807},[338,68031,68032],{},"$2061.89",[1549,68034],{"align":67807},[1531,68036,68037,68045,68047],{},[1549,68038,68039,68040,68044],{"align":67807},"Generated by ",[15,68041,60517],{"href":68042,"rel":68043},"http://pcpartpicker.com",[19]," 2017-02-11 21:29 EST-0500",[1549,68046],{"align":67807},[1549,68048],{"align":67807},[2215,68050,68052],{"id":68051},"beastmode-ii-bm2","Beastmode II (BM2)",[11,68054,68055,67793,68059],{},[15,68056,67792],{"href":68057,"rel":68058},"https://pcpartpicker.com/list/D83rWX",[19],[15,68060,67798],{"href":68061,"rel":68062},"https://pcpartpicker.com/list/D83rWX/by_merchant/",[19],[1525,68064,68065,68075],{},[1528,68066,68067],{},[1531,68068,68069,68071,68073],{},[1534,68070,62583],{"align":67807},[1534,68072,67810],{"align":67807},[1534,68074,67813],{"align":67807},[1544,68076,68077,68093,68106,68122,68137,68150,68163,68179,68192,68209,68222,68238,68254,68267,68283,68293,68303,68313,68326],{},[1531,68078,68079,68083,68090],{},[1549,68080,68081],{"align":67807},[338,68082,61577],{},[1549,68084,68085],{"align":67807},[15,68086,68089],{"href":68087,"rel":68088},"https://pcpartpicker.com/product/Td98TW/intel-cpu-bx80671i76800k",[19],"Intel Core i7-6800K 3.4GHz 6-Core Processor",[1549,68091,68092],{"align":67807},"$408.58 @ OutletPC",[1531,68094,68095,68099,68104],{},[1549,68096,68097],{"align":67807},[338,68098,64210],{},[1549,68100,68101],{"align":67807},[15,68102,67844],{"href":67842,"rel":68103},[19],[1549,68105,67847],{"align":67807},[1531,68107,68108,68112,68119],{},[1549,68109,68110],{"align":67807},[338,68111,61654],{},[1549,68113,68114],{"align":67807},[15,68115,68118],{"href":68116,"rel":68117},"https://pcpartpicker.com/product/wtnG3C/gigabyte-ga-x99-designare-ex-atx-lga2011-3-motherboard-ga-x99-designare-ex",[19],"Gigabyte GA-X99-Designare EX ATX LGA2011-3 Motherboard",[1549,68120,68121],{"align":67807},"$418.95 @ B&H",[1531,68123,68124,68128,68134],{},[1549,68125,68126],{"align":67807},[338,68127,61644],{},[1549,68129,68130],{"align":67807},[15,68131,68133],{"href":61829,"rel":68132},[19],"Corsair Vengeance LPX 16GB (2 x 8GB) DDR4-3000 Memory",[1549,68135,68136],{"align":67807},"$124.99 @ Newegg",[1531,68138,68139,68143,68148],{},[1549,68140,68141],{"align":67807},[338,68142,61644],{},[1549,68144,68145],{"align":67807},[15,68146,68133],{"href":61829,"rel":68147},[19],[1549,68149,68136],{"align":67807},[1531,68151,68152,68156,68161],{},[1549,68153,68154],{"align":67807},[338,68155,67886],{},[1549,68157,68158],{"align":67807},[15,68159,67893],{"href":67891,"rel":68160},[19],[1549,68162,67896],{"align":67807},[1531,68164,68165,68169,68176],{},[1549,68166,68167],{"align":67807},[338,68168,67886],{},[1549,68170,68171],{"align":67807},[15,68172,68175],{"href":68173,"rel":68174},"https://pcpartpicker.com/product/XtjG3C/western-digital-internal-hard-drive-wd2003fzex",[19],"Western Digital BLACK SERIES 2TB 3.5\" 7200RPM Internal Hard Drive",[1549,68177,68178],{"align":67807},"$117.99 @ SuperBiiz",[1531,68180,68181,68185,68190],{},[1549,68182,68183],{"align":67807},[338,68184,67886],{},[1549,68186,68187],{"align":67807},[15,68188,68175],{"href":68173,"rel":68189},[19],[1549,68191,68178],{"align":67807},[1531,68193,68194,68198,68206],{},[1549,68195,68196],{"align":67807},[338,68197,67932],{},[1549,68199,68200,68205],{"align":67807},[15,68201,68204],{"href":68202,"rel":68203},"https://pcpartpicker.com/product/FwcMnQ/msi-video-card-geforcegtx1080armor8goc",[19],"MSI GeForce GTX 1080 8GB Video Card"," (2-Way SLI)",[1549,68207,68208],{"align":67807},"$598.45 @ Amazon",[1531,68210,68211,68215,68220],{},[1549,68212,68213],{"align":67807},[338,68214,67932],{},[1549,68216,68217,68205],{"align":67807},[15,68218,68204],{"href":68202,"rel":68219},[19],[1549,68221,68208],{"align":67807},[1531,68223,68224,68228,68235],{},[1549,68225,68226],{"align":67807},[338,68227,61615],{},[1549,68229,68230],{"align":67807},[15,68231,68234],{"href":68232,"rel":68233},"https://pcpartpicker.com/product/yLvRsY/corsair-case-760tblack",[19],"Corsair 760T Black ATX Full Tower Case",[1549,68236,68237],{"align":67807},"$176.33 @ Amazon",[1531,68239,68240,68244,68251],{},[1549,68241,68242],{"align":67807},[338,68243,67965],{},[1549,68245,68246],{"align":67807},[15,68247,68250],{"href":68248,"rel":68249},"https://pcpartpicker.com/product/dJ6BD3/evga-power-supply-220p21000xr",[19],"EVGA SuperNOVA 1000 P2 1000W 80+ Platinum Certified Fully-Modular ATX Power Supply",[1549,68252,68253],{"align":67807},"$183.80 @ OutletPC",[1531,68255,68256,68260,68265],{},[1549,68257,68258],{"align":67807},[338,68259,67982],{},[1549,68261,68262],{"align":67807},[15,68263,67989],{"href":67987,"rel":68264},[19],[1549,68266,67992],{"align":67807},[1531,68268,68269,68273,68280],{},[1549,68270,68271],{"align":67807},[338,68272,67999],{},[1549,68274,68275],{"align":67807},[15,68276,68279],{"href":68277,"rel":68278},"https://pcpartpicker.com/product/HmkwrH/eset-software-eavhn111rbx2016",[19],"ESET NOD32 Antivirus 2016 (1 Year Subscription) Software",[1549,68281,68282],{"align":67807},"$44.99 @ Adorama",[1531,68284,68285,68289,68291],{},[1549,68286,68287],{"align":67807},[4339,68288,68016],{},[1549,68290],{"align":67807},[1549,68292],{"align":67807},[1531,68294,68295,68298,68301],{},[1549,68296,68297],{"align":67807},"Total (before mail-in rebates)",[1549,68299,68300],{"align":67807},"$3227.38",[1549,68302],{"align":67807},[1531,68304,68305,68308,68311],{},[1549,68306,68307],{"align":67807},"Mail-in rebates",[1549,68309,68310],{"align":67807},"-$20.00",[1549,68312],{"align":67807},[1531,68314,68315,68319,68324],{},[1549,68316,68317],{"align":67807},[338,68318,68027],{},[1549,68320,68321],{"align":67807},[338,68322,68323],{},"$3207.38",[1549,68325],{"align":67807},[1531,68327,68328,68334,68336],{},[1549,68329,68039,68330,68333],{"align":67807},[15,68331,60517],{"href":68042,"rel":68332},[19]," 2017-02-11 21:46 EST-0500",[1549,68335],{"align":67807},[1549,68337],{"align":67807},[855,68339,68340],{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sJ8bj, html code.shiki .sJ8bj{--shiki-default:#6A737D;--shiki-dark:#6A737D}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}",{"title":35,"searchDepth":249,"depth":249,"links":68342},[68343,68344,68345,68346,68347,68348,68349,68356,68357],{"id":60541,"depth":249,"text":60542},{"id":61601,"depth":249,"text":61602},{"id":62204,"depth":249,"text":62205},{"id":61583,"depth":249,"text":61654},{"id":63723,"depth":249,"text":61577},{"id":66907,"depth":249,"text":66908},{"id":67170,"depth":249,"text":67171,"children":68350},[68351,68352,68353,68354,68355],{"id":67222,"depth":312,"text":67223},{"id":67231,"depth":312,"text":67232},{"id":67240,"depth":312,"text":67241},{"id":67249,"depth":312,"text":67250},{"id":67258,"depth":312,"text":67259},{"id":67601,"depth":249,"text":67602},{"id":67778,"depth":249,"text":67779},"2017-01-01","In the summer of 2016 I built two high-end computers, something I haven't done since 2011. I used PCPartPicker to research the components and read about PC builds similar to the ones I had in mind. It's a relatively new site that has a strong community of builders, helpful tools to help with part compatibility as well as extensive user reviews on PC components.",{"layout":29014},"/2017/01/01/pc-data",{"title":60508,"description":68359},"2017/01/01/pc-data",[1125,27051,68365,582],"machine-learning","hUwhLLgIEb7RkaKaDzT6K-ofSuexymRlwozkgexK6dU",{"id":68368,"title":68369,"body":68370,"comments":315,"date":68630,"description":35,"draft":872,"extension":873,"external":874,"image":68376,"meta":68631,"navigation":315,"path":68632,"seo":68633,"stem":68634,"tags":68635,"__hash__":68638},"blog/2016/04/07/home-media-setup.md","Home media center meta-tutorial with miniDLNA, Raspberry Pi, Deluge and Apple TV",{"type":8,"value":68371,"toc":68623},[68372,68377,68380,68383,68394,68397,68425,68429,68437,68446,68450,68459,68468,68472,68485,68491,68494,68500,68524,68527,68533,68536,68542,68560,68563,68569,68572,68580,68605,68609,68618,68620],[11,68373,68374],{},[511,68375],{"alt":7255,"src":68376},"/static/media-setup.png",[11,68378,68379],{},"For a few months now I have been enjoying a new home media system that I threw together with my Raspberry Pi. My setup allows me sit in my living room and stream content from my Raspberry Pi to my 4th Generation Apple TV in full 1080p resolution. The Raspberry Pi is a small, inexpensive stand-alone computer, but it can serve as a powerful media server, and I've been blown away by its consistent performance. Rather than write everything from scratch, I've gathered the tutorials I used when setting up my devices and software and sprinkled it with a few bits of knowledge that I wish I knew when I started.",[11,68381,68382],{},"Here is an overview of my current setup:",[916,68384,68385,68388,68391],{},[919,68386,68387],{},"Raspberry Pi Model 2 B with miniDLNA (media server), Deluge (for torrenting media), TorGuard (anonymous VPN service) and omxplayer (built-in Raspberry Pi media player for playing media right on the Raspberry Pi)",[919,68389,68390],{},"Apple TV with VLC app (for streaming content from my network-connected Raspberry Pi via miniDLNA)",[919,68392,68393],{},"iPhone/iPad with Creation 5 + Creation 2 Video Player (for streaming content from my Raspberry Pi to my mobile devices)",[11,68395,68396],{},"Here's what you'll need:",[916,68398,68399,68402,68405,68408,68411,68419,68422],{},[919,68400,68401],{},"Raspberry Pi (Model 2 or 3)",[919,68403,68404],{},"Wifi-connection",[919,68406,68407],{},"Laptop (you will be SSHing into the Raspberry Pi and managing your torrent downloads through a web-browser interface on your laptop)",[919,68409,68410],{},"Sufficiently large microSD card (mine is 64GB), large USB drive (I recommend 256GB) or external hard drive (1TB)",[919,68412,68413,68418],{},[15,68414,68417],{"href":68415,"rel":68416},"https://torguard.net/aff.php?aff=1933",[19],"TorGuard account"," (optional)",[919,68420,68421],{},"Apple TV (optional)",[919,68423,68424],{},"iPhone or iPad with Creation 5 and Creation 2 Video player app installed (optional)",[168,68426,68428],{"id":68427},"virtual-private-network-vpn","Virtual Private Network (VPN)",[11,68430,68431,68432,68436],{},"If you are planning on downloading Copyrighted content from a public tracker, you should be using a virtual private network (VNP). I have been using ",[15,68433,68435],{"href":68415,"rel":68434},[19],"TorGuard"," for about 6 months and have had excellent service, great download speeds and setup was fairly painless.",[11,68438,68439,68440,68445],{},"Follow along with ",[15,68441,68444],{"href":68442,"rel":68443},"https://torguard.net/knowledgebase.php?action=displayarticle&id=174",[19],"this tutorial"," on how to get TorGuard running on your Raspberry Pi.",[168,68447,68449],{"id":68448},"minidlna","miniDLNA",[11,68451,68452,68453,68458],{},"Next, we will install miniDLNA on your Raspberry Pi. miniDLNA stands for (mini) Digital Network Living Alliance and is a protocol that is used in many devices, including consoles, SmartTVs, and mobile devices. To install it, follow along with ",[15,68454,68457],{"href":68455,"rel":68456},"http://www.instructables.com/id/Raspberry-Pi-Media-Server-MiniDLNA/",[19],"this Instructables tutorial",". Step 4 (Mounting the drive on startup) is not absolutely necessary, but it is a good idea if you will be using a dedicated external USB drive or external hard drive to store your content. I don't have a dedicated USB stick that I use with miniDLNA, but it works just fine playing content from my 64GB microSD card.",[11,68460,68461,68462,68467],{},"Download ",[15,68463,68466],{"href":68464,"rel":68465},"http://www.creation.com.es/creation-5-app/",[19],"Creation 5"," and Creation 2 Video Player on your iPhone. These are free apps with very minimal advertising. Through the Creation 5 app, you should be able to select your miniDLNA server as a media source. You should then see your Music, Image and Video folders that you just configured, but they will all be empty.",[168,68469,68471],{"id":68470},"bittorrent-client-deluge","BitTorrent Client (Deluge)",[11,68473,68474,68475,68480,68481,68484],{},"Now that you have your VPN and miniDLNA set up, you will want to try it out with some new content. I use Deluge, a BitTorrent client written in Python, but there are plenty of other great options out there. Deluge is fairly light-weight, so it works well with the Raspberry Pi. There are a number of ways that you can access Deluge, I prefer to use the Deluge WebUI. Here's ",[15,68476,68479],{"href":68477,"rel":68478},"http://www.howtogeek.com/142044/how-to-turn-a-raspberry-pi-into-an-always-on-bittorrent-box/",[19],"one more tutorial"," from How-to Geek that talks about a few different ways to install and configure Deluge. I recommend jumping to the ",[33,68482,68483],{},"Setting up Deluge for WebUI Access"," section and running the three commands you will need to get started:",[26,68486,68489],{"className":68487,"code":68488,"language":31},[29],"$ sudo apt-get install deluged\n$ sudo apt-get install python-mako\n$ sudo apt-get install deluge-web\n",[33,68490,68488],{"__ignoreMap":35},[11,68492,68493],{},"This gets everything installed. To use Deluge, you will need to run two more commands:",[26,68495,68498],{"className":68496,"code":68497,"language":31},[29],"$ deluged\n$ deluge-web&\n",[33,68499,68497],{"__ignoreMap":35},[11,68501,68502,68505,68506,68508,68509,68512,68513,68516,68517],{},[33,68503,68504],{},"deluged"," runs the Deluge daemon (a background process; the ",[33,68507,50996],{}," at the end of ",[33,68510,68511],{},"deluge"," signifies that it is a daemon) that will start Deluge. ",[33,68514,68515],{},"deluge-web&"," starts that web interface that should be available at http://",[68518,68519,68520,68521,68523],"your",{"raspberry":35,"pi":35,"ip":35,"address":35},":8112. The ",[33,68522,34282],{}," simply keeps the command line available for running other commands.",[11,68525,68526],{},"You can set Deluge to save downloaded files directly into your various miniDLNA folders, but you will need to restart miniDLNA with the following commands before they are visible on your network. SSH into your Raspberry Pi by running the following command:",[26,68528,68531],{"className":68529,"code":68530,"language":31},[29],"$ ssh pi@\u003Cyour raspberry pi ip address>\n",[33,68532,68530],{"__ignoreMap":35},[11,68534,68535],{},"If you don't know your Raspberry Pi's IP address, run following command:",[26,68537,68540],{"className":68538,"code":68539,"language":31},[29],"$ ifconfig\n",[33,68541,68539],{"__ignoreMap":35},[11,68543,68544,68545,68548,68549,68552,68553,15754,68556,68559],{},"Look at the output under ",[33,68546,68547],{},"wlan0"," and it will be the address following ",[33,68550,68551],{},"inet addr",", something like ",[33,68554,68555],{},"192.168.1.5",[33,68557,68558],{},"10.0.1.132",". To clarify, this is your Raspberry Pi's internal IP address; it is assigned to your Raspberry Pi by your home network and it has nothing to do with the external IP address that you set up with TorGuard.",[11,68561,68562],{},"Enter your Raspberry Pi's login and password and then enter the following commands once you have established an SSH connection:",[26,68564,68567],{"className":68565,"code":68566,"language":31},[29],"$ sudo service minidlna restart\n$ sudo service minidlna force-reload\n",[33,68568,68566],{"__ignoreMap":35},[11,68570,68571],{},"Now you should be able to see your content in media players that support miniDLNA.",[11,68573,68574,68575,752],{},"Here are some additional resources for ",[15,68576,68579],{"href":68577,"rel":68578},"https://help.ubuntu.com/community/MiniDLNA",[19],"help with miniDLNA on Ubuntu.com",[11,68581,68582,68583,68588,68589,68592,68593,68598,68599,68604],{},"Before you start downloading content, I would use use TorGuard's ",[15,68584,68587],{"href":68585,"rel":68586},"https://torguard.net/checkmytorrentipaddress.php",[19],"Check My Torrent IP Address"," tool. This is a blank ",[33,68590,68591],{},".torrent"," file that you add to Deluge. In the torrent's tracker information you can see the Public IP address that is listed on the public tracker. This would be the address that ",[15,68594,68597],{"href":68595,"rel":68596},"https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act",[19],"Digital Millennium Copyright Act (DMCA)"," would report to your ISP if it was your public IP address, so make sure it is not ",[15,68600,68603],{"href":68601,"rel":68602},"http://www.whatsmyip.org/",[19],"your public IP address"," that you use in your home. You can select proxy IP address from a number of different countries in the TorGuard setup.",[168,68606,68608],{"id":68607},"vlc-for-tvos","VLC for tvOS",[11,68610,68611,68612,68617],{},"VLC is a great media player, and you probably have it downloaded on your computer. You might not have know that VLC has an app for the Apple TV. This app supports the playback of media from miniDLNA servers like the one you just set up, so you can stream content directly from your Raspberry Pi onto your Apple TV through the VLC app. One thing to note, however, is that VLC for tvOS ",[15,68613,68616],{"href":68614,"rel":68615},"https://forum.videolan.org/viewtopic.php?t=125032",[19],"does not support AC3 audio encodings",", but you should be good to go for almost any other audio/video format on the planet.",[168,68619,2413],{"id":15198},[11,68621,68622],{},"Using the Raspberry Pi certainly isn't necessary for simply downloading torrents, but having it set as a dedicated machine for downloading content and playing back files is a lot of fun and works better than I would have guessed.I don't like running torrents on my MacBook Air, and sometimes I like to use my laptop when I'm watching TV. I would usually have to plug in my laptop to my TV directly and mirror the laptop display onto my TV, but the solution I have eliminates this headache altogether. My setup is by no means perfect, but it gets the job done with a very tolerable amount of effort on my part. Let me know in the comments if you have any critiques or ideas for how to enhance the setup I have described here. Thanks, and good luck with setting up your own home media system.",{"title":35,"searchDepth":249,"depth":249,"links":68624},[68625,68626,68627,68628,68629],{"id":68427,"depth":249,"text":68428},{"id":68448,"depth":249,"text":68449},{"id":68470,"depth":249,"text":68471},{"id":68607,"depth":249,"text":68608},{"id":15198,"depth":249,"text":2413},"2016-04-07",{"layout":29014},"/2016/04/07/home-media-setup",{"title":68369,"description":35},"2016/04/07/home-media-setup",[68636,68637,68511,53509],"mini-dlna","raspberry-pi","cc9fe4z3Jq-M4HtC8mKshDqzO6eRPj_AH10pTyzb3jw",1753130126972]