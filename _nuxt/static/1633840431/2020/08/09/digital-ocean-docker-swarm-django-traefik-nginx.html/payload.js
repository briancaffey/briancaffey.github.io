__NUXT_JSONP__("/2020/08/09/digital-ocean-docker-swarm-django-traefik-nginx.html", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co,cp){return {data:[{article:{slug:"digital-ocean-docker-swarm-django-traefik-nginx.html",layout:"post",title:"Deploying Django applications with docker swarm on DigitalOcean using GitLab CI, Traefik, NGINX and REX-Ray",date:"2020-08-09T00:00:00.000Z",comments:true,image:"\u002Fstatic\u002Fshark.jpg",tags:["django",aI,"digital-ocean","vue","gitlab","rex-ray",V,M,"swarm"],toc:[{id:aJ,depth:I,text:aK},{id:aL,depth:I,text:aM},{id:aN,depth:I,text:aO},{id:aP,depth:I,text:aQ},{id:aR,depth:I,text:aS},{id:aT,depth:I,text:aU},{id:aV,depth:I,text:aW},{id:aX,depth:I,text:W},{id:aY,depth:I,text:aZ},{id:a_,depth:I,text:a$},{id:ba,depth:I,text:as},{id:bb,depth:I,text:Q},{id:E,depth:R,text:E},{id:M,depth:R,text:bc},{id:V,depth:R,text:bd},{id:be,depth:I,text:bf},{id:bg,depth:I,text:bh},{id:bi,depth:R,text:bj},{id:bk,depth:R,text:bl},{id:bm,depth:R,text:bn},{id:bo,depth:R,text:bp},{id:bq,depth:R,text:br}],body:{type:"root",children:[{type:b,tag:h,props:{},children:[{type:a,value:"I recently wrote two articles about deploying Django applications to AWS serverless environments: one on "},{type:b,tag:k,props:{href:"https:\u002F\u002Fbriancaffey.github.io\u002F2020\u002F06\u002F02\u002Fdjango-postgres-vue-gitlab-ecs.html",rel:[w,x,y],target:z},children:[{type:a,value:"AWS Fargate"}]},{type:a,value:" (CloudFront, ALB and ECS Fargate containers) and one on "},{type:b,tag:k,props:{href:"https:\u002F\u002Fbriancaffey.github.io\u002F2020\u002F08\u002F01\u002Fdjango-and-lambda-with-cdk-and-api-gateway.html",rel:[w,x,y],target:z},children:[{type:a,value:"AWS Lambda"}]},{type:a,value:" (Lambda + API Gateway, without using Zappa or Serverless Framework). Both projects focused on automating as much of the setup and operation as possible using DevOps patterns: Infrastructure as Code, GitOps, CI\u002FCD and docker containers. I used AWS resources exclusively (with the exception of GitLab) with the help of "},{type:b,tag:k,props:{href:"https:\u002F\u002Faws.amazon.com\u002Fcdk\u002F",rel:[w,x,y],target:z},children:[{type:a,value:"AWS Cloud Development Kit (CDK)"}]},{type:a,value:", an awesome tool that I have really come to like. In general I really like AWS, and the more I use it I start to think about what I would do without it. Also, a lot of the feedback I got on these projects recommended to \"just use a VPS\" instead of bothering with AWS because it is complicated, expensive, overkill, etc. This got me thinking about how far I could get in deploying a Django application on a server with little or no external services that AWS has spoiled me with. After a little bit of discomfort and confusion, I was able to check off most of what I was hoping to and came away with a few questions as well. If you are interested to know how I got things setup and and hear some of my thoughts on running Django applications in production, continue reading!"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"In this article, I'm going to go over my approach to deploying and running Django applications using DigitalOcean Droplets (Linux-based virtual machine that runs on top of virtualized hardware) and block storage volumes (network-based block devices that provide additional data storage for Droplets)."}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I'll also touch on trade-offs between DigitalOcean and AWS and emphasize aspects of the project that confuse\u002Fd me with these block quotes."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's a link to my project that I'll be referencing: "},{type:b,tag:k,props:{href:bs,rel:[w,x,y],target:z},children:[{type:a,value:bs}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The project setup is a combination of some of the best practices I have picked up along the way as well as some very helpful guides, repositories and blog posts that I'll do my best to reference throughout this article."}]},{type:a,value:f},{type:b,tag:J,props:{id:aJ},children:[{type:b,tag:k,props:{href:"#overview",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aK}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here are some of the key parts of the project that I'll go over:"}]},{type:a,value:f},{type:b,tag:at,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"DigitalOcean and GitLab setup"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Creating an A Record that points to our Droplet IP"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Using a prebuilt VM image that ships with docker and docker-compose"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Setting up the REX-Ray storage driver to automatically provision Digital Ocean block storage volumes"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Setting up a docker swarm cluster"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Setting up a "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" file to build images and push them to a private GitLab CI project registry"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Writing a docker-compose file to configure the services (containers) that will support the application"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Deploying a stack to the docker swarm cluster on DigitalOcean from our GitLab CI environment over SSH"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Django project settings and management commands for our Postgres database and static files"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Monitoring, logging and debugging"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Destroying the environment + cleanup"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Before I dig into all of this, I recommend that you check out "},{type:b,tag:k,props:{href:"https:\u002F\u002Fmattsegal.dev\u002Fdjango-prod-architectures.html",rel:[w,x,y],target:z},children:[{type:a,value:"this article about Django production architectures by Matt Segal"}]},{type:a,value:". This is a great primer for a lot of what I'll be talking about and it includes some great visualizations. "},{type:b,tag:k,props:{href:"https:\u002F\u002Fmattsegal.dev",rel:[w,x,y],target:z},children:[{type:a,value:"mattsegal.dev"}]},{type:a,value:" has lots of good content related to Django, I also recommend checking out "},{type:b,tag:k,props:{href:"https:\u002F\u002Fmattsegal.dev\u002Fnginx-django-reverse-proxy-config.html",rel:[w,x,y],target:z},children:[{type:a,value:"this article about how NGINX is used with Django"}]},{type:a,value:". Thanks for the great resources, Matt!"}]},{type:a,value:f},{type:b,tag:J,props:{id:aL},children:[{type:b,tag:k,props:{href:"#digitalocean-setup",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aM}]},{type:a,value:f},{type:b,tag:at,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Sign up for a new DigitalOcean account if you don't already have one"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Create a DigitalOcean project "},{type:b,tag:k,props:{href:bt,rel:[w,x,y],target:z},children:[{type:a,value:bt}]}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Create a personal access token (we will use this to configure a docker addon that will provision block storage volumes automatically) "},{type:b,tag:k,props:{href:bu,rel:[w,x,y],target:z},children:[{type:a,value:bu}]}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Create and add an SSH key to your account. This is a pretty simple step, but DigitalOcean still has really thorough documentation on how to do this (see "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.digitalocean.com\u002Fdocs\u002Fdroplets\u002Fhow-to\u002Fadd-ssh-keys\u002F",rel:[w,x,y],target:z},children:[{type:a,value:bv}]},{type:a,value:" for more information)"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:J,props:{id:aN},children:[{type:b,tag:k,props:{href:"#prebuilt-docker-vm-image",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aO}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"From the "},{type:b,tag:k,props:{href:"https:\u002F\u002Fcloud.digitalocean.com\u002Fdroplets\u002Fnew",rel:[w,x,y],target:z},children:[{type:a,value:"Create Droplets"}]},{type:a,value:" page, select "},{type:b,tag:e,props:{},children:[{type:a,value:"Marketplace"}]},{type:a,value:" and search for "},{type:b,tag:e,props:{},children:[{type:a,value:aI}]},{type:a,value:". Select the "},{type:b,tag:e,props:{},children:[{type:a,value:"Docker 5:19.03.1~3 18.04"}]},{type:a,value:" image. Note that this VM is Ubuntun 18.04 with Docker Community Edition and docker-compose pre-installed."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Select the basic plan, and then scroll to the left to choose the $5.00\u002Fmonth option. Select a datacenter region. Most of these regions should be OK, but you should verify that the region you have selected supports volumes (they may all support volumes, but there are some DO features that are not supported accross all regiongs, similar to AWS). Do not select a VPC or any of the additional options."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"For Authentication, select the SSH key that you created earlier."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Take note of the Droplet's IP address; we will use this in the next step."}]},{type:a,value:f},{type:b,tag:J,props:{id:aP},children:[{type:b,tag:k,props:{href:"#gitlab-setup",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aQ}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Create a new GitLab project and clone it locally. You can also clone or fork my project and use that as a starting point. Go to "},{type:b,tag:e,props:{},children:[{type:a,value:"Settings \u003E CI\u002FCD \u003E Variables"}]},{type:a,value:" in your GitLab project and add the following environment variables:"}]},{type:a,value:f},{type:b,tag:at,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:bw}]},{type:a,value:": the value should start with "},{type:b,tag:e,props:{},children:[{type:a,value:"-----BEGIN RSA PRIVATE KEY-----"}]},{type:a,value:" and end with "},{type:b,tag:e,props:{},children:[{type:a,value:"-----END RSA PRIVATE KEY-----"}]}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:bx}]},{type:a,value:": the IP address of the droplet you just created"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"POSTGRES_PASSWORD"}]},{type:a,value:": a secure password that we will use for our Postgres database (we will share this with our Django application later on)"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"SECRET_KEY"}]},{type:a,value:": a random secret key to use for our Django application."}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"DEBUG"}]},{type:a,value:": the number "},{type:b,tag:e,props:{},children:[{type:a,value:"0"}]}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:J,props:{id:aR},children:[{type:b,tag:k,props:{href:"#a-record",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aS}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"By the end of this project you will be able to deploy your Django application to a live domain name provided that you have one. All you need to do is create an A Record that points to the Droplet IP. There are no DNS configuration changes to make inside of DigitalOcean. I'm using a domain that I purchased through Route53. You can get a free "},{type:b,tag:e,props:{},children:[{type:a,value:".tk"}]},{type:a,value:" domain from "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.freenom.com\u002Fen\u002Ffreeandpaiddomains.html",rel:[w,x,y],target:z},children:[{type:a,value:"freenom"}]},{type:a,value:". I have used this before and it is a great option for testing things out."}]},{type:a,value:f},{type:b,tag:J,props:{id:aT},children:[{type:b,tag:k,props:{href:"#ssh-into-your-digitalocean-droplet",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aU}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"You can do this with the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"ssh -i ~\u002F.ssh\u002Fa1_rsa root@123.45.578.91\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:by}]},{type:a,value:" is the private key I added to GitHub. You can logout for now, but keep this command handy, because we will be coming back to our Droplet via SSH shortly."}]},{type:a,value:f},{type:b,tag:J,props:{id:aV},children:[{type:b,tag:k,props:{href:"#add-the-rex-ray-docker-plugin",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aW}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This step is very simple, you can follow along with this short guide: "},{type:b,tag:k,props:{href:bz,rel:[w,x,y],target:z},children:[{type:a,value:bz}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"There is basically one command to run:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker plugin install rexray\u002Fdobs DOBS_TOKEN=YOUR_DIGITALOCEAN_TOKEN DOBS_REGION=nyc1 LINUX_VOLUME_FILEMODE=0775\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"You will need to make sure that you replace "},{type:b,tag:e,props:{},children:[{type:a,value:"YOUR_DIGITALOCEAN_TOKEN"}]},{type:a,value:" with the personal access token you added earlier. Also, "},{type:b,tag:e,props:{},children:[{type:a,value:"DOBS_REGION"}]},{type:a,value:" should be the region you selected for your Droplet earlier."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Check that the plugin was installed correctly with:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker plugin ls\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's a quick intro to REX-Ray from "},{type:b,tag:k,props:{href:"https:\u002F\u002Frexray.readthedocs.io\u002Fen\u002Fstable\u002F",rel:[w,x,y],target:z},children:[{type:a,value:"rexray.readthedocs.io"}]},{type:a,value:j}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"REX-Ray is an open source, storage management solution designed to support container runtimes such as Docker and Mesos. REX-Ray enables stateful applications, such as databases, to persist and maintain its data after the life cycle of the container has ended. Built-in high availability enables orchestrators such as Docker Swarm, Kubernetes, and Mesos Frameworks like Marathon to automatically orchestrate storage tasks between hosts in a cluster."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"In the context of this project, REX-Ray will automate the creation of DigitalOcean Block Storage Volumes. We will talk about volumes and how they are used later on in this article."}]},{type:a,value:f},{type:b,tag:J,props:{id:aX},children:[{type:b,tag:k,props:{href:"#gitlab-ciyml",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:W}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" is a file that configures pipelines when code is pushed to GitLab, similar to how GitHub Actions work with GitHub. This single file is a huge topic, if you are unfamiliar with GitLab CI, you might want to have a look over "},{type:b,tag:k,props:{href:"https:\u002F\u002Fdocs.gitlab.com\u002Fee\u002Fci\u002Fyaml\u002F",rel:[w,x,y],target:z},children:[{type:a,value:"this page from the GitLab documentation"}]},{type:a,value:" which goes over all of the configuration options with many examples. Also, "},{type:b,tag:k,props:{href:"https:\u002F\u002Fdocs.gitlab.com\u002Fee\u002Fci\u002Fvariables\u002Fpredefined_variables.html",rel:[w,x,y],target:z},children:[{type:a,value:"this documentation page"}]},{type:a,value:" covers the predefined environment variables that are made available to GitLab CI pipelines. I'm using these in a few different places as we will see shortly."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"CI\u002FCD pipelines that I define with "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" typically contain three stages: "},{type:b,tag:e,props:{},children:[{type:a,value:"test"}]},{type:a,value:au},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:". We will focus on the "},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:" stages for now (reference the article on my Fargate project linked above for reference on setting up unit tests with pytest)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:aw}]},{type:a,value:" is the name of a GitLab CI job that builds and tags a docker image from the source code in the "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" directory of this project and pushes the tagged container image to a private image registry on gitlab.com that we will use later."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's the YAML code for "},{type:b,tag:e,props:{},children:[{type:a,value:aw}]},{type:a,value:j}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,ac]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bA}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:" build\n  "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bB},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bC},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bD},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bE},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bF},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bG},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bH},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bI},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bJ}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:"|"}]},{type:b,tag:c,props:{className:[d,"scalar",l]},children:[{type:a,value:"\n      docker build \\\n        -t $CI_REGISTRY_IMAGE\u002Fbackend:$CI_COMMIT_SHORT_SHA \\\n        -f backend\u002Fdocker\u002FDockerfile.prod \\\n        .\u002Fbackend\u002F"}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" docker push $CI_REGISTRY_IMAGE\u002Fbackend"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"$CI_COMMIT_SHORT_SHA\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"build-nginx"}]},{type:a,value:" is almost identical, but the "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" arguments are slightly different. There are three arguments for the "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" command that I'm using here:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:ax}]},{type:a,value:": the tag to tag the built image with"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:bK}]},{type:a,value:": the Dockerfile to be used for building the image"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"the "},{type:b,tag:e,props:{},children:[{type:a,value:"context"}]},{type:a,value:" that is sent to the docker daemon when we build the image"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:ax}]},{type:a,value:" makes use of two predefined GitLab CI variables: "},{type:b,tag:e,props:{},children:[{type:a,value:ay}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:ak}]},{type:a,value:az},{type:b,tag:e,props:{},children:[{type:a,value:"$CI_REGISTRY_IMAGE"}]},{type:a,value:" is the URL for the private image registry on gitlab.com that we push our images to that is specific to our project: "},{type:b,tag:e,props:{},children:[{type:a,value:"registry.gitlab.com\u002F\u003Cgitlab_username\u003E\u002F\u003Cproject_name\u003E"}]},{type:a,value:", and "},{type:b,tag:e,props:{},children:[{type:a,value:ak}]},{type:a,value:" is an character alphanumeric value that contains the truncated name of the commit hash, this is known as the "},{type:b,tag:e,props:{},children:[{type:a,value:"tag"}]},{type:a,value:", even though we pass in more than just this value. We combine these two values with "},{type:b,tag:e,props:{},children:[{type:a,value:"\u002F"}]},{type:a,value:" and the name of the image we are building, such as "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:", so the full value being passed to "},{type:b,tag:e,props:{},children:[{type:a,value:ax}]},{type:a,value:" is:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"registry.gitlab.com\u002Fgitlab-username\u002Fmy-project\u002Fbackend:abcd1234\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:bK}]},{type:a,value:" is the path to the "},{type:b,tag:e,props:{},children:[{type:a,value:"Dockerfile"}]},{type:a,value:" we are using relative to the directory where we are running the "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" command, which is the root directory of the project."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The final argument defines the context that we are using to build the image, and this is an important part for understanding how Docker works. This argument defines the directory that is zipped up and sent to the docker daemon via the docker API. When we build an image with "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:", we are essentially using the docker CLI to make a POST request to our docker daemon (server) where the POST data contains all of the files that we will have access to in the steps of the Dockerfile (such as "},{type:b,tag:e,props:{},children:[{type:a,value:"ADD"}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:"COPY"}]},{type:a,value:" -- we will get to these soon). There's a key difference between the "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:p},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" commands: the context for "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:bL},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:", but the context for "},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:bL},{type:b,tag:e,props:{},children:[{type:a,value:F}]},{type:a,value:" (the root of the project). This is because we may want access to another top level directory in our project that contains, for example, a Vue.js or React application, that we will build into our NGINX container. In order to be able to access both files in the "},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:" and the folder containing our frontend app, we need to send a context that contains both of these directories. Sending too many files to to the docker daemon when you run docker build will usually cause the "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" command to hang. The first line of output from a "},{type:b,tag:e,props:{},children:[{type:a,value:Y}]},{type:a,value:" command should be something like this:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Sending build context to Docker daemon  24.58kB\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"If this number is too high, you should use a "},{type:b,tag:e,props:{},children:[{type:a,value:".dockerignore"}]},{type:a,value:" file that ignores any files or directories you don't want to send to the docker daemon (similar to how "},{type:b,tag:e,props:{},children:[{type:a,value:".gitignore"}]},{type:a,value:" works with git)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"To be able to pull and push (read and write) images to our private project container image registry, we need login with our docker client using the "},{type:b,tag:e,props:{},children:[{type:a,value:bM}]},{type:a,value:" command in the "},{type:b,tag:e,props:{},children:[{type:a,value:aj}]},{type:a,value:" as well two other predefined GitLab CI variables: "},{type:b,tag:e,props:{},children:[{type:a,value:"CI_JOB_TOKEN"}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:"CI_REGISTRY"}]},{type:a,value:". This all happens using a special service called "},{type:b,tag:e,props:{},children:[{type:a,value:"docker-in-docker"}]},{type:a,value:" which I won't go into too much detail here, but it is a common practice when working with containers in a CI\u002FCD environment that itself which is also based on containers, such as GitLab CI (each job runs in a container -- the key "},{type:b,tag:e,props:{},children:[{type:a,value:S}]},{type:a,value:" -- and can define additional containers -- the "},{type:b,tag:e,props:{},children:[{type:a,value:X}]},{type:a,value:" key -- to help with the CI job). Once the two images for "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:" have been built and pushed, our GitLab CI pipeline moves on to the next stage: "},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:". In the "},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:" stage, we will start these and other containers on our DigitalOcean droplet, so we are getting close, but there is a lot more to explain. Before we deploy our containers, we need to do some one-time setup:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"initialize a single-node docker swarm cluster on our Droplet and"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"create a docker network that our cluster's services (containers) will use"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:J,props:{id:aY},children:[{type:b,tag:k,props:{href:"#setup-a-docker-swarm-cluster",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:aZ}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"To setup a docker swarm cluster, SSH into the Droplet with the command we introduced above ("},{type:b,tag:e,props:{},children:[{type:a,value:"ssh -i ~\u002F.ssh\u002Fa1_rsa root@123.45.578.91"}]},{type:a,value:" where "},{type:b,tag:e,props:{},children:[{type:a,value:by}]},{type:a,value:" is the name of the private key file -- you can ignore the "},{type:b,tag:e,props:{},children:[{type:a,value:"-i ~\u002F.ssh\u002Fa1_rsa"}]},{type:a,value:" part if you are using an SSH key called "},{type:b,tag:e,props:{},children:[{type:a,value:bN}]},{type:a,value:"), and run the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker swarm init --advertise-addr DROPLET_IP\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Replace "},{type:b,tag:e,props:{},children:[{type:a,value:bx}]},{type:a,value:" with your Droplet's IP address (e.g. "},{type:b,tag:e,props:{},children:[{type:a,value:"123.45.578.91"}]},{type:a,value:"). Check out "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Ftutorials\u002Fhow-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-ubuntu-16-04",rel:[w,x,y],target:z},children:[{type:a,value:bv}]},{type:a,value:" for some additional information about using docker swarm on GitLab. It is a little bit outdated, but the main ideas should still hold up. Docker swarm is designed to orchestrate containers running on a group (or swarm) of multiple machines. However, it is perfectly fine to run a single-node cluster as we are doing here."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Docker swarm uses docker-compose files, but using docker swarm is very different from running "},{type:b,tag:e,props:{},children:[{type:a,value:al}]},{type:a,value:", a command which you might see people running both locally and in production and which also uses docker-compose files. As a best practice, you should not be using "},{type:b,tag:e,props:{},children:[{type:a,value:bO}]},{type:a,value:" (the command) in production. Many people do this, and several official tutorials will often end with \"now just run "},{type:b,tag:e,props:{},children:[{type:a,value:al}]},{type:a,value:" and you are done\". The first time I ran containers in the cloud I pulled my git repo into a VM, installed docker and docker-compose and ran "},{type:b,tag:e,props:{},children:[{type:a,value:al}]},{type:a,value:". It is pretty easy and it works very similarly in both local and production environments, but this guide will be using "},{type:b,tag:e,props:{},children:[{type:a,value:bO}]},{type:a,value:" in production. There is more I could say here, but the main point is that docker swarm is a simplified version of something like Kubernetes, but it comes built-in to docker and is very simple to use."}]},{type:a,value:f},{type:b,tag:J,props:{id:a_},children:[{type:b,tag:k,props:{href:"#defining-an-overlay-network",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:a$}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"SSH into your droplet and run the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker network create --driver=overlay traefik-public\n"}]}]}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Usually we define networks in our docker-compose file, but this network needs to be defined first and then referenced in our docker-compose file. Here's a thread on SO that goes into a little bit more on why this is necessary, but I still don't have a very clear idea of why this is the case. With another configuration or perhaps docker-compose version, this may not be needed. I'll update this part of the article if I figure anything out."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Let's go over one more docker concept that will helpful in the next few steps. When you run "},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:" on your local machine, the docker CLI first looks to see if the "},{type:b,tag:e,props:{},children:[{type:a,value:ad}]},{type:a,value:" environment variable is set. If it is not, then docker defaults to "},{type:b,tag:e,props:{},children:[{type:a,value:"unix:\u002F\u002F\u002Fvar\u002Frun\u002Fdocker.sock"}]},{type:a,value:", a UNIX socket. Check out "},{type:b,tag:k,props:{href:"https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F35110146\u002Fcan-anyone-explain-docker-sock",rel:[w,x,y],target:z},children:[{type:a,value:"this SO post"}]},{type:a,value:" titled \"Can anyone explain docker.sock?\""}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"We change the docker host that our local docker CLI is talking to by setting this environment variable, and one nice way to set this environment variables uses an SSH connection:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"DOCKER_HOST=ssh:\u002F\u002Froot@$DOCKER_IP\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"See this article for a more in-depth discussion: "},{type:b,tag:k,props:{href:bP,rel:[w,x,y],target:z},children:[{type:a,value:bP}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"You can try this out locally. Run a container locally, check it with "},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:", then export the "},{type:b,tag:e,props:{},children:[{type:a,value:ad}]},{type:a,value:" environment variable with the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"export DOCKER_HOST=ssh:\u002F\u002Froot@123.45.678.91\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Replacing "},{type:b,tag:e,props:{},children:[{type:a,value:"123.45.678.91"}]},{type:a,value:" with your Droplet IP. Run "},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:" again and you should see nothing (or any other containers that you started on your Droplet). Finally, run:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"unset DOCKER_HOST\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Running "},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:" again you should see the containers on your machine. We will be using this idea in the next step when we look at the "},{type:b,tag:e,props:{},children:[{type:a,value:as}]},{type:a,value:" command."}]},{type:a,value:f},{type:b,tag:J,props:{id:ba},children:[{type:b,tag:k,props:{href:"#docker-stack-deploy",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:as}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Now that we have done our one-time-setup steps, let's look at the "},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:" stage of "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:", the GitLab CI job that will get our containers running on our Droplet. First, let's break down the "},{type:b,tag:e,props:{},children:[{type:a,value:aA}]},{type:a,value:" command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,ac]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aA}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bA}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:" deploy\n  "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bB},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bC},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bD},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"variables"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:ad}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'ssh:\u002F\u002Froot@$DROPLET_IP'"}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" apk update "},{type:b,tag:c,props:{className:[d,"important"]},children:[{type:a,value:"&&"}]},{type:a,value:" apk add openssh"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"client bash\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" mkdir "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"p ~\u002F.ssh\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" echo \"$SSH_PRIVATE_KEY\" "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aB}]},{type:a,value:" ~\u002F.ssh\u002Fid_rsa\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" chmod 600 ~\u002F.ssh\u002Fid_rsa\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" eval \"$(ssh"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"agent "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"s)\"\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bQ},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"add ~\u002F.ssh\u002Fid_rsa\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bQ},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"keyscan "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"H $DROPLET_IP "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aB}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aB}]},{type:a,value:" ~\u002F.ssh\u002Fknown_hosts\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bE},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bF},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bG},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bH},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bI},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bJ}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" docker stack deploy "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"c stack.yml my"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"stack "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"with"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"registry"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"auth\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This job uses the same "},{type:b,tag:e,props:{},children:[{type:a,value:S}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:X}]},{type:a,value:" that our "},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:" stage jobs used. Notice that we set "},{type:b,tag:e,props:{},children:[{type:a,value:ad}]},{type:a,value:" to "},{type:b,tag:e,props:{},children:[{type:a,value:"\"ssh:\u002F\u002Froot@$DROPLET_IP\""}]},{type:a,value:", this means that any docker CLI commands in this job will be communicating with the docker daemon on our Droplet. The "},{type:b,tag:e,props:{},children:[{type:a,value:aj}]},{type:a,value:" has a lot going on, but all we are doing is preparing to use the SSH private key that we previously added to our GitLab project's CI\u002FCD environment variables. The base image for this job, "},{type:b,tag:e,props:{},children:[{type:a,value:"docker:19.03.1"}]},{type:a,value:" is based on Alpine Linus. This version of Linux is super light weight and doesn't come with "},{type:b,tag:e,props:{},children:[{type:a,value:"openssh-client"}]},{type:a,value:an},{type:b,tag:e,props:{},children:[{type:a,value:bR}]},{type:a,value:", so our first step is to install these with the Alpine package manager, "},{type:b,tag:e,props:{},children:[{type:a,value:"apk"}]},{type:a,value:j}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"apk update && apk add openssh-client bash\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Next, we add the "},{type:b,tag:e,props:{},children:[{type:a,value:bw}]},{type:a,value:" environment variable into the body of the "},{type:b,tag:e,props:{},children:[{type:a,value:bN}]},{type:a,value:" private key file, change the permission of this file and then add the key to our SSH agent. Here's an excerpt from "},{type:b,tag:e,props:{},children:[{type:a,value:"man ssh-agent"}]},{type:a,value:" that provides a little bit more context into why we need to run "},{type:b,tag:e,props:{},children:[{type:a,value:"eval \"$(ssh-agent -s)\""}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:"ssh-add ~\u002F.ssh\u002Fid_rsa"}]},{type:a,value:j}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"DESCRIPTION\n     ssh-agent is a program to hold private keys used for public key authentication (RSA, DSA, ECDSA, Ed25519)ssh-agent is usually started in the beginning of an X-session or a login session, and all other windows or programs are started as clients to the ssh-agent program.  Through use of environment variables the agent can be located and automatically used for authentication when logging in to other machines using ssh(1).\n\n     The agent initially does not have any private keys.  Keys are added using ssh(1) (see AddKeysToAgent in ssh_config(5) for details) or ssh-add(1).  Multiple identities may be stored in ssh-agent concurrently and ssh(1) will automatically use them if present.  ssh-add(1) is also used to remove keys from ssh-agent and to query the keys that are held in one.\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Next, "},{type:b,tag:e,props:{},children:[{type:a,value:"ssh-keyscan -H $DROPLET_IP \u003E\u003E ~\u002F.ssh\u002Fknown_hosts"}]},{type:a,value:" tells our SSH agent about our Droplet so that it doesn't prompt us with a "},{type:b,tag:e,props:{},children:[{type:a,value:"Do you want to add this server to known hosts? (yes\u002Fno)"}]},{type:a,value:", or whatever the equivalent of that is for the docker CLI when it attempts to connect to a remote docker daemon over SSH."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Finally, we login to our our GitLab private registry using the same command from before when we built and pushed images to our private registry on gitlab.com:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This essentially gives our DigitalOcean Droplet access to the "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:" images in our private GitLab CI image registry, even tho we are running this command in a contain, in a container that is probably running in Kubernetes on GCP. Next, we are actually going to use these images."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The last command in the "},{type:b,tag:e,props:{},children:[{type:a,value:aA}]},{type:a,value:" job is:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:bS}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Check out this link from the docker docs on docker stacks "},{type:b,tag:k,props:{href:bT,rel:[w,x,y],target:z},children:[{type:a,value:bT}]},{type:a,value:az},{type:b,tag:e,props:{},children:[{type:a,value:"--with-registry-auth"}]},{type:a,value:" is important, our command will complete if this is not included, but our application won't start."}]},{type:a,value:f},{type:b,tag:J,props:{id:bb},children:[{type:b,tag:k,props:{href:"#stackyml",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:Q}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Now we are ready to tackle the last big file in our repo: "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:". This is a the docker-compose file that we use to deploy our project. The only reason we needed to run "},{type:b,tag:e,props:{},children:[{type:a,value:bM}]},{type:a,value:" above is because "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:" references the two images we built and pushed to our GitLab private repo. There's a lot going on in this file, let's start with the "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" service:"}]},{type:a,value:f},{type:b,tag:T,props:{id:E},children:[{type:b,tag:k,props:{href:"#backend",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:E}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,ac]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"version"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'3.4'"}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bU},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:"\u002Fbackend"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bV},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:ak},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:ao}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" main\n  "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bW}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" POSTGRES_PASSWORD\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" SECRET_KEY\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" DEBUG\n  "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bX},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"\u002Fcode\u002Fassets\n  "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:bY}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" postgres\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" redis\n    "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" web\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The "},{type:b,tag:e,props:{},children:[{type:a,value:S}]},{type:a,value:" is essentially what we defined in "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:", but the syntax is slightly different:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"${CI_REGISTRY_IMAGE}\u002Fbackend:${CI_COMMIT_SHORT_SHA}\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"We pass environment variables that we defined in GitLab CI via the "},{type:b,tag:e,props:{},children:[{type:a,value:bW}]},{type:a,value:" key."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:bZ},{type:b,tag:e,props:{},children:[{type:a,value:ap}]},{type:a,value:" is used for storing static assets (CSS, JS, etc.) as well as media assets (images, videos, any other file type). We mount this directory at "},{type:b,tag:e,props:{},children:[{type:a,value:"\u002Fcode\u002Fassets"}]},{type:a,value:" and then define our "},{type:b,tag:e,props:{},children:[{type:a,value:"STATIC_ROOT"}]},{type:a,value:" in Django's "},{type:b,tag:e,props:{},children:[{type:a,value:b_}]},{type:a,value:" to be:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,b$]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"os"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:"path"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:"join"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:a,value:"BASE_DIR"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"assets\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"static\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Later, when we run "},{type:b,tag:e,props:{},children:[{type:a,value:ca}]},{type:a,value:", files are copied to this location in our container, and since this is the location of the volume, the files are actually copied to the volume and will be persisted if we destroy the backend container and restart it. When the container restarts, the volume is mounted again and the static files will still be available to our application."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"network"}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:bY}]},{type:a,value:" related to to the other services that this application will communicate with. "},{type:b,tag:e,props:{},children:[{type:a,value:aq}]},{type:a,value:" is a network defined in the "},{type:b,tag:e,props:{},children:[{type:a,value:ao}]},{type:a,value:" part of "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:", notice that we reference the "},{type:b,tag:e,props:{},children:[{type:a,value:"traefik-public"}]},{type:a,value:" network here that we created earlier, as well."}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Depends on helps with service startup order, but it is a better practice to use "},{type:b,tag:e,props:{},children:[{type:a,value:".\u002Fwait-for-it.sh"}]},{type:a,value:". However, I have never had any issues related to startup order. I'll try adding this later to make things more robust."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:aD}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:aE}]},{type:a,value:" will start before "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:". Our Django application will communicate to these services by their hostnames: "},{type:b,tag:e,props:{},children:[{type:a,value:aD}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:aE}]},{type:a,value:". The fact that "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:au},{type:b,tag:e,props:{},children:[{type:a,value:aD}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:aE}]},{type:a,value:" are all on the same network ("},{type:b,tag:e,props:{},children:[{type:a,value:aq}]},{type:a,value:") means that they can resolve each other by these names. For example, the connection string to redis will look like: "},{type:b,tag:e,props:{},children:[{type:a,value:"redis:\u002F\u002Fredis:6379"}]},{type:a,value:". Let's look at the "},{type:b,tag:e,props:{},children:[{type:a,value:"DATABASES"}]},{type:a,value:" configuration in "},{type:b,tag:e,props:{},children:[{type:a,value:b_}]},{type:a,value:" to see how we connect to Postgres:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,b$]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"DATABASES "},{type:b,tag:c,props:{className:[d,"operator"]},children:[{type:a,value:"="}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"default\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"ENGINE\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"django.db.backends.postgresql_psycopg2\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"NAME\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"POSTGRES_NAME\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"USER\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"POSTGRES_USERNAME\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"PASSWORD\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"POSTGRES_PASSWORD\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"HOST\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"POSTGRES_SERVICE_HOST\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"PORT\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:F}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"\"POSTGRES_SERVICE_PORT\""}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,"number"]},children:[{type:a,value:"5432"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:Z}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:K}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I have defined the "},{type:b,tag:e,props:{},children:[{type:a,value:"HOST"}]},{type:a,value:" to be based on the environment variable "},{type:b,tag:e,props:{},children:[{type:a,value:"POSTGRES_HOST"}]},{type:a,value:", but I have not defined this environment variable, so why didn't I just say "},{type:b,tag:e,props:{},children:[{type:a,value:"\"HOST\": \"postgres\""}]},{type:a,value:"? I could have, but if I want to change the database in the future, the only change will be adding an environment variable; I won't have to worry about hardcoded values."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I'm choosing to run Postgres in a container and not use a managed database (which DigitalOcean does offer) in order to save on costs and also to get more practice managing my own database. I use RDS with AWS and it handles a lot of things that I don't have to worry about, such as backups, and it allows me to quickly restore from a snapshot. I'm interested in learning more about how I can do these tasks with a database that I run myself."}]},{type:a,value:f},{type:b,tag:T,props:{id:M},children:[{type:b,tag:k,props:{href:"#nginx",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bc}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The next service we should go over is "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:", the service that runs the NGINX container that we pushed to our private GitLab image registry. This service has a couple of functions that I'll walk through:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Reverse proxy"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Serve static files for Django"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Serve a frontend Javascript application"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's the definition of this service in "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:j}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,ac]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:U}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bU},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:"\u002Fnginx"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:bV},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:$}]},{type:a,value:ak},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:aa}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:ao}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:aF},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"public\n      "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" main\n    "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:bX},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"\u002Fusr\u002Fsrc\u002Fapp\u002Fassets\n    "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:O}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aG}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cb}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cc}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cd}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ce}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cf}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"For now, ignore the contents under the "},{type:b,tag:e,props:{},children:[{type:a,value:O}]},{type:a,value:" key; we will cover this next when we go over the "},{type:b,tag:e,props:{},children:[{type:a,value:V}]},{type:a,value:cg}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"NGINX acts as a reverse proxy when it sends request starting with "},{type:b,tag:e,props:{},children:[{type:a,value:ch}]},{type:a,value:an},{type:b,tag:e,props:{},children:[{type:a,value:ci}]},{type:a,value:" to the "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" container. Two blocks in "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:" enable this behavior:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"  upstream backend {\n    server backend:8000;\n  }\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This hostname "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" is defined as "},{type:b,tag:e,props:{},children:[{type:a,value:aH}]},{type:a,value:az},{type:b,tag:e,props:{},children:[{type:a,value:aH}]},{type:a,value:" can be resolved by the "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:" service because it is on the same "},{type:b,tag:e,props:{},children:[{type:a,value:aq}]},{type:a,value:" network that "},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" is on. If either of the "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:an},{type:b,tag:e,props:{},children:[{type:a,value:E}]},{type:a,value:" wasn't on the "},{type:b,tag:e,props:{},children:[{type:a,value:aq}]},{type:a,value:" network, NGINX would not be able to make sense of "},{type:b,tag:e,props:{},children:[{type:a,value:aH}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"    # backend urls\n    location ~ ^\u002F(admin|api) {\n      proxy_redirect off;\n      proxy_pass http:\u002F\u002Fbackend;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n    }\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This block does that actual request forwarding. "},{type:b,tag:e,props:{},children:[{type:a,value:"http:\u002F\u002Fbackend"}]},{type:a,value:" references the "},{type:b,tag:e,props:{},children:[{type:a,value:"upstream backend {}"}]},{type:a,value:" block defined above."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:bZ},{type:b,tag:e,props:{},children:[{type:a,value:ap}]},{type:a,value:" is referenced here and mounts to "},{type:b,tag:e,props:{},children:[{type:a,value:cj}]},{type:a,value:". This path is then referenced in "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:", the NGINX configuration file that is used in our custom NGINX-based image:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"    # static files\n    location \u002Fstatic {\n      autoindex on;\n      alias \u002Fusr\u002Fsrc\u002Fapp\u002Fassets\u002Fstatic;\n    }\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"In this block of "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:", we tell NGINX to serve files from "},{type:b,tag:e,props:{},children:[{type:a,value:ck}]},{type:a,value:" for requests that start with "},{type:b,tag:e,props:{},children:[{type:a,value:cl}]},{type:a,value:". A request made to "},{type:b,tag:e,props:{},children:[{type:a,value:"https:\u002F\u002Fmysite.com\u002Fstatic\u002Fbase.css"}]},{type:a,value:" would return a file located at "},{type:b,tag:e,props:{},children:[{type:a,value:ck}]},{type:a,value:" if that file existed. Remember, when we run the "},{type:b,tag:e,props:{},children:[{type:a,value:"collecstatic"}]},{type:a,value:" management command in our Django container, it will collect our static files to "},{type:b,tag:e,props:{},children:[{type:a,value:ap}]},{type:a,value:". Since "},{type:b,tag:e,props:{},children:[{type:a,value:ap}]},{type:a,value:" is mounted to the "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:" service at "},{type:b,tag:e,props:{},children:[{type:a,value:cj}]},{type:a,value:", NGINX will have access to these files by way of the volume mount and they will persist across restarts of the web service and its NGINX container."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Finally, NGINX can serve a Javascript SPA or similar if we choose to use one in our project. To understand how this is done, we need to understand multistage Dockerfiles. Here's the Dockerfile used for the "},{type:b,tag:e,props:{},children:[{type:a,value:M}]},{type:a,value:" container:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"# # build stage\n# FROM node:10-alpine as build-stage\n# WORKDIR \u002Fapp\u002F\n# COPY frontend\u002Fpackage.json \u002Fapp\u002F\n# RUN npm cache verify\n# RUN npm install\n# COPY frontend \u002Fapp\u002F\n# RUN npm run build\n\n# production stage\n# FROM nginx:1.19.1-alpine as production-stage\nFROM nginx:1.19.1-alpine\nCOPY nginx\u002Fprod\u002Fprod.conf \u002Fetc\u002Fnginx\u002Fnginx.conf\nCOPY nginx\u002Fprod\u002Findex.html \u002Fdist\u002F\n# COPY --from=build-stage \u002Fapp\u002Fdist \u002Fdist\u002F\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Currently I don't have a SPA setup, but this is how we could setup one using Vue.js. The important part is this line:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"COPY --from=build-stage \u002Fapp\u002Fdist \u002Fdist\u002F\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This would copy the build files for our Javascript application into the "},{type:b,tag:e,props:{},children:[{type:a,value:cm}]},{type:a,value:" folder of our NGINX container. Another few declarations and blocks in "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:" allow all other requests to be served by the contents of this folder:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"    root \u002Fdist\u002F;\n    index index.html;\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This sets the root and the index document for our NGINX webserver."}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"    # frontend\n    location \u002F {\n      try_files $uri $uri\u002F @rewrites;\n    }\n\n    location @rewrites {\n      rewrite ^(.+)$ \u002Findex.html last;\n    }\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"These two blocks route all other requests to the frontend Javascript app's "},{type:b,tag:e,props:{},children:[{type:a,value:"index.html"}]},{type:a,value:" file location in "},{type:b,tag:e,props:{},children:[{type:a,value:cm}]},{type:a,value:" (any request that doesn't start with "},{type:b,tag:e,props:{},children:[{type:a,value:cl}]},{type:a,value:au},{type:b,tag:e,props:{},children:[{type:a,value:ch}]},{type:a,value:an},{type:b,tag:e,props:{},children:[{type:a,value:ci}]},{type:a,value:"). We may wish to change this behavior if you want Django to serve most of your requests and possibly serve a single page application on another path."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Lastly, the "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:" service's "},{type:b,tag:e,props:{},children:[{type:a,value:"deployment"}]},{type:a,value:" key has some "},{type:b,tag:e,props:{},children:[{type:a,value:aG}]},{type:a,value:" defined for Traefik. Let's come back to these after having a look at the "},{type:b,tag:e,props:{},children:[{type:a,value:V}]},{type:a,value:cg}]},{type:a,value:f},{type:b,tag:T,props:{id:V},children:[{type:b,tag:k,props:{href:"#traefik",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bd}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:"img",props:{alt:"png",src:"https:\u002F\u002Fdocs.traefik.io\u002Fassets\u002Fimg\u002Ftraefik-architecture.png"},children:[]}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. -- "},{type:b,tag:k,props:{href:cn,rel:[w,x,y],target:z},children:[{type:a,value:cn}]}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Traefik has three main functions in my application:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Request TLS certificates from Let's Encrypt that allow us to encrypt our web traffic with HTTPS"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Do TLS termination"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"Route all requests to NGINX"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The one thing that Traefik cannot do is serve static files; it is not a webserver, unlike NGINX which is a webserver. NGINX is also capable of requesting TLS certs from Let's Encrypt, so we don't technically need Traefik, but it is indeed \"fun and easy\", especially when it comes to requesting certificates."}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I have tried setting up Certbot with NGINX a long time ago but I never go it to work, and I didn't like the idea about how to run a chron job to refresh old certs."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"There are two main ways to set up Traefik:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"write a "},{type:b,tag:e,props:{},children:[{type:a,value:"traefik.toml"}]},{type:a,value:" file and build this into your own custom image (similar to what we do with NGINX and "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:Z}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"use a base image and specify all configure options through command line arguments."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I started out with the first approach, and I did get it to work, but I have decided that the second way is better. It requires one less image to build in our deployment process and it is easy to parametrize the command line arguments in "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:" (for now all the values I'm using in the "},{type:b,tag:e,props:{},children:[{type:a,value:V}]},{type:a,value:" service are hard-coded, this is one more item for my ToDo list on this project)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I had a hard time finding good examples of how to use Traefik version 2 with Docker Swarm in the Traefik docs. Their official example for using docker uses "},{type:b,tag:e,props:{},children:[{type:a,value:al}]},{type:a,value:". There is a Swarm example, but it is for an older version of Traefik (1.7). This article titled "},{type:b,tag:k,props:{href:"https:\u002F\u002Fblog.creekorful.com\u002F2019\u002F10\u002Fhow-to-install-traefik-2-docker-swarm\u002F",rel:[w,x,y],target:z},children:[{type:a,value:"How to install Traefik 2.x on a Docker Swarm"}]},{type:a,value:" helped me a lot in figuring out how to get everything working. Thank you for the great article, "},{type:b,tag:k,props:{href:"https:\u002F\u002Fgithub.com\u002Fcreekorful",rel:[w,x,y],target:z},children:[{type:a,value:"Alos"}]},{type:a,value:"!"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's the code that sets up the traefik service:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,ac]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:S}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:aF},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"v2.0.2\n    "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"ports"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'80:80'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'443:443'"}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"command"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--providers.docker.endpoint=unix:\u002F\u002F\u002Fvar\u002Frun\u002Fdocker.sock'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--providers.docker.swarmMode=true'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--providers.docker.exposedbydefault=false'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--providers.docker.network=traefik-public'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--entrypoints.web.address=:80'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--entrypoints.websecure.address=:443'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--certificatesresolvers.letsencryptresolver.acme.httpchallenge=true'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--certificatesresolvers.letsencryptresolver.acme.httpchallenge.entrypoint=web'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--certificatesresolvers.letsencryptresolver.acme.email=brian@email.com'"}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:"'--certificatesresolvers.letsencryptresolver.acme.storage=\u002Fletsencrypt\u002Facme.json'"}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aC}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" letsencrypt"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"\u002Fletsencrypt\n      "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" \u002Fvar\u002Frun\u002Fdocker.sock"},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"\u002Fvar\u002Frun\u002Fdocker.sock\n    "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:ao}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:aF},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:"public\n    "},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:O}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:G},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"placement"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:"constraints"}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:"\n          "},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:" node.role == manager\n"}]}]}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The one thing I don't like about this setup is that it uses a 1GB volume to store one small JSON file. I think that 1GB is the smallest block storage volume I can request using REX-Ray. This only adds $0.10\u002Fmonth to our project costs which is not that bad."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:H},{type:b,tag:e,props:{},children:[{type:a,value:co}]},{type:a,value:" refer to values declared on the "},{type:b,tag:e,props:{},children:[{type:a,value:U}]},{type:a,value:" service. Let's take a look at those values:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,"language-yaml"]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:O}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,m,n]},children:[{type:a,value:aG}]},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:j}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cb}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cc}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cd}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:ce}]},{type:a,value:v},{type:b,tag:c,props:{className:[d,g]},children:[{type:a,value:i}]},{type:a,value:p},{type:b,tag:c,props:{className:[d,l]},children:[{type:a,value:cf}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I still need to setup HTTP -\u003E HTTPS redirecting, so for now only "},{type:b,tag:e,props:{},children:[{type:a,value:co}]},{type:a,value:" is defined, but Alos explains this clearly in his article."}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"For me this is the most complicated part of the setup. I'm still not familiar with exactly how Traefik and Let's Encrypt work. Hopefully I can run through this process a few more times with some variations to better understand the rough edges. Otherwise for this simple way to get TLS certificates. AWS makes this very easy with Amazon Certificate Manager (ACM) which makes the requesting of certificates very simple, especially within CDK."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"That wraps up our overview of "},{type:b,tag:e,props:{},children:[{type:a,value:Q}]},{type:a,value:", we left off with the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:bS}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"We can check out the status of our docker stack deployment by running a few different docker CLI commands. You can either SSH into your Droplet or configure the "},{type:b,tag:e,props:{},children:[{type:a,value:ad}]},{type:a,value:" environment variable that I showed you earlier and run these commands from your local terminal:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker stack ps my-stack --no-trunc\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"--no-trunc"}]},{type:a,value:" is important because important error messages tend to be cut off. This option will show the full version of each column returned by "},{type:b,tag:e,props:{},children:[{type:a,value:"docker stack ps"}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker service ls\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This command shows some the active services on our Droplet."}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker ps\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This command is useful for shelling into a container to run commands and poke around for debugging."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"You can get the Container ID of a running container and access it with the following command:"}]},{type:a,value:f},{type:b,tag:q,props:{className:[r]},children:[{type:b,tag:s,props:{className:[t,u]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"docker exec -it 0da8370ab283 bash\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This assumes that "},{type:b,tag:e,props:{},children:[{type:a,value:bR}]},{type:a,value:" is installed on the container with ID "},{type:b,tag:e,props:{},children:[{type:a,value:"0da8370ab283"}]},{type:a,value:F}]},{type:a,value:f},{type:b,tag:J,props:{id:be},children:[{type:b,tag:k,props:{href:"#management-commands",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bf}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Once the site is deployed we stil need to run a few commands to set up our Djnago application:"}]},{type:a,value:f},{type:b,tag:_,props:{},children:[{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:ca}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"migrate"}]},{type:a,value:f},{type:b,tag:o,props:{},children:[{type:a,value:"createsuperuser"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:N,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"One other ToDo is to figure out how to run these commands through manual GitLab CI jobs."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"That's most of what I wanted to cover on a first pass. This should be a good starting point for working with a Django application in Docker Swarm on DigitalOcean."}]},{type:a,value:f},{type:b,tag:J,props:{id:bg},children:[{type:b,tag:k,props:{href:"#next-steps",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bh}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here are some ideas about the next steps I could take on this project."}]},{type:a,value:f},{type:b,tag:T,props:{id:bi},children:[{type:b,tag:k,props:{href:"#local-environment",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bj}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I'll probably need to create another docker-compose file to bring up everything locally. It might be a good way to experiment with different Traefik settings."}]},{type:a,value:f},{type:b,tag:T,props:{id:bk},children:[{type:b,tag:k,props:{href:"#infrastructure-as-code-setup",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bl}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"There might be a good opportunity to learn more about Terreform or Ansible here. There are a number of manual, one time setup steps. Some of these can't be automated, but some of them would probably fit very neatly into one of these tools. Pulumi would also be a good option to explore as it is more analagous to CDK."}]},{type:a,value:f},{type:b,tag:T,props:{id:bm},children:[{type:b,tag:k,props:{href:"#scaling-out-docker-swarm-services-across-multiple-machines",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bn}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I have only scratched the surface of what docker swarm can do. There are lots of other settings that would be helpful to setup for learning purposes, especially around resource limits for services. For simplicity I haven't touch on any of these options yet. I'm curious to know how many containers I can fit onto one small Droplet, and if resource limits could help with compute and memory-intensive workloads."}]},{type:a,value:f},{type:b,tag:T,props:{id:bo},children:[{type:b,tag:k,props:{href:"#kubernetes-on-digitalocean",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:bp}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"DigitalOcean now offers simplified Kubernetes solutions. It would be interesting to try this out once I get better with docker swarm. I have used Kubernetes a with minikube and to a limited extent with GCP."}]},{type:a,value:f},{type:b,tag:T,props:{id:bq},children:[{type:b,tag:k,props:{href:"#deploying-locally-without-using-gitlab-ci",ariaHidden:A,tabIndex:B},children:[{type:b,tag:c,props:{className:[C,D]},children:[]}]},{type:a,value:br}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"CDK makes deploying locally very easy, especially with the Lambda project I put together. This project as it stands might be a little bit more difficult to deploy locally. It assumes that the images we want to deploy are private. It might be possible, but for now I am fine with deploying through GitLab CI since the pipeline only takes a few minutes to complete for the build and deploy stages."}]}]},dir:"\u002F2020\u002F08\u002F09",path:"\u002F2020\u002F08\u002F09\u002Fdigital-ocean-docker-swarm-django-traefik-nginx.html",extension:".md",createdAt:cp,updatedAt:cp,raw:"\nI recently wrote two articles about deploying Django applications to AWS serverless environments: one on [AWS Fargate](https:\u002F\u002Fbriancaffey.github.io\u002F2020\u002F06\u002F02\u002Fdjango-postgres-vue-gitlab-ecs.html) (CloudFront, ALB and ECS Fargate containers) and one on [AWS Lambda](https:\u002F\u002Fbriancaffey.github.io\u002F2020\u002F08\u002F01\u002Fdjango-and-lambda-with-cdk-and-api-gateway.html) (Lambda + API Gateway, without using Zappa or Serverless Framework). Both projects focused on automating as much of the setup and operation as possible using DevOps patterns: Infrastructure as Code, GitOps, CI\u002FCD and docker containers. I used AWS resources exclusively (with the exception of GitLab) with the help of [AWS Cloud Development Kit (CDK)](https:\u002F\u002Faws.amazon.com\u002Fcdk\u002F), an awesome tool that I have really come to like. In general I really like AWS, and the more I use it I start to think about what I would do without it. Also, a lot of the feedback I got on these projects recommended to \"just use a VPS\" instead of bothering with AWS because it is complicated, expensive, overkill, etc. This got me thinking about how far I could get in deploying a Django application on a server with little or no external services that AWS has spoiled me with. After a little bit of discomfort and confusion, I was able to check off most of what I was hoping to and came away with a few questions as well. If you are interested to know how I got things setup and and hear some of my thoughts on running Django applications in production, continue reading!\n\nIn this article, I'm going to go over my approach to deploying and running Django applications using DigitalOcean Droplets (Linux-based virtual machine that runs on top of virtualized hardware) and block storage volumes (network-based block devices that provide additional data storage for Droplets).\n\n\u003E I'll also touch on trade-offs between DigitalOcean and AWS and emphasize aspects of the project that confuse\u002Fd me with these block quotes.\n\nHere's a link to my project that I'll be referencing: [https:\u002F\u002Fgitlab.com\u002Fbriancaffey\u002Fdigital-ocean-docker-swarm-django-traefik](https:\u002F\u002Fgitlab.com\u002Fbriancaffey\u002Fdigital-ocean-docker-swarm-django-traefik).\n\nThe project setup is a combination of some of the best practices I have picked up along the way as well as some very helpful guides, repositories and blog posts that I'll do my best to reference throughout this article.\n\n## Overview\n\nHere are some of the key parts of the project that I'll go over:\n\n- DigitalOcean and GitLab setup\n- Creating an A Record that points to our Droplet IP\n- Using a prebuilt VM image that ships with docker and docker-compose\n- Setting up the REX-Ray storage driver to automatically provision Digital Ocean block storage volumes\n- Setting up a docker swarm cluster\n- Setting up a `.gitlab-ci.yml` file to build images and push them to a private GitLab CI project registry\n- Writing a docker-compose file to configure the services (containers) that will support the application\n- Deploying a stack to the docker swarm cluster on DigitalOcean from our GitLab CI environment over SSH\n- Django project settings and management commands for our Postgres database and static files\n- Monitoring, logging and debugging\n- Destroying the environment + cleanup\n\nBefore I dig into all of this, I recommend that you check out [this article about Django production architectures by Matt Segal](https:\u002F\u002Fmattsegal.dev\u002Fdjango-prod-architectures.html). This is a great primer for a lot of what I'll be talking about and it includes some great visualizations. [mattsegal.dev](https:\u002F\u002Fmattsegal.dev) has lots of good content related to Django, I also recommend checking out [this article about how NGINX is used with Django](https:\u002F\u002Fmattsegal.dev\u002Fnginx-django-reverse-proxy-config.html). Thanks for the great resources, Matt!\n\n## DigitalOcean setup\n\n- Sign up for a new DigitalOcean account if you don't already have one\n- Create a DigitalOcean project [https:\u002F\u002Fcloud.digitalocean.com\u002Fprojects\u002Fnew](https:\u002F\u002Fcloud.digitalocean.com\u002Fprojects\u002Fnew)\n- Create a personal access token (we will use this to configure a docker addon that will provision block storage volumes automatically) [https:\u002F\u002Fcloud.digitalocean.com\u002Faccount\u002Fapi\u002Ftokens](https:\u002F\u002Fcloud.digitalocean.com\u002Faccount\u002Fapi\u002Ftokens)\n- Create and add an SSH key to your account. This is a pretty simple step, but DigitalOcean still has really thorough documentation on how to do this (see [this article](https:\u002F\u002Fwww.digitalocean.com\u002Fdocs\u002Fdroplets\u002Fhow-to\u002Fadd-ssh-keys\u002F) for more information)\n\n## Prebuilt Docker VM image\n\nFrom the [Create Droplets](https:\u002F\u002Fcloud.digitalocean.com\u002Fdroplets\u002Fnew) page, select `Marketplace` and search for `docker`. Select the `Docker 5:19.03.1~3 18.04` image. Note that this VM is Ubuntun 18.04 with Docker Community Edition and docker-compose pre-installed.\n\nSelect the basic plan, and then scroll to the left to choose the \\$5.00\u002Fmonth option. Select a datacenter region. Most of these regions should be OK, but you should verify that the region you have selected supports volumes (they may all support volumes, but there are some DO features that are not supported accross all regiongs, similar to AWS). Do not select a VPC or any of the additional options.\n\nFor Authentication, select the SSH key that you created earlier.\n\nTake note of the Droplet's IP address; we will use this in the next step.\n\n## GitLab setup\n\nCreate a new GitLab project and clone it locally. You can also clone or fork my project and use that as a starting point. Go to `Settings \u003E CI\u002FCD \u003E Variables` in your GitLab project and add the following environment variables:\n\n- `SSH_PRIVATE_KEY`: the value should start with `-----BEGIN RSA PRIVATE KEY-----` and end with `-----END RSA PRIVATE KEY-----`\n- `DROPLET_IP`: the IP address of the droplet you just created\n- `POSTGRES_PASSWORD`: a secure password that we will use for our Postgres database (we will share this with our Django application later on)\n- `SECRET_KEY`: a random secret key to use for our Django application.\n- `DEBUG`: the number `0`\n\n## A Record\n\nBy the end of this project you will be able to deploy your Django application to a live domain name provided that you have one. All you need to do is create an A Record that points to the Droplet IP. There are no DNS configuration changes to make inside of DigitalOcean. I'm using a domain that I purchased through Route53. You can get a free `.tk` domain from [freenom](https:\u002F\u002Fwww.freenom.com\u002Fen\u002Ffreeandpaiddomains.html). I have used this before and it is a great option for testing things out.\n\n## SSH into your DigitalOcean Droplet\n\nYou can do this with the following command:\n\n```\nssh -i ~\u002F.ssh\u002Fa1_rsa root@123.45.578.91\n```\n\n`a1_rsa` is the private key I added to GitHub. You can logout for now, but keep this command handy, because we will be coming back to our Droplet via SSH shortly.\n\n## Add the REX-Ray docker plugin\n\nThis step is very simple, you can follow along with this short guide: [https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Fquestions\u002Fhow-to-attach-digitalocean-block-storage-to-docker-container](https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Fquestions\u002Fhow-to-attach-digitalocean-block-storage-to-docker-container).\n\nThere is basically one command to run:\n\n```\ndocker plugin install rexray\u002Fdobs DOBS_TOKEN=YOUR_DIGITALOCEAN_TOKEN DOBS_REGION=nyc1 LINUX_VOLUME_FILEMODE=0775\n```\n\nYou will need to make sure that you replace `YOUR_DIGITALOCEAN_TOKEN` with the personal access token you added earlier. Also, `DOBS_REGION` should be the region you selected for your Droplet earlier.\n\nCheck that the plugin was installed correctly with:\n\n```\ndocker plugin ls\n```\n\nHere's a quick intro to REX-Ray from [rexray.readthedocs.io](https:\u002F\u002Frexray.readthedocs.io\u002Fen\u002Fstable\u002F):\n\n\u003E REX-Ray is an open source, storage management solution designed to support container runtimes such as Docker and Mesos. REX-Ray enables stateful applications, such as databases, to persist and maintain its data after the life cycle of the container has ended. Built-in high availability enables orchestrators such as Docker Swarm, Kubernetes, and Mesos Frameworks like Marathon to automatically orchestrate storage tasks between hosts in a cluster.\n\nIn the context of this project, REX-Ray will automate the creation of DigitalOcean Block Storage Volumes. We will talk about volumes and how they are used later on in this article.\n\n## `.gitlab-ci.yml`\n\n`.gitlab-ci.yml` is a file that configures pipelines when code is pushed to GitLab, similar to how GitHub Actions work with GitHub. This single file is a huge topic, if you are unfamiliar with GitLab CI, you might want to have a look over [this page from the GitLab documentation](https:\u002F\u002Fdocs.gitlab.com\u002Fee\u002Fci\u002Fyaml\u002F) which goes over all of the configuration options with many examples. Also, [this documentation page](https:\u002F\u002Fdocs.gitlab.com\u002Fee\u002Fci\u002Fvariables\u002Fpredefined_variables.html) covers the predefined environment variables that are made available to GitLab CI pipelines. I'm using these in a few different places as we will see shortly.\n\nCI\u002FCD pipelines that I define with `.gitlab-ci.yml` typically contain three stages: `test`, `build` and `deploy`. We will focus on the `build` and `deploy` stages for now (reference the article on my Fargate project linked above for reference on setting up unit tests with pytest).\n\n`build-backend` is the name of a GitLab CI job that builds and tags a docker image from the source code in the `backend` directory of this project and pushes the tagged container image to a private image registry on gitlab.com that we will use later.\n\nHere's the YAML code for `build-backend`:\n\n```yml\nbuild-backend:\n  stage: build\n  image: docker:19.03.1\n  services:\n    - docker:19.03.5-dind\n  before_script:\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n  script:\n    - |\n      docker build \\\n        -t $CI_REGISTRY_IMAGE\u002Fbackend:$CI_COMMIT_SHORT_SHA \\\n        -f backend\u002Fdocker\u002FDockerfile.prod \\\n        .\u002Fbackend\u002F\n    - docker push $CI_REGISTRY_IMAGE\u002Fbackend:$CI_COMMIT_SHORT_SHA\n```\n\n`build-nginx` is almost identical, but the `docker build` arguments are slightly different. There are three arguments for the `docker build` command that I'm using here:\n\n1. `-t`: the tag to tag the built image with\n1. `-f`: the Dockerfile to be used for building the image\n1. the `context` that is sent to the docker daemon when we build the image\n\n`-t` makes use of two predefined GitLab CI variables: `CI_REGISTRY_IMAGE` and `CI_COMMIT_SHORT_SHA`. `$CI_REGISTRY_IMAGE` is the URL for the private image registry on gitlab.com that we push our images to that is specific to our project: `registry.gitlab.com\u002F\u003Cgitlab_username\u003E\u002F\u003Cproject_name\u003E`, and `CI_COMMIT_SHORT_SHA` is an character alphanumeric value that contains the truncated name of the commit hash, this is known as the `tag`, even though we pass in more than just this value. We combine these two values with `\u002F` and the name of the image we are building, such as `backend`, so the full value being passed to `-t` is:\n\n```\nregistry.gitlab.com\u002Fgitlab-username\u002Fmy-project\u002Fbackend:abcd1234\n```\n\n`-f` is the path to the `Dockerfile` we are using relative to the directory where we are running the `docker build` command, which is the root directory of the project.\n\nThe final argument defines the context that we are using to build the image, and this is an important part for understanding how Docker works. This argument defines the directory that is zipped up and sent to the docker daemon via the docker API. When we build an image with `docker build`, we are essentially using the docker CLI to make a POST request to our docker daemon (server) where the POST data contains all of the files that we will have access to in the steps of the Dockerfile (such as `ADD` and `COPY` -- we will get to these soon). There's a key difference between the `backend` and `nginx` `docker build` commands: the context for `backend` is `backend`, but the context for `nginx` is `.` (the root of the project). This is because we may want access to another top level directory in our project that contains, for example, a Vue.js or React application, that we will build into our NGINX container. In order to be able to access both files in the `nginx` and the folder containing our frontend app, we need to send a context that contains both of these directories. Sending too many files to to the docker daemon when you run docker build will usually cause the `docker build` command to hang. The first line of output from a `docker build` command should be something like this:\n\n```\nSending build context to Docker daemon  24.58kB\n```\n\nIf this number is too high, you should use a `.dockerignore` file that ignores any files or directories you don't want to send to the docker daemon (similar to how `.gitignore` works with git).\n\nTo be able to pull and push (read and write) images to our private project container image registry, we need login with our docker client using the `docker login` command in the `before_script` as well two other predefined GitLab CI variables: `CI_JOB_TOKEN` and `CI_REGISTRY`. This all happens using a special service called `docker-in-docker` which I won't go into too much detail here, but it is a common practice when working with containers in a CI\u002FCD environment that itself which is also based on containers, such as GitLab CI (each job runs in a container -- the key `image` -- and can define additional containers -- the `services` key -- to help with the CI job). Once the two images for `backend` and `nginx` have been built and pushed, our GitLab CI pipeline moves on to the next stage: `deploy`. In the `deploy` stage, we will start these and other containers on our DigitalOcean droplet, so we are getting close, but there is a lot more to explain. Before we deploy our containers, we need to do some one-time setup:\n\n1. initialize a single-node docker swarm cluster on our Droplet and\n2. create a docker network that our cluster's services (containers) will use\n\n## Setup a docker swarm cluster\n\nTo setup a docker swarm cluster, SSH into the Droplet with the command we introduced above (`ssh -i ~\u002F.ssh\u002Fa1_rsa root@123.45.578.91` where `a1_rsa` is the name of the private key file -- you can ignore the `-i ~\u002F.ssh\u002Fa1_rsa` part if you are using an SSH key called `id_rsa`), and run the following command:\n\n```\ndocker swarm init --advertise-addr DROPLET_IP\n```\n\nReplace `DROPLET_IP` with your Droplet's IP address (e.g. `123.45.578.91`). Check out [this article](https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Ftutorials\u002Fhow-to-create-a-cluster-of-docker-containers-with-docker-swarm-and-digitalocean-on-ubuntu-16-04) for some additional information about using docker swarm on GitLab. It is a little bit outdated, but the main ideas should still hold up. Docker swarm is designed to orchestrate containers running on a group (or swarm) of multiple machines. However, it is perfectly fine to run a single-node cluster as we are doing here.\n\nDocker swarm uses docker-compose files, but using docker swarm is very different from running `docker-compose up`, a command which you might see people running both locally and in production and which also uses docker-compose files. As a best practice, you should not be using `docker-compose` (the command) in production. Many people do this, and several official tutorials will often end with \"now just run `docker-compose up` and you are done\". The first time I ran containers in the cloud I pulled my git repo into a VM, installed docker and docker-compose and ran `docker-compose up`. It is pretty easy and it works very similarly in both local and production environments, but this guide will be using `docker-compose` in production. There is more I could say here, but the main point is that docker swarm is a simplified version of something like Kubernetes, but it comes built-in to docker and is very simple to use.\n\n## Defining an overlay network\n\nSSH into your droplet and run the following command:\n\n```\ndocker network create --driver=overlay traefik-public\n```\n\n\u003E Usually we define networks in our docker-compose file, but this network needs to be defined first and then referenced in our docker-compose file. Here's a thread on SO that goes into a little bit more on why this is necessary, but I still don't have a very clear idea of why this is the case. With another configuration or perhaps docker-compose version, this may not be needed. I'll update this part of the article if I figure anything out.\n\nLet's go over one more docker concept that will helpful in the next few steps. When you run `docker ps` on your local machine, the docker CLI first looks to see if the `DOCKER_HOST` environment variable is set. If it is not, then docker defaults to `unix:\u002F\u002F\u002Fvar\u002Frun\u002Fdocker.sock`, a UNIX socket. Check out [this SO post](https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F35110146\u002Fcan-anyone-explain-docker-sock) titled \"Can anyone explain docker.sock?\"\n\nWe change the docker host that our local docker CLI is talking to by setting this environment variable, and one nice way to set this environment variables uses an SSH connection:\n\n```\nDOCKER_HOST=ssh:\u002F\u002Froot@$DOCKER_IP\n```\n\nSee this article for a more in-depth discussion: [https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Ftutorials\u002Fhow-to-use-a-remote-docker-server-to-speed-up-your-workflow](https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Ftutorials\u002Fhow-to-use-a-remote-docker-server-to-speed-up-your-workflow).\n\nYou can try this out locally. Run a container locally, check it with `docker ps`, then export the `DOCKER_HOST` environment variable with the following command:\n\n```\nexport DOCKER_HOST=ssh:\u002F\u002Froot@123.45.678.91\n```\n\nReplacing `123.45.678.91` with your Droplet IP. Run `docker ps` again and you should see nothing (or any other containers that you started on your Droplet). Finally, run:\n\n```\nunset DOCKER_HOST\n```\n\nRunning `docker ps` again you should see the containers on your machine. We will be using this idea in the next step when we look at the `docker stack deploy` command.\n\n## `docker stack deploy`\n\nNow that we have done our one-time-setup steps, let's look at the `deploy` stage of `.gitlab-ci.yml`, the GitLab CI job that will get our containers running on our Droplet. First, let's break down the `deploy-digital-ocean` command:\n\n```yml\ndeploy-digital-ocean:\n  stage: deploy\n  image: docker:19.03.1\n  services:\n    - docker:19.03.5-dind\n  variables:\n    DOCKER_HOST: 'ssh:\u002F\u002Froot@$DROPLET_IP'\n  before_script:\n    - apk update && apk add openssh-client bash\n    - mkdir -p ~\u002F.ssh\n    - echo \"$SSH_PRIVATE_KEY\" \u003E ~\u002F.ssh\u002Fid_rsa\n    - chmod 600 ~\u002F.ssh\u002Fid_rsa\n    - eval \"$(ssh-agent -s)\"\n    - ssh-add ~\u002F.ssh\u002Fid_rsa\n    - ssh-keyscan -H $DROPLET_IP \u003E\u003E ~\u002F.ssh\u002Fknown_hosts\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n  script:\n    - docker stack deploy -c stack.yml my-stack --with-registry-auth\n```\n\nThis job uses the same `image` and `services` that our `build` stage jobs used. Notice that we set `DOCKER_HOST` to `\"ssh:\u002F\u002Froot@$DROPLET_IP\"`, this means that any docker CLI commands in this job will be communicating with the docker daemon on our Droplet. The `before_script` has a lot going on, but all we are doing is preparing to use the SSH private key that we previously added to our GitLab project's CI\u002FCD environment variables. The base image for this job, `docker:19.03.1` is based on Alpine Linus. This version of Linux is super light weight and doesn't come with `openssh-client` or `bash`, so our first step is to install these with the Alpine package manager, `apk`:\n\n```sh\napk update && apk add openssh-client bash\n```\n\nNext, we add the `SSH_PRIVATE_KEY` environment variable into the body of the `id_rsa` private key file, change the permission of this file and then add the key to our SSH agent. Here's an excerpt from `man ssh-agent` that provides a little bit more context into why we need to run `eval \"$(ssh-agent -s)\"` and `ssh-add ~\u002F.ssh\u002Fid_rsa`:\n\n```\nDESCRIPTION\n     ssh-agent is a program to hold private keys used for public key authentication (RSA, DSA, ECDSA, Ed25519)ssh-agent is usually started in the beginning of an X-session or a login session, and all other windows or programs are started as clients to the ssh-agent program.  Through use of environment variables the agent can be located and automatically used for authentication when logging in to other machines using ssh(1).\n\n     The agent initially does not have any private keys.  Keys are added using ssh(1) (see AddKeysToAgent in ssh_config(5) for details) or ssh-add(1).  Multiple identities may be stored in ssh-agent concurrently and ssh(1) will automatically use them if present.  ssh-add(1) is also used to remove keys from ssh-agent and to query the keys that are held in one.\n```\n\nNext, `ssh-keyscan -H $DROPLET_IP \u003E\u003E ~\u002F.ssh\u002Fknown_hosts` tells our SSH agent about our Droplet so that it doesn't prompt us with a `Do you want to add this server to known hosts? (yes\u002Fno)`, or whatever the equivalent of that is for the docker CLI when it attempts to connect to a remote docker daemon over SSH.\n\nFinally, we login to our our GitLab private registry using the same command from before when we built and pushed images to our private registry on gitlab.com:\n\n```\ndocker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY\n```\n\nThis essentially gives our DigitalOcean Droplet access to the `backend` and `nginx` images in our private GitLab CI image registry, even tho we are running this command in a contain, in a container that is probably running in Kubernetes on GCP. Next, we are actually going to use these images.\n\nThe last command in the `deploy-digital-ocean` job is:\n\n```\ndocker stack deploy -c stack.yml my-stack --with-registry-auth\n```\n\nCheck out this link from the docker docs on docker stacks [https:\u002F\u002Fdocs.docker.com\u002Fengine\u002Fswarm\u002Fstack-deploy\u002F](https:\u002F\u002Fdocs.docker.com\u002Fengine\u002Fswarm\u002Fstack-deploy\u002F). `--with-registry-auth` is important, our command will complete if this is not included, but our application won't start.\n\n## `stack.yml`\n\nNow we are ready to tackle the last big file in our repo: `stack.yml`. This is a the docker-compose file that we use to deploy our project. The only reason we needed to run `docker login` above is because `stack.yml` references the two images we built and pushed to our GitLab private repo. There's a lot going on in this file, let's start with the `backend` service:\n\n### backend\n\n```yml\nversion: '3.4'\nservices:\n  backend:\n  image: ${CI_REGISTRY_IMAGE}\u002Fbackend:${CI_COMMIT_SHORT_SHA}\n  networks:\n    - main\n  environment:\n    - POSTGRES_PASSWORD\n    - SECRET_KEY\n    - DEBUG\n  volumes:\n    - backendassets:\u002Fcode\u002Fassets\n  depends_on:\n    - postgres\n    - redis\n    - web\n```\n\nThe `image` is essentially what we defined in `.gitlab-ci.yml`, but the syntax is slightly different:\n\n```\n${CI_REGISTRY_IMAGE}\u002Fbackend:${CI_COMMIT_SHORT_SHA}\n```\n\nWe pass environment variables that we defined in GitLab CI via the `environment` key.\n\nThe volume `backendassets` is used for storing static assets (CSS, JS, etc.) as well as media assets (images, videos, any other file type). We mount this directory at `\u002Fcode\u002Fassets` and then define our `STATIC_ROOT` in Django's `settings.py` to be:\n\n```py\nos.path.join(BASE_DIR, \"assets\", \"static\")\n```\n\nLater, when we run `collectstatic`, files are copied to this location in our container, and since this is the location of the volume, the files are actually copied to the volume and will be persisted if we destroy the backend container and restart it. When the container restarts, the volume is mounted again and the static files will still be available to our application.\n\n`network` and `depends_on` related to to the other services that this application will communicate with. `main` is a network defined in the `networks` part of `stack.yml`, notice that we reference the `traefik-public` network here that we created earlier, as well.\n\n\u003E Depends on helps with service startup order, but it is a better practice to use `.\u002Fwait-for-it.sh`. However, I have never had any issues related to startup order. I'll try adding this later to make things more robust.\n\n`postgres` and `redis` will start before `backend`. Our Django application will communicate to these services by their hostnames: `postgres` and `redis`. The fact that `backend`, `postgres` and `redis` are all on the same network (`main`) means that they can resolve each other by these names. For example, the connection string to redis will look like: `redis:\u002F\u002Fredis:6379`. Let's look at the `DATABASES` configuration in `settings.py` to see how we connect to Postgres:\n\n```py\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.postgresql_psycopg2\",\n        \"NAME\": os.environ.get(\"POSTGRES_NAME\", \"postgres\"),\n        \"USER\": os.environ.get(\"POSTGRES_USERNAME\", \"postgres\"),\n        \"PASSWORD\": os.environ.get(\"POSTGRES_PASSWORD\", \"postgres\"),\n        \"HOST\": os.environ.get(\"POSTGRES_SERVICE_HOST\", \"postgres\"),\n        \"PORT\": os.environ.get(\"POSTGRES_SERVICE_PORT\", 5432),\n    }\n}\n```\n\nI have defined the `HOST` to be based on the environment variable `POSTGRES_HOST`, but I have not defined this environment variable, so why didn't I just say `\"HOST\": \"postgres\"`? I could have, but if I want to change the database in the future, the only change will be adding an environment variable; I won't have to worry about hardcoded values.\n\nI'm choosing to run Postgres in a container and not use a managed database (which DigitalOcean does offer) in order to save on costs and also to get more practice managing my own database. I use RDS with AWS and it handles a lot of things that I don't have to worry about, such as backups, and it allows me to quickly restore from a snapshot. I'm interested in learning more about how I can do these tasks with a database that I run myself.\n\n### NGINX\n\nThe next service we should go over is `web`, the service that runs the NGINX container that we pushed to our private GitLab image registry. This service has a couple of functions that I'll walk through:\n\n1. Reverse proxy\n2. Serve static files for Django\n3. Serve a frontend Javascript application\n\nHere's the definition of this service in `stack.yml`:\n\n```yml\nservices:\n  web:\n    image: ${CI_REGISTRY_IMAGE}\u002Fnginx:${CI_COMMIT_SHORT_SHA}\n    networks:\n      - traefik-public\n      - main\n    volumes:\n      - backendassets:\u002Fusr\u002Fsrc\u002Fapp\u002Fassets\n    deploy:\n      labels:\n        - 'traefik.enable=true'\n        - 'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'\n        - 'traefik.http.routers.nginx-web.entrypoints=websecure'\n        - 'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'\n        - 'traefik.http.services.nginx-web.loadbalancer.server.port=80'\n```\n\nFor now, ignore the contents under the `deploy` key; we will cover this next when we go over the `traefik` service.\n\nNGINX acts as a reverse proxy when it sends request starting with `\u002Fapi` or `\u002Fadmin` to the `backend` container. Two blocks in `prod.conf` enable this behavior:\n\n```\n  upstream backend {\n    server backend:8000;\n  }\n```\n\nThis hostname `backend` is defined as `backend:8000`. `backend:8000` can be resolved by the `web` service because it is on the same `main` network that `backend` is on. If either of the `web` or `backend` wasn't on the `main` network, NGINX would not be able to make sense of `backend:8000`.\n\n```\n    # backend urls\n    location ~ ^\u002F(admin|api) {\n      proxy_redirect off;\n      proxy_pass http:\u002F\u002Fbackend;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n    }\n```\n\nThis block does that actual request forwarding. `http:\u002F\u002Fbackend` references the `upstream backend {}` block defined above.\n\nThe volume `backendassets` is referenced here and mounts to `\u002Fusr\u002Fsrc\u002Fapp\u002Fassets`. This path is then referenced in `prod.conf`, the NGINX configuration file that is used in our custom NGINX-based image:\n\n```\n    # static files\n    location \u002Fstatic {\n      autoindex on;\n      alias \u002Fusr\u002Fsrc\u002Fapp\u002Fassets\u002Fstatic;\n    }\n```\n\nIn this block of `prod.conf`, we tell NGINX to serve files from `\u002Fusr\u002Fsrc\u002Fapp\u002Fassets\u002Fstatic` for requests that start with `\u002Fstatic`. A request made to `https:\u002F\u002Fmysite.com\u002Fstatic\u002Fbase.css` would return a file located at `\u002Fusr\u002Fsrc\u002Fapp\u002Fassets\u002Fstatic` if that file existed. Remember, when we run the `collecstatic` management command in our Django container, it will collect our static files to `backendassets`. Since `backendassets` is mounted to the `web` service at `\u002Fusr\u002Fsrc\u002Fapp\u002Fassets`, NGINX will have access to these files by way of the volume mount and they will persist across restarts of the web service and its NGINX container.\n\nFinally, NGINX can serve a Javascript SPA or similar if we choose to use one in our project. To understand how this is done, we need to understand multistage Dockerfiles. Here's the Dockerfile used for the `nginx` container:\n\n```Dockerfile\n# # build stage\n# FROM node:10-alpine as build-stage\n# WORKDIR \u002Fapp\u002F\n# COPY frontend\u002Fpackage.json \u002Fapp\u002F\n# RUN npm cache verify\n# RUN npm install\n# COPY frontend \u002Fapp\u002F\n# RUN npm run build\n\n# production stage\n# FROM nginx:1.19.1-alpine as production-stage\nFROM nginx:1.19.1-alpine\nCOPY nginx\u002Fprod\u002Fprod.conf \u002Fetc\u002Fnginx\u002Fnginx.conf\nCOPY nginx\u002Fprod\u002Findex.html \u002Fdist\u002F\n# COPY --from=build-stage \u002Fapp\u002Fdist \u002Fdist\u002F\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n```\n\nCurrently I don't have a SPA setup, but this is how we could setup one using Vue.js. The important part is this line:\n\n```Dockerfile\nCOPY --from=build-stage \u002Fapp\u002Fdist \u002Fdist\u002F\n```\n\nThis would copy the build files for our Javascript application into the `\u002Fdist\u002F` folder of our NGINX container. Another few declarations and blocks in `prod.conf` allow all other requests to be served by the contents of this folder:\n\n```\n    root \u002Fdist\u002F;\n    index index.html;\n```\n\nThis sets the root and the index document for our NGINX webserver.\n\n```\n    # frontend\n    location \u002F {\n      try_files $uri $uri\u002F @rewrites;\n    }\n\n    location @rewrites {\n      rewrite ^(.+)$ \u002Findex.html last;\n    }\n```\n\nThese two blocks route all other requests to the frontend Javascript app's `index.html` file location in `\u002Fdist\u002F` (any request that doesn't start with `\u002Fstatic`, `\u002Fapi` or `\u002Fadmin`). We may wish to change this behavior if you want Django to serve most of your requests and possibly serve a single page application on another path.\n\nLastly, the `web` service's `deployment` key has some `labels` defined for Traefik. Let's come back to these after having a look at the `traefik` service.\n\n### Traefik\n\n![png](https:\u002F\u002Fdocs.traefik.io\u002Fassets\u002Fimg\u002Ftraefik-architecture.png)\n\n\u003E Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. -- [https:\u002F\u002Fdocs.traefik.io\u002F](https:\u002F\u002Fdocs.traefik.io\u002F)\n\nTraefik has three main functions in my application:\n\n1. Request TLS certificates from Let's Encrypt that allow us to encrypt our web traffic with HTTPS\n1. Do TLS termination\n1. Route all requests to NGINX\n\nThe one thing that Traefik cannot do is serve static files; it is not a webserver, unlike NGINX which is a webserver. NGINX is also capable of requesting TLS certs from Let's Encrypt, so we don't technically need Traefik, but it is indeed \"fun and easy\", especially when it comes to requesting certificates.\n\n\u003E I have tried setting up Certbot with NGINX a long time ago but I never go it to work, and I didn't like the idea about how to run a chron job to refresh old certs.\n\nThere are two main ways to set up Traefik:\n\n1. write a `traefik.toml` file and build this into your own custom image (similar to what we do with NGINX and `prod.conf`)\n1. use a base image and specify all configure options through command line arguments.\n\nI started out with the first approach, and I did get it to work, but I have decided that the second way is better. It requires one less image to build in our deployment process and it is easy to parametrize the command line arguments in `stack.yml` (for now all the values I'm using in the `traefik` service are hard-coded, this is one more item for my ToDo list on this project).\n\nI had a hard time finding good examples of how to use Traefik version 2 with Docker Swarm in the Traefik docs. Their official example for using docker uses `docker-compose up`. There is a Swarm example, but it is for an older version of Traefik (1.7). This article titled [How to install Traefik 2.x on a Docker Swarm](https:\u002F\u002Fblog.creekorful.com\u002F2019\u002F10\u002Fhow-to-install-traefik-2-docker-swarm\u002F) helped me a lot in figuring out how to get everything working. Thank you for the great article, [Alos](https:\u002F\u002Fgithub.com\u002Fcreekorful)!\n\nHere's the code that sets up the traefik service:\n\n```yml\nservices:\n  traefik:\n    image: traefik:v2.0.2\n    ports:\n      - '80:80'\n      - '443:443'\n    command:\n      - '--providers.docker.endpoint=unix:\u002F\u002F\u002Fvar\u002Frun\u002Fdocker.sock'\n      - '--providers.docker.swarmMode=true'\n      - '--providers.docker.exposedbydefault=false'\n      - '--providers.docker.network=traefik-public'\n      - '--entrypoints.web.address=:80'\n      - '--entrypoints.websecure.address=:443'\n      - '--certificatesresolvers.letsencryptresolver.acme.httpchallenge=true'\n      - '--certificatesresolvers.letsencryptresolver.acme.httpchallenge.entrypoint=web'\n      - '--certificatesresolvers.letsencryptresolver.acme.email=brian@email.com'\n      - '--certificatesresolvers.letsencryptresolver.acme.storage=\u002Fletsencrypt\u002Facme.json'\n    volumes:\n      - letsencrypt:\u002Fletsencrypt\n      - \u002Fvar\u002Frun\u002Fdocker.sock:\u002Fvar\u002Frun\u002Fdocker.sock\n    networks:\n      - traefik-public\n    deploy:\n      placement:\n        constraints:\n          - node.role == manager\n```\n\n\u003E The one thing I don't like about this setup is that it uses a 1GB volume to store one small JSON file. I think that 1GB is the smallest block storage volume I can request using REX-Ray. This only adds \\$0.10\u002Fmonth to our project costs which is not that bad.\n\n`web` and `websecure` refer to values declared on the `web` service. Let's take a look at those values:\n\n```yaml\ndeploy:\n  labels:\n    - 'traefik.enable=true'\n    - 'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'\n    - 'traefik.http.routers.nginx-web.entrypoints=websecure'\n    - 'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'\n    - 'traefik.http.services.nginx-web.loadbalancer.server.port=80'\n```\n\nI still need to setup HTTP -\u003E HTTPS redirecting, so for now only `websecure` is defined, but Alos explains this clearly in his article.\n\n\u003E For me this is the most complicated part of the setup. I'm still not familiar with exactly how Traefik and Let's Encrypt work. Hopefully I can run through this process a few more times with some variations to better understand the rough edges. Otherwise for this simple way to get TLS certificates. AWS makes this very easy with Amazon Certificate Manager (ACM) which makes the requesting of certificates very simple, especially within CDK.\n\nThat wraps up our overview of `stack.yml`, we left off with the following command:\n\n```\ndocker stack deploy -c stack.yml my-stack --with-registry-auth\n```\n\nWe can check out the status of our docker stack deployment by running a few different docker CLI commands. You can either SSH into your Droplet or configure the `DOCKER_HOST` environment variable that I showed you earlier and run these commands from your local terminal:\n\n```\ndocker stack ps my-stack --no-trunc\n```\n\n`--no-trunc` is important because important error messages tend to be cut off. This option will show the full version of each column returned by `docker stack ps`.\n\n```\ndocker service ls\n```\n\nThis command shows some the active services on our Droplet.\n\n```\ndocker ps\n```\n\nThis command is useful for shelling into a container to run commands and poke around for debugging.\n\nYou can get the Container ID of a running container and access it with the following command:\n\n```\ndocker exec -it 0da8370ab283 bash\n```\n\nThis assumes that `bash` is installed on the container with ID `0da8370ab283`.\n\n## Management commands\n\nOnce the site is deployed we stil need to run a few commands to set up our Djnago application:\n\n1. collectstatic\n1. migrate\n1. createsuperuser\n\n\u003E One other ToDo is to figure out how to run these commands through manual GitLab CI jobs.\n\nThat's most of what I wanted to cover on a first pass. This should be a good starting point for working with a Django application in Docker Swarm on DigitalOcean.\n\n## Next steps\n\nHere are some ideas about the next steps I could take on this project.\n\n### Local environment\n\nI'll probably need to create another docker-compose file to bring up everything locally. It might be a good way to experiment with different Traefik settings.\n\n### Infrastructure as Code setup\n\nThere might be a good opportunity to learn more about Terreform or Ansible here. There are a number of manual, one time setup steps. Some of these can't be automated, but some of them would probably fit very neatly into one of these tools. Pulumi would also be a good option to explore as it is more analagous to CDK.\n\n### Scaling out docker swarm services across multiple machines\n\nI have only scratched the surface of what docker swarm can do. There are lots of other settings that would be helpful to setup for learning purposes, especially around resource limits for services. For simplicity I haven't touch on any of these options yet. I'm curious to know how many containers I can fit onto one small Droplet, and if resource limits could help with compute and memory-intensive workloads.\n\n### Kubernetes on DigitalOcean\n\nDigitalOcean now offers simplified Kubernetes solutions. It would be interesting to try this out once I get better with docker swarm. I have used Kubernetes a with minikube and to a limited extent with GCP.\n\n### Deploying locally, without using GitLab CI\n\nCDK makes deploying locally very easy, especially with the Lambda project I put together. This project as it stands might be a little bit more difficult to deploy locally. It assumes that the images we want to deploy are private. It might be possible, but for now I am fine with deploying through GitLab CI since the pipeline only takes a few minutes to complete for the build and deploy stages.\n"}}],fetch:{},mutations:[]}}("text","element","span","token","code","\n","punctuation","p","-",":","a","string","key","atrule","li"," ","div","nuxt-content-highlight","pre","line-numbers","language-text","\n    ","nofollow","noopener","noreferrer","_blank","true",-1,"icon","icon-link","backend",".","\n      "," and ",2,"h2",",","\n        ","nginx","blockquote","deploy","\n  ","stack.yml",3,"image","h3","web","traefik",".gitlab-ci.yml","services","docker build",")","ol","{","}","(","language-yml","DOCKER_HOST"," os","environ","get","prod.conf"," docker","before_script","CI_COMMIT_SHORT_SHA","docker-compose up","docker ps"," or ","networks","backendassets","main","\"postgres\"","docker stack deploy","ul",", ","build","build-backend","-t","CI_REGISTRY_IMAGE",". ","deploy-digital-ocean","\u003E","volumes","postgres","redis"," traefik","labels","backend:8000","docker","overview","Overview","digitalocean-setup","DigitalOcean setup","prebuilt-docker-vm-image","Prebuilt Docker VM image","gitlab-setup","GitLab setup","a-record","A Record","ssh-into-your-digitalocean-droplet","SSH into your DigitalOcean Droplet","add-the-rex-ray-docker-plugin","Add the REX-Ray docker plugin","gitlab-ciyml","setup-a-docker-swarm-cluster","Setup a docker swarm cluster","defining-an-overlay-network","Defining an overlay network","docker-stack-deploy","stackyml","NGINX","Traefik","management-commands","Management commands","next-steps","Next steps","local-environment","Local environment","infrastructure-as-code-setup","Infrastructure as Code setup","scaling-out-docker-swarm-services-across-multiple-machines","Scaling out docker swarm services across multiple machines","kubernetes-on-digitalocean","Kubernetes on DigitalOcean","deploying-locally-without-using-gitlab-ci","Deploying locally, without using GitLab CI","https:\u002F\u002Fgitlab.com\u002Fbriancaffey\u002Fdigital-ocean-docker-swarm-django-traefik","https:\u002F\u002Fcloud.digitalocean.com\u002Fprojects\u002Fnew","https:\u002F\u002Fcloud.digitalocean.com\u002Faccount\u002Fapi\u002Ftokens","this article","SSH_PRIVATE_KEY","DROPLET_IP","a1_rsa","https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Fquestions\u002Fhow-to-attach-digitalocean-block-storage-to-docker-container","stage","19.03.1\n  ","19.03.5","dind\n  "," docker login ","u gitlab","ci","token ","p $CI_JOB_TOKEN $CI_REGISTRY\n  ","script","-f"," is ","docker login","id_rsa","docker-compose","https:\u002F\u002Fwww.digitalocean.com\u002Fcommunity\u002Ftutorials\u002Fhow-to-use-a-remote-docker-server-to-speed-up-your-workflow"," ssh","bash","docker stack deploy -c stack.yml my-stack --with-registry-auth\n","https:\u002F\u002Fdocs.docker.com\u002Fengine\u002Fswarm\u002Fstack-deploy\u002F"," $","$","environment"," backendassets","depends_on","The volume ","settings.py","language-py","collectstatic","'traefik.enable=true'","'traefik.http.routers.nginx-web.rule=Host(`mysite.com`)'","'traefik.http.routers.nginx-web.entrypoints=websecure'","'traefik.http.routers.nginx-web.tls.certresolver=letsencryptresolver'","'traefik.http.services.nginx-web.loadbalancer.server.port=80'"," service.","\u002Fapi","\u002Fadmin","\u002Fusr\u002Fsrc\u002Fapp\u002Fassets","\u002Fusr\u002Fsrc\u002Fapp\u002Fassets\u002Fstatic","\u002Fstatic","\u002Fdist\u002F","https:\u002F\u002Fdocs.traefik.io\u002F","websecure","2021-10-10T04:32:35.164Z")));