__NUXT_JSONP__("/fr/2022/03/27/ad-hod-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co,cp,cq,cr,cs,ct,cu,cv,cw,cx,cy,cz,cA,cB,cC,cD,cE,cF,cG,cH,cI,cJ,cK,cL,cM,cN,cO,cP,cQ,cR,cS,cT,cU,cV,cW,cX,cY,cZ,c_,c$,da,db,dc,dd,de,df,dg,dh,di,dj,dk,dl,dm,dn,do0,dp,dq,dr,ds,dt,du,dv,dw,dx,dy,dz,dA,dB,dC,dD,dE,dF,dG,dH,dI,dJ,dK,dL,dM,dN,dO,dP,dQ,dR,dS,dT,dU){return {data:[{article:{slug:"ad-hod-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions",description:"How to use AWS to provide isolated ad hoc environments for teams developing web applications",title:"Setting up ad hoc development environments for Django applications with AWS ECS, Terraform and GitHub Actions",date:"2022-05-27",image:ax,tags:["django",ay,"github-actions",az,"ecs","containers","docker"],draft:B,external:null,comments:B,toc:[{id:aA,depth:u,text:aB},{id:aC,depth:u,text:aD},{id:aE,depth:u,text:aF},{id:aG,depth:u,text:aH},{id:_,depth:u,text:_},{id:aI,depth:u,text:aJ},{id:aK,depth:u,text:aL},{id:aM,depth:u,text:$},{id:aN,depth:n,text:aO},{id:aP,depth:n,text:aQ},{id:aR,depth:u,text:aS},{id:aT,depth:n,text:aj},{id:aU,depth:n,text:aV},{id:aW,depth:n,text:aX},{id:aY,depth:n,text:aZ},{id:a_,depth:n,text:a$},{id:ba,depth:n,text:bb},{id:bc,depth:n,text:bd},{id:be,depth:u,text:bf},{id:bg,depth:n,text:ak},{id:bh,depth:n,text:bi},{id:bj,depth:n,text:bk},{id:bl,depth:n,text:bm},{id:bn,depth:n,text:al},{id:bo,depth:n,text:bp},{id:L,depth:n,text:L},{id:bq,depth:u,text:br},{id:bs,depth:n,text:bt},{id:bu,depth:n,text:bv},{id:bw,depth:u,text:bx},{id:by,depth:n,text:bz},{id:bA,depth:n,text:bB},{id:bC,depth:u,text:bD},{id:bE,depth:u,text:bF},{id:bG,depth:u,text:bH},{id:bI,depth:u,text:bJ},{id:bK,depth:u,text:bL},{id:bM,depth:u,text:bN},{id:bO,depth:u,text:bP},{id:bQ,depth:u,text:bR},{id:bS,depth:n,text:bT},{id:bU,depth:n,text:bV},{id:bW,depth:n,text:bX},{id:bY,depth:u,text:bZ},{id:b_,depth:n,text:b$},{id:ca,depth:n,text:cb},{id:cc,depth:n,text:cd},{id:ce,depth:u,text:cf},{id:cg,depth:u,text:$},{id:ch,depth:u,text:ci},{id:cj,depth:n,text:"A: bootstrapped S3 backend for terraform state"},{id:ck,depth:n,text:"B: deploy shared resources for ad hoc environments"},{id:cl,depth:n,text:"C: launch (or update) ad hoc environments"},{id:cm,depth:n,text:"D: destroy an ad hoc environment"},{id:cn,depth:n,text:"E: destroy shared resources"},{id:co,depth:n,text:"F: destroy bootstrap resources"},{id:cp,depth:u,text:cq},{id:cr,depth:n,text:"1: make tf-bootstrap"},{id:cs,depth:n,text:"1: S3 backend Terraform configuration"},{id:ct,depth:n,text:"1: S3 bucket for S3 backend"},{id:cu,depth:n,text:"1: DynamoDB lock table"},{id:cv,depth:n,text:"1: ECR resources"},{id:cw,depth:n,text:"1: S3 backend outputs"},{id:cx,depth:n,text:"1: Terraform state file for S3 backend"},{id:cy,depth:n,text:"1: make examples-simple for setting up shared resources"},{id:cz,depth:n,text:"1: terraform-aws-ad-hoc-environments Terraform module on Terraform Registry"},{id:cA,depth:n,text:"1: VPC module"},{id:cB,depth:n,text:"1: AWS CloudMap"},{id:cC,depth:n,text:"1: EC2 instance (bastion host)"},{id:cD,depth:n,text:"1: RDS instance"},{id:cE,depth:n,text:"1: Application Load Balancer"},{id:cF,depth:n,text:"1: IAM resources"},{id:cG,depth:n,text:"1: ECS roles"},{id:cH,depth:n,text:"1: Shared resources state file"},{id:cI,depth:n,text:"1: workflow_dispatch"},{id:cJ,depth:n,text:"1: create_update_ad_hoc_env GitHub Action"},{id:cK,depth:n,text:"1: Ad hoc environment name"},{id:cL,depth:n,text:"1: terraform-aws-django Terraform module"},{id:cM,depth:n,text:"1: ECS cluster"},{id:cN,depth:n,text:"1: ECS services"},{id:cO,depth:n,text:"1: S3 bucket for ad hoc environment"},{id:cP,depth:n,text:"1: ALB Listener Rules"},{id:cQ,depth:n,text:"1: Postgres database in RDS instance"},{id:cR,depth:n,text:"1: Redis"},{id:cS,depth:n,text:"1: Service Discovery Service (CloudMap)"},{id:cT,depth:n,text:"1: terraform-remote-state"},{id:cU,depth:n,text:"1: destroy_ad_hoc_env"},{id:cV,depth:n,text:"1: make examples-simple-destroy"},{id:cW,depth:n,text:"1: make tf-bootstrap-destroy"},{id:cX,depth:n,text:"1: briancaffey\u002Fdjango-step-by-step GitHub repo"},{id:cY,depth:n,text:"1: briancaffey\u002Fterraform-aws-django GitHub repo"},{id:cZ,depth:n,text:"1: briancaffey\u002Fterraform-aws-ad-hoc-environments GitHub repo"},{id:c_,depth:n,text:c$},{id:da,depth:u,text:db}],body:{type:"root",children:[{type:b,tag:v,props:{id:aA},children:[{type:b,tag:h,props:{href:"#tldr",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aB}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This article shows how software development teams can spin up ad hoc environments for testing and demoing purposes."}]},{type:a,value:c},{type:b,tag:v,props:{id:aC},children:[{type:b,tag:h,props:{href:"#github-links",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aD}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This article has three corresponding code repositories on GitHub that will be referenced in this article:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step",rel:[x,y,z],target:A},children:[{type:a,value:aa}]}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"this repo contains an example microblogging application called Î¼blog"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"includes complete GitHub Action examples for automating creation, updating and removal of ad hoc environments"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-django",rel:[x,y,z],target:A},children:[{type:a,value:am}]}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"a Terraform module that has been published to Terraform Registry"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"includes examples of how to use the module"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments",rel:[x,y,z],target:A},children:[{type:a,value:W}]}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"this repo is also published to Terraform Registry"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"it provides the shared infrastructure for setting up our ad hoc environments"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:aE},children:[{type:b,tag:h,props:{href:"#assumptions",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aF}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There are all sorts of applications, and all sort of engineering teams. For some context of what I'm describing in this article, here are some basic assumptions that I'm making about a fictional application and developed by a fictional engineering team:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"engineering department has a backend team, a frontend team, a devops team and works closely with a product team"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend team primarily develops an API"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"frontend team develops an JavaScript SPA (frontend website)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"SPA consumes backend API"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"product team frequently needs to demo applications to prospective clients"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"development teams don't have deep expertise in infrastructure, containers, CI\u002FCD or automation"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"devops team has some knowledge of the application and is responsible for building automation that will allow anyone on the team to quickly spin up a complete environment for either testing or demoing purposes"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here are assumptions about specific tools and technologies used at the company:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend is a REST API developed with Django and a Postgres database"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend is packaged into a docker container"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"frontend is also packaged into a docker container using multi-stage builds and NGINX"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"frontend does not require any build-time configuration (all configuration needed by frontend is fetched from backend)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend application's configuration is driven by plain-text environment variables at run-time"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"automation pipeline exists for building, tagging and pushing backend and frontend containers to an ECR repository"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"engineering team uses AWS"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"devops team uses AWS ECS for running containerized workloads"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"devops team uses Terraform for provisioning infrastructure"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"devops team uses GitHub Actions for building automation pipelines"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"team is somewhat cost-conscious"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:aG},children:[{type:b,tag:h,props:{href:"#what-are-ad-hoc-environments",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aH}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environments are short-lived environments that are designed to be used for testing a specific set of features or for demoing a specific application configuration in an isolated environment."}]},{type:a,value:c},{type:b,tag:v,props:{id:_},children:[{type:b,tag:h,props:{href:"#eli5",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:_}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"A developer named Brian says: I want to run version 1.2.3 of the backend application and version 4.5.6 of the frontend application. That developer selects these two version values as inputs in a GitHub Action or  Jenkins pipeline or Slack chatbot and then a few moments later gets a message saying:"}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,X]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Hey Brian, your environment is ready!\n\nvisit: https:\u002F\u002Fbrian.dev.company.com to access the environment\n\nthis environment will self-destruct in 1 day!\n\nthank you!\n"}]}]}]},{type:a,value:c},{type:b,tag:v,props:{id:aI},children:[{type:b,tag:h,props:{href:"#trade-offs-to-make-when-designing-ad-hoc-environment-infrastructure-and-automation",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aJ}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"When building infrastructure for ad hoc environments, there are a few things to solve for:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"startup speed"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"shared vs isolated resources"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"costs"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"automation complexity"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"degree of similarity to production environments"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Startup speed can be measured by the time between when an environment is requested and when that environment can be used. In this period of time, an automation pipeline may do the following:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"run "},{type:b,tag:e,props:{},children:[{type:a,value:an}]},{type:a,value:Y},{type:b,tag:e,props:{},children:[{type:a,value:ab}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:Z}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"run scripts to prepare the application such as database migrations"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"seeding initial sample data with a script or database dump"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"message the user with information about the environment (URLs, commands for accessing a bastion host or interactive shell, etc.)"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Some things in AWS take more time to provision than others. RDS and ElastiCache instances can take a long time to create relative to S3 buckets or IAM roles. RDS and ElastiCache instances are also more costly than other resources. We could use a single, shared RDS instance placed in a private subnet of a shared VPC. Each ad hoc environment could use a different named database in the RDS instance in the form "},{type:b,tag:e,props:{},children:[{type:a,value:"{ad-hoc-env-name}-db"}]},{type:a,value:". We can do port forwarding from the GitHub Action script as described "},{type:b,tag:h,props:{href:"https:\u002F\u002Fitnext.io\u002Fsecure-access-to-a-private-network-from-github-actions-47dab60cf37d",rel:[x,y,z],target:A},children:[{type:a,value:ac}]},{type:a,value:dc}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,X]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"ssh -o StrictHostKeyChecking=no -t -t  \\\n  -i .\u002Fgithub_id_rsa \"${SHH_USER}@${BASTION_IP}\" \\\n  -L \"3306:${MYSQL_IP}:3306\" &\n"}]}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This would require that we have a bastion in order to access RDS in a private database. Now our setup is more complex, but the complexity is justified since we now:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"won't incur the costs of lots of RDS instances"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"will start up faster because it doesn't need to create an RDS per environment"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"have slightly less resource isolation (but this is an acceptable trade-off)"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The postgres database is just one part of our application, and already there a few decisions that we have to make when it comes to some of the trade-offs mentioned above."}]},{type:a,value:c},{type:b,tag:v,props:{id:aK},children:[{type:b,tag:h,props:{href:"#more-on-shared-resources-vs-per-environment-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aL}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here's a detailed overview of the resources that are shared and the resources that are specific to each ad hoc environment."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Shared resources include:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:aj}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"IAM policies"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Security groups"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"RDS instance"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Service Discovery namespace"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Application Load Balancer"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Bastion host"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environment resources include:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:ak}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"ECS Tasks and Services (for backend and frontend applications)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"CloudWatch logging groups"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"ALB Target groups"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"ALB listener rules"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Route53 record that point to the load balancer"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:al}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Service Discovery Service"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Shared resources can be defined in one terraform configuration and deployed once. These resources will be long-lived as long as the application is under active development and the team requires on-demand provisioning of ad hoc environments."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environment resources can be defined in another terraform configuration that references outputs from the shared resource configuration using "},{type:b,tag:e,props:{},children:[{type:a,value:L}]},{type:a,value:". Each ad hoc environment can be defined by a "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file that contains the name of the ad hoc environment (such as "},{type:b,tag:e,props:{},children:[{type:a,value:dd}]},{type:a,value:Y},{type:b,tag:e,props:{},children:[{type:a,value:"brian2"}]},{type:a,value:Y},{type:b,tag:e,props:{},children:[{type:a,value:"demo-feature-abc"}]},{type:a,value:", etc.). This "},{type:b,tag:e,props:{},children:[{type:a,value:ao}]},{type:a,value:" value will also be the name of the Terraform workspace and will be used to name and tag AWS resources associated with the corresponding ad hoc environment."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:ap},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file will allow developers to use a simple, standard file interface for defining application specific values, such as the version of the backend and frontend. This brings developers into the concepts and practices of \"infrastructure as code\" and \"configuration as code\" and also helps the entire team keep track of how different environments are configured."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environment "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" files are stored in a directory of a special git repository that also defines the ad hoc environment terraform configuration. Currently, the "},{type:b,tag:e,props:{},children:[{type:a,value:de}]},{type:a,value:" files are stored "},{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Ftree\u002Fmain\u002Fterraform\u002Flive\u002Fdev",rel:[x,y,z],target:A},children:[{type:a,value:ac}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Now let's look at the two terraform configurations used for defining shared resources and ad hoc environment resources."}]},{type:a,value:c},{type:b,tag:v,props:{id:aM},children:[{type:b,tag:h,props:{href:"#diagram",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here's an overview of the resources used for the ad hoc environments. The "},{type:b,tag:ad,props:{},children:[{type:a,value:"letters represent shared resources"}]},{type:a,value:" and the "},{type:b,tag:ad,props:{},children:[{type:a,value:"numbers represent per-environment resources"}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:aq,props:{alt:df,src:ax},children:[]}]},{type:a,value:c},{type:b,tag:o,props:{id:aN},children:[{type:b,tag:h,props:{href:"#shared-architecture",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aO}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"A. VPC (created using the "},{type:b,tag:h,props:{href:dg,rel:[x,y,z],target:A},children:[{type:a,value:"official AWS VPC Module"}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"B. Public subnets for bastion host, NAT Gateways and Load Balancer"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"C. Private subnets for application workloads and RDS"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"D. Application Load Balancer that is shared between all ad hoc environments. A pre-provisioned wildcard ACM certificate is attached to the load balancer that is used to secure traffic for load-balanced ECS services"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"E. Service discovery namespace that provides a namespace for application workloads to access the redis service running in ECS"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"F. IAM roles needed for ECS tasks to access AWS services"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"G. RDS instance using postgres engine that is shared between all ad hoc environments"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"H. Bastion host used to access RDS from GitHub Actions (needed for creating per-environment databases)"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"I. NAT Gateway used to give traffic in private subnets a route to the public internet"}]},{type:a,value:c},{type:b,tag:o,props:{id:aP},children:[{type:b,tag:h,props:{href:"#environment-specific-architecture",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aQ}]},{type:a,value:c},{type:b,tag:ar,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"ECS Cluster that groups all ECS tasks for a single ad hoc environment"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Listener rules and target groups that direct traffic from the load balancer to the ECS services for an ad hoc environment."}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Redis service running in ECS that provides caching and serves as a task broker for celery"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Route53 records that point to the load balancer"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Frontend service that serves the Vue.js application over NGINX"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"API service that serves the backend with Gunicorn"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Celery worker that process jobs in the default queue"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Celery beat that schedules celery tasks"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:dh}]},{type:a,value:di}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"migrate"}]},{type:a,value:di}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"CloudWatch log groups are created for each ECS task in an ad hoc environment"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Each ad hoc environment gets a database in the shared RDS instance"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:aR},children:[{type:b,tag:h,props:{href:"#shared-resources-terraform-configuration",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aS}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Let's have a detailed look at the terraform configuration for shared resources that will support ad hoc environments."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:aq,props:{alt:"Shared AWS resources for ad hoc environment",src:"\u002Fimage\u002Fshared-resources.png"},children:[]}]},{type:a,value:c},{type:b,tag:o,props:{id:aT},children:[{type:b,tag:h,props:{href:"#vpc",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aj}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:dj},{type:b,tag:h,props:{href:dg,rel:[x,y,z],target:A},children:[{type:a,value:"AWS VPC module"}]},{type:a,value:" for creating the shared VPC with Terraform. This module provides a high level interface that will provision lots of the components that are needed for a VPC following best practices, and it is less code for the DevOps team to manage."}]},{type:a,value:c},{type:b,tag:o,props:{id:aU},children:[{type:b,tag:h,props:{href:"#cloud-map-service-discovery-namespace",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aV}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Cloud Map is used in order to allow services in our ECS cluster to communicate with each other. The only reason that Cloud Map is needed is so that the backend services (API, celery workers, beat) can communicate with Redis, which will be an important service for our application, providing caching and also serving as a broker for celery. If we were to use Django Channels for websockets, the Redis service would also function as the backend for Django Channels."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We will only need to specify "},{type:b,tag:e,props:{},children:[{type:a,value:"service_registries"}]},{type:a,value:" on the redis service in our ECS cluster. What this will do is provide an address that our other services can use to communicate with redis. This address is created in the form of a Route 53 record, and it points to the private IP address of the redis service. If the private IP of the redis service is updated, the Route 53 record record for our redis service will be updated as well."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In order for service discovery to work in the VPC that we created, we need to add the following options to the terraform AWS VPC module:"}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,X]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"# DNS settings\nenable_dns_hostnames = true\nenable_dns_support   = true\n"}]}]}]},{type:a,value:c},{type:b,tag:o,props:{id:aW},children:[{type:b,tag:h,props:{href:"#security-groups",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aX}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There are two important security groups that we will set up as part of the shared infrastructure layer to be used by each ad hoc environment: one security group for the load balancer, and one security group where all of our ECS services will run."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The load balancer security group will allow all traffic on port "},{type:b,tag:e,props:{},children:[{type:a,value:"80"}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:"443"}]},{type:a,value:" for "},{type:b,tag:e,props:{},children:[{type:a,value:"HTTP"}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:"HTTPS"}]},{type:a,value:" traffic. The ECS security group will only allow inbound traffic from the application load balancer security group."}]},{type:a,value:c},{type:b,tag:o,props:{id:aY},children:[{type:b,tag:h,props:{href:"#iam-roles",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:aZ}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There are two important IAM roles that we will need for our ECS tasks. We need a task execution role that our ECS tasks will use to interact with other AWS services, such as S3, Secrets Manager, etc."}]},{type:a,value:c},{type:b,tag:o,props:{id:a_},children:[{type:b,tag:h,props:{href:"#rds-instance",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:a$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We will create one RDS instance in one of the private subnets in our VPC. This RDS instance will have one Postgres database per ad hoc environment. This RDS instance has a security group that allows all traffic from our ECS security group."}]},{type:a,value:c},{type:b,tag:o,props:{id:ba},children:[{type:b,tag:h,props:{href:"#load-balancer",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bb}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We will use one load balancer for all ad hoc environments. This load balancer will have a wildcard ACM certificate attached to it ("},{type:b,tag:e,props:{},children:[{type:a,value:"*.dev.example.com"}]},{type:a,value:", for example). Each ad hoc environment will create a Route 53 record that will point to this load balancer's public DNS name. For example, "},{type:b,tag:e,props:{},children:[{type:a,value:"brian.dev.example.com"}]},{type:a,value:" will be the address of my ad hoc environment. Requests to this address will then be routed to either the frontend ECS service or the backend ECS service depending on request header values that will be set on the listener rules."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"By default, a load balancer supports up to 50 listener rules, so we can create plenty of ad hoc environments before we need to increase the default quota. There will be a discussion at the end of this article about AWS service quotas."}]},{type:a,value:c},{type:b,tag:o,props:{id:bc},children:[{type:b,tag:h,props:{href:"#bastion-host",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bd}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The bastion host will be created in one of the VPC's public subnets. This will primarily be used for connecting to RDS to create new databases for new ad hoc environments, or for manually manipulating data in an ad hoc environment for debugging."}]},{type:a,value:c},{type:b,tag:v,props:{id:be},children:[{type:b,tag:h,props:{href:"#environment-specific-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bf}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Now that we have defined a shared set of infrastructure that our ad hoc environments will use, let's have a look at the resources that will be specific to ad hoc environments that will be added on top of the shared resources."}]},{type:a,value:c},{type:b,tag:o,props:{id:bg},children:[{type:b,tag:h,props:{href:"#ecs-cluster",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:ak}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The ECS Cluster is a simple grouping of ECS tasks and services."}]},{type:a,value:c},{type:b,tag:o,props:{id:bh},children:[{type:b,tag:h,props:{href:"#ecs-tasks-and-services",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bi}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Each environment will have a unique collection of ECS tasks and services that will be used to run the application."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There are four important ECS services in our application that are used to run \"long-running\" ECS tasks. Long-running tasks are tasks that start processes that run indefinitely, rather than running until completion. The long-running tasks in our application include:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend web application (gunciron web server)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend celery worker"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"backend celery beat"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"frontend web site"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:dk}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The infrastructure code also defines some tasks that are not long-running but rather short lived tasks that run until completion and do not start again. These services include:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:dh}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"database migrations"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"any other ad-hoc task that we want to run, usually wrapped in a Django management command"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:o,props:{id:bj},children:[{type:b,tag:h,props:{href:"#alb-listener-rules-and-target-groups",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bk}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"These two resources work together to expose certain ECS tasks to the internet. The pattern used to do this involves:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"creating an "},{type:b,tag:e,props:{},children:[{type:a,value:as}]},{type:a,value:" that define a "},{type:b,tag:e,props:{},children:[{type:a,value:"health_check"}]},{type:a,value:", references a VPC and a port"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"defining a "},{type:b,tag:e,props:{},children:[{type:a,value:"load_balancer"}]},{type:a,value:" block on an "},{type:b,tag:e,props:{},children:[{type:a,value:"aws_ecs_service"}]},{type:a,value:" resource that references the "},{type:b,tag:e,props:{},children:[{type:a,value:as}]}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"an "},{type:b,tag:e,props:{},children:[{type:a,value:"aws_lb_listener_rule"}]},{type:a,value:" rule is defined to forward traffic matching certain hostname, path or header values that we define to the "},{type:b,tag:e,props:{},children:[{type:a,value:as}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:o,props:{id:bl},children:[{type:b,tag:h,props:{href:"#route-53-record",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bm}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The Route 53 resource we create for ad hoc environment will define a subdomain that will use for the ad hoc environment, such as "},{type:b,tag:e,props:{},children:[{type:a,value:"brian.example.dev"}]},{type:a,value:" (where "},{type:b,tag:e,props:{},children:[{type:a,value:"example.dev"}]},{type:a,value:" is a domain name used internally by the DevOps team). The load balancer created as part of the shared resources will have a wildcard certificate that will give us secure connections for URLs following the pattern "},{type:b,tag:e,props:{},children:[{type:a,value:"*.example.dev"}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:o,props:{id:bn},children:[{type:b,tag:h,props:{href:"#s3-bucket",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:al}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"An S3 bucket will be provisioned per ad hoc environment. This S3 bucket will store both static files as well as media uploads (images)."}]},{type:a,value:c},{type:b,tag:o,props:{id:bo},children:[{type:b,tag:h,props:{href:"#database-within-rds-instance",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bp}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We will create one database in our RDS instance per ad hoc environment for data isolation."}]},{type:a,value:c},{type:b,tag:o,props:{id:L},children:[{type:b,tag:h,props:{href:"#terraform_remote_state",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:L}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:dl,rel:[x,y,z],target:A},children:[{type:b,tag:e,props:{},children:[{type:a,value:L}]}]},{type:a,value:" is a feature of terraform that will allow us to separate the shared infrastructure terraform configuration ("},{type:b,tag:h,props:{href:dm},children:[{type:a,value:ae}]},{type:a,value:") from the ad hoc environment terraform infrastructure ("},{type:b,tag:h,props:{href:dm},children:[{type:a,value:ae}]},{type:a,value:dn}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Our terraform configuration for ad hoc environments will define variables that will be provided by "},{type:b,tag:e,props:{},children:[{type:a,value:L}]},{type:a,value:" that will reference values from our shared resource terraform configuration. This is what that looks like in Terraform code:"}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,"language-hcl"]},children:[{type:b,tag:e,props:{},children:[{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# terraform_remote_state"}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:ay}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"required_version"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"\u003E=1.1.7\""}]},{type:a,value:at},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:"required_providers"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:au},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:az}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:do0},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:dp}]},{type:a,value:"  "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"hashicorp\u002Faws\""}]},{type:a,value:do0},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"version"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"4.4.0\""}]},{type:a,value:au},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:at},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:dq},{type:b,tag:d,props:{className:[m,af,ag]},children:[{type:a,value:" \"s3\" "}]}]},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:c},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:"provider"},{type:b,tag:d,props:{className:[m,af,ag]},children:[{type:a,value:" \"aws\" "}]}]},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:ah}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.region\n"},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# shared resources"}]},{type:a,value:c},{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# see https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments"}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:"data "},{type:b,tag:d,props:{className:[m,af,ag]},children:[{type:a,value:"\"terraform_remote_state\""}]}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"shared\""}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:dq}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"s3\""}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"config"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:au},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"bucket"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.s3_bucket\n    "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"key"}]},{type:a,value:dr},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.key\n    "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:ah}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:ds},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:c},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# main"}]},{type:a,value:T},{type:b,tag:d,props:{className:[m,U]},children:[{type:a,value:"module"},{type:b,tag:d,props:{className:[m,af,ag]},children:[{type:a,value:" \"main\" "}]}]},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:I}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:dp}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,V]},children:[{type:a,value:"\"..\u002F..\u002Fmodules\u002Fad-hoc\""}]},{type:a,value:at},{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# shared resources -- taken from terraform_remote_state data source above"}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"vpc_id"}]},{type:a,value:"                         "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.vpc_id\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"private_subnets"}]},{type:a,value:"                "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.private_subnets\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"public_subnets"}]},{type:a,value:"                 "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.public_subnets\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"listener_arn"}]},{type:a,value:dt},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.listener_arn\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"alb_dns_name"}]},{type:a,value:dt},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.alb_dns_name\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"service_discovery_namespace_id"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.service_discovery_namespace_id\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"task_role_arn"}]},{type:a,value:"                  "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.task_role_arn\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"execution_role_arn"}]},{type:a,value:du},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.execution_role_arn\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"rds_address"}]},{type:a,value:"                    "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.rds_address\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"alb_default_tg_arn"}]},{type:a,value:du},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.alb_default_tg_arn\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"ecs_sg_id"}]},{type:a,value:"                      "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" data.terraform_remote_state.shared.outputs.ecs_sg_id\n\n  "},{type:b,tag:d,props:{className:[m,S]},children:[{type:a,value:"# per environment settings -- taken from .tfvars files"}]},{type:a,value:J},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"ecr_be_repo_url"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.ecr_be_repo_url\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"ecr_fe_repo_url"}]},{type:a,value:w},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.ecr_fe_repo_url\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:ah}]},{type:a,value:"          "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:ds},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"frontend_url"}]},{type:a,value:dr},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.frontend_url\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"zone_name"}]},{type:a,value:"       "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.zone_name\n  "},{type:b,tag:d,props:{className:[m,s]},children:[{type:a,value:"record_name"}]},{type:a,value:"     "},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:t}]},{type:a,value:" var.record_name\n"},{type:b,tag:d,props:{className:[m,p]},children:[{type:a,value:K}]},{type:a,value:c}]}]}]},{type:a,value:c},{type:b,tag:v,props:{id:bq},children:[{type:b,tag:h,props:{href:"#how-to-setup-an-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:br}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Now that we have been over the resources that will be created to support our ad hoc environments, let's talk about how we can enable individuals on our team to create and update ad hoc environments."}]},{type:a,value:c},{type:b,tag:o,props:{id:bs},children:[{type:b,tag:h,props:{href:"#design-decisions",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bt}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The devops team will decide on the interface that will be used for creating an ad hoc environment. Since we are using Terraform, this interface will be a Terraform configuration. The minimum amount of information that our ad hoc environment configuration needs is image tags for the frontend and backend images to use. Other configurations will be provided by default values set in "},{type:b,tag:e,props:{},children:[{type:a,value:"variables.tf"}]},{type:a,value:", and these defaults can easily be overridden by passing values to "},{type:b,tag:e,props:{},children:[{type:a,value:ab}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:Z}]},{type:a,value:". I'm choosing to use "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" as the way to pass configuration values to our ad hoc environments where "},{type:b,tag:e,props:{},children:[{type:a,value:ao}]},{type:a,value:" is the name of the ad hoc environment being created. This will give us the following benefits:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"all ad hoc environments will be visible to the entire team in git since each ad hoc environment will have a "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file associated with it"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"adding additional customization to an ad hoc environment does not add additional complexity to our automation pipeline since all customization is added through a single file that will be referenced by "},{type:b,tag:e,props:{},children:[{type:a,value:"$WORKSPACE.tfvars"}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The downsides of this approach are:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"creating ad hoc environments requires knowledge of git, so non-technical product team members might need help from the engineering team when setting up an ad hoc environment"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"there is an additional \"manual\" step of creating a "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file that must be done before running a pipeline to create an ad hoc environment"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Provided that a "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file has been created and pushed to the correct branch ("},{type:b,tag:e,props:{},children:[{type:a,value:"main"}]},{type:a,value:", for example) of our \"live\" Terraform repo, creating or updating an ad hoc environment will be as simple as running a pipeline in GitHub Actions that specifies the "},{type:b,tag:e,props:{},children:[{type:a,value:ao}]},{type:a,value:" of our ad hoc environment. If no such "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file exists, our pipeline will fail."}]},{type:a,value:c},{type:b,tag:o,props:{id:bu},children:[{type:b,tag:h,props:{href:"#github-action",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bv}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Creating ad hoc environments will involve manually triggering a GitHub Action that runs on "},{type:b,tag:e,props:{},children:[{type:a,value:dv}]},{type:a,value:dc}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,X]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"on:\n  workflow_dispatch:\n    inputs:\n      workspace:\n        description: 'Name of terraform workspace to use'\n        required: true\n        default: 'dev'\n        type: string\n"}]}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We only have to enter the name of the ad hoc environment we want to create or update. The ad hoc environment name is used as the Terraform workspace name. This name is also the name of the "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file that must be created per environment."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This workflow will do "},{type:b,tag:e,props:{},children:[{type:a,value:an}]},{type:a,value:Y},{type:b,tag:e,props:{},children:[{type:a,value:ab}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:Z}]},{type:a,value:" using the "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file. When everything has been created, we will use the AWS CLI to prepare the environment so that it can be used. We will use the "},{type:b,tag:e,props:{},children:[{type:a,value:dw}]},{type:a,value:" command to run database migrations needed so that the application code can interact with the database."}]},{type:a,value:c},{type:b,tag:v,props:{id:bw},children:[{type:b,tag:h,props:{href:"#how-to-update-code-in-an-existing-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bx}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Assuming that we have deployed an ad hoc environment called "},{type:b,tag:e,props:{},children:[{type:a,value:dd}]},{type:a,value:" with version "},{type:b,tag:e,props:{},children:[{type:a,value:"v1.0.0"}]},{type:a,value:" of the backend application and "},{type:b,tag:e,props:{},children:[{type:a,value:"v2.0.0"}]},{type:a,value:" of the frontend application, let's think about the process of updating the application to "},{type:b,tag:e,props:{},children:[{type:a,value:"v1.1.0"}]},{type:a,value:" of the backend and "},{type:b,tag:e,props:{},children:[{type:a,value:"v2.1.0"}]},{type:a,value:" of the frontend."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The simplest approach to updating the application would be edit the "},{type:b,tag:e,props:{},children:[{type:a,value:dx}]},{type:a,value:" file with the new versions:"}]},{type:a,value:c},{type:b,tag:O,props:{className:[P]},children:[{type:b,tag:Q,props:{className:[R,X]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"# brian.tfvars\nbe_image_tag = \"v1.1.0\"\nfe_image_tag = \"v2.1.0\"\n"}]}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"If we run the same pipeline that we initially used to deploy ad hoc environment (with "},{type:b,tag:e,props:{},children:[{type:a,value:an}]},{type:a,value:Y},{type:b,tag:e,props:{},children:[{type:a,value:ab}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:Z}]},{type:a,value:") against the updated "},{type:b,tag:e,props:{},children:[{type:a,value:dx}]},{type:a,value:" file, this will result in a rolling update of the frontend and backend services ("},{type:b,tag:h,props:{href:"https:\u002F\u002Fdocs.aws.amazon.com\u002FAmazonECS\u002Flatest\u002Fdeveloperguide\u002Fdeployment-type-ecs.html",rel:[x,y,z],target:A},children:[{type:a,value:"more on rolling updates here"}]},{type:a,value:dn}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"If there are database migrations included in the new version of the code that is going out, we will need to run the database migrations after the "},{type:b,tag:e,props:{},children:[{type:a,value:Z}]},{type:a,value:" completes. One of the top level outputs of our ad hov environment terraform configuration includes a complete "},{type:b,tag:e,props:{},children:[{type:a,value:"run-task"}]},{type:a,value:" command that will run our database migrations when called from GitHub Actions (or our local machine)."}]},{type:a,value:c},{type:b,tag:o,props:{id:by},children:[{type:b,tag:h,props:{href:"#order-of-operations",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bz}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"For ad hoc environments, it is probably fine to update the services and then run the database migrations. Ad hoc environments may only have a single \"user\" -- the developer, so "},{type:b,tag:ad,props:{},children:[{type:a,value:"we don't need to worry about any errors that may occur if requests are made against the new version of code before database migrations have been applied"}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Let's consider a simple example to illustrate what can go wrong here. If we add a "},{type:b,tag:e,props:{},children:[{type:a,value:"total_views"}]},{type:a,value:" to our blog post model to track the total number of page views a post has, we would add a field to the model, generate migration file with "},{type:b,tag:e,props:{},children:[{type:a,value:"makemigrations"}]},{type:a,value:", and then update our views and model serializers to make use of this new field. In the time between updating our service and running the database migrations, any requests to endpoints that access the new database field will fail since the table does not yet exist."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"If we first run database migrations "},{type:b,tag:ad,props:{},children:[{type:a,value:"and then"}]},{type:a,value:" update application code (ECS services), then we can avoid errors about fields not existing. In our production application, we want to aim for fewer errors, so we should be using this \"order of operations\": first run new database migrations and then update application code."}]},{type:a,value:c},{type:b,tag:o,props:{id:bA},children:[{type:b,tag:h,props:{href:"#github-action-for-application-updates",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bB}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We need a GitHub Action that can do the following:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"fetch the container definitions JSON for the backend tasks ("},{type:b,tag:e,props:{},children:[{type:a,value:dy}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"write new container definitions JSON with the new backend image tag"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"register new task definitions with the new container definition JSON files for each task ("},{type:b,tag:e,props:{},children:[{type:a,value:dz}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"call run-task with the newly updated migration ECS task ("},{type:b,tag:e,props:{},children:[{type:a,value:dw}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"wait for the task to exit and display the logs"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"update the backend services (gunicorn, celery, celery beat) ("},{type:b,tag:e,props:{},children:[{type:a,value:dA}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:dB},{type:b,tag:e,props:{},children:[{type:a,value:dC}]},{type:a,value:E}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In order have the correct arguments for all of the AWS CLI calls used in the above workflow, we can fetch values from "},{type:b,tag:e,props:{},children:[{type:a,value:"terraform output"}]},{type:a,value:" and save values into environment variables, and then use these value in a later stage of our GitHub Actions workflow."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The process described above is needed for updating the backend application. Updating the frontend application involves a similar process to the backend update:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"fetch the container definition JSON for the frontend tasks ("},{type:b,tag:e,props:{},children:[{type:a,value:dy}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"write new container definition JSON with the new frontend image tag"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"register new task definitions with the new container definition JSON file for the frontend task ("},{type:b,tag:e,props:{},children:[{type:a,value:dz}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"update the frontend service ("},{type:b,tag:e,props:{},children:[{type:a,value:dA}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:dB},{type:b,tag:e,props:{},children:[{type:a,value:dC}]},{type:a,value:E}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:bC},children:[{type:b,tag:h,props:{href:"#how-to-destroy-an-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bD}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Destroying an ad hoc environment is as simple as creating or updating an ad hoc environment. We simply provide the name of the terraform workspace and corresponding "},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" file and another GitHub Action will run "},{type:b,tag:e,props:{},children:[{type:a,value:dD}]},{type:a,value:" with the correct inputs."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:ap},{type:b,tag:e,props:{},children:[{type:a,value:D}]},{type:a,value:" will not be automatically deleted."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"No data from the ad hoc environment will be deleted. The S3 bucket objects will be destroyed, as well as the database for the specific ad hoc environment."}]},{type:a,value:c},{type:b,tag:v,props:{id:bE},children:[{type:b,tag:h,props:{href:"#keeping-track-of-ad-hoc-environments",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bF}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We need to think about how we can keep track of our active ad hoc environments. Active environments will incur additional AWS costs, and we do not want developers or the product team to create lots of environments and then leave them running without actively using them."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We may decide to have some long-lived ad hoc environments, but those would be managed primarily by the DevOps team."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"One way to check the active ad hoc environments would be to use the AWS CLI. We could list the ECS clusters in our development account, and this would show the number of ad hoc environments running. We could go farther and list the ad hoc environments by when they were last updated. We could then request developers or team members to remove ad hoc environments that are not in use."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Or we could have a policy that all ad-hoc environments are deleted automatically at the end of the week."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There could be a lot of ways to track active ad hoc environments, so I won't go into more alternatives here."}]},{type:a,value:c},{type:b,tag:v,props:{id:bG},children:[{type:b,tag:h,props:{href:"#other-considerations",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bH}]},{type:a,value:c},{type:b,tag:r,props:{className:[ai]},children:[{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Creating new databases without using SSH or bastion host"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:bI},children:[{type:b,tag:h,props:{href:"#aws-usage-quotas",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bJ}]},{type:a,value:c},{type:b,tag:r,props:{className:[ai]},children:[{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Make sure that there are no default usage quotas that would be exceeded for some number of concurrent ad hoc environments."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:bK},children:[{type:b,tag:h,props:{href:"#options-for-ad-hoc-environment-settings",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bL}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"At the very minimum, our ad hoc environments need to know what versions of the frontend and backend application to use (the image tag for the frontend and backend images that have been pushed to ECR)."}]},{type:a,value:c},{type:b,tag:r,props:{className:[ai]},children:[{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Public or private - do you want anyone on the internet to be able to access your ad hoc environment, or should the environment only be accessible over a private VPN connection. A "},{type:b,tag:e,props:{},children:[{type:a,value:"public"}]},{type:a,value:" boolean could default to "},{type:b,tag:e,props:{},children:[{type:a,value:"false"}]},{type:a,value:", and you could have the environment only available"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Custom AWS tags"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Custom environment variables"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:bM},children:[{type:b,tag:h,props:{href:"#aws-tagging",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bN}]},{type:a,value:c},{type:b,tag:v,props:{id:bO},children:[{type:b,tag:h,props:{href:"#deploying-the-application-from-your-local-machine",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bP}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Deploying the application from your local machine can have a faster feedback loop while developing. This allows us to iterate on the deployment pipeline before automating it in GitHub Actions or another automation platform."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In order to deploy the application, you will need to do the following:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"set up resources for an S3 backend that can be used by both the shared infrastructure and the ad hoc environments\n"},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"this can be done with the "},{type:b,tag:e,props:{},children:[{type:a,value:dE}]},{type:a,value:" Makefile targets in the "},{type:b,tag:e,props:{},children:[{type:a,value:aa}]},{type:a,value:" repo"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"take note of the S3 bucket name and dynamodb table name"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"store these in a file called "},{type:b,tag:e,props:{},children:[{type:a,value:dF}]},{type:a,value:" that will be used in both the shared infrastructure deployment and the ad hoc environment deployment"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Install the AWS CLI and configure credentials with sufficient permissions (admin)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"build and push the frontend and backend application images to the ECR repositories that were set up with the "},{type:b,tag:e,props:{},children:[{type:a,value:dE}]},{type:a,value:" commands"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Deploy base layer application\n"},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"in the "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" repo, deploy the "},{type:b,tag:e,props:{},children:[{type:a,value:dG}]},{type:a,value:" terraform configuration. This requires two inputs:\n"},{type:b,tag:ar,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"the "},{type:b,tag:e,props:{},children:[{type:a,value:dH}]},{type:a,value:" for the wildcard certificate that will be used on the shared load balancer"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"the AWS Key Pair name that will be used for SSHing into the bastion host"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"The bucket name, region and key will be used"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:bQ},children:[{type:b,tag:h,props:{href:"#spin-everything-up",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bR}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Starting with an empty AWS account, here is how to get ad hoc environments set up."}]},{type:a,value:c},{type:b,tag:o,props:{id:bS},children:[{type:b,tag:h,props:{href:"#s3-backend-resources-and-ecr-repositories",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bT}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We first want to set up an S3 backend that we can use for storing our Terraform state files as well as ECR repositories for storing the images used for the frontend and backend components of our application. There are a few ways to do this:"}]},{type:a,value:c},{type:b,tag:ar,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Set things up manually (not recommended)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Set things up using Terraform (this is what I'm doing currently)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Set things up using a CloudFormation template"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"Use Terraform Cloud"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"I'll be using option 2. This is sometimes referred to as \"bootstrapping\". When using CDK for IaC, you will do something similar that sets up S3 buckets and ECR repositories used to manage the assets used for application deployment."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Have a look at "},{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Fblob\u002Fmain\u002Fterraform\u002Fbootstrap\u002FREADME.md",rel:[x,y,z],target:A},children:[{type:a,value:"this README.md file"}]},{type:a,value:" for more information about bootstrapping a Terraform S3 backend."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:dj},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:" command from the "},{type:b,tag:e,props:{},children:[{type:a,value:aa}]},{type:a,value:" GitHub repo to do this. Before running "},{type:b,tag:e,props:{},children:[{type:a,value:av}]},{type:a,value:", we need to copy the "},{type:b,tag:e,props:{},children:[{type:a,value:"bootstrap.tfvars.template"}]},{type:a,value:" file to "},{type:b,tag:e,props:{},children:[{type:a,value:dI}]},{type:a,value:" and add values for "},{type:b,tag:e,props:{},children:[{type:a,value:ah}]},{type:a,value:M},{type:b,tag:e,props:{},children:[{type:a,value:"backend_name"}]},{type:a,value:". Setting up these resources takes just a few seconds."}]},{type:a,value:c},{type:b,tag:o,props:{id:bU},children:[{type:b,tag:h,props:{href:"#set-up-shared-infrastructure",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bV}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The next step is to setup shared infrastructure components that our ad hoc environments will use. We will use the S3 backend that was setup in the previous step to store the Terraform state for our shared resources. The shared resources is configured with a dedicated Terraform module that I have published to the Terraform Registry ("},{type:b,tag:h,props:{href:dJ,rel:[x,y,z],target:A},children:[{type:a,value:ae}]},{type:a,value:"). The git repo for this Terraform module contains a simple example (located in "},{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments\u002Ftree\u002Fmain\u002Fexamples\u002Fsimple",rel:[x,y,z],target:A},children:[{type:b,tag:e,props:{},children:[{type:a,value:dG}]}]},{type:a,value:") that I use to set up the shared resources."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"We will need to provide a "},{type:b,tag:e,props:{},children:[{type:a,value:dF}]},{type:a,value:" file that will be used to configure the S3 backend where we will store the terraform state for the shared resources."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This terraform configuration will not be updated frequently, so there is no GitHub Actions pipeline that will be used for updating and deploying the shared resources. For larger infrastructure teams, it might make sense to build a GitHub Actions pipeline for introducing changes to the shared resources rather than deploying from a local machine."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The shared resources terraform configuration requires two inputs:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:dH}]},{type:a,value:": the ARN of a pre-provisioned wildcard ACM certificate that will be used to provide secure connections to the shared load balancer over the public internet. (My approach to ad hoc environments assumes that the environments can be accessed over the public internet without a VPN connection, but your requirements may be different)"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"key_name"}]},{type:a,value:": the name of a pre-existing EC2 key pair name that will be used to access the bastion host from GitHub Actions. We need this in order to create databases in our RDS instance per ad hoc environment"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:o,props:{id:bW},children:[{type:b,tag:h,props:{href:"#setting-up-an-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bX}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Now that we have set up shared resources, we can start"}]},{type:a,value:c},{type:b,tag:v,props:{id:bY},children:[{type:b,tag:h,props:{href:"#spin-everything-down",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:bZ}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"To get rid of all of the AWS resources used to support ad hoc environments, we need to delete the following:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"all ad hoc environments"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"shared resources"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:a,value:"S3 bucket and dynamodb table that were used to store terraform state"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:o,props:{id:b_},children:[{type:b,tag:h,props:{href:"#delete-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:b$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This can be done by running the "},{type:b,tag:e,props:{},children:[{type:a,value:dD}]},{type:a,value:" GitHub Actions job with the name of the ad hoc environment to be destroyed."}]},{type:a,value:c},{type:b,tag:o,props:{id:ca},children:[{type:b,tag:h,props:{href:"#delete-shared-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:cb}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Currently this can be done from the command line using the "},{type:b,tag:e,props:{},children:[{type:a,value:dK}]},{type:a,value:" Makefile command in the "},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" repo."}]},{type:a,value:c},{type:b,tag:o,props:{id:cc},children:[{type:b,tag:h,props:{href:"#delete-the-s3-bucket-and-dynamodb-table-that-are-used-to-store-and-lock-state-files",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:cd}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This can be done from the "},{type:b,tag:e,props:{},children:[{type:a,value:aa}]},{type:a,value:" repo using the "},{type:b,tag:e,props:{},children:[{type:a,value:dL}]},{type:a,value:" command."}]},{type:a,value:c},{type:b,tag:v,props:{id:ce},children:[{type:b,tag:h,props:{href:"#state-management--resource-lifecycle",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:cf}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Let's take a look at the way that state is stored and shared between different components of our ad hoc environment setup."}]},{type:a,value:c},{type:b,tag:v,props:{id:cg},children:[{type:b,tag:h,props:{href:"#diagram-1",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here's a diagram that shows both where state is stored as well as the lifecycle of resources used in ad hoc environments."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:aq,props:{alt:df,src:"\u002Fstatic\u002Fadhoc\u002Flifecycle.drawio.png"},children:[]}]},{type:a,value:c},{type:b,tag:v,props:{id:ch},children:[{type:b,tag:h,props:{href:"#high-level-overview-of-lifecycle-a---f",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:ci}]},{type:a,value:c},{type:b,tag:o,props:{id:cj},children:[{type:b,tag:h,props:{href:"#a-bootstrapped-s3-backend-for-terraform-state",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"A"}]},{type:a,value:": bootstrapped S3 backend for terraform state"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"These resource are long lived and they provide the ability to store Terraform state (a JSON object that described what is deployed to our AWS account) as well as Elastic Container Registry repos that store builds of our frontend and backend application."}]},{type:a,value:c},{type:b,tag:o,props:{id:ck},children:[{type:b,tag:h,props:{href:"#b-deploy-shared-resources-for-ad-hoc-environments",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"B"}]},{type:a,value:": deploy shared resources for ad hoc environments"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environments share resources. This helps keeps startup speeds low and also helps to keep costs down."}]},{type:a,value:c},{type:b,tag:o,props:{id:cl},children:[{type:b,tag:h,props:{href:"#c-launch-or-update-ad-hoc-environments",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"C"}]},{type:a,value:": launch (or update) ad hoc environments"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Ad hoc environments can be launched by developers for testing a feature, or setting up an environment for a product demo. The same process used for launching a new environment can be used for updating an existing environment. Application versions can be updated through updating Terraform, or through a GitHub Action that makes a series of AWS CLI calls to make rolling updates to ECS services. This is triggered by a manual GitHub Action."}]},{type:a,value:c},{type:b,tag:o,props:{id:cm},children:[{type:b,tag:h,props:{href:"#d-destroy-an-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"D"}]},{type:a,value:": destroy an ad hoc environment"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Once an ad hoc environment is no longer needed, it can be destroyed by running another GitHub Action with the name of the ad hoc environment to destroy as the only input."}]},{type:a,value:c},{type:b,tag:o,props:{id:cn},children:[{type:b,tag:h,props:{href:"#e-destroy-shared-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"E"}]},{type:a,value:": destroy shared resources"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"If ad hoc environments are no longer needed altogether, then the shared resources can be deleted as they incur costs."}]},{type:a,value:c},{type:b,tag:o,props:{id:co},children:[{type:b,tag:h,props:{href:"#f-destroy-bootstrap-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:"F"}]},{type:a,value:": destroy bootstrap resources"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Once all ad hoc environments and shared resources are removed, the S3 backend resource can also be removed."}]},{type:a,value:c},{type:b,tag:v,props:{id:cp},children:[{type:b,tag:h,props:{href:"#ad-hoc-infrastructure-component-and-process-details-1---25",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:cq}]},{type:a,value:c},{type:b,tag:o,props:{id:cr},children:[{type:b,tag:h,props:{href:"#1-make-tf-bootstrap",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:av}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This command does a "},{type:b,tag:e,props:{},children:[{type:a,value:dM}]},{type:a,value:" to create the S3 backend resources needed for storing state files for both shared infrastructure and per-environment resources (using Terraform workspaces)."}]},{type:a,value:c},{type:b,tag:o,props:{id:cs},children:[{type:b,tag:h,props:{href:"#1-s3-backend-terraform-configuration",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": S3 backend Terraform configuration"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Ftree\u002Fmain\u002Fterraform\u002Fbootstrap",rel:[x,y,z],target:A},children:[{type:a,value:dN}]},{type:a,value:" to this Terraform configuration."}]},{type:a,value:c},{type:b,tag:o,props:{id:ct},children:[{type:b,tag:h,props:{href:"#1-s3-bucket-for-s3-backend",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": S3 bucket for S3 backend"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The S3 bucket that is used to store Terraform state files. The bucket name is provided in "},{type:b,tag:e,props:{},children:[{type:a,value:dI}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:o,props:{id:cu},children:[{type:b,tag:h,props:{href:"#1-dynamodb-lock-table",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": DynamoDB lock table"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"More information about state locking can be found "},{type:b,tag:h,props:{href:"https:\u002F\u002Fwww.terraform.io\u002Flanguage\u002Fsettings\u002Fbackends\u002Fs3#dynamodb-table-permissions",rel:[x,y,z],target:A},children:[{type:a,value:ac}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:o,props:{id:cv},children:[{type:b,tag:h,props:{href:"#1-ecr-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": ECR resources"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The bootstrap terraform configuration includes Elastic Container Registry repos used for storing frontend and backend application container images that will be used in the ECS clusters for each ad hoc environment."}]},{type:a,value:c},{type:b,tag:o,props:{id:cw},children:[{type:b,tag:h,props:{href:"#1-s3-backend-outputs",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": S3 backend outputs"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The S3 backend Terraform configuration has Terraform outputs that can be used"}]},{type:a,value:c},{type:b,tag:o,props:{id:cx},children:[{type:b,tag:h,props:{href:"#1-terraform-state-file-for-s3-backend",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Terraform state file for S3 backend"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The state file that stores the state for our AWS resources that store state for our ad hoc environments is stored on my local machine. This is not ideal, but the S3 backend resource are fairly simple and will not be changed as we use and develop our ad hoc environment infrastructure setup. Also, these resources can easily be delete manually from the AWS console."}]},{type:a,value:c},{type:b,tag:o,props:{id:cy},children:[{type:b,tag:h,props:{href:"#1-make-examples-simple-for-setting-up-shared-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dO}]},{type:a,value:" for setting up shared resources"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:dO}]},{type:a,value:" is a Makefile command from the "},{type:b,tag:e,props:{},children:[{type:a,value:dP}]},{type:a,value:" repo. Like "},{type:b,tag:e,props:{},children:[{type:a,value:"tf-bootstrap"}]},{type:a,value:", it also wraps "},{type:b,tag:e,props:{},children:[{type:a,value:dM}]},{type:a,value:" commands, but for another Terraform configuration."}]},{type:a,value:c},{type:b,tag:o,props:{id:cz},children:[{type:b,tag:h,props:{href:"#1-terraform-aws-ad-hoc-environments-terraform-module-on-terraform-registry",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" Terraform module on Terraform Registry"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:ap},{type:b,tag:e,props:{},children:[{type:a,value:W}]},{type:a,value:" module has been published to the Terraform Registry and can be viewed "},{type:b,tag:h,props:{href:dJ,rel:[x,y,z],target:A},children:[{type:a,value:ac}]},{type:a,value:N}]},{type:a,value:c},{type:b,tag:o,props:{id:cA},children:[{type:b,tag:h,props:{href:"#1-vpc-module",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": VPC module"}]},{type:a,value:c},{type:b,tag:o,props:{id:cB},children:[{type:b,tag:h,props:{href:"#1-aws-cloudmap",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": AWS CloudMap"}]},{type:a,value:c},{type:b,tag:o,props:{id:cC},children:[{type:b,tag:h,props:{href:"#1-ec2-instance-bastion-host",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": EC2 instance (bastion host)"}]},{type:a,value:c},{type:b,tag:o,props:{id:cD},children:[{type:b,tag:h,props:{href:"#1-rds-instance",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": RDS instance"}]},{type:a,value:c},{type:b,tag:o,props:{id:cE},children:[{type:b,tag:h,props:{href:"#1-application-load-balancer",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Application Load Balancer"}]},{type:a,value:c},{type:b,tag:o,props:{id:cF},children:[{type:b,tag:h,props:{href:"#1-iam-resources",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": IAM resources"}]},{type:a,value:c},{type:b,tag:o,props:{id:cG},children:[{type:b,tag:h,props:{href:"#1-ecs-roles",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": ECS roles"}]},{type:a,value:c},{type:b,tag:o,props:{id:cH},children:[{type:b,tag:h,props:{href:"#1-shared-resources-state-file",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Shared resources state file"}]},{type:a,value:c},{type:b,tag:o,props:{id:cI},children:[{type:b,tag:h,props:{href:"#1-workflow_dispatch",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dv}]}]},{type:a,value:c},{type:b,tag:o,props:{id:cJ},children:[{type:b,tag:h,props:{href:"#1-create_update_ad_hoc_env-github-action",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dQ}]},{type:a,value:" GitHub Action"}]},{type:a,value:c},{type:b,tag:o,props:{id:cK},children:[{type:b,tag:h,props:{href:"#1-ad-hoc-environment-name",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Ad hoc environment name"}]},{type:a,value:c},{type:b,tag:o,props:{id:cL},children:[{type:b,tag:h,props:{href:"#1-terraform-aws-django-terraform-module",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:" Terraform module"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fbriancaffey\u002Fdjango\u002Faws\u002Flatest",rel:[x,y,z],target:A},children:[{type:a,value:dN}]},{type:a,value:" to the "},{type:b,tag:e,props:{},children:[{type:a,value:am}]},{type:a,value:" module on the Terraform Registry."}]},{type:a,value:c},{type:b,tag:o,props:{id:cM},children:[{type:b,tag:h,props:{href:"#1-ecs-cluster",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": ECS cluster"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"One ECS cluster is created for each ad hoc environment. The ECS cluster resources is is a grouping of ECS services and tasks."}]},{type:a,value:c},{type:b,tag:o,props:{id:cN},children:[{type:b,tag:h,props:{href:"#1-ecs-services",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": ECS services"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The application uses the following ECS services for ad hoc environments:"}]},{type:a,value:c},{type:b,tag:r,props:{},children:[{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"gunicorn"}]},{type:a,value:" - web server process for the API"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"celery"}]},{type:a,value:" - web worker that processes tasks"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"celery beat"}]},{type:a,value:" - job scheduler that creates tasks for celery workers to process"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"nginx"}]},{type:a,value:" - for serving the frontend application"}]},{type:a,value:c},{type:b,tag:g,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:dk}]},{type:a,value:" - a stateful service that is used for application data caching and brokering tasks"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:o,props:{id:cO},children:[{type:b,tag:h,props:{href:"#1-s3-bucket-for-ad-hoc-environment",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": S3 bucket for ad hoc environment"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Each ad hoc environment will have a dedicated S3 bucket for storing static and media files. In the context of the example application we are running, these files are all image files"}]},{type:a,value:c},{type:b,tag:o,props:{id:cP},children:[{type:b,tag:h,props:{href:"#1-alb-listener-rules",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": ALB Listener Rules"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"There is a single load balancer shared across all ad hoc environments. Each ad hoc environments has associated listener rules that route requests based on the path to the correct application"}]},{type:a,value:c},{type:b,tag:o,props:{id:cQ},children:[{type:b,tag:h,props:{href:"#1-postgres-database-in-rds-instance",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Postgres database in RDS instance"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Each ad hoc environment uses a dedicated postgres database that lives in an RDS instance that is shared by all ad hoc environments."}]},{type:a,value:c},{type:b,tag:o,props:{id:cR},children:[{type:b,tag:h,props:{href:"#1-redis",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Redis"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Redis is a key-value database that is used for application data caching and brokering tasks. Running redis servers as an ECS service is an alternative to using a managed ElastiCache instance per environment."}]},{type:a,value:c},{type:b,tag:o,props:{id:cS},children:[{type:b,tag:h,props:{href:"#1-service-discovery-service-cloudmap",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:": Service Discovery Service (CloudMap)"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In order for our backend application containers to communicate with the Redis ECS service, we need to set up a Service Discovery service for Redis. This will maintain an internal Route 53 record that points to the private IP of the Redis ECS service."}]},{type:a,value:c},{type:b,tag:o,props:{id:cT},children:[{type:b,tag:h,props:{href:"#1-terraform-remote-state",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dR}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:dl,rel:[x,y,z],target:A},children:[{type:b,tag:e,props:{},children:[{type:a,value:dR}]}]},{type:a,value:" is used to share state between two Terraform configurations. For example, the endpoint of the RDS instance needs to shared from the shared resources Terraform Configuration to the ad hoc environment configuration."}]},{type:a,value:c},{type:b,tag:o,props:{id:cU},children:[{type:b,tag:h,props:{href:"#1-destroy_ad_hoc_env",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dS}]}]},{type:a,value:c},{type:b,tag:o,props:{id:cV},children:[{type:b,tag:h,props:{href:"#1-make-examples-simple-destroy",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dK}]}]},{type:a,value:c},{type:b,tag:o,props:{id:cW},children:[{type:b,tag:h,props:{href:"#1-make-tf-bootstrap-destroy",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dL}]}]},{type:a,value:c},{type:b,tag:o,props:{id:cX},children:[{type:b,tag:h,props:{href:"#1-briancaffeydjango-step-by-step-github-repo",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:"briancaffey\u002Fdjango-step-by-step"}]},{type:a,value:aw}]},{type:a,value:c},{type:b,tag:o,props:{id:cY},children:[{type:b,tag:h,props:{href:"#1-briancaffeyterraform-aws-django-github-repo",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:"briancaffey\u002Fterraform-aws-django"}]},{type:a,value:aw}]},{type:a,value:c},{type:b,tag:o,props:{id:cZ},children:[{type:b,tag:h,props:{href:"#1-briancaffeyterraform-aws-ad-hoc-environments-github-repo",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:b,tag:e,props:{},children:[{type:a,value:q}]},{type:a,value:C},{type:b,tag:e,props:{},children:[{type:a,value:dP}]},{type:a,value:aw}]},{type:a,value:c},{type:b,tag:o,props:{id:c_},children:[{type:b,tag:h,props:{href:"#google-drive-link-to-this-drawio-diagram",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:c$}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:b,tag:h,props:{href:"https:\u002F\u002Fdrive.google.com\u002Ffile\u002Fd\u002F1Te427LEPSlGinEfncH39gArHxwB1cMp0\u002Fview?usp=sharing",rel:[x,y,z],target:A},children:[{type:a,value:"Google Drive link to this diagram"}]},{type:a,value:". It is view-only, but you can duplicate it and edit the copy."}]},{type:a,value:c},{type:b,tag:v,props:{id:da},children:[{type:b,tag:h,props:{href:"#todos",ariaHidden:i,tabIndex:j},children:[{type:b,tag:d,props:{className:[k,l]},children:[]}]},{type:a,value:db}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"A list of things that need to be fixed or added to this article"}]},{type:a,value:c},{type:b,tag:r,props:{className:[ai]},children:[{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{checked:B,disabled:B,type:H},children:[]},{type:a,value:dT},{type:b,tag:e,props:{},children:[{type:a,value:dQ}]}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{checked:B,disabled:B,type:H},children:[]},{type:a,value:dT},{type:b,tag:e,props:{},children:[{type:a,value:dS}]}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Build Route 53 resources based on ad hoc environment name (e.g. "},{type:b,tag:e,props:{},children:[{type:a,value:"my-env.dev.example.com"}]},{type:a,value:E}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Mention CloudFormation template as an option for managing S3 backend resources "},{type:b,tag:h,props:{href:"https:\u002F\u002Fwww.bti360.com\u002Fmng-terraform-state-cloudformation\u002F",rel:[x,y,z],target:A},children:[{type:a,value:ae}]}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Use Fargate Spot launch type instead of regular Fargate in order to lower costs for ad hoc environments"}]},{type:a,value:c},{type:b,tag:g,props:{className:[F]},children:[{type:b,tag:G,props:{disabled:B,type:H},children:[]},{type:a,value:" Add a dedicated directory for storing "},{type:b,tag:e,props:{},children:[{type:a,value:de}]},{type:a,value:" files"}]},{type:a,value:c}]}]},dir:"\u002F2022\u002F03\u002F27",path:"\u002F2022\u002F03\u002F27\u002Fad-hod-developer-environments-for-django-with-aws-ecs-terraform-and-github-actions",extension:".md",createdAt:dU,updatedAt:dU,raw:"\n## tl;dr\n\nThis article shows how software development teams can spin up ad hoc environments for testing and demoing purposes.\n\n## GitHub Links\n\nThis article has three corresponding code repositories on GitHub that will be referenced in this article:\n\n- [django-step-by-step](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step)\n  - this repo contains an example microblogging application called Î¼blog\n  - includes complete GitHub Action examples for automating creation, updating and removal of ad hoc environments\n\n- [terraform-aws-django](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-django)\n  - a Terraform module that has been published to Terraform Registry\n  - includes examples of how to use the module\n\n- [terraform-aws-ad-hoc-environments](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments)\n  - this repo is also published to Terraform Registry\n  - it provides the shared infrastructure for setting up our ad hoc environments\n\n## Assumptions\n\nThere are all sorts of applications, and all sort of engineering teams. For some context of what I'm describing in this article, here are some basic assumptions that I'm making about a fictional application and developed by a fictional engineering team:\n\n- engineering department has a backend team, a frontend team, a devops team and works closely with a product team\n- backend team primarily develops an API\n- frontend team develops an JavaScript SPA (frontend website)\n- SPA consumes backend API\n- product team frequently needs to demo applications to prospective clients\n- development teams don't have deep expertise in infrastructure, containers, CI\u002FCD or automation\n- devops team has some knowledge of the application and is responsible for building automation that will allow anyone on the team to quickly spin up a complete environment for either testing or demoing purposes\n\nHere are assumptions about specific tools and technologies used at the company:\n\n- backend is a REST API developed with Django and a Postgres database\n- backend is packaged into a docker container\n- frontend is also packaged into a docker container using multi-stage builds and NGINX\n- frontend does not require any build-time configuration (all configuration needed by frontend is fetched from backend)\n- backend application's configuration is driven by plain-text environment variables at run-time\n- automation pipeline exists for building, tagging and pushing backend and frontend containers to an ECR repository\n- engineering team uses AWS\n- devops team uses AWS ECS for running containerized workloads\n- devops team uses Terraform for provisioning infrastructure\n- devops team uses GitHub Actions for building automation pipelines\n- team is somewhat cost-conscious\n\n## What are ad hoc environments?\n\nAd hoc environments are short-lived environments that are designed to be used for testing a specific set of features or for demoing a specific application configuration in an isolated environment.\n\n## eli5\n\nA developer named Brian says: I want to run version 1.2.3 of the backend application and version 4.5.6 of the frontend application. That developer selects these two version values as inputs in a GitHub Action or  Jenkins pipeline or Slack chatbot and then a few moments later gets a message saying:\n\n    Hey Brian, your environment is ready!\n\n    visit: https:\u002F\u002Fbrian.dev.company.com to access the environment\n\n    this environment will self-destruct in 1 day!\n\n    thank you!\n\n## Trade-offs to make when designing ad hoc environment infrastructure and automation\n\nWhen building infrastructure for ad hoc environments, there are a few things to solve for:\n\n- startup speed\n- shared vs isolated resources\n- costs\n- automation complexity\n- degree of similarity to production environments\n\nStartup speed can be measured by the time between when an environment is requested and when that environment can be used. In this period of time, an automation pipeline may do the following:\n\n- run `terraform init`, `terraform plan` and `terraform apply`\n- run scripts to prepare the application such as database migrations\n- seeding initial sample data with a script or database dump\n- message the user with information about the environment (URLs, commands for accessing a bastion host or interactive shell, etc.)\n\nSome things in AWS take more time to provision than others. RDS and ElastiCache instances can take a long time to create relative to S3 buckets or IAM roles. RDS and ElastiCache instances are also more costly than other resources. We could use a single, shared RDS instance placed in a private subnet of a shared VPC. Each ad hoc environment could use a different named database in the RDS instance in the form `{ad-hoc-env-name}-db`. We can do port forwarding from the GitHub Action script as described [here](https:\u002F\u002Fitnext.io\u002Fsecure-access-to-a-private-network-from-github-actions-47dab60cf37d):\n\n    ssh -o StrictHostKeyChecking=no -t -t  \\\n      -i .\u002Fgithub_id_rsa \"${SHH_USER}@${BASTION_IP}\" \\\n      -L \"3306:${MYSQL_IP}:3306\" &\n\nThis would require that we have a bastion in order to access RDS in a private database. Now our setup is more complex, but the complexity is justified since we now:\n\n- won't incur the costs of lots of RDS instances\n- will start up faster because it doesn't need to create an RDS per environment\n- have slightly less resource isolation (but this is an acceptable trade-off)\n\nThe postgres database is just one part of our application, and already there a few decisions that we have to make when it comes to some of the trade-offs mentioned above.\n\n## More on shared resources vs per-environment resources\n\nHere's a detailed overview of the resources that are shared and the resources that are specific to each ad hoc environment.\n\nShared resources include:\n\n- VPC\n- IAM policies\n- Security groups\n- RDS instance\n- Service Discovery namespace\n- Application Load Balancer\n- Bastion host\n\nAd hoc environment resources include:\n\n- ECS Cluster\n- ECS Tasks and Services (for backend and frontend applications)\n- CloudWatch logging groups\n- ALB Target groups\n- ALB listener rules\n- Route53 record that point to the load balancer\n- S3 bucket\n- Service Discovery Service\n\nShared resources can be defined in one terraform configuration and deployed once. These resources will be long-lived as long as the application is under active development and the team requires on-demand provisioning of ad hoc environments.\n\nAd hoc environment resources can be defined in another terraform configuration that references outputs from the shared resource configuration using `terraform_remote_state`. Each ad hoc environment can be defined by a `\u003Cname\u003E.tfvars` file that contains the name of the ad hoc environment (such as `brian`, `brian2`, `demo-feature-abc`, etc.). This `\u003Cname\u003E` value will also be the name of the Terraform workspace and will be used to name and tag AWS resources associated with the corresponding ad hoc environment.\n\nThe `\u003Cname\u003E.tfvars` file will allow developers to use a simple, standard file interface for defining application specific values, such as the version of the backend and frontend. This brings developers into the concepts and practices of \"infrastructure as code\" and \"configuration as code\" and also helps the entire team keep track of how different environments are configured.\n\nAd hoc environment `\u003Cname\u003E.tfvars` files are stored in a directory of a special git repository that also defines the ad hoc environment terraform configuration. Currently, the `tfvars` files are stored [here](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Ftree\u002Fmain\u002Fterraform\u002Flive\u002Fdev).\n\nNow let's look at the two terraform configurations used for defining shared resources and ad hoc environment resources.\n\n## Diagram\n\nHere's an overview of the resources used for the ad hoc environments. The **letters represent shared resources** and the **numbers represent per-environment resources**.\n\n![png](\u002Fstatic\u002Fadhoc.png)\n\n### Shared architecture\n\nA. VPC (created using the [official AWS VPC Module](https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fterraform-aws-modules\u002Fvpc\u002Faws\u002Flatest))\n\nB. Public subnets for bastion host, NAT Gateways and Load Balancer\n\nC. Private subnets for application workloads and RDS\n\nD. Application Load Balancer that is shared between all ad hoc environments. A pre-provisioned wildcard ACM certificate is attached to the load balancer that is used to secure traffic for load-balanced ECS services\n\nE. Service discovery namespace that provides a namespace for application workloads to access the redis service running in ECS\n\nF. IAM roles needed for ECS tasks to access AWS services\n\nG. RDS instance using postgres engine that is shared between all ad hoc environments\n\nH. Bastion host used to access RDS from GitHub Actions (needed for creating per-environment databases)\n\nI. NAT Gateway used to give traffic in private subnets a route to the public internet\n\n### Environment-specific architecture\n\n1. ECS Cluster that groups all ECS tasks for a single ad hoc environment\n2. Listener rules and target groups that direct traffic from the load balancer to the ECS services for an ad hoc environment.\n3. Redis service running in ECS that provides caching and serves as a task broker for celery\n4. Route53 records that point to the load balancer\n5. Frontend service that serves the Vue.js application over NGINX\n6. API service that serves the backend with Gunicorn\n7. Celery worker that process jobs in the default queue\n8. Celery beat that schedules celery tasks\n9. `collectstatic` task\n10. `migrate` task\n11. CloudWatch log groups are created for each ECS task in an ad hoc environment\n12. Each ad hoc environment gets a database in the shared RDS instance\n\n\n## Shared resources terraform configuration\n\nLet's have a detailed look at the terraform configuration for shared resources that will support ad hoc environments.\n\n![Shared AWS resources for ad hoc environment](\u002Fimage\u002Fshared-resources.png)\n\n### VPC\n\nWe can use the [AWS VPC module](https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fterraform-aws-modules\u002Fvpc\u002Faws\u002Flatest) for creating the shared VPC with Terraform. This module provides a high level interface that will provision lots of the components that are needed for a VPC following best practices, and it is less code for the DevOps team to manage.\n\n### Cloud Map Service Discovery Namespace\n\nCloud Map is used in order to allow services in our ECS cluster to communicate with each other. The only reason that Cloud Map is needed is so that the backend services (API, celery workers, beat) can communicate with Redis, which will be an important service for our application, providing caching and also serving as a broker for celery. If we were to use Django Channels for websockets, the Redis service would also function as the backend for Django Channels.\n\nWe will only need to specify `service_registries` on the redis service in our ECS cluster. What this will do is provide an address that our other services can use to communicate with redis. This address is created in the form of a Route 53 record, and it points to the private IP address of the redis service. If the private IP of the redis service is updated, the Route 53 record record for our redis service will be updated as well.\n\nIn order for service discovery to work in the VPC that we created, we need to add the following options to the terraform AWS VPC module:\n\n\n    # DNS settings\n    enable_dns_hostnames = true\n    enable_dns_support   = true\n\n### Security Groups\n\nThere are two important security groups that we will set up as part of the shared infrastructure layer to be used by each ad hoc environment: one security group for the load balancer, and one security group where all of our ECS services will run.\n\nThe load balancer security group will allow all traffic on port `80` and `443` for `HTTP` and `HTTPS` traffic. The ECS security group will only allow inbound traffic from the application load balancer security group.\n\n### IAM Roles\n\nThere are two important IAM roles that we will need for our ECS tasks. We need a task execution role that our ECS tasks will use to interact with other AWS services, such as S3, Secrets Manager, etc.\n\n### RDS Instance\n\nWe will create one RDS instance in one of the private subnets in our VPC. This RDS instance will have one Postgres database per ad hoc environment. This RDS instance has a security group that allows all traffic from our ECS security group.\n\n### Load Balancer\n\nWe will use one load balancer for all ad hoc environments. This load balancer will have a wildcard ACM certificate attached to it (`*.dev.example.com`, for example). Each ad hoc environment will create a Route 53 record that will point to this load balancer's public DNS name. For example, `brian.dev.example.com` will be the address of my ad hoc environment. Requests to this address will then be routed to either the frontend ECS service or the backend ECS service depending on request header values that will be set on the listener rules.\n\nBy default, a load balancer supports up to 50 listener rules, so we can create plenty of ad hoc environments before we need to increase the default quota. There will be a discussion at the end of this article about AWS service quotas.\n\n### Bastion Host\n\nThe bastion host will be created in one of the VPC's public subnets. This will primarily be used for connecting to RDS to create new databases for new ad hoc environments, or for manually manipulating data in an ad hoc environment for debugging.\n\n## Environment-specific resources\n\nNow that we have defined a shared set of infrastructure that our ad hoc environments will use, let's have a look at the resources that will be specific to ad hoc environments that will be added on top of the shared resources.\n\n### ECS Cluster\n\nThe ECS Cluster is a simple grouping of ECS tasks and services.\n\n### ECS Tasks and Services\n\nEach environment will have a unique collection of ECS tasks and services that will be used to run the application.\n\nThere are four important ECS services in our application that are used to run \"long-running\" ECS tasks. Long-running tasks are tasks that start processes that run indefinitely, rather than running until completion. The long-running tasks in our application include:\n\n- backend web application (gunciron web server)\n- backend celery worker\n- backend celery beat\n- frontend web site\n- redis\n\nThe infrastructure code also defines some tasks that are not long-running but rather short lived tasks that run until completion and do not start again. These services include:\n\n- collectstatic\n- database migrations\n- any other ad-hoc task that we want to run, usually wrapped in a Django management command\n\n### ALB Listener Rules and Target Groups\n\nThese two resources work together to expose certain ECS tasks to the internet. The pattern used to do this involves:\n\n- creating an `aws_lb_target_group` that define a `health_check`, references a VPC and a port\n- defining a `load_balancer` block on an `aws_ecs_service` resource that references the `aws_lb_target_group`\n- an `aws_lb_listener_rule` rule is defined to forward traffic matching certain hostname, path or header values that we define to the `aws_lb_target_group`\n\n### Route 53 record\n\nThe Route 53 resource we create for ad hoc environment will define a subdomain that will use for the ad hoc environment, such as `brian.example.dev` (where `example.dev` is a domain name used internally by the DevOps team). The load balancer created as part of the shared resources will have a wildcard certificate that will give us secure connections for URLs following the pattern `*.example.dev`.\n\n### S3 bucket\n\nAn S3 bucket will be provisioned per ad hoc environment. This S3 bucket will store both static files as well as media uploads (images).\n\n### Database within RDS instance\n\nWe will create one database in our RDS instance per ad hoc environment for data isolation.\n\n### terraform_remote_state\n\n[`terraform_remote_state`](https:\u002F\u002Fwww.terraform.io\u002Flanguage\u002Fstate\u002Fremote-state-data) is a feature of terraform that will allow us to separate the shared infrastructure terraform configuration ([link]()) from the ad hoc environment terraform infrastructure ([link]()).\n\nOur terraform configuration for ad hoc environments will define variables that will be provided by `terraform_remote_state` that will reference values from our shared resource terraform configuration. This is what that looks like in Terraform code:\n\n```hcl\n# terraform_remote_state\n\nterraform {\n  required_version = \"\u003E=1.1.7\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp\u002Faws\"\n      version = \"4.4.0\"\n    }\n  }\n\n  backend \"s3\" {}\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n\n# shared resources\n# see https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments\n\ndata \"terraform_remote_state\" \"shared\" {\n  backend = \"s3\"\n  config = {\n    bucket = var.s3_bucket\n    key    = var.key\n    region = var.region\n  }\n}\n\n# main\n\nmodule \"main\" {\n  source = \"..\u002F..\u002Fmodules\u002Fad-hoc\"\n\n  # shared resources -- taken from terraform_remote_state data source above\n  vpc_id                         = data.terraform_remote_state.shared.outputs.vpc_id\n  private_subnets                = data.terraform_remote_state.shared.outputs.private_subnets\n  public_subnets                 = data.terraform_remote_state.shared.outputs.public_subnets\n  listener_arn                   = data.terraform_remote_state.shared.outputs.listener_arn\n  alb_dns_name                   = data.terraform_remote_state.shared.outputs.alb_dns_name\n  service_discovery_namespace_id = data.terraform_remote_state.shared.outputs.service_discovery_namespace_id\n  task_role_arn                  = data.terraform_remote_state.shared.outputs.task_role_arn\n  execution_role_arn             = data.terraform_remote_state.shared.outputs.execution_role_arn\n  rds_address                    = data.terraform_remote_state.shared.outputs.rds_address\n  alb_default_tg_arn             = data.terraform_remote_state.shared.outputs.alb_default_tg_arn\n  ecs_sg_id                      = data.terraform_remote_state.shared.outputs.ecs_sg_id\n\n  # per environment settings -- taken from .tfvars files\n  ecr_be_repo_url = var.ecr_be_repo_url\n  ecr_fe_repo_url = var.ecr_fe_repo_url\n  region          = var.region\n  frontend_url    = var.frontend_url\n  zone_name       = var.zone_name\n  record_name     = var.record_name\n}\n```\n\n## How to setup an ad hoc environment\n\nNow that we have been over the resources that will be created to support our ad hoc environments, let's talk about how we can enable individuals on our team to create and update ad hoc environments.\n\n### Design decisions\n\nThe devops team will decide on the interface that will be used for creating an ad hoc environment. Since we are using Terraform, this interface will be a Terraform configuration. The minimum amount of information that our ad hoc environment configuration needs is image tags for the frontend and backend images to use. Other configurations will be provided by default values set in `variables.tf`, and these defaults can easily be overridden by passing values to `terraform plan` and `terraform apply`. I'm choosing to use `\u003Cname\u003E.tfvars` as the way to pass configuration values to our ad hoc environments where `\u003Cname\u003E` is the name of the ad hoc environment being created. This will give us the following benefits:\n\n- all ad hoc environments will be visible to the entire team in git since each ad hoc environment will have a `\u003Cname\u003E.tfvars` file associated with it\n- adding additional customization to an ad hoc environment does not add additional complexity to our automation pipeline since all customization is added through a single file that will be referenced by `$WORKSPACE.tfvars`\n\nThe downsides of this approach are:\n\n- creating ad hoc environments requires knowledge of git, so non-technical product team members might need help from the engineering team when setting up an ad hoc environment\n- there is an additional \"manual\" step of creating a `\u003Cname\u003E.tfvars` file that must be done before running a pipeline to create an ad hoc environment\n\nProvided that a `\u003Cname\u003E.tfvars` file has been created and pushed to the correct branch (`main`, for example) of our \"live\" Terraform repo, creating or updating an ad hoc environment will be as simple as running a pipeline in GitHub Actions that specifies the `\u003Cname\u003E` of our ad hoc environment. If no such `\u003Cname\u003E.tfvars` file exists, our pipeline will fail.\n\n### GitHub Action\n\nCreating ad hoc environments will involve manually triggering a GitHub Action that runs on `workflow_dispatch`:\n\n    on:\n      workflow_dispatch:\n        inputs:\n          workspace:\n            description: 'Name of terraform workspace to use'\n            required: true\n            default: 'dev'\n            type: string\n\nWe only have to enter the name of the ad hoc environment we want to create or update. The ad hoc environment name is used as the Terraform workspace name. This name is also the name of the `\u003Cname\u003E.tfvars` file that must be created per environment.\n\nThis workflow will do `terraform init`, `terraform plan` and `terraform apply` using the `\u003Cname\u003E.tfvars` file. When everything has been created, we will use the AWS CLI to prepare the environment so that it can be used. We will use the `aws ecs run-task` command to run database migrations needed so that the application code can interact with the database.\n\n## How to update code in an existing ad hoc environment\n\nAssuming that we have deployed an ad hoc environment called `brian` with version `v1.0.0` of the backend application and `v2.0.0` of the frontend application, let's think about the process of updating the application to `v1.1.0` of the backend and `v2.1.0` of the frontend.\n\nThe simplest approach to updating the application would be edit the `brian.tfvars` file with the new versions:\n\n    # brian.tfvars\n    be_image_tag = \"v1.1.0\"\n    fe_image_tag = \"v2.1.0\"\n\nIf we run the same pipeline that we initially used to deploy ad hoc environment (with `terraform init`, `terraform plan` and `terraform apply`) against the updated `brian.tfvars` file, this will result in a rolling update of the frontend and backend services ([more on rolling updates here](https:\u002F\u002Fdocs.aws.amazon.com\u002FAmazonECS\u002Flatest\u002Fdeveloperguide\u002Fdeployment-type-ecs.html)).\n\nIf there are database migrations included in the new version of the code that is going out, we will need to run the database migrations after the `terraform apply` completes. One of the top level outputs of our ad hov environment terraform configuration includes a complete `run-task` command that will run our database migrations when called from GitHub Actions (or our local machine).\n\n### Order of Operations\n\nFor ad hoc environments, it is probably fine to update the services and then run the database migrations. Ad hoc environments may only have a single \"user\" -- the developer, so **we don't need to worry about any errors that may occur if requests are made against the new version of code before database migrations have been applied**.\n\nLet's consider a simple example to illustrate what can go wrong here. If we add a `total_views` to our blog post model to track the total number of page views a post has, we would add a field to the model, generate migration file with `makemigrations`, and then update our views and model serializers to make use of this new field. In the time between updating our service and running the database migrations, any requests to endpoints that access the new database field will fail since the table does not yet exist.\n\nIf we first run database migrations **and then** update application code (ECS services), then we can avoid errors about fields not existing. In our production application, we want to aim for fewer errors, so we should be using this \"order of operations\": first run new database migrations and then update application code.\n\n### GitHub Action for application updates\n\nWe need a GitHub Action that can do the following:\n\n- fetch the container definitions JSON for the backend tasks (`aws ecs describe-task-definition`)\n- write new container definitions JSON with the new backend image tag\n- register new task definitions with the new container definition JSON files for each task (`aws ecs register-task-definition`)\n- call run-task with the newly updated migration ECS task (`aws ecs run-task`)\n- wait for the task to exit and display the logs\n- update the backend services (gunicorn, celery, celery beat) (`aws ecs update-service`)\n- wait for the new backend services to be stable (`aws ecs wait services-stable`)\n\nIn order have the correct arguments for all of the AWS CLI calls used in the above workflow, we can fetch values from `terraform output` and save values into environment variables, and then use these value in a later stage of our GitHub Actions workflow.\n\nThe process described above is needed for updating the backend application. Updating the frontend application involves a similar process to the backend update:\n\n- fetch the container definition JSON for the frontend tasks (`aws ecs describe-task-definition`)\n- write new container definition JSON with the new frontend image tag\n- register new task definitions with the new container definition JSON file for the frontend task (`aws ecs register-task-definition`)\n- update the frontend service (`aws ecs update-service`)\n- wait for the new backend services to be stable (`aws ecs wait services-stable`)\n\n## How to destroy an ad hoc environment\n\nDestroying an ad hoc environment is as simple as creating or updating an ad hoc environment. We simply provide the name of the terraform workspace and corresponding `\u003Cname\u003E.tfvars` file and another GitHub Action will run `terraform destroy` with the correct inputs.\n\nThe `\u003Cname\u003E.tfvars` will not be automatically deleted.\n\nNo data from the ad hoc environment will be deleted. The S3 bucket objects will be destroyed, as well as the database for the specific ad hoc environment.\n\n## Keeping track of ad hoc environments\n\nWe need to think about how we can keep track of our active ad hoc environments. Active environments will incur additional AWS costs, and we do not want developers or the product team to create lots of environments and then leave them running without actively using them.\n\nWe may decide to have some long-lived ad hoc environments, but those would be managed primarily by the DevOps team.\n\nOne way to check the active ad hoc environments would be to use the AWS CLI. We could list the ECS clusters in our development account, and this would show the number of ad hoc environments running. We could go farther and list the ad hoc environments by when they were last updated. We could then request developers or team members to remove ad hoc environments that are not in use.\n\nOr we could have a policy that all ad-hoc environments are deleted automatically at the end of the week.\n\nThere could be a lot of ways to track active ad hoc environments, so I won't go into more alternatives here.\n\n## Other considerations\n\n- [ ] Creating new databases without using SSH or bastion host\n\n## AWS Usage Quotas\n\n- [ ] Make sure that there are no default usage quotas that would be exceeded for some number of concurrent ad hoc environments.\n\n## Options for ad hoc environment settings\n\nAt the very minimum, our ad hoc environments need to know what versions of the frontend and backend application to use (the image tag for the frontend and backend images that have been pushed to ECR).\n\n- [ ] Public or private - do you want anyone on the internet to be able to access your ad hoc environment, or should the environment only be accessible over a private VPN connection. A `public` boolean could default to `false`, and you could have the environment only available\n\n- [ ] Custom AWS tags\n\n- [ ] Custom environment variables\n\n## AWS Tagging\n\n## Deploying the application from your local machine\n\nDeploying the application from your local machine can have a faster feedback loop while developing. This allows us to iterate on the deployment pipeline before automating it in GitHub Actions or another automation platform.\n\nIn order to deploy the application, you will need to do the following:\n\n- set up resources for an S3 backend that can be used by both the shared infrastructure and the ad hoc environments\n  - this can be done with the `tf-bootstrap-*` Makefile targets in the `django-step-by-step` repo\n  - take note of the S3 bucket name and dynamodb table name\n  - store these in a file called `backend.config` that will be used in both the shared infrastructure deployment and the ad hoc environment deployment\n- Install the AWS CLI and configure credentials with sufficient permissions (admin)\n- build and push the frontend and backend application images to the ECR repositories that were set up with the `tf-bootstrap-*` commands\n- Deploy base layer application\n  - in the `terraform-aws-ad-hoc-environments` repo, deploy the `examples\u002Fsimple` terraform configuration. This requires two inputs:\n    1. the `certificate_arn` for the wildcard certificate that will be used on the shared load balancer\n    2. the AWS Key Pair name that will be used for SSHing into the bastion host\n  - The bucket name, region and key will be used\n\n## Spin everything up\n\nStarting with an empty AWS account, here is how to get ad hoc environments set up.\n\n### S3 backend resources and ECR repositories\n\nWe first want to set up an S3 backend that we can use for storing our Terraform state files as well as ECR repositories for storing the images used for the frontend and backend components of our application. There are a few ways to do this:\n\n1. Set things up manually (not recommended)\n2. Set things up using Terraform (this is what I'm doing currently)\n3. Set things up using a CloudFormation template\n4. Use Terraform Cloud\n\nI'll be using option 2. This is sometimes referred to as \"bootstrapping\". When using CDK for IaC, you will do something similar that sets up S3 buckets and ECR repositories used to manage the assets used for application deployment.\n\nHave a look at [this README.md file](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Fblob\u002Fmain\u002Fterraform\u002Fbootstrap\u002FREADME.md) for more information about bootstrapping a Terraform S3 backend.\n\nWe can use the `make tf-bootstrap` command from the `django-step-by-step` GitHub repo to do this. Before running `make tf-bootstrap`, we need to copy the `bootstrap.tfvars.template` file to `bootstrap.tfvars` and add values for `region` and `backend_name`. Setting up these resources takes just a few seconds.\n\n### Set up shared infrastructure\n\nThe next step is to setup shared infrastructure components that our ad hoc environments will use. We will use the S3 backend that was setup in the previous step to store the Terraform state for our shared resources. The shared resources is configured with a dedicated Terraform module that I have published to the Terraform Registry ([link](https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fbriancaffey\u002Fad-hoc-environments\u002Faws\u002Flatest)). The git repo for this Terraform module contains a simple example (located in [`examples\u002Fsimple`](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fterraform-aws-ad-hoc-environments\u002Ftree\u002Fmain\u002Fexamples\u002Fsimple)) that I use to set up the shared resources.\n\nWe will need to provide a `backend.config` file that will be used to configure the S3 backend where we will store the terraform state for the shared resources.\n\nThis terraform configuration will not be updated frequently, so there is no GitHub Actions pipeline that will be used for updating and deploying the shared resources. For larger infrastructure teams, it might make sense to build a GitHub Actions pipeline for introducing changes to the shared resources rather than deploying from a local machine.\n\nThe shared resources terraform configuration requires two inputs:\n\n- `certificate_arn`: the ARN of a pre-provisioned wildcard ACM certificate that will be used to provide secure connections to the shared load balancer over the public internet. (My approach to ad hoc environments assumes that the environments can be accessed over the public internet without a VPN connection, but your requirements may be different)\n- `key_name`: the name of a pre-existing EC2 key pair name that will be used to access the bastion host from GitHub Actions. We need this in order to create databases in our RDS instance per ad hoc environment\n\n### Setting up an ad hoc environment\n\nNow that we have set up shared resources, we can start\n\n## Spin everything down\n\nTo get rid of all of the AWS resources used to support ad hoc environments, we need to delete the following:\n\n- all ad hoc environments\n- shared resources\n- S3 bucket and dynamodb table that were used to store terraform state\n\n### Delete ad hoc environment\n\nThis can be done by running the `terraform destroy` GitHub Actions job with the name of the ad hoc environment to be destroyed.\n\n### Delete shared resources\n\nCurrently this can be done from the command line using the `make examples-simple-destroy` Makefile command in the `terraform-aws-ad-hoc-environments` repo.\n\n### Delete the S3 bucket and DynamoDB table that are used to store and lock state files\n\nThis can be done from the `django-step-by-step` repo using the `make tf-bootstrap-destroy` command.\n\n\n## State management & resource lifecycle\n\nLet's take a look at the way that state is stored and shared between different components of our ad hoc environment setup.\n\n## Diagram\n\nHere's a diagram that shows both where state is stored as well as the lifecycle of resources used in ad hoc environments.\n\n![png](\u002Fstatic\u002Fadhoc\u002Flifecycle.drawio.png)\n\n## High level overview of lifecycle (A - F)\n\n### `A`: bootstrapped S3 backend for terraform state\n\nThese resource are long lived and they provide the ability to store Terraform state (a JSON object that described what is deployed to our AWS account) as well as Elastic Container Registry repos that store builds of our frontend and backend application.\n\n### `B`: deploy shared resources for ad hoc environments\n\nAd hoc environments share resources. This helps keeps startup speeds low and also helps to keep costs down.\n\n### `C`: launch (or update) ad hoc environments\n\nAd hoc environments can be launched by developers for testing a feature, or setting up an environment for a product demo. The same process used for launching a new environment can be used for updating an existing environment. Application versions can be updated through updating Terraform, or through a GitHub Action that makes a series of AWS CLI calls to make rolling updates to ECS services. This is triggered by a manual GitHub Action.\n\n### `D`: destroy an ad hoc environment\n\nOnce an ad hoc environment is no longer needed, it can be destroyed by running another GitHub Action with the name of the ad hoc environment to destroy as the only input.\n\n### `E`: destroy shared resources\n\nIf ad hoc environments are no longer needed altogether, then the shared resources can be deleted as they incur costs.\n\n### `F`: destroy bootstrap resources\n\nOnce all ad hoc environments and shared resources are removed, the S3 backend resource can also be removed.\n\n## Ad hoc infrastructure component and process details (1 - 25)\n\n### `1`: `make tf-bootstrap`\n\nThis command does a `terraform init \u002F plan \u002F apply` to create the S3 backend resources needed for storing state files for both shared infrastructure and per-environment resources (using Terraform workspaces).\n\n### `1`: S3 backend Terraform configuration\n\n[Link](https:\u002F\u002Fgithub.com\u002Fbriancaffey\u002Fdjango-step-by-step\u002Ftree\u002Fmain\u002Fterraform\u002Fbootstrap) to this Terraform configuration.\n\n### `1`: S3 bucket for S3 backend\n\nThe S3 bucket that is used to store Terraform state files. The bucket name is provided in `bootstrap.tfvars`.\n\n### `1`: DynamoDB lock table\n\nMore information about state locking can be found [here](https:\u002F\u002Fwww.terraform.io\u002Flanguage\u002Fsettings\u002Fbackends\u002Fs3#dynamodb-table-permissions).\n\n### `1`: ECR resources\n\nThe bootstrap terraform configuration includes Elastic Container Registry repos used for storing frontend and backend application container images that will be used in the ECS clusters for each ad hoc environment.\n\n### `1`: S3 backend outputs\n\nThe S3 backend Terraform configuration has Terraform outputs that can be used\n\n### `1`: Terraform state file for S3 backend\n\nThe state file that stores the state for our AWS resources that store state for our ad hoc environments is stored on my local machine. This is not ideal, but the S3 backend resource are fairly simple and will not be changed as we use and develop our ad hoc environment infrastructure setup. Also, these resources can easily be delete manually from the AWS console.\n\n### `1`: `make examples-simple` for setting up shared resources\n\n`make examples-simple` is a Makefile command from the `briancaffey\u002Fterraform-aws-ad-hoc-environments` repo. Like `tf-bootstrap`, it also wraps `terraform init \u002F plan \u002F apply` commands, but for another Terraform configuration.\n\n### `1`: `terraform-aws-ad-hoc-environments` Terraform module on Terraform Registry\n\nThe `terraform-aws-ad-hoc-environments` module has been published to the Terraform Registry and can be viewed [here](https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fbriancaffey\u002Fad-hoc-environments\u002Faws\u002Flatest).\n\n### `1`: VPC module\n\n### `1`: AWS CloudMap\n\n### `1`: EC2 instance (bastion host)\n\n### `1`: RDS instance\n\n### `1`: Application Load Balancer\n\n### `1`: IAM resources\n\n### `1`: ECS roles\n\n### `1`: Shared resources state file\n\n### `1`: `workflow_dispatch`\n\n### `1`: `create_update_ad_hoc_env` GitHub Action\n\n### `1`: Ad hoc environment name\n\n### `1`: `terraform-aws-django` Terraform module\n\n[Link](https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fbriancaffey\u002Fdjango\u002Faws\u002Flatest) to the `terraform-aws-django` module on the Terraform Registry.\n\n### `1`: ECS cluster\n\nOne ECS cluster is created for each ad hoc environment. The ECS cluster resources is is a grouping of ECS services and tasks.\n\n### `1`: ECS services\n\nThe application uses the following ECS services for ad hoc environments:\n\n- `gunicorn` - web server process for the API\n- `celery` - web worker that processes tasks\n- `celery beat` - job scheduler that creates tasks for celery workers to process\n- `nginx` - for serving the frontend application\n- `redis` - a stateful service that is used for application data caching and brokering tasks\n\n### `1`: S3 bucket for ad hoc environment\n\nEach ad hoc environment will have a dedicated S3 bucket for storing static and media files. In the context of the example application we are running, these files are all image files\n\n### `1`: ALB Listener Rules\n\nThere is a single load balancer shared across all ad hoc environments. Each ad hoc environments has associated listener rules that route requests based on the path to the correct application\n\n### `1`: Postgres database in RDS instance\n\nEach ad hoc environment uses a dedicated postgres database that lives in an RDS instance that is shared by all ad hoc environments.\n\n### `1`: Redis\n\nRedis is a key-value database that is used for application data caching and brokering tasks. Running redis servers as an ECS service is an alternative to using a managed ElastiCache instance per environment.\n\n### `1`: Service Discovery Service (CloudMap)\n\nIn order for our backend application containers to communicate with the Redis ECS service, we need to set up a Service Discovery service for Redis. This will maintain an internal Route 53 record that points to the private IP of the Redis ECS service.\n\n### `1`: `terraform-remote-state`\n\n[`terraform-remote-state`](https:\u002F\u002Fwww.terraform.io\u002Flanguage\u002Fstate\u002Fremote-state-data) is used to share state between two Terraform configurations. For example, the endpoint of the RDS instance needs to shared from the shared resources Terraform Configuration to the ad hoc environment configuration.\n\n### `1`: `destroy_ad_hoc_env`\n\n### `1`: `make examples-simple-destroy`\n\n### `1`: `make tf-bootstrap-destroy`\n\n### `1`: `briancaffey\u002Fdjango-step-by-step` GitHub repo\n\n### `1`: `briancaffey\u002Fterraform-aws-django` GitHub repo\n\n### `1`: `briancaffey\u002Fterraform-aws-ad-hoc-environments` GitHub repo\n\n### Google Drive link to this draw.io diagram\n\n[Google Drive link to this diagram](https:\u002F\u002Fdrive.google.com\u002Ffile\u002Fd\u002F1Te427LEPSlGinEfncH39gArHxwB1cMp0\u002Fview?usp=sharing). It is view-only, but you can duplicate it and edit the copy.\n\n## TODOs\n\nA list of things that need to be fixed or added to this article\n\n- [x] rename GitHub Action to `create_update_ad_hoc_env`\n- [x] rename GitHub Action to `destroy_ad_hoc_env`\n- [ ] Build Route 53 resources based on ad hoc environment name (e.g. `my-env.dev.example.com`)\n- [ ] Mention CloudFormation template as an option for managing S3 backend resources [link](https:\u002F\u002Fwww.bti360.com\u002Fmng-terraform-state-cloudformation\u002F)\n- [ ] Use Fargate Spot launch type instead of regular Fargate in order to lower costs for ad hoc environments\n- [ ] Add a dedicated directory for storing `tfvars` files\n"}}],fetch:{},mutations:[]}}("text","element","\n","span","code","p","li","a","true",-1,"icon","icon-link","token",3,"h3","punctuation","1","ul","property","=",2,"h2"," ","nofollow","noopener","noreferrer","_blank",true,": ","\u003Cname\u003E.tfvars",")","task-list-item","input","checkbox","{","\n  ","}","terraform_remote_state"," and ",".","div","nuxt-content-highlight","pre","line-numbers","comment","\n\n","keyword","string","terraform-aws-ad-hoc-environments","language-text",", ","terraform apply","eli5","Diagram","django-step-by-step","terraform plan","here","strong","link","type","variable","region","contains-task-list","VPC","ECS Cluster","S3 bucket","terraform-aws-django","terraform init","\u003Cname\u003E","The ","img","ol","aws_lb_target_group","\n\n  ","\n    ","make tf-bootstrap"," GitHub repo","\u002Fstatic\u002Fadhoc.png","terraform","aws","tldr","tl;dr","github-links","GitHub Links","assumptions","Assumptions","what-are-ad-hoc-environments","What are ad hoc environments?","trade-offs-to-make-when-designing-ad-hoc-environment-infrastructure-and-automation","Trade-offs to make when designing ad hoc environment infrastructure and automation","more-on-shared-resources-vs-per-environment-resources","More on shared resources vs per-environment resources","diagram","shared-architecture","Shared architecture","environment-specific-architecture","Environment-specific architecture","shared-resources-terraform-configuration","Shared resources terraform configuration","vpc","cloud-map-service-discovery-namespace","Cloud Map Service Discovery Namespace","security-groups","Security Groups","iam-roles","IAM Roles","rds-instance","RDS Instance","load-balancer","Load Balancer","bastion-host","Bastion Host","environment-specific-resources","Environment-specific resources","ecs-cluster","ecs-tasks-and-services","ECS Tasks and Services","alb-listener-rules-and-target-groups","ALB Listener Rules and Target Groups","route-53-record","Route 53 record","s3-bucket","database-within-rds-instance","Database within RDS instance","how-to-setup-an-ad-hoc-environment","How to setup an ad hoc environment","design-decisions","Design decisions","github-action","GitHub Action","how-to-update-code-in-an-existing-ad-hoc-environment","How to update code in an existing ad hoc environment","order-of-operations","Order of Operations","github-action-for-application-updates","GitHub Action for application updates","how-to-destroy-an-ad-hoc-environment","How to destroy an ad hoc environment","keeping-track-of-ad-hoc-environments","Keeping track of ad hoc environments","other-considerations","Other considerations","aws-usage-quotas","AWS Usage Quotas","options-for-ad-hoc-environment-settings","Options for ad hoc environment settings","aws-tagging","AWS Tagging","deploying-the-application-from-your-local-machine","Deploying the application from your local machine","spin-everything-up","Spin everything up","s3-backend-resources-and-ecr-repositories","S3 backend resources and ECR repositories","set-up-shared-infrastructure","Set up shared infrastructure","setting-up-an-ad-hoc-environment","Setting up an ad hoc environment","spin-everything-down","Spin everything down","delete-ad-hoc-environment","Delete ad hoc environment","delete-shared-resources","Delete shared resources","delete-the-s3-bucket-and-dynamodb-table-that-are-used-to-store-and-lock-state-files","Delete the S3 bucket and DynamoDB table that are used to store and lock state files","state-management--resource-lifecycle","State management & resource lifecycle","diagram-1","high-level-overview-of-lifecycle-a---f","High level overview of lifecycle (A - F)","a-bootstrapped-s3-backend-for-terraform-state","b-deploy-shared-resources-for-ad-hoc-environments","c-launch-or-update-ad-hoc-environments","d-destroy-an-ad-hoc-environment","e-destroy-shared-resources","f-destroy-bootstrap-resources","ad-hoc-infrastructure-component-and-process-details-1---25","Ad hoc infrastructure component and process details (1 - 25)","1-make-tf-bootstrap","1-s3-backend-terraform-configuration","1-s3-bucket-for-s3-backend","1-dynamodb-lock-table","1-ecr-resources","1-s3-backend-outputs","1-terraform-state-file-for-s3-backend","1-make-examples-simple-for-setting-up-shared-resources","1-terraform-aws-ad-hoc-environments-terraform-module-on-terraform-registry","1-vpc-module","1-aws-cloudmap","1-ec2-instance-bastion-host","1-rds-instance","1-application-load-balancer","1-iam-resources","1-ecs-roles","1-shared-resources-state-file","1-workflow_dispatch","1-create_update_ad_hoc_env-github-action","1-ad-hoc-environment-name","1-terraform-aws-django-terraform-module","1-ecs-cluster","1-ecs-services","1-s3-bucket-for-ad-hoc-environment","1-alb-listener-rules","1-postgres-database-in-rds-instance","1-redis","1-service-discovery-service-cloudmap","1-terraform-remote-state","1-destroy_ad_hoc_env","1-make-examples-simple-destroy","1-make-tf-bootstrap-destroy","1-briancaffeydjango-step-by-step-github-repo","1-briancaffeyterraform-aws-django-github-repo","1-briancaffeyterraform-aws-ad-hoc-environments-github-repo","google-drive-link-to-this-drawio-diagram","Google Drive link to this draw.io diagram","todos","TODOs",":","brian","tfvars","png","https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fterraform-aws-modules\u002Fvpc\u002Faws\u002Flatest","collectstatic"," task","We can use the ","redis","https:\u002F\u002Fwww.terraform.io\u002Flanguage\u002Fstate\u002Fremote-state-data","",").","\n      ","source","backend","    "," var.region\n  ","                   ","             ","workflow_dispatch","aws ecs run-task","brian.tfvars","aws ecs describe-task-definition","aws ecs register-task-definition","aws ecs update-service","wait for the new backend services to be stable (","aws ecs wait services-stable","terraform destroy","tf-bootstrap-*","backend.config","examples\u002Fsimple","certificate_arn","bootstrap.tfvars","https:\u002F\u002Fregistry.terraform.io\u002Fmodules\u002Fbriancaffey\u002Fad-hoc-environments\u002Faws\u002Flatest","make examples-simple-destroy","make tf-bootstrap-destroy","terraform init \u002F plan \u002F apply","Link","make examples-simple","briancaffey\u002Fterraform-aws-ad-hoc-environments","create_update_ad_hoc_env","terraform-remote-state","destroy_ad_hoc_env"," rename GitHub Action to ","2022-05-23T23:40:07.289Z")));