__NUXT_JSONP__("/jp/2023/08/27/python-vue-chinese-llama-2-and-the-three-body-problem", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG){return {data:[{article:{slug:"python-vue-chinese-llama-2-and-the-three-body-problem",description:null,title:"Python, Vue, Chinese-LLaMA-2 and The Three-Body Problem",date:"2023-06-17",image:"\u002Fimg\u002Fthree-body-problem\u002Finvokeai\u002Fconfucius\u002F1.png",tags:["three-body-problem",ax,ay,"ai","chinese","python","nvidia","cuda","gpu","stable-diffusion","langchain","vue"],draft:am,comments:am,toc:[{id:az,depth:X,text:aA},{id:aB,depth:X,text:aC},{id:aD,depth:X,text:aE},{id:aF,depth:X,text:aG},{id:aH,depth:X,text:aI},{id:aJ,depth:X,text:aK}],body:{type:"root",children:[{type:b,tag:"h1",props:{id:"tldr"},children:[{type:b,tag:w,props:{href:"#tldr",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:"tl;dr"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This articles brings together several of my interest, both old and new:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"The Chinese Sci-Fi book series 'Three-Body Problem'"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Chinese language"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"NLP techniques"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"AI"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Large Language Models (LLMs)"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Stable Diffusion"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Data visualization and 3D graphics"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Mathematics"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"NVIDIA \u002F CUDA"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I translated 'The Three-Body Problem' into English using Meta's LLaMa 2, "},{type:b,tag:o,props:{},children:[{type:a,value:"llama.cpp"}]},{type:a,value:aL},{type:b,tag:o,props:{},children:[{type:a,value:aM}]},{type:a,value:" and the "},{type:b,tag:w,props:{href:"https:\u002F\u002Fhuggingface.co\u002Fziqingyang\u002Fchinese-llama-2-7b",rel:[F,G,H],target:I},children:[{type:b,tag:o,props:{},children:[{type:a,value:"Chinese-LLaMa-2"}]},{type:a,value:" model from Hugging Face"}]},{type:a,value:". I implemented a basic Retrieval Augmented Generated (RAG) program using LangChain with a local vector database that allowed me to ask the LLM questions about the book. To visualize scenes from the book I wrote prompts for Stable Diffusion. Finally, I dipped my toes into nbody simulations with both CuPy (Python library for CUDA) and Three.js. This article will walk through code samples and give more background on the tools, libraries and languages used."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"When discussing particulars of the Three Body novel series, I will add the emoji for the Chinese character for \"secret\" „äôÔ∏è (Áßò)."}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The \"„äô\" emote, also known as the \"Chinese Character\" emote, is a symbol used in Japanese culture to indicate secrecy or confidentiality. It is derived from the Chinese character \"Áßò\" (m√¨), which means \"secret.\" In online communication, it is often used to suggest that something should be kept private or not discussed openly."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:Y,props:{id:az},children:[{type:b,tag:w,props:{href:"#back-story",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aA}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"About a month ago my company announced that another round of layoffs was coming the following week. I'm on an engineering team that had already been impacted by two rounds of layoffs in the past year, and I was fully expecting to be let go this time. In an impulse-buy I ordered a book at the top of my reading list from Amazon: \"The Three-Body Problem\". It is a Sci-Fi trilogy written by Liu Cixin, a Chinese computer engineer who started writing the book as a series of essays that were published in China's \"World of Sci-Fi\" publication. I bought the original trilogy of books written in Chinese as well as the English translation, six books in total."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"Images of Three Body Problem Book Series",src:"\u002Fimg\u002Fthree-body-problem\u002Fbooks.png"},children:[]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I started learning Chinese in college, adding a major in Chinese Language to the mathematics major I decided on in my sophomore year. I did an exchange program through my college with Fudan University, a top Chinese University. In 2007, living and studying Chinese in Shanghai as a 19 year old American was a really fun time. I was placed in an advanced-level course with a diverse group of students where English was not the lowest common linguistic denominator. We had a demanding cirriculum that emphasized reading, listening and speaking Chinese. Alcohol, food and partying was an effective catalyst for absorbing the Chinese language. I also took a course on Differential Equations with my college's mathematics department chair who was on sebatical at Fudan during the same time."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"3-6-9 drinking game rules",src:"\u002Fimg\u002Fthree-body-problem\u002F3-6-9.jpeg"},children:[]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"My favorite memory of that semester at Fudan was travelling on an epic over-night sleeper train from Shanghai to Guangxi province with a school-sponsored class trip to see Guilin. Multiple games of sam-yuk-gu (3-6-9) ran in parallel across the matrix of 3-by-2 sleeper car bunk beds like workloads distributed across multiple GPU cores. The rules of 3-6-9 are simple: sit in a circle counting up from 1. If your number contains a 3, 6 or 9, you clap once for each occurance of the number your hands instead of saying your number. The first person to break the rules takes a drink. Our volume level swelled to a fever pitch as the drinking games created a feedback loop that rapidly diminished our alcohol supply. Everyone passed out later in the evening as the train pushed forward through the night. The next morning we all boarded a river cruise in a daze to see Guilin's stunning limestone peaks:"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"20 yuan note with Guilin rock formations",src:"\u002Fimg\u002Fthree-body-problem\u002F20_yuan_note.jpeg"},children:[]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"After college, my second job took me back to China where I specialized in the technologies, policies and applications of large scale battery storage systems for use in renewable energy projects. I worked for a man in Beijing who founded China's energy storage industry association after pioneering the commercialization of vinadium redox flow batteries for renewable energy projects. The job exposed me to exciting battery projects all over China, and also sharpened my technical Chinese skills as I was frequently reading, translating and intepreting in a bi-linguagl, hi-tech environment. At one of the annual industry conferences our organization hosted, I had the opportunity to be an interpreter for a conversation between American battery entrepreurs and representative from China's State Grid. State Grid is China's state-owned power utility and was the world's 3rd largest company by revenue in 2022 behind Walmart and Amazon."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"State Grid HQ in Xi Cheng",src:"\u002Fimg\u002Fthree-body-problem\u002Finvokeai\u002Fcastles.png"},children:[]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"My first introduction to the 'Three-Body' book came from one of my very best friends from college. He lived at the inner-most leaf-node of one of Beijing's more labrythnian hutongs next to a family that raised and trained pigeons. Visiting my friend where he lived often involved exchanges with his pigeon master neighbor in the narrow alleys of their corner of the hutong. In one such exchange the neighbor quipped to me with a croaking voice: ‰ªñÊÄï‰Ω†Áê¥ÔºÅ, or \"he's afraid of your guitar!\". The pigeons cooing at my were apparently fearful of the backpack case for my classical guitar which extended over my head by about a foot, an unwelcomed advance into their roost! This simple pleasentry became a refrain that echoed every time we joked about life in the hutong. My friend and I bonded over Chinese language, classical guitar among many other things. I strongly considered his recommendation to check out ‰∏â‰Ωì (Three Body), the Chinese Sci-Fi novel about alien life in a solar system with three stars as he described it, but with already lots going on in Beijing I never had the chance to check out the book."}]},{type:a,value:f},{type:b,tag:"hr",props:{},children:[]},{type:a,value:f},{type:b,tag:ac,props:{width:ad,height:315,src:"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002F5lj99Uz1d50?si=TwrypbY4vTfeWGRf",title:"YouTube video player",frameBorder:D,allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowFullScreen:am},children:[]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Almost 10 years later I came across across a preview for the Netflix production of \"3 Body Problem\" scheduled to come out in early 2024. With the possibility of loosing my job weighing heavily on me, I picked up the books on Amazon on an impulse buy hoping to have something to do if I was layed off this time. I ended up reading the first book on my Kindle. Reading Chinese on the kindle is nice because your can use built in dictionaries and Google translate for looking up difficult passages with just a few taps on the Kindle or Kindle iPad app."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Even though these look ups are simple, it can still be a distraction to have to do lots of look ups for certain chapters or passages that were hard to follow without doing so. I got the idea to build a simple online reader with built-in translation of each word. I scraped the Chinese text of the book from an online Â±±ÂØ®Áâà (a term from Shen Zhen--the heart of China's electronics industry--which referes to an unofficial version of something, a copy-cat or knock-off). This online version of the book contains some simple watermarks that I was able to remove, and I noticed that some words are rarely replaced with incorrect synonym characters, for examlpe:"}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Âç±Èô© would be written Âç±ËÑ∏, this jumped out at me as an obvious intentional typo. The first character of the word \"dangerous\" Âç± is the same, but in the second character, the phonetic component on the right side of the character is the same (‰Ω•), but the left side is replaced with another radical Êúà Ôºàüà∑Ô∏èÔºâ which means moon or flesh, making the meaning of the character ËÑ∏ \"face\", not danger or Èô©."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"For the most part the text from the website seemed to be a faithful reproduction of the book. I found two open source projects that could be use to parse text and translate words:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"CC-CEDICT\n"},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"an open-source Chinese-\u003EEnglish dictionary"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:b,tag:w,props:{href:aN,rel:[F,G,H],target:I},children:[{type:a,value:aN}]}]},{type:a,value:f}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Chinese Natural Language Processing (spaCy) (NTNU)\n"},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:b,tag:w,props:{href:aO,rel:[F,G,H],target:I},children:[{type:a,value:aO}]}]},{type:a,value:f}]},{type:a,value:f}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The spaCy tool parses words which in Chinese are typically two characters long, and sometimes three or four characters in length. The Chinese -\u003E English dictionary contains tranlsations both for both individual Chinese characters and multiple character words. I wrote some Python scripts to loop over each paragraph of each chapter of each book and parse then Chinese words and look up definitions from them in the Chinese dictonary with over 150K entries. I saved each chapter as a file containing the parsed text and definitions as well as pinyin. I also added the frequency rank for each character to visualize the \"difficulty\" of reading certain passages. The darker the background of an individual character, the less frequent the character occurs in written Chinese. Hovering over a word you can see the meaning of an individual character as well as the word or \"linguistic token\" that the NLP software parsed."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I built a simple UI to parse and display the text and related \"metadata\" using Vue, a popular library that rival the dominant UI framework that Facebook, now Meta, is famous for open sourcing. I love Vue's simple API and have had a lot of luck getting it to do what I want it to do."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I'm also a big fan of Vue's author and BDFL, Evan You. His chinese name is Â∞§Èõ®Ê∫™ ÔºàYou YuXiÔºâ. The word \"You\" in Chinese is pronounced \"yo\" as in \"yo-yo\", but Evan seems to have adopted the pronciation \"you\" as in \"you and me\" which in Chinese pinyin would actually be \"yu\". Chinese and English in my experience are totally butchered. For example my name is \"Brian\" but it is more than not misspelled as \"Brain\" but native Chinese speakers. ü§∑‚Äç‚ôÇÔ∏è So I typically adopt the phonetic translation of Brian: Â∏ÉËé±ÊÅ© (Bu Lai En) or Â∞èÂ∏É (Little Bu)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here are some quick facts about the first book in the series:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"36 chapters in the book"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"188380 total charactes in the book"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"average of 69.78 chapters per paragraph"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"2859 unique characters in the book (TODO: look this up)"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"total of 2512 in the book"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"average of 74.99 characters per paragraph"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"Character Frequency",src:"\u002Fimg\u002Fthree-body-problem\u002Fchar_freq.png"},children:[]}]},{type:a,value:f},{type:b,tag:v,props:{className:["wrap"]},children:[{type:a,value:f},{type:b,tag:ac,props:{className:[aP],src:"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Ffreq\u002F",width:ad,height:aQ},children:[]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:Y,props:{id:aB},children:[{type:b,tag:w,props:{href:"#llms-metas-llama-2-and-chinese-llama-2",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aC}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"In recent months I have been following the development of some interesting open source AI projects. Two projects in particular are InvokeAI, an image generation tool based on stable diffusion, and "},{type:b,tag:w,props:{href:"https:\u002F\u002Fai.meta.com\u002Fllama\u002F",rel:[F,G,H],target:I},children:[{type:a,value:"LLaMA 2"}]},{type:a,value:", the next generation of Meta's open source large language model (LLM). The casing of the word "},{type:b,tag:o,props:{},children:[{type:a,value:"LLaMA"}]},{type:a,value:" indicates that LLaMa 2 takes its name from the acronym LLM (Large Language Model), which is a general term for the technology that underpins the massively popular ChatGPT from OpenAI. LLMs are so hot right now. Before going deeper into LLMs we need a quick Chinese lesson."}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"ËçâÊ≥•È©¨ is a non-technical word that referes to animals like Llama or Alpaca. It can be directly translated as \"Grass Mud Horse\" it is phonetically similar to the most common Chinese profanity: Êìç‰Ω†Â¶à, which literally means \"fuck your mother\". The characters in the two words are nearly synonymous: the sounds of both words are \"cao ni ma\", but the tones are different, which in Chinese changes the meaning completely. In English this would be like two words spelled the same way that have emphsis placed on different syllables. The llama is basically a legendary Chinese internet meme subversive in the face of Chinese censorship. ü¶ô was approved as part of Unicode 11.0 in 2018."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"There's a lot more to say about the significance of the Llama in Chinese internet culture, have a look at "},{type:b,tag:w,props:{href:"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGrass_Mud_Horse",rel:[F,G,H],target:I},children:[{type:a,value:"the Wikipedia article on "},{type:b,tag:"strong",props:{},children:[{type:a,value:"Grass Mud Horse"}]}]},{type:a,value:" if you want to go deeper. I think it is safe to assume that the naming of LLaMa has something to do with Facebook\u002FMeta's hacker mentality and the large number of Chinese people working on AI at Meta. This mostly likely is OK with Mark Z now since "},{type:b,tag:w,props:{href:"https:\u002F\u002Fwww.voanews.com\u002Fa\u002Fsilicon-valley-technology_how-facebooks-zuckerberg-went-courting-criticizing-beijing\u002F6195455.html",rel:[F,G,H],target:I},children:[{type:a,value:"his attitude toward China has changed"}]},{type:a,value:" in the last few years from one of courtship to one of criticism."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I requested access to Meta's LLaMa 2 models as soon as they came out and was able to get the models to run on my computer. I have an I9-13900K CPU and an NVIDIA 4090 graphics card which I was able to get a hold of in Febrauary after resellers had made these cards almost impossible to buy from major retailers. I also started lurking in a subreddit called "},{type:b,tag:w,props:{href:"https:\u002F\u002Fwww.reddit.com\u002Fr\u002FLocalLLaMA\u002F",rel:[F,G,H],target:I},children:[{type:b,tag:o,props:{},children:[{type:a,value:"r\u002FLocalLLaMa"}]}]},{type:a,value:" with over fifty thousand members. The forum has a lot of helpful information about running Large Language Models on computer hardware like mine which is most commonly used play computer games. Another annoucement that caught my attention in July was the release of "},{type:b,tag:w,props:{href:"https:\u002F\u002Fgithub.com\u002Fymcui\u002FChinese-LLaMA-Alpaca-2",rel:[F,G,H],target:I},children:[{type:a,value:"Chinese LLaMa 2"}]},{type:a,value:", a Large Language Model trained on Chinese and English which does very well against Chinese Language Benchmarks."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"image of Chinese LLaMa 2",src:"\u002Fimg\u002Fthree-body-problem\u002Fchinese_llama_2.png"},children:[]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:w,props:{href:aR,rel:[F,G,H],target:I},children:[{type:a,value:aR}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:V,props:{alt:"image of CMMLU",src:"\u002Fimg\u002Fthree-body-problem\u002Fcmmlu.jpeg"},children:[]}]},{type:a,value:f},{type:b,tag:Y,props:{id:aD},children:[{type:b,tag:w,props:{href:"#translation",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aE}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"„äôÔ∏è From the documents of the the Red Coast Base we learn about the plans to develop a universal language based on math and physics that can be understood by any sufficiently advanced civilization. There is also a plan to build a full linguistic system on top of this universal language which effectively allows for communication with alien civilizations in Chinese and Esperanto."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The books idea of universal translation made me think a lot about LLMs that are so popular today. In essence they are calculators for words. When you feed a prompt to an LLM, it first puts the prompt through a process called tokenization. Tokenization takes a string of text and breaks it down into tokens (defined by the Large Language Model you are using). These tokens are numbers. Here's an example of tokenization in action:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,_]},children:[{type:b,tag:o,props:{},children:[{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" json\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" os\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:"from"}]},{type:a,value:" llama_cpp "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:" LlamaTokenizer\n\nllm "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:aS},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"\n    model_path"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"\u002Fpath\u002Fto\u002Fmodels\u002Fggml-model-q4_0.bin\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:"\n    n_ctx"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"4096"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:"\n    n_gpu_layers"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:aT}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\ntokenizer "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:" LlamaTokenizer"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:ax},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:ay},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\nTEXT"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"Âú®ÈÇ£‰∏™Â∑≤Ë¢´ÂøòÂç¥ÁöÑÊó•Â≠êÈáåÔºåÂÆÉÁöÑ‰∏ñÁïåÈ¢†Ë¶Ü‰∫Ü„ÄÇÊ≥•ÂúüÈ£ûËµ∞ÔºåÂá∫Áé∞‰∫Ü‰∏ÄÊù°ÂèàÊ∑±ÂèàÂÆΩÁöÑÂ≥°Ë∞∑ÔºåÁÑ∂ÂêéÊ≥•ÂúüÂèàËΩ∞ÈöÜÈöÜÂú∞È£ûÂõûÊù•ÔºåÂ≥°Ë∞∑Ê∂àÂ§±‰∫ÜÔºåÂú®ÂéüÊù•Â≥°Ë∞∑ÁöÑÂ∞ΩÂ§¥Âá∫Áé∞‰∫Ü‰∏ÄÂ∫ßÈªëËâ≤ÁöÑÂ≠§Â≥∞„ÄÇÂÖ∂ÂÆûÔºåÂú®ËøôÁâáÂπøÈòîÁöÑÁñÜÂüü‰∏äÔºåËøôÁßç‰∫ãÂ∏∏Â∏∏ÂèëÁîüÔºåÊ≥•ÂúüÈ£ûËµ∞ÂèàÈ£ûÂõûÔºåÂ≥°Ë∞∑Âá∫Áé∞ÂèàÊ∂àÂ§±ÔºåÁÑ∂ÂêéÊòØÂ≠§Â≥∞Èôç‰∏¥ÔºåÂ•ΩÂÉèÊòØÁªôÊØèÊ¨°ÁÅæÂèòÊâì‰∏ä‰∏Ä‰∏™ÈÜíÁõÆÁöÑÊ†áËÆ∞„ÄÇË§êËöÅÂíåÂá†Áôæ‰∏™ÂêåÊóèÂ∏¶ÁùÄÂπ∏Â≠òÁöÑËöÅÂêéÂêëÂ§™Èò≥ËêΩ‰∏ãÁöÑÊñπÂêëËµ∞‰∫Ü‰∏ÄÊÆµË∑ØÔºåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÂ∏ùÂõΩ„ÄÇ\""}]},{type:a,value:"\ntokens "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aU},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"TEXT"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"print(str(tokens[:4]) + \" ...\")\n"}]}]}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"[1, 30505, 32380, 36812] ..."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"for token in tokens:\n    text = tokenizer.decode([token])\n    print(text, end=\" \")\n"}]}]}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Âú® ÈÇ£‰∏™ Â∑≤Ë¢´ Âøò Âç¥ ÁöÑÊó•Â≠ê Èáå Ôºå ÂÆÉÁöÑ ‰∏ñÁïå È¢†Ë¶Ü ‰∫Ü „ÄÇ Ê≥• Âúü È£û Ëµ∞ Ôºå Âá∫Áé∞‰∫Ü ‰∏ÄÊù° Âèà Ê∑± Âèà ÂÆΩ ÁöÑ Â≥°Ë∞∑ Ôºå ÁÑ∂Âêé Ê≥• Âúü Âèà ËΩ∞ ÈöÜ ÈöÜ Âú∞ È£û ÂõûÊù• Ôºå Â≥°Ë∞∑ Ê∂àÂ§± ‰∫Ü Ôºå Âú® ÂéüÊù• Â≥°Ë∞∑ ÁöÑ Â∞ΩÂ§¥ Âá∫Áé∞‰∫Ü ‰∏ÄÂ∫ß ÈªëËâ≤ ÁöÑ Â≠§ Â≥∞ „ÄÇ ÂÖ∂ÂÆû Ôºå Âú®Ëøô Áâá ÂπøÈòî ÁöÑ ÁñÜ Âüü ‰∏ä Ôºå ËøôÁßç‰∫ã Â∏∏Â∏∏ ÂèëÁîü Ôºå Ê≥• Âúü È£û Ëµ∞ Âèà È£û Âõû Ôºå Â≥°Ë∞∑ Âá∫Áé∞ Âèà Ê∂àÂ§± Ôºå ÁÑ∂Âêé ÊòØ Â≠§ Â≥∞ Èôç‰∏¥ Ôºå Â•ΩÂÉèÊòØ Áªô ÊØèÊ¨° ÁÅæ Âèò Êâì ‰∏ä ‰∏Ä‰∏™ ÈÜíÁõÆ ÁöÑ Ê†áËÆ∞ „ÄÇ Ë§ê ËöÅ Âíå Âá†Áôæ ‰∏™ Âêå Êóè Â∏¶ÁùÄ Âπ∏ Â≠ò ÁöÑ ËöÅ Âêé Âêë Â§™Èò≥ ËêΩ ‰∏ãÁöÑ ÊñπÂêë Ëµ∞‰∫Ü ‰∏ÄÊÆµ Ë∑Ø Ôºå Âª∫Á´ã‰∫Ü Êñ∞ÁöÑ Â∏ùÂõΩ „ÄÇ"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,_]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"english_text "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"This is an example of tokenization using a large language model.\""}]},{type:a,value:"\nenglish_tokens "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:aU},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"english_text"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:ae}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:"str"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"english_tokens"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ao}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:"+"}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\" ...\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:ap},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:aV}]},{type:a,value:" token "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:aW}]},{type:a,value:" english_tokens"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:"\n    text "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"decode"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:d},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:ae}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,"string-interpolation"]},children:[{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"f\"'"}]},{type:b,tag:c,props:{className:[d,"interpolation"]},children:[{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:aq}]},{type:a,value:a},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:ar}]}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"'\""}]}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:" end"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\" \""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"[1, 4013, 338, 385] ..."}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"'' 'This' ' is' ' an' ' example' ' of' ' token' 'ization' ' using' ' a' ' large' ' language' ' model' '.'"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This is a good time to talk about some of the obvious differences between Chinese and English."}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"The Chinese does not use spaces between words like English does"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Chinese words are typically formed from 2 or more characters"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Chinese does not conjugate verbs"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Chinese characters not have capitization like ASCII characters"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"(TODO: compare how other LLMs tokenize Chinese text.)"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"After using basic NLP techniques to translate individual words, I did some basic prompt engineering to get the LLM to translate the books in the Three-Body problem paragraph by paragraph. My computer was able to translate the first book overnight in under 500 minutes. Here's the prompt I used:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"completion_prompt = f\"### Chinese:\\n‰Ω†Â•Ω\\n\\n### English:\\nHello\\n\\n### Chinese:\\n${text}\\n\\n### English:\\n\"\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"My initial impression of the Chinese LLaMa 2's accuracy at translation tasks is that it is almost unbelievably good, with the occasional occurance of task failure include some of these failure modes:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"responding to a request with the same Chinese text that I asked it to translate"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"translating the only the first sentence of a multi-sentence paragraph"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"not translating the text at all"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Translating most of the text to English while some words are left not translated in the response: For example: Ëøô‰ΩçÊòØÁ∫¢Â≤∏Âü∫Âú∞ÁöÑÈõ∑ÂøóÊàêÊîøÂßî -\u003E This is Red Bank Base's Li Zhi-chengÊîøÂßî. ÊîøÂßî is an abreviation of Political commissar (ÊîøÊ≤ªÂßîÂëò) In the military, a political commissar or political officer is a supervisory officer responsible for the political education and organization of the unit to which they are assigned, with the intention of ensuring political control of the military."}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"responding only with \"```\" (this may have something to do with the prompt I used)"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Butchering names with different translations offered between paragraphs, particularly ÁôΩÊ≤êÈúñ which it translated as White Moxin, White Mullin, White Mow Lin. None of the varieties used the pinyin translation of the characters: Bai Mu Lin. This isn't necessarily a failure of the translation, just interesting to see this type of variation across translated paragraphs."}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Some of the dialog started with the Chinese word for subtitles (Â≠óÂπïÔºö), which was not included as part of the translation, also understandable"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"It was nice to have the English book as a reference for qualitative analysis of the translation tasks I gave to the Chinese LLaMa 2 model. I look forward to reading the second and third books with all characters, words and paragraphs fully translated (for the most part)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here are the results of my translation of Three-Body Problem with Chinese-Alpaca-2 7 Billion parameter model from Hugging Face with 4 bit quantization:"}]},{type:a,value:f},{type:b,tag:ac,props:{className:[aP],src:"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Freader\u002F?book=three_body&chapterNumber=3",width:ad,height:aQ},children:[]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I used "},{type:b,tag:o,props:{},children:[{type:a,value:"nvtop"}]},{type:a,value:" to monitor GPU usage:"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"[image of nvtop]"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here are some of the statistics from individual paragraph translations:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"# model initialization\nllama.cpp: loading model from \u002Fhome\u002Fbrian\u002Fgithub\u002Fllama.cpp\u002Fmodels\u002F7B\u002FChinese-Alpaca-2\u002Fggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 55296\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 5504\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal: using CUDA for GPU acceleration\nllama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\nllama_model_load_internal: offloading 0 repeating layers to GPU\nllama_model_load_internal: offloaded 0\u002F35 layers to GPU\nllama_model_load_internal: total VRAM used: 512 MB\nllama_new_context_with_model: kv self size  = 2048.00 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:b,tag:o,props:{},children:[{type:a,value:"BLAS = 1"}]},{type:a,value:" means that it is able to use the GPU, we also see "},{type:b,tag:o,props:{},children:[{type:a,value:"using CUDA for GPU acceleration"}]},{type:a,value:l}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"# translation task logs\nllama_print_timings:        load time =   640.04 ms\nllama_print_timings:      sample time =    20.00 ms \u002F    45 runs   (    0.44 ms per token,  2249.66 tokens per second)\nllama_print_timings: prompt eval time =   640.00 ms \u002F    77 tokens (    8.31 ms per token,   120.31 tokens per second)\nllama_print_timings:        eval time =  3898.99 ms \u002F    44 runs   (   88.61 ms per token,    11.28 tokens per second)\nllama_print_timings:       total time =  4631.62 ms\n"}]}]}]},{type:a,value:f},{type:b,tag:Y,props:{id:aF},children:[{type:b,tag:w,props:{href:"#-hugging-face",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aG}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Going over the "},{type:b,tag:w,props:{href:"https:\u002F\u002Fhuggingface.co\u002Flearn",rel:[F,G,H],target:I},children:[{type:a,value:"Hugging Face learning resources"}]},{type:a,value:" were very useful for building foundational knowledge in LLMs and understanding the terminologly and alphabet soup."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I also learned that Hugging Face python library offers language-to-language translation via it's "},{type:b,tag:o,props:{},children:[{type:a,value:"pipeline"}]},{type:a,value:" interface. This could be interesting to try, but I wasn't able to make it work with the Chinese-LLaMa-2 model."}]},{type:a,value:f},{type:b,tag:Y,props:{id:aH},children:[{type:b,tag:w,props:{href:"#retrieval-augmented-generation-rag-reading-comprehension-for-computers",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aI}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Currently there is a lot of interest in the LLM community \"chatting with your documents\", and it certainly something I am interested in. After taking a first-pass at translation and getting decent results, I started looking into what I later learned is refered to in the community as RAG. RAG stands for retrieval augmented generation. Here's my understanding of RAG:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Your body of documents (the Three-Body Problem books, in my case) is first grouped into digestable chunks (called documents)"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"These chunks are processed into embeddings which are stored in a vector database\n"},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Word embedding is an AI technique representing words and sentences in a very large vector space"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Check out this article from Microsoft for more info: "},{type:b,tag:w,props:{href:aX,rel:[F,G,H],target:I},children:[{type:a,value:aX}]}]},{type:a,value:f}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Your request to the LLM is first passed into a vector database and it pulls out certain documents (\"chunks\") that it determines are relavant to your query."}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"With addional (supposedly relevant) data from your documents, the LLM responses should be better"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I found out that you can do RAG using "},{type:b,tag:o,props:{},children:[{type:a,value:aM}]},{type:a,value:" along with a really popular open-source python library started by Harrison Chase called LangChain."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Here's how I approached building a RAG program using Langchain:"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"[Link to LangChain Notebook]"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"With my vector database full of embeddings created from the complete text of the first book, I was ready to to starting RAGing."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I wanted to use RAG to test the LLM's reading comprehnsion with the following questions:"}]},{type:a,value:f},{type:b,tag:O,props:{},children:[{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"„äôÔ∏è Which methods of predicting the momevement of the sun were used in the three body game?"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"What is Wang Miao's daughter's name?"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"What is the Frontiers of Science?"}]},{type:a,value:f},{type:b,tag:m,props:{},children:[{type:a,value:"Who is Shi Qiang?"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"There book invites a lot of deep questions about our place in the universe, but I wanted to start with some factual questions to get a sense of how well RAG works. The results from my first attempts at RAG were terrible, so I'm interested in doing more tests with different parameters, different types of prompt structures and learning more from the LangChain community about how to improve RAG results. This is all still pretty new to me! But it is great to know that there is a big community of people working on enhancing LLMs with tools and frameworks like LangChain."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"„äôÔ∏è While I was unable to get good results with my first pass in LangChain, here are some fun images I generated with InvokeAI. The prompt I used aims to describe one such attempt at predicting the movement of the sun in the three body game, which is a system of etiquette that can be used to understand patterns in celstial movement developed by Confucius, a player who appears in Wang Miao's experience in the Three-Body VR game."}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Prompt: Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:aY,props:{},children:[{type:a,value:f},{type:b,tag:aZ,props:{":count":a_,dir:"confucius"},children:[{type:a,value:f}]}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Prompt metadata:"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,"language-json"]},children:[{type:b,tag:o,props:{},children:[{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:aq}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"app_version\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"3.0.2post1\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"generation_mode\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"txt2img\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"positive_prompt\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"negative_prompt\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"width\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:a$}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"height\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:a$}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"seed\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"1748220044"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"rand_device\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"cpu\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"cfg_scale\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"10.5"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"steps\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"35"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"scheduler\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"euler_k\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"clip_skip\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"model\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:aq}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"model_name\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"stable-diffusion-2-1\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"base_model\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"sd-2\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"model_type\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"main\""}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:ar}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"controlnets\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:C},{type:b,tag:c,props:{className:[d,B]},children:[{type:a,value:"\"loras\""}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:r}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:ar}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle"}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:aY,props:{},children:[{type:a,value:f},{type:b,tag:aZ,props:{":count":ao,dir:"computer"},children:[{type:a,value:f}]}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"{\n  \"app_version\": \"3.0.2post1\",\n  \"generation_mode\": \"txt2img\",\n  \"positive_prompt\": \"array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle\",\n  \"negative_prompt\": \"\",\n  \"width\": 768,\n  \"height\": 768,\n  \"seed\": 740286719,\n  \"rand_device\": \"cpu\",\n  \"cfg_scale\": 7.5,\n  \"steps\": 50,\n  \"scheduler\": \"euler\",\n  \"clip_skip\": 0,\n  \"model\": {\n    \"model_name\": \"stable-diffusion-2-1\",\n    \"base_model\": \"sd-2\",\n    \"model_type\": \"main\"\n  },\n  \"controlnets\": [],\n  \"loras\": []\n}\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Congrats to the InvokeAI team on the 3.0 release. It has been awesome to use and the current docker compose setup is a huge improvement on the 2.x version."}]},{type:a,value:f},{type:b,tag:Y,props:{id:aJ},children:[{type:b,tag:w,props:{href:"#n-body-simulations-cuda-and-cupy",ariaHidden:R,tabIndex:S},children:[{type:b,tag:c,props:{className:[T,U]},children:[]}]},{type:a,value:aK}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"üöß Warning: this section contains errors that I'm still working out. My nbody CuPy simulation can run by I don't think the forces are calucalated properly. I dumped the simulation data into blender to model the position of three spheres and the spheres instantly shot off in different directions. üöß"}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"\"Other than Stable Eras, all times are Chaotic Eras\""}]},{type:a,value:f}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"This is one of my favorite lines from the book. It said twice by King Wen of Zhou during Wang Miao's first experience in the Three-Body game where we start to learn about the peculiarities of irregular gravity in a solar system with three suns."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"NVIDIA released CUDA was realease in 2006, back when the company's stock price was around $5, compared to its recent peak of around $450 in July 2023. If the Three-Body Problem book was written any later, it would perhaps make references to a juggernaut private enterprise that develops specialized hardware for physics simulations like the n-body problem."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The nbody problem is supposed to be unsolvable, but it can be silumated pretty easily using graphics cards. NVIDIA provides some sample code for running nbody simulations, but I wasn't able to get these samples working due to appears to be driver issues. I could write a long article about my misadventures in installing and uninstalling CUDA and NVIDIA drivers to get things working for all things related that use the GPU: stable diffusion, LLMs and CUDA programs. I have a decent understanding now of how things should be installed but I'm still slightly nervous of unintentionally breaking things in my development environment. As a DevOps engineer, Murphy's law elementary. Everything I love about containers for simplifying developer environments seems to not really apply to graphics cards and their drivers, even when you are using the "},{type:b,tag:o,props:{},children:[{type:a,value:"nvidia-container-toolkit"}]},{type:a,value:l}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"When my drivers were working correctly, I was in a \"Stable Era\" of development and both my programs and my working knowledge of GPUs evolved. Sometimes when I tried changing versions or reinstalling things or tuning parameters the wrong way, I found myself sent into a \"Chaotic Era\" where either my scripts and notebooks were left in cold, unrunable states due to driver issues, or I would get lost in the steps needed to prepare the model to run on with llama.cpp on the GPU: quantization and building with support for "},{type:b,tag:w,props:{href:"https:\u002F\u002Fwww.netlib.org\u002Fblas\u002F",rel:[F,G,H],target:I},children:[{type:a,value:"BLAS (Basic Linear Algebra Subprograms)"}]},{type:a,value:l}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"„äôÔ∏è Yang Dong is driven to suicide by her conclusion that physics no longer exist when her particle physics experiment yeilded inconsistent results due to intentional interference by an alien civilization. During \"Chaotic Eras\" of my development, the conclusion I reach that nearly drove me insane was simply that CUDA didn't exist anymore. In the end, I just needed to restart my computer. I also felt a little like Ding Yi with moving his pool table around his apartment for the expirment he demonstrates to Wang Miao."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Currently I have most things working correctly with the following configurations:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"brian@a2:~$ nvidia-smi\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage\u002FCap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n|  0%   40C    P0              62W \u002F 450W |     83MiB \u002F 24564MiB |      0%      Default |\n|                                         |                      |                  N\u002FA |\n+-----------------------------------------+----------------------+----------------------+\n"}]}]}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Thu_Nov_18_09:45:30_PST_2021\nCuda compilation tools, release 11.5, V11.5.119\nBuild cuda_11.5.r11.5\u002Fcompiler.30672275_0\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I'm still not sure why "},{type:b,tag:o,props:{},children:[{type:a,value:"nvcc"}]},{type:a,value:" (NVIDIA CUDA compiler) reads one version of CUDA (11.5) and "},{type:b,tag:o,props:{},children:[{type:a,value:"nvidia-smi"}]},{type:a,value:" reports a higher version (12.2)."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"I wrote a simple program with the help of ChatGPT for running nbody problem simulations. The program uses CuPy, a Python library that exposes APIs for doing matrix multiplication to predict the position of three bodies using Euclidian Integration. Here's the script:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,ba]},children:[{type:b,tag:o,props:{},children:[{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" numpy "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:as}]},{type:a,value:" np\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" cupy "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:as}]},{type:a,value:" cp\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" time\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:W}]},{type:a,value:" json\n\n"},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# Simulation parameters"}]},{type:a,value:"\nNUM_PARTICLES "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:a,value:"\nDIMENSIONS "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# 3D space"}]},{type:a,value:"\nNUM_STEPS "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:aT}]},{type:a,value:"\nDT "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"0.1"}]},{type:a,value:ap},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# Generate initial positions and velocities"}]},{type:a,value:"\nnp_positions "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bb},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bc},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bd},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:be},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\nnp_velocities "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bb},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bc},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bd},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:be},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\ncp_positions "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bf},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:bg},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"np_velocities"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\nnp_ticks "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bh},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bf},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\ncp_ticks "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bi},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:ap},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# nbody simulation loop"}]},{type:a,value:"\nstart_time "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:bj},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bk},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:aV}]},{type:a,value:" step "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:aW}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:"range"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"NUM_STEPS"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# this gets pairwise differences"}]},{type:a,value:"\n    diff "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:bl},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,ai]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:a,value:bl},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,ai]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:"\n    distances "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"sqrt"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"cp"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:bm}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:au},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:bn}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bo}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# avoid division by zero"}]},{type:a,value:"\n    epsilon "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bp}]},{type:a,value:"\n    inv_distances "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bq}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:br}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bs},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bt},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:bu},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# calculate forces"}]},{type:a,value:"\n    cp_forces "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:bm}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:au},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"T "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:aa}]},{type:a,value:" inv_distances"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:bn}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"T"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:at},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bv}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:bw}]},{type:a,value:"\n    cp_velocities "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:ak}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:aa}]},{type:a,value:" cp_forces\n    cp_positions "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:ak}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:aa}]},{type:a,value:" cp_velocities\n    cp_ticks "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"append"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"cp_ticks"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bh},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"cp_positions"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\nsim_time "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:bj},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bk},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:a,value:" start_time\n"},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:ae}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"Simulation time:\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:" sim_time"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:bx},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:"class"}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,"class-name"]},children:[{type:a,value:by}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bz},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bA},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:$},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:"def"}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,"function"]},children:[{type:a,value:bB}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bC},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:av},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:"if"}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:"isinstance"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:"obj"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:af},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"ndarray"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:"\n            "},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:bD}]},{type:a,value:av},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"tolist"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:bD}]},{type:a,value:" json"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bA},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bB},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bC},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:av},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\n\nnp_ticks "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:" cp_ticks"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"get"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:bx},{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# this is data we can work with in python and write to a file"}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:"with"}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,Z]},children:[{type:a,value:"open"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"ticks.json\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,A]},children:[{type:a,value:"\"w\""}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:as}]},{type:a,value:" f"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:a,value:"\n    f"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"write"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bz},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:"dumps"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bi},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:" cls"},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:by},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"To better understand the matrix math here I walked through a simple example of what each step does:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,ba]},children:[{type:b,tag:o,props:{},children:[{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:"# particle coordinates (x,y,z) in 3D space"}]},{type:a,value:"\npositions "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bv}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ao}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"5"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"7"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:a_}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:"9"}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The first operation creates an array for pairwise distances for each dimension:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,_]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"diff "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:bE},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,ai]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:a,value:bE},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,ai]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:r}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:a,value:f},{type:b,tag:c,props:{className:[d,s]},children:[{type:a,value:ae}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:au},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:"\n\narray"},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:bG},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:aw}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:bG},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bF}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:ab}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:E}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:Q},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:t}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:L},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:D}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:u}]},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The rows of zeros correspond to a particle's "},{type:b,tag:o,props:{},children:[{type:a,value:"x"}]},{type:a,value:aL},{type:b,tag:o,props:{},children:[{type:a,value:"y"}]},{type:a,value:" and "},{type:b,tag:o,props:{},children:[{type:a,value:"z"}]},{type:a,value:" distances to itself, which are all zero by axioms of Euclidian vector spaces."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The next operation calculates the distance between each particle:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"distances = cp.sqrt(cp.sum(diff**2, axis=2))\nprint(distances)\n\narray([[ 0.        ,  4.9244289 , 10.11187421],\n       [ 4.9244289 ,  0.        ,  5.19615242],\n       [10.11187421,  5.19615242,  0.        ]])\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The diagonal or zeros represents that fact that a particle "},{type:b,tag:o,props:{},children:[{type:a,value:"n"}]},{type:a,value:" has a distance of zero to iself."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The next step calculates inverse distances:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,_]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"epsilon "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bp}]},{type:a,value:"\ninv_distances "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:n}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,k]},children:[{type:a,value:bq}]},{type:a,value:g},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:br}]},{type:a,value:P},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:l}]},{type:a,value:bs},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:p}]},{type:a,value:bt},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:j}]},{type:a,value:bu},{type:b,tag:c,props:{className:[d,e]},children:[{type:a,value:q}]},{type:a,value:f}]}]}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"array([[1.00000000e+05, 2.03069233e-01, 9.88936353e-02],\n       [2.03069233e-01, 1.00000000e+05, 1.92450090e-01],\n       [9.88936353e-02, 1.92450090e-01, 1.00000000e+05]])\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The next step is the most elegant part of the simulation in my opinion:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,M]},children:[{type:b,tag:o,props:{},children:[{type:a,value:"cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The "},{type:b,tag:o,props:{},children:[{type:a,value:".T"}]},{type:a,value:" operation transposes a matrix, multiplies by the cube of inverse distances, then transposes the matrix again before summing along the first axis. Transposing a matrix basically swaps rows and columns."}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"The next two steps are also pretty elegant:"}]},{type:a,value:f},{type:b,tag:v,props:{className:[x]},children:[{type:b,tag:y,props:{className:[z,_]},children:[{type:b,tag:o,props:{},children:[{type:b,tag:c,props:{className:[d,N]},children:[{type:a,value:bw}]},{type:a,value:bg},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:ak}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:aa}]},{type:a,value:" cp_forces\ncp_positions "},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:ak}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,i]},children:[{type:a,value:aa}]},{type:a,value:" cp_velocities\n"}]}]}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"In the last step I append the updated positions to an array that holds every \"tick\" (the positions of each particle between each time interval, "},{type:b,tag:o,props:{},children:[{type:a,value:"DT"}]},{type:a,value:" - \"delta time\")"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"A simpler way to view particle physics can be acheived with Three.js:"}]},{type:a,value:f},{type:b,tag:ac,props:{src:"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Fthree\u002F",width:ad,height:350},children:[]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"Three.js and WebGL and still feels like alien technologies to me. It doesn't feel like these simulations should be run so fluidly on my computer in the browser. Wang Miao's comments about hidden data in the Three-Body game resonated with me when I came across this Three.js example:"}]},{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"„äôÔ∏è"}]},{type:a,value:f},{type:b,tag:J,props:{},children:[{type:a,value:f},{type:b,tag:h,props:{},children:[{type:a,value:"„Ää‰∏â‰Ωì„ÄãÊ≠£ÊòØËøôÊ†∑ÔºåÂÆÉÁöÑÊµ∑Èáè‰ø°ÊÅØÊòØÈöêËóèÂú®Ê∑±Â§ÑÁöÑÔºåÊ±™Ê∑ºËÉΩÊÑüËßâÂà∞Ôºå‰ΩÜËØ¥‰∏çÊ∏Ö„ÄÇ‰ªñÁ™ÅÁÑ∂ÊÇüÂá∫Ôºå„Ää‰∏â‰Ωì„ÄãÁöÑ‰∏çÂØªÂ∏∏Âú®‰∫éÔºå‰∏éÂÖ∂‰ªñÁöÑÊ∏∏ÊàèÁõ∏ÊØîÔºåÂÆÉÁöÑËÆæËÆ°ËÄÖÊòØÂèçÂÖ∂ÈÅìËÄåË°å‰πã‚Äî‚Äî‰∏ÄËà¨Ê∏∏ÊàèÁöÑËÆæËÆ°ËÄÖÈÉΩÊòØÂ∞ΩÂèØËÉΩÂú∞Â¢ûÂä†ÊòæÁ§∫ÁöÑ‰ø°ÊÅØÈáèÔºå‰ª•‰∫ßÁîüÁúüÂÆûÊÑüÔºõ‰ΩÜ„Ää‰∏â‰Ωì„ÄãÁöÑËÆæËÆ°ËÄÖÂç¥ÊòØÂú®ÊûÅÂäõÂéãÁº©‰ø°ÊÅØÈáèÔºå‰ª•ÈöêËóèÊüêÁßçÂ∑®Â§ßÁöÑÁúüÂÆûÔºåÂ∞±ÂÉèÈÇ£Âº†Áúã‰ººÁ©∫Êó∑ÁöÑÂ§©Á©∫ÁÖßÁâá„ÄÇ"}]},{type:a,value:f}]}]},dir:"\u002F2023\u002F08\u002F27",path:"\u002F2023\u002F08\u002F27\u002Fpython-vue-chinese-llama-2-and-the-three-body-problem",extension:".md",createdAt:"2023-08-30T03:58:38.255Z",updatedAt:"2023-08-30T03:58:38.259Z",raw:"\n# tl;dr\n\nThis articles brings together several of my interest, both old and new:\n\n- The Chinese Sci-Fi book series 'Three-Body Problem'\n- Chinese language\n- NLP techniques\n- AI\n- Large Language Models (LLMs)\n- Stable Diffusion\n- Data visualization and 3D graphics\n- Mathematics\n- NVIDIA \u002F CUDA\n\nI translated 'The Three-Body Problem' into English using Meta's LLaMa 2, `llama.cpp`, `llama-cpp-python` and the [`Chinese-LLaMa-2` model from Hugging Face](https:\u002F\u002Fhuggingface.co\u002Fziqingyang\u002Fchinese-llama-2-7b). I implemented a basic Retrieval Augmented Generated (RAG) program using LangChain with a local vector database that allowed me to ask the LLM questions about the book. To visualize scenes from the book I wrote prompts for Stable Diffusion. Finally, I dipped my toes into nbody simulations with both CuPy (Python library for CUDA) and Three.js. This article will walk through code samples and give more background on the tools, libraries and languages used.\n\nWhen discussing particulars of the Three Body novel series, I will add the emoji for the Chinese character for \"secret\" „äôÔ∏è (Áßò).\n\n\u003E The \"„äô\" emote, also known as the \"Chinese Character\" emote, is a symbol used in Japanese culture to indicate secrecy or confidentiality. It is derived from the Chinese character \"Áßò\" (m√¨), which means \"secret.\" In online communication, it is often used to suggest that something should be kept private or not discussed openly.\n\n## Back story\n\nAbout a month ago my company announced that another round of layoffs was coming the following week. I'm on an engineering team that had already been impacted by two rounds of layoffs in the past year, and I was fully expecting to be let go this time. In an impulse-buy I ordered a book at the top of my reading list from Amazon: \"The Three-Body Problem\". It is a Sci-Fi trilogy written by Liu Cixin, a Chinese computer engineer who started writing the book as a series of essays that were published in China's \"World of Sci-Fi\" publication. I bought the original trilogy of books written in Chinese as well as the English translation, six books in total.\n\n![Images of Three Body Problem Book Series](\u002Fimg\u002Fthree-body-problem\u002Fbooks.png)\n\nI started learning Chinese in college, adding a major in Chinese Language to the mathematics major I decided on in my sophomore year. I did an exchange program through my college with Fudan University, a top Chinese University. In 2007, living and studying Chinese in Shanghai as a 19 year old American was a really fun time. I was placed in an advanced-level course with a diverse group of students where English was not the lowest common linguistic denominator. We had a demanding cirriculum that emphasized reading, listening and speaking Chinese. Alcohol, food and partying was an effective catalyst for absorbing the Chinese language. I also took a course on Differential Equations with my college's mathematics department chair who was on sebatical at Fudan during the same time.\n\n![3-6-9 drinking game rules](\u002Fimg\u002Fthree-body-problem\u002F3-6-9.jpeg)\n\nMy favorite memory of that semester at Fudan was travelling on an epic over-night sleeper train from Shanghai to Guangxi province with a school-sponsored class trip to see Guilin. Multiple games of sam-yuk-gu (3-6-9) ran in parallel across the matrix of 3-by-2 sleeper car bunk beds like workloads distributed across multiple GPU cores. The rules of 3-6-9 are simple: sit in a circle counting up from 1. If your number contains a 3, 6 or 9, you clap once for each occurance of the number your hands instead of saying your number. The first person to break the rules takes a drink. Our volume level swelled to a fever pitch as the drinking games created a feedback loop that rapidly diminished our alcohol supply. Everyone passed out later in the evening as the train pushed forward through the night. The next morning we all boarded a river cruise in a daze to see Guilin's stunning limestone peaks:\n\n![20 yuan note with Guilin rock formations](\u002Fimg\u002Fthree-body-problem\u002F20_yuan_note.jpeg)\n\nAfter college, my second job took me back to China where I specialized in the technologies, policies and applications of large scale battery storage systems for use in renewable energy projects. I worked for a man in Beijing who founded China's energy storage industry association after pioneering the commercialization of vinadium redox flow batteries for renewable energy projects. The job exposed me to exciting battery projects all over China, and also sharpened my technical Chinese skills as I was frequently reading, translating and intepreting in a bi-linguagl, hi-tech environment. At one of the annual industry conferences our organization hosted, I had the opportunity to be an interpreter for a conversation between American battery entrepreurs and representative from China's State Grid. State Grid is China's state-owned power utility and was the world's 3rd largest company by revenue in 2022 behind Walmart and Amazon.\n\n![State Grid HQ in Xi Cheng](\u002Fimg\u002Fthree-body-problem\u002Finvokeai\u002Fcastles.png)\n\nMy first introduction to the 'Three-Body' book came from one of my very best friends from college. He lived at the inner-most leaf-node of one of Beijing's more labrythnian hutongs next to a family that raised and trained pigeons. Visiting my friend where he lived often involved exchanges with his pigeon master neighbor in the narrow alleys of their corner of the hutong. In one such exchange the neighbor quipped to me with a croaking voice: ‰ªñÊÄï‰Ω†Áê¥ÔºÅ, or \"he's afraid of your guitar!\". The pigeons cooing at my were apparently fearful of the backpack case for my classical guitar which extended over my head by about a foot, an unwelcomed advance into their roost! This simple pleasentry became a refrain that echoed every time we joked about life in the hutong. My friend and I bonded over Chinese language, classical guitar among many other things. I strongly considered his recommendation to check out ‰∏â‰Ωì (Three Body), the Chinese Sci-Fi novel about alien life in a solar system with three stars as he described it, but with already lots going on in Beijing I never had the chance to check out the book.\n\n***\n\n\u003Ciframe width=\"100%\" height=\"315\" src=\"https:\u002F\u002Fwww.youtube.com\u002Fembed\u002F5lj99Uz1d50?si=TwrypbY4vTfeWGRf\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003E\u003C\u002Fiframe\u003E\n\nAlmost 10 years later I came across across a preview for the Netflix production of \"3 Body Problem\" scheduled to come out in early 2024. With the possibility of loosing my job weighing heavily on me, I picked up the books on Amazon on an impulse buy hoping to have something to do if I was layed off this time. I ended up reading the first book on my Kindle. Reading Chinese on the kindle is nice because your can use built in dictionaries and Google translate for looking up difficult passages with just a few taps on the Kindle or Kindle iPad app.\n\nEven though these look ups are simple, it can still be a distraction to have to do lots of look ups for certain chapters or passages that were hard to follow without doing so. I got the idea to build a simple online reader with built-in translation of each word. I scraped the Chinese text of the book from an online Â±±ÂØ®Áâà (a term from Shen Zhen--the heart of China's electronics industry--which referes to an unofficial version of something, a copy-cat or knock-off). This online version of the book contains some simple watermarks that I was able to remove, and I noticed that some words are rarely replaced with incorrect synonym characters, for examlpe:\n\n\u003E Âç±Èô© would be written Âç±ËÑ∏, this jumped out at me as an obvious intentional typo. The first character of the word \"dangerous\" Âç± is the same, but in the second character, the phonetic component on the right side of the character is the same (‰Ω•), but the left side is replaced with another radical Êúà Ôºàüà∑Ô∏èÔºâ which means moon or flesh, making the meaning of the character ËÑ∏ \"face\", not danger or Èô©.\n\nFor the most part the text from the website seemed to be a faithful reproduction of the book. I found two open source projects that could be use to parse text and translate words:\n\n- CC-CEDICT\n    - an open-source Chinese-\u003EEnglish dictionary\n    - [https:\u002F\u002Fwww.mdbg.net\u002Fchinese\u002Fdictionary?page=cedict](https:\u002F\u002Fwww.mdbg.net\u002Fchinese\u002Fdictionary?page=cedict)\n- Chinese Natural Language Processing (spaCy) (NTNU)\n    - [https:\u002F\u002Falvinntnu.github.io\u002Fpython-notes\u002Fnlp\u002Fnlp-spacy-zh.html](https:\u002F\u002Falvinntnu.github.io\u002Fpython-notes\u002Fnlp\u002Fnlp-spacy-zh.html)\n\nThe spaCy tool parses words which in Chinese are typically two characters long, and sometimes three or four characters in length. The Chinese -\u003E English dictionary contains tranlsations both for both individual Chinese characters and multiple character words. I wrote some Python scripts to loop over each paragraph of each chapter of each book and parse then Chinese words and look up definitions from them in the Chinese dictonary with over 150K entries. I saved each chapter as a file containing the parsed text and definitions as well as pinyin. I also added the frequency rank for each character to visualize the \"difficulty\" of reading certain passages. The darker the background of an individual character, the less frequent the character occurs in written Chinese. Hovering over a word you can see the meaning of an individual character as well as the word or \"linguistic token\" that the NLP software parsed.\n\nI built a simple UI to parse and display the text and related \"metadata\" using Vue, a popular library that rival the dominant UI framework that Facebook, now Meta, is famous for open sourcing. I love Vue's simple API and have had a lot of luck getting it to do what I want it to do.\n\nI'm also a big fan of Vue's author and BDFL, Evan You. His chinese name is Â∞§Èõ®Ê∫™ ÔºàYou YuXiÔºâ. The word \"You\" in Chinese is pronounced \"yo\" as in \"yo-yo\", but Evan seems to have adopted the pronciation \"you\" as in \"you and me\" which in Chinese pinyin would actually be \"yu\". Chinese and English in my experience are totally butchered. For example my name is \"Brian\" but it is more than not misspelled as \"Brain\" but native Chinese speakers. ü§∑‚Äç‚ôÇÔ∏è So I typically adopt the phonetic translation of Brian: Â∏ÉËé±ÊÅ© (Bu Lai En) or Â∞èÂ∏É (Little Bu).\n\nHere are some quick facts about the first book in the series:\n\n- 36 chapters in the book\n- 188380 total charactes in the book\n- average of 69.78 chapters per paragraph\n- 2859 unique characters in the book (TODO: look this up)\n- total of 2512 in the book\n- average of 74.99 characters per paragraph\n\n![Character Frequency](\u002Fimg\u002Fthree-body-problem\u002Fchar_freq.png)\n\n\u003Cdiv class='wrap'\u003E\n\u003Ciframe class=\"p-4\" src=\"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Ffreq\u002F\" width=100% height=550\u003E\u003C\u002Fiframe\u003E\n\u003C\u002Fdiv\u003E\n\n## LLMs, Meta's LLaMA 2 and Chinese-LLaMA-2\n\nIn recent months I have been following the development of some interesting open source AI projects. Two projects in particular are InvokeAI, an image generation tool based on stable diffusion, and [LLaMA 2](https:\u002F\u002Fai.meta.com\u002Fllama\u002F), the next generation of Meta's open source large language model (LLM). The casing of the word `LLaMA` indicates that LLaMa 2 takes its name from the acronym LLM (Large Language Model), which is a general term for the technology that underpins the massively popular ChatGPT from OpenAI. LLMs are so hot right now. Before going deeper into LLMs we need a quick Chinese lesson.\n\n\u003E ËçâÊ≥•È©¨ is a non-technical word that referes to animals like Llama or Alpaca. It can be directly translated as \"Grass Mud Horse\" it is phonetically similar to the most common Chinese profanity: Êìç‰Ω†Â¶à, which literally means \"fuck your mother\". The characters in the two words are nearly synonymous: the sounds of both words are \"cao ni ma\", but the tones are different, which in Chinese changes the meaning completely. In English this would be like two words spelled the same way that have emphsis placed on different syllables. The llama is basically a legendary Chinese internet meme subversive in the face of Chinese censorship. ü¶ô was approved as part of Unicode 11.0 in 2018.\n\nThere's a lot more to say about the significance of the Llama in Chinese internet culture, have a look at [the Wikipedia article on **Grass Mud Horse**](https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGrass_Mud_Horse) if you want to go deeper. I think it is safe to assume that the naming of LLaMa has something to do with Facebook\u002FMeta's hacker mentality and the large number of Chinese people working on AI at Meta. This mostly likely is OK with Mark Z now since [his attitude toward China has changed](https:\u002F\u002Fwww.voanews.com\u002Fa\u002Fsilicon-valley-technology_how-facebooks-zuckerberg-went-courting-criticizing-beijing\u002F6195455.html) in the last few years from one of courtship to one of criticism.\n\nI requested access to Meta's LLaMa 2 models as soon as they came out and was able to get the models to run on my computer. I have an I9-13900K CPU and an NVIDIA 4090 graphics card which I was able to get a hold of in Febrauary after resellers had made these cards almost impossible to buy from major retailers. I also started lurking in a subreddit called [`r\u002FLocalLLaMa`](https:\u002F\u002Fwww.reddit.com\u002Fr\u002FLocalLLaMA\u002F) with over fifty thousand members. The forum has a lot of helpful information about running Large Language Models on computer hardware like mine which is most commonly used play computer games. Another annoucement that caught my attention in July was the release of [Chinese LLaMa 2](https:\u002F\u002Fgithub.com\u002Fymcui\u002FChinese-LLaMA-Alpaca-2), a Large Language Model trained on Chinese and English which does very well against Chinese Language Benchmarks.\n\n![image of Chinese LLaMa 2](\u002Fimg\u002Fthree-body-problem\u002Fchinese_llama_2.png)\n\nhttps:\u002F\u002Fgithub.com\u002Fhaonan-li\u002FCMMLU\n\n![image of CMMLU](\u002Fimg\u002Fthree-body-problem\u002Fcmmlu.jpeg)\n\n## Translation\n\n„äôÔ∏è From the documents of the the Red Coast Base we learn about the plans to develop a universal language based on math and physics that can be understood by any sufficiently advanced civilization. There is also a plan to build a full linguistic system on top of this universal language which effectively allows for communication with alien civilizations in Chinese and Esperanto.\n\nThe books idea of universal translation made me think a lot about LLMs that are so popular today. In essence they are calculators for words. When you feed a prompt to an LLM, it first puts the prompt through a process called tokenization. Tokenization takes a string of text and breaks it down into tokens (defined by the Large Language Model you are using). These tokens are numbers. Here's an example of tokenization in action:\n\n```python\nimport json\nimport os\nfrom llama_cpp import Llama, LlamaTokenizer\n\nllm = Llama(\n    model_path=\"\u002Fpath\u002Fto\u002Fmodels\u002Fggml-model-q4_0.bin\",\n    n_ctx=4096,\n    n_gpu_layers=30\n)\n\ntokenizer = LlamaTokenizer(llama=llm)\n\nTEXT=\"Âú®ÈÇ£‰∏™Â∑≤Ë¢´ÂøòÂç¥ÁöÑÊó•Â≠êÈáåÔºåÂÆÉÁöÑ‰∏ñÁïåÈ¢†Ë¶Ü‰∫Ü„ÄÇÊ≥•ÂúüÈ£ûËµ∞ÔºåÂá∫Áé∞‰∫Ü‰∏ÄÊù°ÂèàÊ∑±ÂèàÂÆΩÁöÑÂ≥°Ë∞∑ÔºåÁÑ∂ÂêéÊ≥•ÂúüÂèàËΩ∞ÈöÜÈöÜÂú∞È£ûÂõûÊù•ÔºåÂ≥°Ë∞∑Ê∂àÂ§±‰∫ÜÔºåÂú®ÂéüÊù•Â≥°Ë∞∑ÁöÑÂ∞ΩÂ§¥Âá∫Áé∞‰∫Ü‰∏ÄÂ∫ßÈªëËâ≤ÁöÑÂ≠§Â≥∞„ÄÇÂÖ∂ÂÆûÔºåÂú®ËøôÁâáÂπøÈòîÁöÑÁñÜÂüü‰∏äÔºåËøôÁßç‰∫ãÂ∏∏Â∏∏ÂèëÁîüÔºåÊ≥•ÂúüÈ£ûËµ∞ÂèàÈ£ûÂõûÔºåÂ≥°Ë∞∑Âá∫Áé∞ÂèàÊ∂àÂ§±ÔºåÁÑ∂ÂêéÊòØÂ≠§Â≥∞Èôç‰∏¥ÔºåÂ•ΩÂÉèÊòØÁªôÊØèÊ¨°ÁÅæÂèòÊâì‰∏ä‰∏Ä‰∏™ÈÜíÁõÆÁöÑÊ†áËÆ∞„ÄÇË§êËöÅÂíåÂá†Áôæ‰∏™ÂêåÊóèÂ∏¶ÁùÄÂπ∏Â≠òÁöÑËöÅÂêéÂêëÂ§™Èò≥ËêΩ‰∏ãÁöÑÊñπÂêëËµ∞‰∫Ü‰∏ÄÊÆµË∑ØÔºåÂª∫Á´ã‰∫ÜÊñ∞ÁöÑÂ∏ùÂõΩ„ÄÇ\"\ntokens = tokenizer.encode(TEXT)\n```\n\n```\nprint(str(tokens[:4]) + \" ...\")\n```\n\n\u003E [1, 30505, 32380, 36812] ...\n\n```\nfor token in tokens:\n    text = tokenizer.decode([token])\n    print(text, end=\" \")\n```\n\n\u003E  Âú® ÈÇ£‰∏™ Â∑≤Ë¢´ Âøò Âç¥ ÁöÑÊó•Â≠ê Èáå Ôºå ÂÆÉÁöÑ ‰∏ñÁïå È¢†Ë¶Ü ‰∫Ü „ÄÇ Ê≥• Âúü È£û Ëµ∞ Ôºå Âá∫Áé∞‰∫Ü ‰∏ÄÊù° Âèà Ê∑± Âèà ÂÆΩ ÁöÑ Â≥°Ë∞∑ Ôºå ÁÑ∂Âêé Ê≥• Âúü Âèà ËΩ∞ ÈöÜ ÈöÜ Âú∞ È£û ÂõûÊù• Ôºå Â≥°Ë∞∑ Ê∂àÂ§± ‰∫Ü Ôºå Âú® ÂéüÊù• Â≥°Ë∞∑ ÁöÑ Â∞ΩÂ§¥ Âá∫Áé∞‰∫Ü ‰∏ÄÂ∫ß ÈªëËâ≤ ÁöÑ Â≠§ Â≥∞ „ÄÇ ÂÖ∂ÂÆû Ôºå Âú®Ëøô Áâá ÂπøÈòî ÁöÑ ÁñÜ Âüü ‰∏ä Ôºå ËøôÁßç‰∫ã Â∏∏Â∏∏ ÂèëÁîü Ôºå Ê≥• Âúü È£û Ëµ∞ Âèà È£û Âõû Ôºå Â≥°Ë∞∑ Âá∫Áé∞ Âèà Ê∂àÂ§± Ôºå ÁÑ∂Âêé ÊòØ Â≠§ Â≥∞ Èôç‰∏¥ Ôºå Â•ΩÂÉèÊòØ Áªô ÊØèÊ¨° ÁÅæ Âèò Êâì ‰∏ä ‰∏Ä‰∏™ ÈÜíÁõÆ ÁöÑ Ê†áËÆ∞ „ÄÇ Ë§ê ËöÅ Âíå Âá†Áôæ ‰∏™ Âêå Êóè Â∏¶ÁùÄ Âπ∏ Â≠ò ÁöÑ ËöÅ Âêé Âêë Â§™Èò≥ ËêΩ ‰∏ãÁöÑ ÊñπÂêë Ëµ∞‰∫Ü ‰∏ÄÊÆµ Ë∑Ø Ôºå Âª∫Á´ã‰∫Ü Êñ∞ÁöÑ Â∏ùÂõΩ „ÄÇ\n\n```python\nenglish_text = \"This is an example of tokenization using a large language model.\"\nenglish_tokens = tokenizer.encode(english_text)\nprint(str(english_tokens[:4]) + \" ...\")\n\nfor token in english_tokens:\n    text = tokenizer.decode([token])\n    print(f\"'{text}'\", end=\" \")\n```\n\n\u003E [1, 4013, 338, 385] ...\n\n\u003E '' 'This' ' is' ' an' ' example' ' of' ' token' 'ization' ' using' ' a' ' large' ' language' ' model' '.'\n\nThis is a good time to talk about some of the obvious differences between Chinese and English.\n\n- The Chinese does not use spaces between words like English does\n- Chinese words are typically formed from 2 or more characters\n- Chinese does not conjugate verbs\n- Chinese characters not have capitization like ASCII characters\n\n(TODO: compare how other LLMs tokenize Chinese text.)\n\nAfter using basic NLP techniques to translate individual words, I did some basic prompt engineering to get the LLM to translate the books in the Three-Body problem paragraph by paragraph. My computer was able to translate the first book overnight in under 500 minutes. Here's the prompt I used:\n\n```\ncompletion_prompt = f\"### Chinese:\\n‰Ω†Â•Ω\\n\\n### English:\\nHello\\n\\n### Chinese:\\n${text}\\n\\n### English:\\n\"\n```\n\n\nMy initial impression of the Chinese LLaMa 2's accuracy at translation tasks is that it is almost unbelievably good, with the occasional occurance of task failure include some of these failure modes:\n\n- responding to a request with the same Chinese text that I asked it to translate\n- translating the only the first sentence of a multi-sentence paragraph\n- not translating the text at all\n- Translating most of the text to English while some words are left not translated in the response: For example: Ëøô‰ΩçÊòØÁ∫¢Â≤∏Âü∫Âú∞ÁöÑÈõ∑ÂøóÊàêÊîøÂßî -\u003E This is Red Bank Base's Li Zhi-chengÊîøÂßî. ÊîøÂßî is an abreviation of Political commissar (ÊîøÊ≤ªÂßîÂëò) In the military, a political commissar or political officer is a supervisory officer responsible for the political education and organization of the unit to which they are assigned, with the intention of ensuring political control of the military.\n- responding only with \"```\" (this may have something to do with the prompt I used)\n- Butchering names with different translations offered between paragraphs, particularly ÁôΩÊ≤êÈúñ which it translated as White Moxin, White Mullin, White Mow Lin. None of the varieties used the pinyin translation of the characters: Bai Mu Lin. This isn't necessarily a failure of the translation, just interesting to see this type of variation across translated paragraphs.\n- Some of the dialog started with the Chinese word for subtitles (Â≠óÂπïÔºö), which was not included as part of the translation, also understandable\n\nIt was nice to have the English book as a reference for qualitative analysis of the translation tasks I gave to the Chinese LLaMa 2 model. I look forward to reading the second and third books with all characters, words and paragraphs fully translated (for the most part).\n\nHere are the results of my translation of Three-Body Problem with Chinese-Alpaca-2 7 Billion parameter model from Hugging Face with 4 bit quantization:\n\n\u003Ciframe class=\"p-4\" src=\"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Freader\u002F?book=three_body&chapterNumber=3\" width=100% height=550\u003E\u003C\u002Fiframe\u003E\n\n\nI used `nvtop` to monitor GPU usage:\n\n[image of nvtop]\n\nHere are some of the statistics from individual paragraph translations:\n\n```\n# model initialization\nllama.cpp: loading model from \u002Fhome\u002Fbrian\u002Fgithub\u002Fllama.cpp\u002Fmodels\u002F7B\u002FChinese-Alpaca-2\u002Fggml-model-q4_0.bin\nllama_model_load_internal: format     = ggjt v3 (latest)\nllama_model_load_internal: n_vocab    = 55296\nllama_model_load_internal: n_ctx      = 4096\nllama_model_load_internal: n_embd     = 4096\nllama_model_load_internal: n_mult     = 5504\nllama_model_load_internal: n_head     = 32\nllama_model_load_internal: n_head_kv  = 32\nllama_model_load_internal: n_layer    = 32\nllama_model_load_internal: n_rot      = 128\nllama_model_load_internal: n_gqa      = 1\nllama_model_load_internal: rnorm_eps  = 1.0e-06\nllama_model_load_internal: n_ff       = 11008\nllama_model_load_internal: freq_base  = 10000.0\nllama_model_load_internal: freq_scale = 1\nllama_model_load_internal: ftype      = 2 (mostly Q4_0)\nllama_model_load_internal: model size = 7B\nllama_model_load_internal: ggml ctx size =    0.08 MB\nllama_model_load_internal: using CUDA for GPU acceleration\nllama_model_load_internal: mem required  = 4299.79 MB (+ 2048.00 MB per state)\nllama_model_load_internal: offloading 0 repeating layers to GPU\nllama_model_load_internal: offloaded 0\u002F35 layers to GPU\nllama_model_load_internal: total VRAM used: 512 MB\nllama_new_context_with_model: kv self size  = 2048.00 MB\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\n```\n\n`BLAS = 1` means that it is able to use the GPU, we also see `using CUDA for GPU acceleration`.\n\n```\n# translation task logs\nllama_print_timings:        load time =   640.04 ms\nllama_print_timings:      sample time =    20.00 ms \u002F    45 runs   (    0.44 ms per token,  2249.66 tokens per second)\nllama_print_timings: prompt eval time =   640.00 ms \u002F    77 tokens (    8.31 ms per token,   120.31 tokens per second)\nllama_print_timings:        eval time =  3898.99 ms \u002F    44 runs   (   88.61 ms per token,    11.28 tokens per second)\nllama_print_timings:       total time =  4631.62 ms\n```\n\n## ü§ó Hugging Face\n\nGoing over the [Hugging Face learning resources](https:\u002F\u002Fhuggingface.co\u002Flearn) were very useful for building foundational knowledge in LLMs and understanding the terminologly and alphabet soup.\n\nI also learned that Hugging Face python library offers language-to-language translation via it's `pipeline` interface. This could be interesting to try, but I wasn't able to make it work with the Chinese-LLaMa-2 model.\n\n## Retrieval Augmented Generation (RAG): Reading Comprehension for Computers\n\nCurrently there is a lot of interest in the LLM community \"chatting with your documents\", and it certainly something I am interested in. After taking a first-pass at translation and getting decent results, I started looking into what I later learned is refered to in the community as RAG. RAG stands for retrieval augmented generation. Here's my understanding of RAG:\n\n- Your body of documents (the Three-Body Problem books, in my case) is first grouped into digestable chunks (called documents)\n- These chunks are processed into embeddings which are stored in a vector database\n    - Word embedding is an AI technique representing words and sentences in a very large vector space\n    - Check out this article from Microsoft for more info: https:\u002F\u002Flearn.microsoft.com\u002Fen-us\u002Fsemantic-kernel\u002Fmemories\u002Fembeddings#what-are-embeddings-to-a-programmer\n- Your request to the LLM is first passed into a vector database and it pulls out certain documents (\"chunks\") that it determines are relavant to your query.\n- With addional (supposedly relevant) data from your documents, the LLM responses should be better\n\nI found out that you can do RAG using `llama-cpp-python` along with a really popular open-source python library started by Harrison Chase called LangChain.\n\nHere's how I approached building a RAG program using Langchain:\n\n[Link to LangChain Notebook]\n\nWith my vector database full of embeddings created from the complete text of the first book, I was ready to to starting RAGing.\n\nI wanted to use RAG to test the LLM's reading comprehnsion with the following questions:\n\n- „äôÔ∏è Which methods of predicting the momevement of the sun were used in the three body game?\n- What is Wang Miao's daughter's name?\n- What is the Frontiers of Science?\n- Who is Shi Qiang?\n\nThere book invites a lot of deep questions about our place in the universe, but I wanted to start with some factual questions to get a sense of how well RAG works. The results from my first attempts at RAG were terrible, so I'm interested in doing more tests with different parameters, different types of prompt structures and learning more from the LangChain community about how to improve RAG results. This is all still pretty new to me! But it is great to know that there is a big community of people working on enhancing LLMs with tools and frameworks like LangChain.\n\n„äôÔ∏è While I was unable to get good results with my first pass in LangChain, here are some fun images I generated with InvokeAI. The prompt I used aims to describe one such attempt at predicting the movement of the sun in the three body game, which is a system of etiquette that can be used to understand patterns in celstial movement developed by Confucius, a player who appears in Wang Miao's experience in the Three-Body VR game.\n\n\u003E Prompt: Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style\n\n\u003Cclient-only\u003E\n\u003Ccarousel :count=\"8\" dir=\"confucius\" \u002F\u003E\n\u003C\u002Fclient-only\u003E\n\n\u003E Prompt metadata:\n\n```json\n{\n  \"app_version\": \"3.0.2post1\",\n  \"generation_mode\": \"txt2img\",\n  \"positive_prompt\": \"Ceremonies and etiquette system related to the sun and multiple celestial++ bodies Confucius artistic style\",\n  \"negative_prompt\": \"\",\n  \"width\": 768,\n  \"height\": 768,\n  \"seed\": 1748220044,\n  \"rand_device\": \"cpu\",\n  \"cfg_scale\": 10.5,\n  \"steps\": 35,\n  \"scheduler\": \"euler_k\",\n  \"clip_skip\": 0,\n  \"model\": {\n    \"model_name\": \"stable-diffusion-2-1\",\n    \"base_model\": \"sd-2\",\n    \"model_type\": \"main\"\n  },\n  \"controlnets\": [],\n  \"loras\": []\n}\n```\n\n\u003E array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle\n\n\u003Cclient-only\u003E\n\u003Ccarousel :count=\"4\" dir=\"computer\" \u002F\u003E\n\u003C\u002Fclient-only\u003E\n\n```\n{\n  \"app_version\": \"3.0.2post1\",\n  \"generation_mode\": \"txt2img\",\n  \"positive_prompt\": \"array of chinese++ warriors++ on a electronics+ circuit+ board qing+ dynasty style art logic puzzle\",\n  \"negative_prompt\": \"\",\n  \"width\": 768,\n  \"height\": 768,\n  \"seed\": 740286719,\n  \"rand_device\": \"cpu\",\n  \"cfg_scale\": 7.5,\n  \"steps\": 50,\n  \"scheduler\": \"euler\",\n  \"clip_skip\": 0,\n  \"model\": {\n    \"model_name\": \"stable-diffusion-2-1\",\n    \"base_model\": \"sd-2\",\n    \"model_type\": \"main\"\n  },\n  \"controlnets\": [],\n  \"loras\": []\n}\n```\n\nCongrats to the InvokeAI team on the 3.0 release. It has been awesome to use and the current docker compose setup is a huge improvement on the 2.x version.\n\n## n-body simulations, CUDA and CuPy\n\nüöß Warning: this section contains errors that I'm still working out. My nbody CuPy simulation can run by I don't think the forces are calucalated properly. I dumped the simulation data into blender to model the position of three spheres and the spheres instantly shot off in different directions. üöß\n\n\u003E \"Other than Stable Eras, all times are Chaotic Eras\"\n\nThis is one of my favorite lines from the book. It said twice by King Wen of Zhou during Wang Miao's first experience in the Three-Body game where we start to learn about the peculiarities of irregular gravity in a solar system with three suns.\n\nNVIDIA released CUDA was realease in 2006, back when the company's stock price was around $5, compared to its recent peak of around $450 in July 2023. If the Three-Body Problem book was written any later, it would perhaps make references to a juggernaut private enterprise that develops specialized hardware for physics simulations like the n-body problem.\n\nThe nbody problem is supposed to be unsolvable, but it can be silumated pretty easily using graphics cards. NVIDIA provides some sample code for running nbody simulations, but I wasn't able to get these samples working due to appears to be driver issues. I could write a long article about my misadventures in installing and uninstalling CUDA and NVIDIA drivers to get things working for all things related that use the GPU: stable diffusion, LLMs and CUDA programs. I have a decent understanding now of how things should be installed but I'm still slightly nervous of unintentionally breaking things in my development environment. As a DevOps engineer, Murphy's law elementary. Everything I love about containers for simplifying developer environments seems to not really apply to graphics cards and their drivers, even when you are using the `nvidia-container-toolkit`.\n\nWhen my drivers were working correctly, I was in a \"Stable Era\" of development and both my programs and my working knowledge of GPUs evolved. Sometimes when I tried changing versions or reinstalling things or tuning parameters the wrong way, I found myself sent into a \"Chaotic Era\" where either my scripts and notebooks were left in cold, unrunable states due to driver issues, or I would get lost in the steps needed to prepare the model to run on with llama.cpp on the GPU: quantization and building with support for [BLAS (Basic Linear Algebra Subprograms)](https:\u002F\u002Fwww.netlib.org\u002Fblas\u002F).\n\n„äôÔ∏è Yang Dong is driven to suicide by her conclusion that physics no longer exist when her particle physics experiment yeilded inconsistent results due to intentional interference by an alien civilization. During \"Chaotic Eras\" of my development, the conclusion I reach that nearly drove me insane was simply that CUDA didn't exist anymore. In the end, I just needed to restart my computer. I also felt a little like Ding Yi with moving his pool table around his apartment for the expirment he demonstrates to Wang Miao.\n\nCurrently I have most things working correctly with the following configurations:\n\n```\nbrian@a2:~$ nvidia-smi\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage\u002FCap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n|  0%   40C    P0              62W \u002F 450W |     83MiB \u002F 24564MiB |      0%      Default |\n|                                         |                      |                  N\u002FA |\n+-----------------------------------------+----------------------+----------------------+\n```\n\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Thu_Nov_18_09:45:30_PST_2021\nCuda compilation tools, release 11.5, V11.5.119\nBuild cuda_11.5.r11.5\u002Fcompiler.30672275_0\n```\n\nI'm still not sure why `nvcc` (NVIDIA CUDA compiler) reads one version of CUDA (11.5) and `nvidia-smi` reports a higher version (12.2).\n\nI wrote a simple program with the help of ChatGPT for running nbody problem simulations. The program uses CuPy, a Python library that exposes APIs for doing matrix multiplication to predict the position of three bodies using Euclidian Integration. Here's the script:\n\n```py\nimport numpy as np\nimport cupy as cp\nimport time\nimport json\n\n# Simulation parameters\nNUM_PARTICLES = 3\nDIMENSIONS = 3 # 3D space\nNUM_STEPS = 30\nDT = 0.1\n\n# Generate initial positions and velocities\nnp_positions = np.random.randn(NUM_PARTICLES, DIMENSIONS)\nnp_velocities = np.random.randn(NUM_PARTICLES, DIMENSIONS)\n\ncp_positions = cp.array(np_positions)\ncp_velocities = cp.array(np_velocities)\n\nnp_ticks = np.expand_dims(np_positions, axis=0)\ncp_ticks = cp.array(np_ticks)\n\n# nbody simulation loop\nstart_time = time.time()\nfor step in range(NUM_STEPS):\n\n    # this gets pairwise differences\n    diff = cp_positions[:, None, :] - cp_positions[None, :, :]\n    distances = cp.sqrt(cp.sum(diff**2, axis=2))\n\n    # avoid division by zero\n    epsilon = 1e-5\n    inv_distances = 1.0 \u002F cp.maximum(distances, epsilon)\n\n    # calculate forces\n    cp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n\n    # update velocities and positions\n    cp_velocities += DT * cp_forces\n    cp_positions += DT * cp_velocities\n    cp_ticks = cp.append(cp_ticks, cp.expand_dims(cp_positions, 0), 0)\n\nsim_time = time.time() - start_time\nprint(\"Simulation time:\", sim_time)\n\n\nclass NumpyArrayEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\n\nnp_ticks = cp_ticks.get()\n\n\n# this is data we can work with in python and write to a file\nwith open(\"ticks.json\", \"w\") as f:\n    f.write(json.dumps(np_ticks, cls=NumpyArrayEncoder))\n```\n\nTo better understand the matrix math here I walked through a simple example of what each step does:\n\n```py\n# particle coordinates (x,y,z) in 3D space\npositions = cp.array([[1,2.5,3], [4,5,6], [7,8,9]])\n```\n\nThe first operation creates an array for pairwise distances for each dimension:\n\n```python\ndiff = positions[None, :, :] - positions[:, None, :]\nprint(diff)\n\narray([[[ 0. ,  0. ,  0. ],\n        [ 3. ,  2.5,  3. ],\n        [ 6. ,  5.5,  6. ]],\n\n       [[-3. , -2.5, -3. ],\n        [ 0. ,  0. ,  0. ],\n        [ 3. ,  3. ,  3. ]],\n\n       [[-6. , -5.5, -6. ],\n        [-3. , -3. , -3. ],\n        [ 0. ,  0. ,  0. ]]])\n```\n\nThe rows of zeros correspond to a particle's `x`, `y` and `z` distances to itself, which are all zero by axioms of Euclidian vector spaces.\n\nThe next operation calculates the distance between each particle:\n\n```\ndistances = cp.sqrt(cp.sum(diff**2, axis=2))\nprint(distances)\n\narray([[ 0.        ,  4.9244289 , 10.11187421],\n       [ 4.9244289 ,  0.        ,  5.19615242],\n       [10.11187421,  5.19615242,  0.        ]])\n```\n\nThe diagonal or zeros represents that fact that a particle `n` has a distance of zero to iself.\n\nThe next step calculates inverse distances:\n\n```python\nepsilon = 1e-5\ninv_distances = 1.0 \u002F cp.maximum(distances, epsilon)\n```\n\n```\narray([[1.00000000e+05, 2.03069233e-01, 9.88936353e-02],\n       [2.03069233e-01, 1.00000000e+05, 1.92450090e-01],\n       [9.88936353e-02, 1.92450090e-01, 1.00000000e+05]])\n```\n\nThe next step is the most elegant part of the simulation in my opinion:\n\n```\ncp_forces = cp.sum((diff.T * inv_distances**3).T, axis=1)\n```\n\nThe `.T` operation transposes a matrix, multiplies by the cube of inverse distances, then transposes the matrix again before summing along the first axis. Transposing a matrix basically swaps rows and columns.\n\nThe next two steps are also pretty elegant:\n\n```python\n# update velocities and positions\ncp_velocities += DT * cp_forces\ncp_positions += DT * cp_velocities\n```\n\nIn the last step I append the updated positions to an array that holds every \"tick\" (the positions of each particle between each time interval, `DT` - \"delta time\")\n\nA simpler way to view particle physics can be acheived with Three.js:\n\n\u003Ciframe src=\"https:\u002F\u002Fbriancaffey.github.io\u002Fthree-body-problem\u002Fthree\u002F\" width=100% height=350\u003E\u003C\u002Fiframe\u003E\n\nThree.js and WebGL and still feels like alien technologies to me. It doesn't feel like these simulations should be run so fluidly on my computer in the browser. Wang Miao's comments about hidden data in the Three-Body game resonated with me when I came across this Three.js example:\n\n„äôÔ∏è\n\u003E „Ää‰∏â‰Ωì„ÄãÊ≠£ÊòØËøôÊ†∑ÔºåÂÆÉÁöÑÊµ∑Èáè‰ø°ÊÅØÊòØÈöêËóèÂú®Ê∑±Â§ÑÁöÑÔºåÊ±™Ê∑ºËÉΩÊÑüËßâÂà∞Ôºå‰ΩÜËØ¥‰∏çÊ∏Ö„ÄÇ‰ªñÁ™ÅÁÑ∂ÊÇüÂá∫Ôºå„Ää‰∏â‰Ωì„ÄãÁöÑ‰∏çÂØªÂ∏∏Âú®‰∫éÔºå‰∏éÂÖ∂‰ªñÁöÑÊ∏∏ÊàèÁõ∏ÊØîÔºåÂÆÉÁöÑËÆæËÆ°ËÄÖÊòØÂèçÂÖ∂ÈÅìËÄåË°å‰πã‚Äî‚Äî‰∏ÄËà¨Ê∏∏ÊàèÁöÑËÆæËÆ°ËÄÖÈÉΩÊòØÂ∞ΩÂèØËÉΩÂú∞Â¢ûÂä†ÊòæÁ§∫ÁöÑ‰ø°ÊÅØÈáèÔºå‰ª•‰∫ßÁîüÁúüÂÆûÊÑüÔºõ‰ΩÜ„Ää‰∏â‰Ωì„ÄãÁöÑËÆæËÆ°ËÄÖÂç¥ÊòØÂú®ÊûÅÂäõÂéãÁº©‰ø°ÊÅØÈáèÔºå‰ª•ÈöêËóèÊüêÁßçÂ∑®Â§ßÁöÑÁúüÂÆûÔºåÂ∞±ÂÉèÈÇ£Âº†Áúã‰ººÁ©∫Êó∑ÁöÑÂ§©Á©∫ÁÖßÁâá„ÄÇ\n"}}],fetch:{},mutations:[]}}("text","element","span","token","punctuation","\n"," ","p","operator",",","number",".","li","=","code","(",")",":","keyword","[","]","div","a","nuxt-content-highlight","pre","line-numbers","string","property","\n  ","0","3","nofollow","noopener","noreferrer","_blank","blockquote","-","  ","language-text","comment","ul"," cp","\n        ","true",-1,"icon","icon-link","img","import",2,"h2","builtin","language-python","\n    ","*","6","iframe","100%","print"," np","array","\n\n    ","boolean","None","+="," DT ",true," tokenizer","4","\n\n","{","}","as"," axis","diff"," obj","2.5","llama","llm","back-story","Back story","llms-metas-llama-2-and-chinese-llama-2","LLMs, Meta's LLaMA 2 and Chinese-LLaMA-2","translation","Translation","-hugging-face","ü§ó Hugging Face","retrieval-augmented-generation-rag-reading-comprehension-for-computers","Retrieval Augmented Generation (RAG): Reading Comprehension for Computers","n-body-simulations-cuda-and-cupy","n-body simulations, CUDA and CuPy",", ","llama-cpp-python","https:\u002F\u002Fwww.mdbg.net\u002Fchinese\u002Fdictionary?page=cedict","https:\u002F\u002Falvinntnu.github.io\u002Fpython-notes\u002Fnlp\u002Fnlp-spacy-zh.html","p-4",550,"https:\u002F\u002Fgithub.com\u002Fhaonan-li\u002FCMMLU"," Llama","30","encode","for","in","https:\u002F\u002Flearn.microsoft.com\u002Fen-us\u002Fsemantic-kernel\u002Fmemories\u002Fembeddings#what-are-embeddings-to-a-programmer","client-only","carousel","8","768","language-py","random","randn","NUM_PARTICLES"," DIMENSIONS","np_positions","\ncp_velocities ","expand_dims","np_ticks"," time","time"," cp_positions","sum","**","2","1e-5","1.0","\u002F","maximum","distances"," epsilon","1","# update velocities and positions","\n\n\n","NumpyArrayEncoder","json","JSONEncoder","default","self","return"," positions","5.5","\n\n       ")));