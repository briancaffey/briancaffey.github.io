__NUXT_JSONP__("/in/2024/02/17/rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br){return {data:[{article:{slug:"rocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",description:"This article discusses my entry for NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket Leauge BotChat",title:"Rocket League BotChat powered by TensorRT-LLM: My submission for NVIDIA's Generative AI on RTX PCs Developer Contest",date:"2024-02-17",image:"\u002Fimg\u002Frlbc\u002Fcover.png",tags:["nvidia","rtx","gpu","tensorrt-llm","ai",an,"llama","rocket-league","gaming","windows"],draft:ao,comments:ao,toc:[{id:ap,depth:C,text:aq},{id:ar,depth:C,text:as},{id:at,depth:C,text:au},{id:av,depth:C,text:aw},{id:ax,depth:C,text:ay},{id:az,depth:M,text:aA},{id:aB,depth:M,text:aC},{id:aD,depth:C,text:aE},{id:aF,depth:M,text:aG},{id:aH,depth:M,text:aI},{id:aJ,depth:M,text:aK},{id:aL,depth:C,text:aM},{id:aN,depth:M,text:aO},{id:aP,depth:M,text:aQ},{id:S,depth:M,text:aR},{id:aS,depth:C,text:aT},{id:aU,depth:C,text:aV},{id:aW,depth:C,text:aX}],body:{type:"root",children:[{type:b,tag:D,props:{id:ap},children:[{type:b,tag:k,props:{href:"#tldr",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aq}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This article is about my submission to NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA's TensorRT-LLM library."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Here's my project submission post on ùïè:"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"[link to x.com post]"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Rocket Leauge BotChat - powered by TensorRT-LLM\n‚öΩÔ∏èüöó‚ö°Ô∏èü§ñüí¨"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"My submission for NVIDIA's Gen AI on RTX PCs Developer Contest\n@NVIDIAAIDev #RocketLeague #GTC24 #NVIDIA #LLM #LLaMA"}]},{type:a,value:e},{type:b,tag:D,props:{id:ar},children:[{type:b,tag:k,props:{href:"#nvidias-gen-ai-developer-contest",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:as}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The following email caught my attention last month:"}]},{type:a,value:e},{type:b,tag:ac,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The part about ‚Äúon Windows PC‚Äù made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had never used any of the Windows-specific development frameworks like .NET or Visual Studio."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I built my first personal computer in 2016. It uses an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives so I could dual boot into both Windows and Ubuntu operating systems.Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Last year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my Ubuntu operating system, and frequently had to switch between using Ubuntu for working with Stable Diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems is ultimately what led me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently. And just like that, I became a Windows developer! This got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"Windows winfetch screenshot",src:"\u002Fimg\u002Frlbc\u002Fwinfetch.png"},children:[]}]},{type:a,value:e},{type:b,tag:D,props:{id:at},children:[{type:b,tag:k,props:{href:"#coming-up-with-an-idea",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:au}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The contest description and some related NVIDIA articles about the contest helped me with brainstorming:"}]},{type:a,value:e},{type:b,tag:ac,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Whether it‚Äôs a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:ac,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This contest is focused on NVIDIA's consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software and new AI techniques including image generation and LLM (Large Language Model) inference."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"A stacked bar chart showing the composition of Nvidia's revenue each quarter going back to fiscal 2019.",src:"https:\u002F\u002Fg.foolcdn.com\u002Fimage\u002F?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&op=resize&w=700"},children:[]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Gaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don't play these types of games and it didn't seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Rocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and we quickly came to learn that the in-game chat can be very entertaining, or annoying, or toxic and in some cases, sportsmanlike."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"One other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas \"bakkes\" Bakke, a Norwegian software engineer). Rocket League's in-game chat inspired the idea for my submission to NVIDIA's Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Bots are built into the Rocket League game and you can with or against them in offline matches, but the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow. These bots are very good, but they are not infallible. Also, there is a known problem in Rocket League where some players use these bots to play in ranked online matches to gain an unfair advantage against human players. Playing through the night, these bots can ascend to the upper levels of the ranked gameplay undetected. My plugin is only intended to be used in offline games against AI bots. My goal was to find the most advanced Rocket League bot and have it to send me in-game chat messages while playing against it."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"RLBot Ascii Art",src:"\u002Fimg\u002Frlbc\u002Frlbot.png"},children:[]}]},{type:a,value:e},{type:b,tag:D,props:{id:av},children:[{type:b,tag:k,props:{href:"#putting-together-the-puzzle-pieces",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aw}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"With this idea in mind, I looked into the overall feasibility. I really had no idea if this would this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept."}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"The Bakkesmod Plugin Wiki "},{type:b,tag:k,props:{href:aY,rel:[E,F,G],target:H},children:[{type:a,value:aY}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:b,tag:k,props:{href:"https:\u002F\u002Fwiki.bakkesplugins.com\u002Fcode_snippets\u002Fusing_http_wrapper\u002F",rel:[E,F,G],target:H},children:[{type:b,tag:i,props:{},children:[{type:a,value:"HttpWrapper"}]}]},{type:a,value:" for sending HTTP requests from Bakkesmod"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:b,tag:k,props:{href:"https:\u002F\u002Fwiki.bakkesplugins.com\u002Ffunctions\u002Fstat_events\u002F",rel:[E,F,G],target:H},children:[{type:b,tag:i,props:{},children:[{type:a,value:"StatEvents"}]}]},{type:a,value:" that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save)."}]},{type:a,value:e}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"The Bakkesmod plugin template: "},{type:b,tag:k,props:{href:aZ,rel:[E,F,G],target:H},children:[{type:a,value:aZ}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly"}]},{type:a,value:e}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Plugin Tutorial: "},{type:b,tag:k,props:{href:a_,rel:[E,F,G],target:H},children:[{type:a,value:a_}]}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Open-source chat-related Bakkesmod plugins on GitHub\n"},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"BetterChat: "},{type:b,tag:k,props:{href:a$,rel:[E,F,G],target:H},children:[{type:a,value:a$}]}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Translate: "},{type:b,tag:k,props:{href:ba,rel:[E,F,G],target:H},children:[{type:a,value:ba}]}]},{type:a,value:e}]},{type:a,value:e}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Starting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to "},{type:b,tag:i,props:{},children:[{type:a,value:"localhost:8000\u002Fhello"}]},{type:a,value:". I set up a Hello World Flask app running on "},{type:b,tag:i,props:{},children:[{type:a,value:"localhost:8000"}]},{type:a,value:" and I was able to get a response from my Hello World server! There seemed to be no weird network or permission errors that would prevent my game code from communicating."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Next I started looking into how to build and run optimized LLMs with NVIDIA's TensorRT-LLM library, the software that this contest is promoting. The contest announcement included another interesting building block that I thought could be very useful: an example repo showing how to run "},{type:b,tag:i,props:{},children:[{type:a,value:Z}]},{type:a,value:" optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev)."}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:b,tag:i,props:{},children:[{type:a,value:Z}]},{type:a,value:" is an open source model from Meta that is trained on code and can help with code generation tasks"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like "},{type:b,tag:i,props:{},children:[{type:a,value:Z}]},{type:a,value:" and generates an engine that can be used for doing inference"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"VSCode is an open source code editor developed by Microsoft"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"To get this working you need to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:"}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"using a \"bare-metal\" virtual environment on Windows (with PowerShell)"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"using WSL"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"At the time of writing, using building a TensorRT-LLM engine on Windows can only be done with version "},{type:b,tag:i,props:{},children:[{type:a,value:"v0.6.1"}]},{type:a,value:" of the TensorRT-LLM repo and version "},{type:b,tag:i,props:{},children:[{type:a,value:"v0.7.1"}]},{type:a,value:" of the "},{type:b,tag:i,props:{},children:[{type:a,value:"tensorrt_llm"}]},{type:a,value:" Python package."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"With WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Once the engines are built you can use them to run the example from the "},{type:b,tag:i,props:{},children:[{type:a,value:bb}]},{type:a,value:" repo."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,"language-json"]},children:[{type:b,tag:i,props:{},children:[{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,$]},children:[{type:a,value:"\"title\""}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:P}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"CodeLlama-13b-instruct-hf\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,$]},children:[{type:a,value:"\"apiBase\""}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:P}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"http:\u002F\u002F192.168.5.96:5000\u002F\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,$]},children:[{type:a,value:"\"provider\""}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:P}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"openai\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,$]},children:[{type:a,value:"\"apiKey\""}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:P}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"None\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:_},{type:b,tag:c,props:{className:[d,$]},children:[{type:a,value:"\"model\""}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:P}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"gpt-4\""}]},{type:a,value:e},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The Continue.dev extension using "},{type:b,tag:i,props:{},children:[{type:a,value:Z}]},{type:a,value:" accelerated and optimized by TensorRT-LLM is very fast. According to "},{type:b,tag:k,props:{href:"https:\u002F\u002Fblog.continue.dev\u002Fprogramming-languages\u002F",rel:[E,F,G],target:H},children:[{type:a,value:"this post on Continue.dev's blog"}]},{type:a,value:", C++ is a \"first tier\" language:"}]},{type:a,value:e},{type:b,tag:ac,props:{},children:[{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode \u002F TP3, MBXP \u002F Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don‚Äôt believe that LLMs are as helpful for C++ as some of the other languages in this tier."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Most of the time I'm working with either Python and TypeScript. I've read about C++ but haven't used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with Continue.dev was helpful for tackling smaller problems a REPL-like environment. For example, I used the "},{type:b,tag:i,props:{},children:[{type:a,value:"nlohmann\u002Fjson"}]},{type:a,value:" JSON library to figure out how to handle JSON. Coming from Python and JavaScript, the syntax is quite different. For example, here is how to build the "},{type:b,tag:i,props:{},children:[{type:a,value:S}]},{type:a,value:" for the body of an OpenAI API request:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:S},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:"push_back"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:bc}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:bd},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:" content "},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"In Python the code for appending a message to a list of messages would be written differently:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,"language-python"]},children:[{type:b,tag:i,props:{},children:[{type:a,value:S},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"append"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:bc}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:P}]},{type:a,value:bd},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:P}]},{type:a,value:" content"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:D,props:{id:ax},children:[{type:b,tag:k,props:{href:"#development-environment",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:ay}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"While working on different projects using web technologies and frameworks across the Python and JavaScript ecosystems, I have an appreciation for well-structured development environments. Development environments refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running a local environment. The local environment (the developer's computer) should be a close proxy the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality. I really like the development environment for this project. Here's a screenshot that shows the different parts of the development environment for Rocket League BotChat:"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"Screenshot of Rocket League BotChat development environment",src:"\u002Fimg\u002Frlbc\u002Fdevenv2.png"},children:[]}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Rocket League (running with the "},{type:b,tag:i,props:{},children:[{type:a,value:"-dev"}]},{type:a,value:" flag turned on). The console is helpful viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Visual Studio for working on the plugin code. Controol+Shift+B rebuilds the code and automatically reloads the plugin in the game"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"OpenAI-compatible LLM server powered by TensorRT-LLM (using "},{type:b,tag:i,props:{},children:[{type:a,value:be}]},{type:a,value:" with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using "},{type:b,tag:i,props:{},children:[{type:a,value:Z}]},{type:a,value:" with AWQ INT4 quantization) running in a virtual environment on Windows"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:O,props:{id:az},children:[{type:b,tag:k,props:{href:"#building-the-tensorrt-llm-engines",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aA}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I was able to build and run the TensorRT LLM engines for my game plugin's inference and the Continue.dev extension's inference both in Python virtual environments on Windows and on Ubuntu in WSL. Here's what these commands look like."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"For building the "},{type:b,tag:i,props:{},children:[{type:a,value:be}]},{type:a,value:" model with INT4 AWQ quantization on Windows 11 I used this command:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,bf]},children:[{type:b,tag:i,props:{},children:[{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:bg},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:bh}]},{type:a,value:" C:\\Users\\My PC\\GitHub\\TensorRT"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"LLM\\examples\\llama\u003E python build"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:bi},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"model_dir D:\\llama\\Llama"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"hf\\ "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"quant_ckpt_path D:\\llama\\Llama"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"hf\\llama_tp1_rank0"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"npz "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"dtype float16 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"use_gpt_attention_plugin float16 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"use_gemm_plugin float16 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"use_weight_only "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"weight_only_precision int4_awq "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"per_group "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"enable_context_fmha "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"max_batch_size 1 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"max_input_len 3500 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"max_output_len 1024 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"output_dir D:\\llama\\Llama"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ag},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ah},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:ai},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"hf\\single"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"gpu\\ "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"vocab_size 32064\n"}]}]}]},{type:a,value:e},{type:b,tag:O,props:{id:aB},children:[{type:b,tag:k,props:{href:"#running-the-tensorrt-llm-engines",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aC}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Using Windows PowerShell to start the CodeLlama server for Continue.dev:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,bf]},children:[{type:b,tag:i,props:{},children:[{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:bg},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:bh}]},{type:a,value:" C:\\Users\\My PC\\GitHub\\trt"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:an},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"-as"}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"openai"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:s}]},{type:a,value:"windows\u003E python "},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"\\app"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:bi},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"trt_engine_path "},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\""}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"trt_engine_name llama_float16_tp1_rank0"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"engine "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"tokenizer_dir_path "},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"D:\\llama\\CodeLlama-13b-Instruct-hf\\\""}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"port 5000 "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:n}]},{type:a,value:"host 0"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:aj},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:aj},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"0\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Tip: Adding "},{type:b,tag:i,props:{},children:[{type:a,value:bj}]},{type:a,value:" isn't required here, but it allows me to use the CodeLlama\u002FTensorRT-LLM server with VSCode any computer on my local network using my PC's local IP address in the Continue.dev configuration."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Using docker in WSL to start the Llama-2-13b-chat-hf LLM server:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,"language-text"]},children:[{type:b,tag:i,props:{},children:[{type:a,value:"root@0a5b5b75f079:\u002Fcode\u002Fgit\u002FTensorRT-LLM\u002Fexamples\u002Fserver\u002Fflask# python3 app.py --trt_engine_path \u002Fllama\u002FLlama-2-13b-chat-hf\u002Ftrt_engines\u002F1-gpu\u002F --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path \u002Fllama\u002FLlama-2-13b-chat-hf\u002F --port 5001 --host 0.0.0.0\n"}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Note: Here I also add "},{type:b,tag:i,props:{},children:[{type:a,value:bj}]},{type:a,value:", but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"BakkesMod includes a console window that came in handy for debugging errors during development."}]},{type:a,value:e},{type:b,tag:D,props:{id:aD},children:[{type:b,tag:k,props:{href:"#how-it-works",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aE}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Here's a quick look at key parts of the plugin source code that make it work."}]},{type:a,value:e},{type:b,tag:O,props:{id:aF},children:[{type:b,tag:k,props:{href:"#hooking-events",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aG}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Hooking events the core of how this plugin works. "},{type:b,tag:i,props:{},children:[{type:a,value:"StatTickerMessage"}]},{type:a,value:" events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car."}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:bk},{type:b,tag:c,props:{className:[d,aa]},children:[{type:a,value:"\u002F\u002F Hooks different types of events that are handled in onStatTickerMessage"}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,aa]},children:[{type:a,value:"\u002F\u002F See https:\u002F\u002Fwiki.bakkesplugins.com\u002Ffunctions\u002Fstat_events\u002F"}]},{type:a,value:"\n    gameWrapper"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ak}]},{type:b,tag:c,props:{className:[d,"generic-function"]},children:[{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:"HookEventWithCallerPost"}]},{type:b,tag:c,props:{className:[d,"generic","class-name"]},children:[{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"\u003C"}]},{type:a,value:"ServerWrapper"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"\u003E"}]}]}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:bl}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:a,value:"ServerWrapper caller"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:"void"}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:"*"}]},{type:a,value:" params"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:" std"},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string eventname"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:"onStatTickerMessage"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:a,value:"params"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:O,props:{id:aH},children:[{type:b,tag:k,props:{href:"#handling-events-and-building-the-prompt",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aI}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"We can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the "},{type:b,tag:i,props:{},children:[{type:a,value:am}]},{type:a,value:" method."}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:bk},{type:b,tag:c,props:{className:[d,aa]},children:[{type:a,value:"\u002F\u002F handle different events like scoring a goal or making a save"}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:bm}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:a,value:"statEvent"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:"GetEventName"}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:bn}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"Goal\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:"\n\n        "},{type:b,tag:c,props:{className:[d,aa]},children:[{type:a,value:"\u002F\u002F was the goal scored by the human player or the bot?"}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:bm}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:a,value:"playerPRI"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"memory_address "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:bn}]},{type:a,value:" receiver"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:t}]},{type:a,value:"memory_address"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:am}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"Your human opponent just scored a goal against you! \""}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ab}]},{type:a,value:bo},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:bp}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:"else"}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:J}]},{type:a,value:al},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:am}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"You just scored a goal against the human player! \""}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ab}]},{type:a,value:bo},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:bp}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:U},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:K}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:O,props:{id:aJ},children:[{type:b,tag:k,props:{href:"#making-requests-and-handling-responses",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aK}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI's API."}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:"std"},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string message "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:" response_json"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"choices\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,"number"]},children:[{type:a,value:aj}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"message\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:af}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The "},{type:b,tag:i,props:{},children:[{type:a,value:bq}]},{type:a,value:" method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time."}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:"gameWrapper"},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ak}]},{type:b,tag:c,props:{className:[d,L]},children:[{type:a,value:bq}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:v}]},{type:a,value:S},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:V}]},{type:a,value:"i"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:X}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:u}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,W]},children:[{type:a,value:bl}]},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ak}]},{type:a,value:"bot_name"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:w}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"That's it! The code isn't that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages ("},{type:b,tag:i,props:{},children:[{type:a,value:"\u003C\u002Fs\u003E"}]},{type:a,value:"). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt."}]},{type:a,value:e},{type:b,tag:D,props:{id:aL},children:[{type:b,tag:k,props:{href:"#rocket-league-botchat-ui",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aM}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Most BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here's what the UI for Rocket League BotChat looks like:"}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"Rocket League BotChat Plugin UI",src:"\u002Fimg\u002Frlbc\u002Frlbcui.png"},children:[]}]},{type:a,value:e},{type:b,tag:O,props:{id:aN},children:[{type:b,tag:k,props:{href:"#system-prompt",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aO}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here's the default prompt that I used:"}]},{type:a,value:e},{type:b,tag:y,props:{className:[z]},children:[{type:b,tag:A,props:{className:[B,T]},children:[{type:b,tag:i,props:{},children:[{type:a,value:"    std"},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string ai_player "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"You are an elite AI player in the car soccer game Rocket League. \""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string one_v_one "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"You are playing a 1v1 match against a human player. \""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string instructions "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"You will send short chat messages to your human opponent in response to what happens in the game. \""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string details "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:j},{type:b,tag:c,props:{className:[d,m]},children:[{type:a,value:"\"Respond to the human player with brief messages no more than 12 words long.\""}]},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:ad},{type:b,tag:c,props:{className:[d,aa]},children:[{type:a,value:"\u002F\u002F initial system prompt"}]},{type:a,value:ae},{type:b,tag:c,props:{className:[d,Q,f]},children:[{type:a,value:R}]},{type:a,value:"string initial_system_prompt "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:Y}]},{type:a,value:" ai_player "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ab}]},{type:a,value:" one_v_one "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ab}]},{type:a,value:" instructions "},{type:b,tag:c,props:{className:[d,h]},children:[{type:a,value:ab}]},{type:a,value:" details"},{type:b,tag:c,props:{className:[d,f]},children:[{type:a,value:x}]},{type:a,value:e}]}]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"The last part about \"no more than 12 words long\" was the most effective way of controlling the responses from the LLM. I tried changing the "},{type:b,tag:i,props:{},children:[{type:a,value:"max_output_len"}]},{type:a,value:" when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user."}]},{type:a,value:e},{type:b,tag:O,props:{id:aP},children:[{type:b,tag:k,props:{href:"#temperature-and-seed",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aQ}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"These values are included in the body of the request to the LLM, but I didn't have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn't really work."}]},{type:a,value:e},{type:b,tag:O,props:{id:S},children:[{type:b,tag:k,props:{href:"#messages",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aR}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the \"user\" (which are messages about game events) and the \"assistant\" (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt."}]},{type:a,value:e},{type:b,tag:D,props:{id:aS},children:[{type:b,tag:k,props:{href:"#demo-video-for-contest-submission",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aT}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I used Blender's sequence editor to create a demo video for my contest submission. I don't edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here's how I approached creating the demo video for my project."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"Blender video sequence editor UI used to create my project video",src:"\u002Fimg\u002Frlbc\u002Fblender.png"},children:[]}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used "},{type:b,tag:k,props:{href:"https:\u002F\u002Fopen.spotify.com\u002Ftrack\u002F68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51",rel:[E,F,G],target:H},children:[{type:a,value:"\"Dads in Space\" by Steven Walking"}]}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Get stock Rocket League footage from YouTube with "},{type:b,tag:i,props:{},children:[{type:a,value:"youtube-dl"}]},{type:a,value:" (this is an amazing tool!). I mostly used footage from the "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=e1tqWldCYOI&pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D",rel:[E,F,G],target:H},children:[{type:a,value:"RLCS 2023 Winter Major Trailer"}]},{type:a,value:". This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps."}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!"}]},{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Use ComfyUI to animate some of the images used in the contest description and use these in my video"}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"ComfyUI workflow for animating images using img2vid model",src:"\u002Fimg\u002Frlbc\u002Fcomfyui.png"},children:[]}]},{type:a,value:e},{type:b,tag:I,props:{},children:[{type:a,value:e},{type:b,tag:l,props:{},children:[{type:a,value:"Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice."}]},{type:a,value:e}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:N,props:{alt:"Eleven Labs Voice Generation Web UI",src:"\u002Fimg\u002Frlbc\u002Felevenlabs.png"},children:[]}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:b,tag:k,props:{href:"#"},children:[{type:a,value:"Embed twitter video here"}]}]},{type:a,value:e},{type:b,tag:D,props:{id:aU},children:[{type:b,tag:k,props:{href:"#shortcomings-of-my-project",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aV}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"This plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in \"user\" and \"assistant\" prompts getting out of order which breaks assertions on the "},{type:b,tag:i,props:{},children:[{type:a,value:bb}]},{type:a,value:" repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"There are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like \"shot on goal\" to avoid prompt messages getting out of order. This could also be addressed with more code logic."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Prompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like "},{type:b,tag:i,props:{},children:[{type:a,value:"SendMessage Good shot, bot!"}]},{type:a,value:", for example."}]},{type:a,value:e},{type:b,tag:D,props:{id:aW},children:[{type:b,tag:k,props:{href:"#whats-next",ariaHidden:o,tabIndex:p},children:[{type:b,tag:c,props:{className:[q,r]},children:[]}]},{type:a,value:aX}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Participating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a \"Rocket League Coach\" plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"I think the gaming industry's adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I'm excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM."}]},{type:a,value:e},{type:b,tag:g,props:{},children:[{type:a,value:"Thanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!"}]}]},dir:"\u002F2024\u002F02\u002F17",path:"\u002F2024\u002F02\u002F17\u002Frocket-league-botchat-nvidia-generative-ai-on-rtx-pcs-developer-contest",extension:".md",createdAt:br,updatedAt:br,raw:"\n## tl;dr\n\nThis article is about my submission to NVIDIA's Generative AI on RTX PCs Developer Contest: Rocket League BotChat. Rocket League BotChat is a BakkesMod plugin for Rocket League that allows bots to send chat messages based on in-game events. It is designed to be used with a local LLM service optimized and accelerated with NVIDIA's TensorRT-LLM library.\n\nHere's my project submission post on ùïè:\n\n[link to x.com post]\n\nRocket Leauge BotChat - powered by TensorRT-LLM\n‚öΩÔ∏èüöó‚ö°Ô∏èü§ñüí¨\n\nMy submission for NVIDIA's Gen AI on RTX PCs Developer Contest\n@NVIDIAAIDev #RocketLeague #GTC24 #NVIDIA #LLM #LLaMA\n\n## NVIDIA's Gen AI Developer Contest\n\nThe following email caught my attention last month:\n\n\u003E Generative AI on RTX PCs Developer Contest: Build your next innovative Gen AI project using NVIDIA TensorRT or TensorRT-LLM on Windows PC with NVIDIA RTX systems\n\nThe part about ‚Äúon Windows PC‚Äù made me think: why would a developer contest focus on a particular operating system? I use all three of the major operating systems: macOS, Ubuntu and Windows 11, but most of the development work I do is on macOS and Ubuntu. I discovered WSL (Windows Subsystem for Linux) a few years ago and really enjoy using that for development as well, but I had never considered doing development work on Windows outside of WSL. I had never used any of the Windows-specific development frameworks like .NET or Visual Studio.\n\nI built my first personal computer in 2016. It uses an NVIDIA GeForce GTX 1080 graphics card. When I built another personal computer last year in 2023, getting the NVIDIA GeForce RTX 4090 graphics card was a big step up. I bought two NVMe drives so I could dual boot into both Windows and Ubuntu operating systems.Switching between the operating systems requires turning off the computer, going into the BIOS settings and changing the boot order and restarting the computer.\n\nLast year I started learning more about AI image generation using Stable Diffusion with programs like Automatic1111, InvokeAI and ComfyUI. I set up everything on my Ubuntu operating system, and frequently had to switch between using Ubuntu for working with Stable Diffusion and Windows for gaming and other Windows-specific software. The friction of having to constantly switch operating systems is ultimately what led me to move my stable diffusion software workflows to Windows. All of my models and images are stored to external drives, so moving things over to Windows was pretty easy.\n\nI learned PowerShell and got more familiar with how Windows works as a development machine. Environment variables and system variables are one example of how Windows does things differently. And just like that, I became a Windows developer! This got me interested in coming up with an idea for the NVIDIA Generative AI on NVIDIA RTX PCs Developer Contest.\n\n![Windows winfetch screenshot](\u002Fimg\u002Frlbc\u002Fwinfetch.png)\n\n## Coming up with an Idea\n\nThe contest description and some related NVIDIA articles about the contest helped me with brainstorming:\n\n\u003E Whether it‚Äôs a RAG-based chatbot, a plug-in for an existing application, or a code generation tool, the possibilities are endless.\n\n\u003E Many use cases would benefit from running LLMs locally on Windows PCs, including gaming, creativity, productivity, and developer experiences.\n\nThis contest is focused on NVIDIA's consumer hardware line: GeForce RTX. It has a diverse set of use cases including gaming, crypto mining, VR, simulation software and new AI techniques including image generation and LLM (Large Language Model) inference.\n\n![A stacked bar chart showing the composition of Nvidia's revenue each quarter going back to fiscal 2019.](https:\u002F\u002Fg.foolcdn.com\u002Fimage\u002F?url=https%3A%2F%2Fg.foolcdn.com%2Feditorial%2Fimages%2F764886%2Fnvda_revenue_bar.png&op=resize&w=700)\n\nGaming seemed like an interesting avenue for me to explore. PC gaming is still an industry that is developed primarily for Windows operating systems, and the gaming industry has been the largest revenue driver of NVIDIA in recent years, only recently surpassed by the data center segment. GPUs are needed to render graphics of enormous open-world environments. Some story-driven games include huge amounts of dialogue that can be considered as huge literary works in their own right. Red Dead Redemption and Genshin Impact are two massively popular games of this type. There might be an interesting project idea that could use LLMs and RAG (retrieval augmented generation), but I don't play these types of games and it didn't seem practical for a project that would be built in just over a month. I thought about trying to build something for a simpler game that I already know.\n\nRocket League is a vehicular soccer game that is played on both game consoles and on PCs. It is an eSports with a very high skill ceiling and a massive player base (85 million active players in the last 30 days). I started playing it during the pandemic with some of my friends and we quickly came to learn that the in-game chat can be very entertaining, or annoying, or toxic and in some cases, sportsmanlike.\n\nOne other thing I learned about Rocket League is that it has an active modding community. Developers create plugins for the game for all different purposes, such as coaching, practice drills, capturing replays, tracking player statistics, etc. Most Rocket League Mods are written in a popular framework called Bakkesmod (developed Andreas \"bakkes\" Bakke, a Norwegian software engineer). Rocket League's in-game chat inspired the idea for my submission to NVIDIA's Generative AI Developer Contest: Rocket League BotChat. The idea for my project is to build a plugin with Bakkesmod that allows Rocket League bots to send chat messages based on game events using an LLM accelerated and optimized by TensorRT-LLM (more on TensorRT-LLM soon!)\n\nBots are built into the Rocket League game and you can with or against them in offline matches, but the built-in bots are not very good. Another 3rd-party project called RLBot allows players to play against community-developed AI bots that are developed with machine learning frameworks like TensorFlow. These bots are very good, but they are not infallible. Also, there is a known problem in Rocket League where some players use these bots to play in ranked online matches to gain an unfair advantage against human players. Playing through the night, these bots can ascend to the upper levels of the ranked gameplay undetected. My plugin is only intended to be used in offline games against AI bots. My goal was to find the most advanced Rocket League bot and have it to send me in-game chat messages while playing against it.\n\n![RLBot Ascii Art](\u002Fimg\u002Frlbc\u002Frlbot.png)\n\n## Putting together the puzzle pieces\n\nWith this idea in mind, I looked into the overall feasibility. I really had no idea if this would this would work. I looked through the Bakkesmod documentation and found some helpful resources that gave me confidence that I could pull something together for at least a proof-of-concept.\n\n- The Bakkesmod Plugin Wiki [https:\u002F\u002Fwiki.bakkesplugins.com\u002F](https:\u002F\u002Fwiki.bakkesplugins.com\u002F)\n\t- [`HttpWrapper`](https:\u002F\u002Fwiki.bakkesplugins.com\u002Fcode_snippets\u002Fusing_http_wrapper\u002F) for sending HTTP requests from Bakkesmod\n\t- [`StatEvents`](https:\u002F\u002Fwiki.bakkesplugins.com\u002Ffunctions\u002Fstat_events\u002F) that allow for running custom code when specific event functions are triggered in the game (such as scoring a goal, or making a save).\n- The Bakkesmod plugin template: https:\u002F\u002Fgithub.com\u002FMartinii89\u002FBakkesmodPluginTemplate\n\t- This provides a great starting-off point for developing Bakkesmod plugins. Plugins for Bakkesmod are written in C++ and this repo provides an organized file structure that allows your to get started quickly\n- Plugin Tutorial: https:\u002F\u002Fwiki.bakkesplugins.com\u002Fplugin_tutorial\u002Fgetting_started\u002F\n- Open-source chat-related Bakkesmod plugins on GitHub\n  - BetterChat: [https:\u002F\u002Fgithub.com\u002FJulienML\u002FBetterChat](https:\u002F\u002Fgithub.com\u002FJulienML\u002FBetterChat)\n  - Translate: [https:\u002F\u002Fgithub.com\u002F0xleft\u002Ftrnslt](https:\u002F\u002Fgithub.com\u002F0xleft\u002Ftrnslt)\n\nStarting with the Plugin Template, I wrote a simple console command that when triggered sends an HTTP request to `localhost:8000\u002Fhello`. I set up a Hello World Flask app running on `localhost:8000` and I was able to get a response from my Hello World server! There seemed to be no weird network or permission errors that would prevent my game code from communicating.\n\nNext I started looking into how to build and run optimized LLMs with NVIDIA's TensorRT-LLM library, the software that this contest is promoting. The contest announcement included another interesting building block that I thought could be very useful: an example repo showing how to run `CodeLlama-13b-instruct-hf` optimized by TensorRT-LLM to provide inference for a VSCode extension called Continue (Continue.dev).\n\n- `CodeLlama-13b-instruct-hf` is an open source model from Meta that is trained on code and can help with code generation tasks\n- TensorRT-LLM is a Python library that accelerates and optimizes inference performance of large language models. It takes a Large Language Model like `CodeLlama-13b-instruct-hf` and generates an engine that can be used for doing inference\n- VSCode is an open source code editor developed by Microsoft\n- Continue.dev is a startup backed by Y Combinator that is developing an open-source autopilot (code assistant) for VSCode and JetBrains that works with local LLMs or paid services like ChatGPT\n\nTo get this working you need to build the TensorRT-LLM engine. Building TensorRT-LLM engines on Windows can be done in one of two ways:\n\n- using a \"bare-metal\" virtual environment on Windows (with PowerShell)\n- using WSL\n\nAt the time of writing, using building a TensorRT-LLM engine on Windows can only be done with version `v0.6.1` of the TensorRT-LLM repo and version `v0.7.1` of the `tensorrt_llm` Python package.\n\nWith WSL you can use the up-to-date versions of the TensorRT-LLM repo (main branch). The engines produced by Windows and WSL (Ubuntu) are not interchangeable and you will get errors if you try to use an engine created with one operating system on another operating system.\n\nOnce the engines are built you can use them to run the example from the `trt-llm-as-openai-windows` repo.\n\nThe example repo exposes an OpenAI-compatible API locally that can do chat completions. You then need to configure the Continue.dev extension to use the local LLM service:\n\n```json\n{\n  \"title\": \"CodeLlama-13b-instruct-hf\",\n  \"apiBase\": \"http:\u002F\u002F192.168.5.96:5000\u002F\",\n  \"provider\": \"openai\",\n  \"apiKey\": \"None\",\n  \"model\": \"gpt-4\"\n}\n```\n\nThe Continue.dev extension using `CodeLlama-13b-instruct-hf` accelerated and optimized by TensorRT-LLM is very fast. According to [this post on Continue.dev's blog](https:\u002F\u002Fblog.continue.dev\u002Fprogramming-languages\u002F), C++ is a \"first tier\" language:\n\n\u003E C++ has one of the largest presences on GitHub and Stack Overflow. This shows up in its representation in public LLM datasets, where it is one of the languages with the most data. Its performance is near the top of the MultiPL-E, BabelCode \u002F TP3, MBXP \u002F Multilingual HumanEval, and HumanEval-X benchmarks. However, given that C++ is often used when code performance and exact algorithm implementation is very important, many developers don‚Äôt believe that LLMs are as helpful for C++ as some of the other languages in this tier.\n\nMost of the time I'm working with either Python and TypeScript. I've read about C++ but haven't used it for anything before doing this project. I primarily used Microsoft Visual Studio to build the plugin, but VSCode with Continue.dev was helpful for tackling smaller problems a REPL-like environment. For example, I used the `nlohmann\u002Fjson` JSON library to figure out how to handle JSON. Coming from Python and JavaScript, the syntax is quite different. For example, here is how to build the `messages` for the body of an OpenAI API request:\n\n```cpp\nmessages.push_back({ {\"role\", role}, {\"content\", content } });\n```\n\nIn Python the code for appending a message to a list of messages would be written differently:\n\n```python\nmessages.append({\"role\": role, \"content\": content})\n```\n\n## Development environment\n\nWhile working on different projects using web technologies and frameworks across the Python and JavaScript ecosystems, I have an appreciation for well-structured development environments. Development environments refers to the tools and processes by which a developer can make a change to source code and see these changes reflected in some version of the application running a local environment. The local environment (the developer's computer) should be a close proxy the production environment where the code will ultimately deployed to for end users. For this project the local development environment is our PC itself, which simplifies things. A development environment should support hot-reloading so incremental changes can be run to test functionality. I really like the development environment for this project. Here's a screenshot that shows the different parts of the development environment for Rocket League BotChat:\n\n![Screenshot of Rocket League BotChat development environment](\u002Fimg\u002Frlbc\u002Fdevenv2.png)\n\n- Rocket League (running with the `-dev` flag turned on). The console is helpful viewing log messages and the plugin settings panel can be used to view and change plugin configuration values. The BakkesMod plugin also needs to be running in order to inject plugin code into the game engine\n- Visual Studio for working on the plugin code. Controol+Shift+B rebuilds the code and automatically reloads the plugin in the game\n- OpenAI-compatible LLM server powered by TensorRT-LLM (using `Llama-2-13b-chat-hf` with AWQ INT4 quantization) running in a docker container on Ubuntu in WSL\n- VSCode for debugging C++ code with Continue.dev extension powered by TensorRT-LLM (using `CodeLlama-13b-instruct-hf` with AWQ INT4 quantization) running in a virtual environment on Windows\n\n### Building the TensorRT-LLM engines\n\nI was able to build and run the TensorRT LLM engines for my game plugin's inference and the Continue.dev extension's inference both in Python virtual environments on Windows and on Ubuntu in WSL. Here's what these commands look like.\n\nFor building the `Llama-2-13b-chat-hf` model with INT4 AWQ quantization on Windows 11 I used this command:\n\n```powershell\n(.venv) PS C:\\Users\\My PC\\GitHub\\TensorRT-LLM\\examples\\llama\u003E python build.py --model_dir D:\\llama\\Llama-2-13b-chat-hf\\ --quant_ckpt_path D:\\llama\\Llama-2-13b-chat-hf\\llama_tp1_rank0.npz --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16 --use_weight_only --weight_only_precision int4_awq --per_group --enable_context_fmha --max_batch_size 1 --max_input_len 3500 --max_output_len 1024 --output_dir D:\\llama\\Llama-2-13b-chat-hf\\single-gpu\\ --vocab_size 32064\n```\n\n### Running the TensorRT-LLM engines\n\nUsing Windows PowerShell to start the CodeLlama server for Continue.dev:\n\n```powershell\n(.venv) PS C:\\Users\\My PC\\GitHub\\trt-llm-as-openai-windows\u003E python .\\app.py --trt_engine_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\trt_engines\\1-gpu\\\" --trt_engine_name llama_float16_tp1_rank0.engine --tokenizer_dir_path \"D:\\llama\\CodeLlama-13b-Instruct-hf\\\" --port 5000 --host 0.0.0.0\n```\n\nTip: Adding `--host 0.0.0.0` isn't required here, but it allows me to use the CodeLlama\u002FTensorRT-LLM server with VSCode any computer on my local network using my PC's local IP address in the Continue.dev configuration.\n\nUsing docker in WSL to start the Llama-2-13b-chat-hf LLM server:\n\n```\nroot@0a5b5b75f079:\u002Fcode\u002Fgit\u002FTensorRT-LLM\u002Fexamples\u002Fserver\u002Fflask# python3 app.py --trt_engine_path \u002Fllama\u002FLlama-2-13b-chat-hf\u002Ftrt_engines\u002F1-gpu\u002F --trt_engine_name  llama_float16_t_rank0.engine --tokenizer_dir_path \u002Fllama\u002FLlama-2-13b-chat-hf\u002F --port 5001 --host 0.0.0.0\n```\n\nNote: Here I also add `--host 0.0.0.0`, but this is required in order for the service in the docker container to be reached from WSL by the game running on Windows.\n\nBakkesMod includes a console window that came in handy for debugging errors during development.\n\n## How it works\n\nHere's a quick look at key parts of the plugin source code that make it work.\n\n### Hooking events\n\nHooking events the core of how this plugin works. `StatTickerMessage` events cover most of the events that are triggered in Rocket League, such as scoring a goal, making a save or demolishing a car.\n\n```cpp\n\t\u002F\u002F Hooks different types of events that are handled in onStatTickerMessage\n\t\u002F\u002F See https:\u002F\u002Fwiki.bakkesplugins.com\u002Ffunctions\u002Fstat_events\u002F\n\tgameWrapper-\u003EHookEventWithCallerPost\u003CServerWrapper\u003E(\"Function TAGame.GFxHUD_TA.HandleStatTickerMessage\",\n\t\t[this](ServerWrapper caller, void* params, std::string eventname) {\n\t\t\tonStatTickerMessage(params);\n\t\t});\n```\n\n### Handling events and building the prompt\n\nWe can unpack values from the event to determine the player to which the event should be attributed. The code then translates the game event and related data into an English sentence. This is appended to a vector of message objects with the `appendToPrompt` method.\n\n```cpp\n\t\u002F\u002F handle different events like scoring a goal or making a save\n\tif (statEvent.GetEventName() == \"Goal\") {\n\n\t\t\u002F\u002F was the goal scored by the human player or the bot?\n\t\tif (playerPRI.memory_address == receiver.memory_address) {\n\t\t\tappendToPrompt(\"Your human opponent just scored a goal against you! \" + score_sentence, \"user\");\n\t\t}\n\t\telse {\n\t\t\tappendToPrompt(\"You just scored a goal against the human player! \" + score_sentence, \"user\");\n\t\t}\n\t}\n```\n\n### Making requests and handling responses\n\nThe last main part of the code is making a request to the LLM server with the prompt that we have formed above based on game messages. This code should look familiar to anyone who has worked with OpenAI's API.\n\n```cpp\nstd::string message = response_json[\"choices\"][0][\"message\"][\"content\"];\n```\n\nThe `LogToChatbox` method is used to send a message to the in-game chat box with the name of the bot that is sending the message. Since messages could possibly be longer than the limit of 120 characters, I send messages to the chatbox in chunks of 120 characters at a time.\n\n```cpp\ngameWrapper-\u003ELogToChatbox(messages[i], this-\u003Ebot_name);\n```\n\nThat's it! The code isn't that complicated. I had to sanitize the message so that it would not include emoji or the stop character that the LLM server would include in messages (`\u003C\u002Fs\u003E`). Oddly, I had a hard time getting the LLM to not use emoji even when I instructed it to not use emoji in the system prompt.\n\n## Rocket League BotChat UI\n\nMost BakkesMod plugins for RocketLeague UIs that allow for controlling settings. Here's what the UI for Rocket League BotChat looks like:\n\n![Rocket League BotChat Plugin UI](\u002Fimg\u002Frlbc\u002Frlbcui.png)\n\n### System prompt\n\nThe system prompt instructs the bot on how it shoud reply. This is an important part of the prompt engineering for this project, and I used Postman to experiment with lots of different types of instructions. Here's the default prompt that I used:\n\n```cpp\n\tstd::string ai_player = \"You are an elite AI player in the car soccer game Rocket League. \";\n\tstd::string one_v_one = \"You are playing a 1v1 match against a human player. \";\n\tstd::string instructions = \"You will send short chat messages to your human opponent in response to what happens in the game. \";\n\tstd::string details = \"Respond to the human player with brief messages no more than 12 words long.\";\n\t\u002F\u002F initial system prompt\n\tstd::string initial_system_prompt = ai_player + one_v_one + instructions + details;\n```\n\nThe last part about \"no more than 12 words long\" was the most effective way of controlling the responses from the LLM. I tried changing the `max_output_len` when building the TensorRT engine, but this degraded the quality of the responses. The system prompt can be changed by the user.\n\n### Temperature and Seed\n\nThese values are included in the body of the request to the LLM, but I didn't have much luck with these. Early on I had issues with getting sufficient variation in the responses from the LLM, so I tried using random values for seed and temperature, but this didn't really work.\n\n### Messages\n\nThis section of the UI displays the messages that are used in requests to the LLM. In order keep the prompt within the context window limit, I only used the most recent six messages sent from the \"user\" (which are messages about game events) and the \"assistant\" (which are LLM responses from the bot). Whenever the user changes the system prompt, the messages vector is reset to only include the new system prompt.\n\n## Demo Video for Contest Submission\n\nI used Blender's sequence editor to create a demo video for my contest submission. I don't edit a lot of videos, but it is a fun process and I learned a lot about Blender and non-linear video editing in the process. Here's how I approached creating the demo video for my project.\n\n![Blender video sequence editor UI used to create my project video](\u002Fimg\u002Frlbc\u002Fblender.png)\n\n- Structure the video in three main parts: introduction to my project and the contest, description of how it works, demo of my project in action\n- Find an upbeat song from playlists included in Rocket League with no vocals to use as background music. I used [\"Dads in Space\" by Steven Walking](https:\u002F\u002Fopen.spotify.com\u002Ftrack\u002F68ahXxPJrxcEvQFjRmC2ja?si=2147d6d652064d51)\n- Get stock Rocket League footage from YouTube with `youtube-dl` (this is an amazing tool!). I mostly used footage from the [RLCS 2023 Winter Major Trailer](https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=e1tqWldCYOI&pp=ygUQcmxjcyB3aW50ZXIgMjAyMw%3D%3D). This video was uploaded at 24 fps, and my Blender Video project frame rate was set to 29.97, so I used ffmpeg to convert this video from 24 fps to 29.97 fps.\n- Record myself playing Rocket League with my plugin enabled using NVIDIA Share. Miraculously, I was able to score against the Nexto bot!\n- Use ComfyUI to animate some of the images used in the contest description and use these in my video\n\n![ComfyUI workflow for animating images using img2vid model](\u002Fimg\u002Frlbc\u002Fcomfyui.png)\n\n- Use ElevenLabs to narrate a simple voice over script that describes the video content. This tuned out a lot better than I expected. I paid $1 for the ElevenLabs creator plan and got lots of tokens to experiment with different settings for voice generation using a clone of my voice.\n\n![Eleven Labs Voice Generation Web UI](\u002Fimg\u002Frlbc\u002Felevenlabs.png)\n\n[Embed twitter video here](#)\n\n## Shortcomings of my project\n\nThis plugin is a proof of concept and it has some shortcomings. One issue is that some events that my plugin listens to can happen in rapid succession. This results in \"user\" and \"assistant\" prompts getting out of order which breaks assertions on the `trt-llm-as-openai-windows` repo. It would make more sense to have the bot send messages not immediately after the events are triggered, but on a different type of schedule that allows for multiple events to happen before sending the prompt to the LLM.\n\nThere are lots of events that are triggered that would be interesting things for the bot to react to, but I decided not to prompt on every event since the above situation would be triggered frequently. For example, suppose I listen for events like taking a shot on goal and scoring a goal. If the goal is scored immediately after the shot is taken, then the second prompt is sent before the response for the first prompt comes back. For this reason I decided to simply not listen to events like \"shot on goal\" to avoid prompt messages getting out of order. This could also be addressed with more code logic.\n\nPrompt engineering is something that can always be improved. It is hard to measure and testing it is subjective. I am pleased with the results I was able to capture for the demo video, but the quality of the LLM responses can very depending on what happens during gameplay. One idea I had to address this would be to provide multiple English translations for any given event, and then select one at random. This might help improve the variety of responses, for example.\n\nI faced some limitations that are built in to the game iteself. For example, it is not possible for a player to send messages to the in-game chat in offline matches, which makes sense! I built a backdoor for doing this through the BakkesMod developer console, so you can send messages to the bot by typing something like `SendMessage Good shot, bot!`, for example.\n\n## What's next?\n\nParticipating in this contest was a great opportunity to learn more about LLMs and how to use them to extend programs in a Windows environment. It was also a lot of fun to build something by putting together new tools like TensorRT-LLM. Seeing the bot send me chat messages was very satisfying when I first got it to work! Overall it is a pretty simple implementation, but this idea could be extended to produce useful application. I could imagine a \"Rocket League Coach\" plugin that expands on this idea to give helpful feedback based on higher-level data, statistical trends, training goals, etc.\n\nI think the gaming industry's adoption of LLMs for new games will be BIG, and it will present a huge opportunity for LLM optimization and acceleration software like TensorRT-LLM that I was able to use in my Rocket League BotChat. This is not to discredit the work of writers which play an important role in game development. I'm excited to see what other developers have built for this contest, especially submissions that are building mods for games using TensorRT-LLM.\n\nThanks NVIDIA and the TensorRT and TensorRT-LLM teams for organizing this contest! Keep on building!!\n"}}],fetch:{},mutations:[]}}("text","element","span","token","\n","punctuation","p","operator","code"," ","a","li","string","--","true",-1,"icon","icon-link","-",".",",","(",")",";","div","nuxt-content-highlight","pre","line-numbers",2,"h2","nofollow","noopener","noreferrer","_blank","ul","{","}","function",3,"img","h3",":","double-colon","::","messages","language-cpp","\n        ","[","keyword","]","=","CodeLlama-13b-instruct-hf","\n  ","property","comment","+","blockquote","\n    ","\n    std","\"content\"","2","13b","chat","0","-\u003E","\n            ","appendToPrompt","llm",true,"tldr","tl;dr","nvidias-gen-ai-developer-contest","NVIDIA's Gen AI Developer Contest","coming-up-with-an-idea","Coming up with an Idea","putting-together-the-puzzle-pieces","Putting together the puzzle pieces","development-environment","Development environment","building-the-tensorrt-llm-engines","Building the TensorRT-LLM engines","running-the-tensorrt-llm-engines","Running the TensorRT-LLM engines","how-it-works","How it works","hooking-events","Hooking events","handling-events-and-building-the-prompt","Handling events and building the prompt","making-requests-and-handling-responses","Making requests and handling responses","rocket-league-botchat-ui","Rocket League BotChat UI","system-prompt","System prompt","temperature-and-seed","Temperature and Seed","Messages","demo-video-for-contest-submission","Demo Video for Contest Submission","shortcomings-of-my-project","Shortcomings of my project","whats-next","What's next?","https:\u002F\u002Fwiki.bakkesplugins.com\u002F","https:\u002F\u002Fgithub.com\u002FMartinii89\u002FBakkesmodPluginTemplate","https:\u002F\u002Fwiki.bakkesplugins.com\u002Fplugin_tutorial\u002Fgetting_started\u002F","https:\u002F\u002Fgithub.com\u002FJulienML\u002FBetterChat","https:\u002F\u002Fgithub.com\u002F0xleft\u002Ftrnslt","trt-llm-as-openai-windows","\"role\""," role","Llama-2-13b-chat-hf","language-powershell","venv","PS","py ","--host 0.0.0.0","    ","this","if","=="," score_sentence","\"user\"","LogToChatbox","2024-02-17T22:35:34.319Z")));