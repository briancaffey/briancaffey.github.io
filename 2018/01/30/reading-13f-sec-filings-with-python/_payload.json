[{"data":1,"prerenderedAt":658},["ShallowReactive",2],{"reading-13f-sec-filings-with-python":3},{"id":4,"title":5,"body":6,"comments":242,"date":642,"description":643,"draft":644,"extension":645,"external":646,"image":647,"meta":648,"navigation":242,"path":650,"seo":651,"stem":652,"tags":653,"__hash__":657},"blog/2018/01/30/reading-13f-sec-filings-with-python.md","Reading 13F SEC filings with python",{"type":7,"value":8,"toc":639},"minimark",[9,23,49,60,63,72,82,90,101,104,127,135,138,256,259,309,316,321,324,416,427,525,528,534,540,543,630,635],[10,11,12,16,17,22],"p",{},[13,14,15],"strong",{},"Update",": This project has been updated, please see ",[18,19,21],"a",{"href":20},"/2020/11/29/weekend-project-update-open-sec-data","this article"," to read about the most recent updates.",[24,25,26,35,42],"ul",{},[27,28,29,34],"li",{},[18,30,31],{"href":31,"rel":32},"https://opensecdata.ga",[33],"nofollow"," (project staging website, deployed to docker swarm cluster running on DigitalOcean)",[27,36,37,41],{},[18,38,39],{"href":39,"rel":40},"https://gitlab.com/briancaffey/sec-filings-app",[33]," (main repository, requires GitLab account)",[27,43,44,48],{},[18,45,46],{"href":46,"rel":47},"https://github.com/briancaffey/sec-filings-app",[33]," (mirror, no account required to view)",[50,51,52],"blockquote",{},[10,53,54,55],{},"The SEC Form 13F is a filing with the Securities and Exchange Commission (SEC) also known as the Information Required of Institutional Investment Managers Form. It is a quarterly filing required of institutional investment managers with over $100 million in qualifying assets. -",[18,56,59],{"href":57,"rel":58},"https://www.investopedia.com/terms/f/form-13f.asp",[33],"Investopedia",[10,61,62],{},"In this article I will show how to collect and parse 13F filing data from the SEC.",[10,64,65,66,71],{},"First, use ",[18,67,70],{"href":68,"rel":69},"https://www.sec.gov/edgar/searchedgar/companysearch.html",[33],"EDGAR"," to search the company of interest.",[50,73,74],{},[10,75,76,77],{},"EDGAR, the Electronic Data Gathering, Analysis, and Retrieval system, performs automated collection, validation, indexing, acceptance, and forwarding of submissions by companies and others who are required by law to file forms with the U.S. Securities and Exchange Commission (the \"SEC\"). -",[18,78,81],{"href":79,"rel":80},"https://en.wikipedia.org/wiki/EDGAR",[33],"Wikipedia",[10,83,84,85,89],{},"Click on the Central Index Key (CIK) of the company you are search for, and then click on ",[86,87,88],"code",{},"Documents",".",[10,91,92,93,96,97,100],{},"You'll want to grab the HTML version of the ",[86,94,95],{},"Information Table",". I have saved them in a folder with their file names cooresponding to their dates (",[86,98,99],{},"YYYY-MM-DD"," format).",[10,102,103],{},"For this example, I have manually collected the files for a few years of data filed by a hedge fund. Here are the files I'll be working with:",[105,106,111],"pre",{"className":107,"code":108,"language":109,"meta":110,"style":110},"language-python shiki shiki-themes github-light github-dark","files = os.listdir(\"13f/\")\nprint(*sorted(files), sep=\"\\n\")\n","python","",[86,112,113,121],{"__ignoreMap":110},[114,115,118],"span",{"class":116,"line":117},"line",1,[114,119,120],{},"files = os.listdir(\"13f/\")\n",[114,122,124],{"class":116,"line":123},2,[114,125,126],{},"print(*sorted(files), sep=\"\\n\")\n",[105,128,133],{"className":129,"code":131,"language":132},[130],"language-text","2014-02-14.html\n2014-05-15.html\n2014-08-14.html\n2014-11-14.html\n2015-02-17.html\n2015-05-14.html\n2015-08-14.html\n2015-11-12.html\n2016-02-16.html\n2016-05-16.html\n2016-08-12.html\n2016-11-14.html\n2017-02-14.html\n2017-05-15.html\n2017-08-10.html\n2017-10-30.html\n","text",[86,134,131],{"__ignoreMap":110},[10,136,137],{},"Here's a quick script we can use to parse information from each filing document:",[105,139,141],{"className":107,"code":140,"language":109,"meta":110,"style":110},"def scrape_13f(file):\n    date = file\n    html = open(\"13f/\"+file).read()\n    soup = BeautifulSoup(html, 'lxml')\n    rows = soup.find_all('tr')[11:]\n    positions = []\n    for row in rows:\n        dic = {}\n        position = row.find_all('td')\n        dic[\"NAME_OF_ISSUER\"] = position[0].text\n        dic[\"TITLE_OF_CLASS\"] = position[1].text\n        dic[\"CUSIP\"] = position[2].text\n        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n        dic[\"DATE\"] = date.strip(\".html\")\n        positions.append(dic)\n\n    df = pd.DataFrame(positions)\n    return df\n",[86,142,143,148,153,159,165,171,177,183,189,195,201,207,213,219,225,231,237,244,250],{"__ignoreMap":110},[114,144,145],{"class":116,"line":117},[114,146,147],{},"def scrape_13f(file):\n",[114,149,150],{"class":116,"line":123},[114,151,152],{},"    date = file\n",[114,154,156],{"class":116,"line":155},3,[114,157,158],{},"    html = open(\"13f/\"+file).read()\n",[114,160,162],{"class":116,"line":161},4,[114,163,164],{},"    soup = BeautifulSoup(html, 'lxml')\n",[114,166,168],{"class":116,"line":167},5,[114,169,170],{},"    rows = soup.find_all('tr')[11:]\n",[114,172,174],{"class":116,"line":173},6,[114,175,176],{},"    positions = []\n",[114,178,180],{"class":116,"line":179},7,[114,181,182],{},"    for row in rows:\n",[114,184,186],{"class":116,"line":185},8,[114,187,188],{},"        dic = {}\n",[114,190,192],{"class":116,"line":191},9,[114,193,194],{},"        position = row.find_all('td')\n",[114,196,198],{"class":116,"line":197},10,[114,199,200],{},"        dic[\"NAME_OF_ISSUER\"] = position[0].text\n",[114,202,204],{"class":116,"line":203},11,[114,205,206],{},"        dic[\"TITLE_OF_CLASS\"] = position[1].text\n",[114,208,210],{"class":116,"line":209},12,[114,211,212],{},"        dic[\"CUSIP\"] = position[2].text\n",[114,214,216],{"class":116,"line":215},13,[114,217,218],{},"        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n",[114,220,222],{"class":116,"line":221},14,[114,223,224],{},"        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n",[114,226,228],{"class":116,"line":227},15,[114,229,230],{},"        dic[\"DATE\"] = date.strip(\".html\")\n",[114,232,234],{"class":116,"line":233},16,[114,235,236],{},"        positions.append(dic)\n",[114,238,240],{"class":116,"line":239},17,[114,241,243],{"emptyLinePlaceholder":242},true,"\n",[114,245,247],{"class":116,"line":246},18,[114,248,249],{},"    df = pd.DataFrame(positions)\n",[114,251,253],{"class":116,"line":252},19,[114,254,255],{},"    return df\n",[10,257,258],{},"Using this function we can get a quick snapshot of this hedge fund by filing total over the last 4 years:",[105,260,262],{"className":107,"code":261,"language":109,"meta":110,"style":110},"fund_growth = [sum(scrape_13f(file).VALUE) for file in sorted(files)]\ndates = [f.strip('.html') for f in sorted(files)]\nplt.figure(figsize=(10,5))\nplt.title('Total Fund Size')\nplt.xlabel('Filing Date')\nplt.ylabel('USD')\nplt.bar(dates, fund_growth)\nplt.yticks()\nplt.xticks(rotation='vertical')\n",[86,263,264,269,274,279,284,289,294,299,304],{"__ignoreMap":110},[114,265,266],{"class":116,"line":117},[114,267,268],{},"fund_growth = [sum(scrape_13f(file).VALUE) for file in sorted(files)]\n",[114,270,271],{"class":116,"line":123},[114,272,273],{},"dates = [f.strip('.html') for f in sorted(files)]\n",[114,275,276],{"class":116,"line":155},[114,277,278],{},"plt.figure(figsize=(10,5))\n",[114,280,281],{"class":116,"line":161},[114,282,283],{},"plt.title('Total Fund Size')\n",[114,285,286],{"class":116,"line":167},[114,287,288],{},"plt.xlabel('Filing Date')\n",[114,290,291],{"class":116,"line":173},[114,292,293],{},"plt.ylabel('USD')\n",[114,295,296],{"class":116,"line":179},[114,297,298],{},"plt.bar(dates, fund_growth)\n",[114,300,301],{"class":116,"line":185},[114,302,303],{},"plt.yticks()\n",[114,305,306],{"class":116,"line":191},[114,307,308],{},"plt.xticks(rotation='vertical')\n",[10,310,311],{},[312,313],"img",{"alt":314,"src":315},"png","/static/sec/fund_size.png",[317,318,320],"h2",{"id":319},"fund-positions-with-bubble-chart","Fund Positions with Bubble Chart",[10,322,323],{},"Next, it would be great to get a snapshot of the stocks owned by this fund in a given year. Let's use a D3 bubble chart. The names for each stock are quite long, so first let's convert them to stock ticker values. Here's a quick script I hacked together using a Fidelity lookup service:",[105,325,327],{"className":107,"code":326,"language":109,"meta":110,"style":110},"cusip_nums = set()\nfor file in files:\n    cusip_nums = cusip_nums | set(scrape_13f(file).CUSIP)\n\nticker_dic = {c:\"\" for c in cusip_nums}\nfor c in list(ticker_dic.keys()):\n    url = \"http://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=stock&by=cusip&criteria=\"+c+\"&submit=Search\"\n    html = requests.get(url).text\n    soup = BeautifulSoup(html, 'lxml')\n    ticker_elem = soup.find('tr', attrs={\"bgcolor\":\"#666666\"})\n    ticker = \"\"\n    try:\n        ticker = ticker_elem.next_sibling.next_sibling.find('a').text\n        ticker_dic[c] = ticker\n    except:\n        pass\n\n    time.sleep(1)\n",[86,328,329,334,339,344,348,353,358,363,368,372,377,382,387,392,397,402,407,411],{"__ignoreMap":110},[114,330,331],{"class":116,"line":117},[114,332,333],{},"cusip_nums = set()\n",[114,335,336],{"class":116,"line":123},[114,337,338],{},"for file in files:\n",[114,340,341],{"class":116,"line":155},[114,342,343],{},"    cusip_nums = cusip_nums | set(scrape_13f(file).CUSIP)\n",[114,345,346],{"class":116,"line":161},[114,347,243],{"emptyLinePlaceholder":242},[114,349,350],{"class":116,"line":167},[114,351,352],{},"ticker_dic = {c:\"\" for c in cusip_nums}\n",[114,354,355],{"class":116,"line":173},[114,356,357],{},"for c in list(ticker_dic.keys()):\n",[114,359,360],{"class":116,"line":179},[114,361,362],{},"    url = \"http://quotes.fidelity.com/mmnet/SymLookup.phtml?reqforlookup=REQUESTFORLOOKUP&productid=mmnet&isLoggedIn=mmnet&rows=50&for=stock&by=cusip&criteria=\"+c+\"&submit=Search\"\n",[114,364,365],{"class":116,"line":185},[114,366,367],{},"    html = requests.get(url).text\n",[114,369,370],{"class":116,"line":191},[114,371,164],{},[114,373,374],{"class":116,"line":197},[114,375,376],{},"    ticker_elem = soup.find('tr', attrs={\"bgcolor\":\"#666666\"})\n",[114,378,379],{"class":116,"line":203},[114,380,381],{},"    ticker = \"\"\n",[114,383,384],{"class":116,"line":209},[114,385,386],{},"    try:\n",[114,388,389],{"class":116,"line":215},[114,390,391],{},"        ticker = ticker_elem.next_sibling.next_sibling.find('a').text\n",[114,393,394],{"class":116,"line":221},[114,395,396],{},"        ticker_dic[c] = ticker\n",[114,398,399],{"class":116,"line":227},[114,400,401],{},"    except:\n",[114,403,404],{"class":116,"line":233},[114,405,406],{},"        pass\n",[114,408,409],{"class":116,"line":239},[114,410,243],{"emptyLinePlaceholder":242},[114,412,413],{"class":116,"line":246},[114,414,415],{},"    time.sleep(1)\n",[10,417,418,419,422,423,426],{},"I couldn't get all the CUSIP numbers, but I was able to get most of them. Some of the CUSIP numbers have changed for certain stocks and couldn't be looked up with this service. For now I won't fill these in. With the ",[86,420,421],{},"ticker_dic"," dictionary, we can make a quick edit to our ",[86,424,425],{},"scrape_13f"," function to populate ticker data for each holding:",[105,428,430],{"className":107,"code":429,"language":109,"meta":110,"style":110},"ticker_dict = {'00206R102': 'T', '00507V109': 'ATVI', '00724F101': 'ADBE', ... }\n\ndef scrape_13f(file):\n    date = file\n    html = open(\"13f/\"+file).read()\n    soup = BeautifulSoup(html, 'lxml')\n    rows = soup.find_all('tr')[11:]\n    positions = []\n    for row in rows:\n        dic = {}\n        position = row.find_all('td')\n        dic[\"NAME_OF_ISSUER\"] = position[0].text\n        dic[\"TITLE_OF_CLASS\"] = position[1].text\n        dic[\"CUSIP\"] = position[2].text\n        dic[\"VALUE\"] = int(position[3].text.replace(',', ''))*1000\n        dic[\"SHARES\"] = int(position[4].text.replace(',', ''))\n        dic[\"DATE\"] = date.strip(\".html\")\n        dic[\"TICKER\"] = ticker_dict[position[2].text]\n        positions.append(dic)\n\n    df = pd.DataFrame(positions)\n    return df\n",[86,431,432,437,441,445,449,453,457,461,465,469,473,477,481,485,489,493,497,501,506,510,515,520],{"__ignoreMap":110},[114,433,434],{"class":116,"line":117},[114,435,436],{},"ticker_dict = {'00206R102': 'T', '00507V109': 'ATVI', '00724F101': 'ADBE', ... }\n",[114,438,439],{"class":116,"line":123},[114,440,243],{"emptyLinePlaceholder":242},[114,442,443],{"class":116,"line":155},[114,444,147],{},[114,446,447],{"class":116,"line":161},[114,448,152],{},[114,450,451],{"class":116,"line":167},[114,452,158],{},[114,454,455],{"class":116,"line":173},[114,456,164],{},[114,458,459],{"class":116,"line":179},[114,460,170],{},[114,462,463],{"class":116,"line":185},[114,464,176],{},[114,466,467],{"class":116,"line":191},[114,468,182],{},[114,470,471],{"class":116,"line":197},[114,472,188],{},[114,474,475],{"class":116,"line":203},[114,476,194],{},[114,478,479],{"class":116,"line":209},[114,480,200],{},[114,482,483],{"class":116,"line":215},[114,484,206],{},[114,486,487],{"class":116,"line":221},[114,488,212],{},[114,490,491],{"class":116,"line":227},[114,492,218],{},[114,494,495],{"class":116,"line":233},[114,496,224],{},[114,498,499],{"class":116,"line":239},[114,500,230],{},[114,502,503],{"class":116,"line":246},[114,504,505],{},"        dic[\"TICKER\"] = ticker_dict[position[2].text]\n",[114,507,508],{"class":116,"line":252},[114,509,236],{},[114,511,513],{"class":116,"line":512},20,[114,514,243],{"emptyLinePlaceholder":242},[114,516,518],{"class":116,"line":517},21,[114,519,249],{},[114,521,523],{"class":116,"line":522},22,[114,524,255],{},[10,526,527],{},"Let's check this:",[105,529,532],{"className":530,"code":531,"language":132},[130],"df = scrape_13f(files[2])\nprint(df[[\"CUSIP\", \"NAME_OF_ISSUER\", \"TICKER\"]].head())\n",[86,533,531],{"__ignoreMap":110},[105,535,538],{"className":536,"code":537,"language":132},[130],"       CUSIP         NAME_OF_ISSUER TICKER\n0  88579Y101                  3M CO    MMM\n1  G1151C101  ACCENTURE PLC IRELAND    ACN\n2  02209S103       ALTRIA GROUP INC     MO\n3  03076C106    AMERIPRISE FINL INC    AMP\n4  035710409    ANNALY CAP MGMT INC    NLY\n",[86,539,537],{"__ignoreMap":110},[10,541,542],{},"Let's take a look at the last filing, Q4 2017.",[105,544,546],{"className":107,"code":545,"language":109,"meta":110,"style":110},"q4_2017 = sorted(files)[-1]\ndf_q4_2017 = scrape_13f(q4_2017)\n\ntop_20 = df_q4_2017.sort_values(by=\"VALUE\", ascending=False)[[\"TICKER\", \"VALUE\"]][:40]\na = top_20.TICKER\nb = top_20.VALUE\nc = range(len(b))\n\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111)\nax.bar(c, b)\n\nplt.xticks(c, a, rotation=90)\nplt.title('Top 40 Stock Holdings by Value')\nplt.xlabel('Stock Ticker')\nplt.ylabel('USD (10 MM))')\nplt.show()\n",[86,547,548,553,558,562,567,572,577,582,586,591,596,601,605,610,615,620,625],{"__ignoreMap":110},[114,549,550],{"class":116,"line":117},[114,551,552],{},"q4_2017 = sorted(files)[-1]\n",[114,554,555],{"class":116,"line":123},[114,556,557],{},"df_q4_2017 = scrape_13f(q4_2017)\n",[114,559,560],{"class":116,"line":155},[114,561,243],{"emptyLinePlaceholder":242},[114,563,564],{"class":116,"line":161},[114,565,566],{},"top_20 = df_q4_2017.sort_values(by=\"VALUE\", ascending=False)[[\"TICKER\", \"VALUE\"]][:40]\n",[114,568,569],{"class":116,"line":167},[114,570,571],{},"a = top_20.TICKER\n",[114,573,574],{"class":116,"line":173},[114,575,576],{},"b = top_20.VALUE\n",[114,578,579],{"class":116,"line":179},[114,580,581],{},"c = range(len(b))\n",[114,583,584],{"class":116,"line":185},[114,585,243],{"emptyLinePlaceholder":242},[114,587,588],{"class":116,"line":191},[114,589,590],{},"fig = plt.figure(figsize=(15,5))\n",[114,592,593],{"class":116,"line":197},[114,594,595],{},"ax = fig.add_subplot(111)\n",[114,597,598],{"class":116,"line":203},[114,599,600],{},"ax.bar(c, b)\n",[114,602,603],{"class":116,"line":209},[114,604,243],{"emptyLinePlaceholder":242},[114,606,607],{"class":116,"line":215},[114,608,609],{},"plt.xticks(c, a, rotation=90)\n",[114,611,612],{"class":116,"line":221},[114,613,614],{},"plt.title('Top 40 Stock Holdings by Value')\n",[114,616,617],{"class":116,"line":227},[114,618,619],{},"plt.xlabel('Stock Ticker')\n",[114,621,622],{"class":116,"line":233},[114,623,624],{},"plt.ylabel('USD (10 MM))')\n",[114,626,627],{"class":116,"line":239},[114,628,629],{},"plt.show()\n",[10,631,632],{},[312,633],{"alt":314,"src":634},"/static/sec/2017_filing.png",[636,637,638],"style",{},"html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":110,"searchDepth":123,"depth":123,"links":640},[641],{"id":319,"depth":123,"text":320},"2018-01-30","How to read SEC filing data with Python",false,"md",null,"/static/sec/sec.jpg",{"layout":649},"post","/2018/01/30/reading-13f-sec-filings-with-python",{"title":5,"description":643},"2018/01/30/reading-13f-sec-filings-with-python",[654,109,655,656],"sec","data","scraping","D6TNxFV67Xh4BsDCS5kZQv22BbqytCEaC26V9sAMfks",1753130132314]